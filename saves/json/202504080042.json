[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.03632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03632v1",
                "updated": "2025-04-04T17:56:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:56:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs"
                },
                "summary": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems."
                },
                "authors": [
                    {
                        "name": "Huda Ibeid"
                    },
                    {
                        "name": "Vikram Narayana"
                    },
                    {
                        "name": "Jeongnim Kim"
                    },
                    {
                        "name": "Anthony Nguyen"
                    },
                    {
                        "name": "Vitali Morozov"
                    },
                    {
                        "name": "Ye Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ye Luo"
                },
                "author": "Ye Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v2",
                "updated": "2025-04-04T16:51:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    51,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02073v2",
                "updated": "2025-04-04T15:30:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    30,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2023-07-05T07:30:53Z",
                "published_parsed": [
                    2023,
                    7,
                    5,
                    7,
                    30,
                    53,
                    2,
                    186,
                    0
                ],
                "title": "Performance Modeling of Data Storage Systems using Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Modeling of Data Storage Systems using Generative Models"
                },
                "summary": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning."
                },
                "authors": [
                    {
                        "name": "Abdalaziz Rashid Al-Maeeni"
                    },
                    {
                        "name": "Aziz Temirkhanov"
                    },
                    {
                        "name": "Artem Ryzhikov"
                    },
                    {
                        "name": "Mikhail Hushchyn"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Hushchyn"
                },
                "author": "Mikhail Hushchyn",
                "arxiv_doi": "0.1109/ACCESS.2025.3552409",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/0.1109/ACCESS.2025.3552409",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.02073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access 2025 ( Volume: 13) 49643 - 49658",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03499v1",
                "updated": "2025-04-04T14:55:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:55:27Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "title": "Optimistic Learning for Communication Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimistic Learning for Communication Networks"
                },
                "summary": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach."
                },
                "authors": [
                    {
                        "name": "George Iosifidis"
                    },
                    {
                        "name": "Naram Mhaisen"
                    },
                    {
                        "name": "Douglas J. Leith"
                    }
                ],
                "author_detail": {
                    "name": "Douglas J. Leith"
                },
                "author": "Douglas J. Leith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v3",
                "updated": "2025-04-04T13:27:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    27,
                    49,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02533v2",
                "updated": "2025-04-04T11:12:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    12,
                    18,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-03T12:36:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions"
                },
                "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based ISA extensibility. Our implementation\nshows $30\\times$ to $84\\times$ performance improvement when operating on 8-bit\ndata over the same system with a traditional cache when executing a worst-case\n32-bit CNN workload, with only $41.3\\%$ area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based ISA extensibility. Our implementation\nshows $30\\times$ to $84\\times$ performance improvement when operating on 8-bit\ndata over the same system with a traditional cache when executing a worst-case\n32-bit CNN workload, with only $41.3\\%$ area overhead."
                },
                "authors": [
                    {
                        "name": "Vincenzo Petrolo"
                    },
                    {
                        "name": "Flavia Guella"
                    },
                    {
                        "name": "Michele Caon"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Guido Masera"
                    },
                    {
                        "name": "Maurizio Martina"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Martina"
                },
                "author": "Maurizio Martina",
                "arxiv_comment": "6 pages, 4 figures, accepted at the Design Automation Conference\n  (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v1",
                "updated": "2025-04-04T03:30:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16444v3",
                "updated": "2025-04-03T22:49:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    49,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2024-05-26T06:00:17Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    6,
                    0,
                    17,
                    6,
                    147,
                    0
                ],
                "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion"
                },
                "summary": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03048v1",
                "updated": "2025-04-03T21:53:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T21:53:51Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "title": "LLM Library Learning Fails: A LEGO-Prover Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Library Learning Fails: A LEGO-Prover Case Study"
                },
                "summary": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines."
                },
                "authors": [
                    {
                        "name": "Ian Berlot-Attwell"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Xujie Si"
                    }
                ],
                "author_detail": {
                    "name": "Xujie Si"
                },
                "author": "Xujie Si",
                "arxiv_comment": "24 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02976v1",
                "updated": "2025-04-03T18:54:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:54:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching"
                },
                "summary": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates."
                },
                "authors": [
                    {
                        "name": "Nooshin Bahador"
                    }
                ],
                "author_detail": {
                    "name": "Nooshin Bahador"
                },
                "author": "Nooshin Bahador",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02972v1",
                "updated": "2025-04-03T18:47:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:47:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "Improved Compact Genetic Algorithms with Efficient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Compact Genetic Algorithms with Efficient Caching"
                },
                "summary": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy."
                },
                "authors": [
                    {
                        "name": "Prasanta Dutta"
                    },
                    {
                        "name": "Anirban Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Mukhopadhyay"
                },
                "author": "Anirban Mukhopadhyay",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02921v1",
                "updated": "2025-04-03T17:08:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:08:42Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service."
                },
                "authors": [
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Seo Jin Park"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v2",
                "updated": "2025-04-03T13:28:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    28,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity ."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v1",
                "updated": "2025-04-03T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02268v1",
                "updated": "2025-04-03T04:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data"
                },
                "summary": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Justin Cechmanek"
                    },
                    {
                        "name": "Tyler Hutcherson"
                    },
                    {
                        "name": "Srijith Rajamohan"
                    },
                    {
                        "name": "Jen Agarwal"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    },
                    {
                        "name": "Manvinder Singh"
                    },
                    {
                        "name": "Benoit Dion"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Dion"
                },
                "arxiv_affiliation": "Redis",
                "author": "Benoit Dion",
                "arxiv_comment": "Initial study on embedding fine tuning for semantic cache. It also\n  explores synthetic data. Total pages are 12, including refrences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02220v1",
                "updated": "2025-04-03T02:24:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T02:24:21Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "title": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations"
                },
                "summary": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Helen Mayer"
                    },
                    {
                        "name": "James Richards"
                    }
                ],
                "author_detail": {
                    "name": "James Richards"
                },
                "author": "James Richards",
                "arxiv_comment": "International Conference on Computing Technologies and Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v2",
                "updated": "2025-04-03T01:23:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    1,
                    23,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22875v2",
                "updated": "2025-04-02T18:51:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    18,
                    51,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-28T21:02:32Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "title": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading"
                },
                "summary": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing."
                },
                "authors": [
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhi Wang"
                },
                "author": "Yinzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01582v1",
                "updated": "2025-04-02T10:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "title": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors"
                },
                "summary": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues."
                },
                "authors": [
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Xiaoxuan Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Zhihang Tan"
                    },
                    {
                        "name": "Wenbo Xu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v4",
                "updated": "2025-04-02T04:57:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    57,
                    15,
                    2,
                    92,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v5",
                "updated": "2025-04-02T01:58:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    58,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v1",
                "updated": "2025-04-02T01:49:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "11 pages, 6 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01157v1",
                "updated": "2025-04-01T19:48:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T19:48:17Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB"
                },
                "summary": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden."
                },
                "authors": [
                    {
                        "name": "Anas Dorbani"
                    },
                    {
                        "name": "Sunny Yasser"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01104v1",
                "updated": "2025-04-01T18:21:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:21:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Fundamentals of Caching Layered Data objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamentals of Caching Layered Data objects"
                },
                "summary": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers."
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "arxiv_comment": "An abridged version of this paper has been accepted at the 45th IEEE\n  International Conference on Distributed Computing Systems (ICDCS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01084v1",
                "updated": "2025-04-01T18:00:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:00:48Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "title": "Surfactants Screen Slide Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfactants Screen Slide Electrification"
                },
                "summary": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop."
                },
                "authors": [
                    {
                        "name": "Xiaomei Li"
                    },
                    {
                        "name": "Zhongyuan Ni"
                    },
                    {
                        "name": "Xiaoteng Zhou"
                    },
                    {
                        "name": "Lisa S. Bauer"
                    },
                    {
                        "name": "Diego Diaz"
                    },
                    {
                        "name": "Gabriele Schfer"
                    },
                    {
                        "name": "Hans-Jrgen Butt"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Jrgen Butt"
                },
                "author": "Hans-Jrgen Butt",
                "arxiv_comment": "13 pages, 4 figures, 50 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00999v1",
                "updated": "2025-04-01T17:39:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:39:19Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization"
                },
                "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Luyuan Zhang"
                    },
                    {
                        "name": "Zedong Wang"
                    },
                    {
                        "name": "Juanxi Tian"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Qingsong Xie"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "CVPR2025 (in process for more analysis and extension)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v1",
                "updated": "2025-04-01T17:08:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v2",
                "updated": "2025-04-01T14:21:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    21,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00726v1",
                "updated": "2025-04-01T12:34:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T12:34:58Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "title": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning"
                },
                "summary": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL."
                },
                "authors": [
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Weibo He"
                    },
                    {
                        "name": "Wanglei Feng"
                    },
                    {
                        "name": "Zhenyu Wen"
                    },
                    {
                        "name": "Bin Qian"
                    },
                    {
                        "name": "Blesson Varghese"
                    }
                ],
                "author_detail": {
                    "name": "Blesson Varghese"
                },
                "author": "Blesson Varghese",
                "arxiv_comment": "Poster accepted at IEEE ICDCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00557v1",
                "updated": "2025-04-01T09:10:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T09:10:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features"
                },
                "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity."
                },
                "authors": [
                    {
                        "name": "Jewon Lee"
                    },
                    {
                        "name": "Ki-Ung Song"
                    },
                    {
                        "name": "Seungmin Yang"
                    },
                    {
                        "name": "Donguk Lim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Bo-Kyeong Kim"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "accepted at CVPR 2025 Workshop on ELVM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00474v1",
                "updated": "2025-04-01T07:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T07:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "title": "High specific impulse electrospray propulsion with small capillary\n  emitters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High specific impulse electrospray propulsion with small capillary\n  emitters"
                },
                "summary": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$."
                },
                "authors": [
                    {
                        "name": "Manel Caballero-Prez"
                    },
                    {
                        "name": "Marc Galobardes-Esteban"
                    },
                    {
                        "name": "Manuel Gamero-Castao"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Gamero-Castao"
                },
                "author": "Manuel Gamero-Castao",
                "arxiv_comment": "29 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v1",
                "updated": "2025-03-31T12:32:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24000v1",
                "updated": "2025-03-31T12:23:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving"
                },
                "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "21 pages, 18 figures, published to MLSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23988v1",
                "updated": "2025-03-31T11:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments"
                },
                "summary": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups."
                },
                "authors": [
                    {
                        "name": "Elayne Lemos"
                    },
                    {
                        "name": "Rodrigo Oliveira"
                    },
                    {
                        "name": "Jairson Rodrigues"
                    },
                    {
                        "name": "Rosalvo F. Oliveira Neto"
                    }
                ],
                "author_detail": {
                    "name": "Rosalvo F. Oliveira Neto"
                },
                "author": "Rosalvo F. Oliveira Neto",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; I.2.0; B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v1",
                "updated": "2025-03-31T11:13:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v2",
                "updated": "2025-03-31T10:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23897v1",
                "updated": "2025-03-31T09:46:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:46:56Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model"
                },
                "summary": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released."
                },
                "authors": [
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Bihan Wen"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v4",
                "updated": "2025-03-31T03:28:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    3,
                    28,
                    44,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v2",
                "updated": "2025-03-31T02:19:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    19,
                    29,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v2",
                "updated": "2025-03-30T11:14:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    14,
                    17,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages,fix figure mistake(inv/fwd skipping) in fig2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23397v1",
                "updated": "2025-03-30T11:09:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T11:09:06Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "title": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update"
                },
                "summary": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads."
                },
                "authors": [
                    {
                        "name": "Yuan Chen"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Wenhai Li"
                    },
                    {
                        "name": "Lingfeng Deng"
                    }
                ],
                "author_detail": {
                    "name": "Lingfeng Deng"
                },
                "author": "Lingfeng Deng",
                "arxiv_comment": "14 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23388v1",
                "updated": "2025-03-30T10:34:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T10:34:45Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation"
                },
                "summary": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC."
                },
                "authors": [
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Hebei Li"
                    },
                    {
                        "name": "Faisal Nadeem Khan"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v2",
                "updated": "2025-03-30T09:46:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    46,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v2",
                "updated": "2025-03-30T09:19:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    19,
                    53,
                    6,
                    89,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v1",
                "updated": "2025-03-30T08:51:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12820v2",
                "updated": "2025-03-30T08:13:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    13,
                    50,
                    6,
                    89,
                    0
                ],
                "published": "2024-07-01T13:05:42Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    5,
                    42,
                    0,
                    183,
                    0
                ],
                "title": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference"
                },
                "summary": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding."
                },
                "authors": [
                    {
                        "name": "Hailin Zhang"
                    },
                    {
                        "name": "Xiaodong Ji"
                    },
                    {
                        "name": "Yilin Chen"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23294v1",
                "updated": "2025-03-30T03:20:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T03:20:34Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "title": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference"
                },
                "summary": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets."
                },
                "authors": [
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Jiguang Wan"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the Design, Automation, and Test in Europe 2025 (DATE\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v2",
                "updated": "2025-03-30T02:45:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    2,
                    45,
                    0,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18278v2",
                "updated": "2025-03-29T23:00:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    23,
                    0,
                    27,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-24T01:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model"
                },
                "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Lingyi Huang"
                    },
                    {
                        "name": "Yu Gong"
                    },
                    {
                        "name": "Chendi Li"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Ponnuswamy Sadayappan"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v2",
                "updated": "2025-03-29T04:43:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    4,
                    43,
                    11,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v1",
                "updated": "2025-03-29T01:06:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22796v1",
                "updated": "2025-03-28T18:00:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T18:00:12Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers"
                },
                "summary": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity."
                },
                "authors": [
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Rundong Su"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Mingzhu Shen Yibo Fan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v3",
                "updated": "2025-03-28T16:15:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    15,
                    19,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v3",
                "updated": "2025-03-28T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    11,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22329v1",
                "updated": "2025-03-28T11:08:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:08:34Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "title": "A Refined Analysis of Massive Activations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Refined Analysis of Massive Activations in LLMs"
                },
                "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations."
                },
                "authors": [
                    {
                        "name": "Louis Owen"
                    },
                    {
                        "name": "Nilabhra Roy Chowdhury"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Fabian Gra"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Gra"
                },
                "author": "Fabian Gra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22196v1",
                "updated": "2025-03-28T07:26:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T07:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices"
                },
                "summary": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Renshou Wu"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22017v1",
                "updated": "2025-03-27T22:16:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T22:16:57Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "title": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype"
                },
                "summary": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device."
                },
                "authors": [
                    {
                        "name": "Jianping Zeng"
                    },
                    {
                        "name": "Shuyi Pei"
                    },
                    {
                        "name": "Da Zhang"
                    },
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Amir Beygi"
                    },
                    {
                        "name": "Xuebin Yao"
                    },
                    {
                        "name": "Ramdas Kachare"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Zongwang Li"
                    },
                    {
                        "name": "Marie Nguyen"
                    },
                    {
                        "name": "Rekha Pitchumani"
                    },
                    {
                        "name": "Yang Soek Ki"
                    },
                    {
                        "name": "Changhee Jung"
                    }
                ],
                "author_detail": {
                    "name": "Changhee Jung"
                },
                "author": "Changhee Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v2",
                "updated": "2025-03-27T17:48:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v1",
                "updated": "2025-03-27T17:37:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v4",
                "updated": "2025-03-27T15:21:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    21,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17922v2",
                "updated": "2025-03-27T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    14,
                    11,
                    37,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-23T03:36:52Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    36,
                    52,
                    6,
                    82,
                    0
                ],
                "title": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for\n  Efficient LLM Inference"
                },
                "summary": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements in long-context inference capabilities of large\nlanguage models (LLMs), the KV cache has become one of the foundational\ncomponents. However, its substantial GPU memory consumption makes KV cache\ncompression a key technique for enabling efficient LLM inference in industrial\nscenarios. While recent studies have focused on optimizing the memory occupied\nby the KV cache, they overlook two critical factors: preserving semantic\ncoherence and considering task-specific characteristic during compression. To\naddress these limitations, we propose a novel task-adaptive KV cache window\nselection method, WindowKV. WindowKV dynamically selects local semantic windows\nconsisting of consecutive tokens, according to task-specific characteristics,\nensuring the retained KV cache captures continuous, essential context.\nAdditionally, we introduce an intra-group layer KV cache indices sharing\nstrategy to reduce computational overhead, achieving a balance between\nperformance and efficiency. We rigorously evaluate WindowKV on the LongBench\nbenchmark, and the results demonstrate that it maintains a performance\ncomparable to full KV cache retention while using only 12% of the original KV\ncache, significantly reducing memory requirements. Furthermore, our method also\nachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,\nhighlighting its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Youhui Zuo"
                    },
                    {
                        "name": "Sibo Wei"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Wenpeng Lu"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17038v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17038v3",
                "updated": "2025-03-27T12:14:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    12,
                    14,
                    56,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-21T10:48:35Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    10,
                    48,
                    35,
                    4,
                    80,
                    0
                ],
                "title": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation"
                },
                "summary": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of embedded hardware platforms poses significant\nchallenges for real-time workloads. Architectural features such as Intel RDT,\nArm QoS, and Arm MPAM are either unavailable on commercial embedded platforms\nor designed primarily for server environments optimized for average-case\nperformance and might fail to deliver the expected real-time guarantees. Arm\nDynamIQ Shared Unit (DSU) includes isolation features-among others, hardware\nper-way cache partitioning-that can improve the real-time guarantees of complex\nembedded multicore systems and facilitate real-time analysis. However, the DSU\nalso targets average cases, and its real-time capabilities have not yet been\nevaluated. This paper presents the first comprehensive analysis of three\nreal-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and\nNVIDIA Orin platforms. We integrate support for the DSU at the operating system\nand hypervisor level and conduct a large-scale evaluation using both synthetic\nand real-world benchmarks with varying types and intensities of interference.\nOur results make extensive use of performance counters and indicate that,\nalthough effective, the quality of partitioning and isolation provided by the\nDSU depends on the type and the intensity of the interfering workloads. In\naddition, we uncover and analyze in detail the correlation between benchmarks\nand different types and intensities of interference."
                },
                "authors": [
                    {
                        "name": "Ashutosh Pradhan"
                    },
                    {
                        "name": "Daniele Ottaviano"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Haozheng Huang"
                    },
                    {
                        "name": "Alexander Zuepke"
                    },
                    {
                        "name": "Andrea Bastoni"
                    },
                    {
                        "name": "Marco Caccamo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Caccamo"
                },
                "author": "Marco Caccamo",
                "arxiv_comment": "Accepted for publication in the Proceedings of the 31st IEEE\n  Real-Time and Embedded Technology and Applications Symposium (RTAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17038v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17038v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; C.4; D.4.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v3",
                "updated": "2025-03-27T11:46:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    46,
                    22,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights are available at\nhttps://github.com/ali-vilab/CDT."
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v2",
                "updated": "2025-03-27T09:53:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    9,
                    53,
                    15,
                    3,
                    86,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Grkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_doi": "10.1109/TVLSI.2025.3527225",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVLSI.2025.3527225",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.17606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems (\n  Volume: 33, Issue: 4, April 2025)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11108v2",
                "updated": "2025-03-27T07:02:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    2,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-14T06:01:42Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    1,
                    42,
                    4,
                    73,
                    0
                ],
                "title": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer\n  Decoding"
                },
                "summary": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in the tensor version of transformers presents a\nsignificant bottleneck during inference. While previous work analyzes the\nfundamental space complexity barriers in standard attention mechanisms [Haris\nand Onak, 2025], our work generalizes the space complexity barriers result to\ntensor attention version. Our theoretical contributions rely on a reduction\nfrom communication complexity and deduce the memory lower bound for\ntensor-structured attention mechanisms when $d = \\Omega(\\log n)$. Furthermore,\nwe introduce two types of tensor attention cache and present a trade-off\nbetween time and memory for two scenarios. Overall, our work provides a\ntheoretical foundation for us to understand the time-memory tradeoff of\nKV-Cache compression in tensor attention decoding and offers more perspectives\nin developing more memory-efficient tensor attention Transformer architectures."
                },
                "authors": [
                    {
                        "name": "Yifang Chen"
                    },
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yu Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yu Tian"
                },
                "author": "Yu Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16302v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16302v2",
                "updated": "2025-03-26T15:08:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    15,
                    8,
                    12,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-20T16:23:44Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    16,
                    23,
                    44,
                    3,
                    79,
                    0
                ],
                "title": "Unleashing Vecset Diffusion Model for Fast Shape Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Vecset Diffusion Model for Fast Shape Generation"
                },
                "summary": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D shape generation has greatly flourished through the development of\nso-called \"native\" 3D diffusion, particularly through the Vecset Diffusion\nModel (VDM). While recent advancements have shown promising results in\ngenerating high-resolution 3D shapes, VDM still struggles with high-speed\ngeneration. Challenges exist because of difficulties not only in accelerating\ndiffusion sampling but also VAE decoding in VDM, areas under-explored in\nprevious works. To address these challenges, we present FlashVDM, a systematic\nframework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables\nflexible diffusion sampling with as few as 5 inference steps and comparable\nquality, which is made possible by stabilizing consistency distillation with\nour newly introduced Progressive Flow Distillation. For VAE, we introduce a\nlightning vecset decoder equipped with Adaptive KV Selection, Hierarchical\nVolume Decoding, and Efficient Network Design. By exploiting the locality of\nthe vecset and the sparsity of shape surface in the volume, our decoder\ndrastically lowers FLOPs, minimizing the overall decoding overhead. We apply\nFlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic\nevaluation, we show that our model significantly outperforms existing fast 3D\ngeneration methods, achieving comparable performance to the state-of-the-art\nwhile reducing inference time by over 45x for reconstruction and 32x for\ngeneration. Code and models are available at\nhttps://github.com/Tencent/FlashVDM."
                },
                "authors": [
                    {
                        "name": "Zeqiang Lai"
                    },
                    {
                        "name": "Yunfei Zhao"
                    },
                    {
                        "name": "Zibo Zhao"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Fuyun Wang"
                    },
                    {
                        "name": "Huiwen Shi"
                    },
                    {
                        "name": "Xianghui Yang"
                    },
                    {
                        "name": "Qingxiang Lin"
                    },
                    {
                        "name": "Jingwei Huang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "Technical report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16302v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16302v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v3",
                "updated": "2025-03-26T13:59:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    59,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN\n  Inference on NVIDIA GPUs"
                },
                "summary": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud service providers heavily colocate high-priority, latency-sensitive\n(LS), and low-priority, best-effort (BE) DNN inference services on the same GPU\nto improve resource utilization in data centers. Among the critical shared GPU\nresources, there has been very limited analysis on the dynamic allocation of\ncompute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU\nresource management solutions are either hardware-specific, or unable to\ndynamically allocate resources to different tenants, or both; (2) NVIDIA\ndoesn't expose interfaces for VRAM bandwidth allocation, and the software stack\nand VRAM channel architectures are black-box, both of which limit the\nsoftware-level resource management. These drive prior work to design either\nconservative sharing policies detrimental to throughput, or static resource\npartitioning only applicable to a few GPU models.\n  To bridge this gap, this paper proposes SGDRC, a fully software-defined\ndynamic VRAM bandwidth and compute unit management solution for concurrent DNN\ninference services. SGDRC aims at guaranteeing service quality, maximizing the\noverall throughput, and providing general applicability to NVIDIA GPUs. SGDRC\nfirst reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs\nthrough comprehensive reverse engineering and eliminates VRAM channel conflicts\nusing software-level cache coloring. SGDRC applies bimodal tensors and tidal SM\nmasking to dynamically allocate VRAM bandwidth and compute units, and guides\nthe allocation of resources based on offline profiling. We evaluate 11\nmainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show\nthat compared with the state-of-the-art GPU sharing solutions, SGDRC achieves\nthe highest SLO attainment rates (99.0% on average), and improves overall\nthroughput by up to 1.47x and BE job throughput by up to 2.36x."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yunzhe Li"
                    },
                    {
                        "name": "Zhifeng Jiang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_doi": "10.1145/3710848.3710863",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3710848.3710863",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13996v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 19 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20481v1",
                "updated": "2025-03-26T12:10:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T12:10:53Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    12,
                    10,
                    53,
                    2,
                    85,
                    0
                ],
                "title": "Analyzing Modern NVIDIA GPU cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Modern NVIDIA GPU cores"
                },
                "summary": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are the most popular platform for accelerating HPC workloads, such as\nartificial intelligence and science simulations. However, most\nmicroarchitectural research in academia relies on GPU core pipeline designs\nbased on architectures that are more than 15 years old.\n  This paper reverse engineers modern NVIDIA GPU cores, unveiling many key\naspects of its design and explaining how GPUs leverage hardware-compiler\ntechniques where the compiler guides hardware during execution. In particular,\nit reveals how the issue logic works including the policy of the issue\nscheduler, the structure of the register file and its associated cache, and\nmultiple features of the memory pipeline. Moreover, it analyses how a simple\ninstruction prefetcher based on a stream buffer fits well with modern NVIDIA\nGPUs and is likely to be used. Furthermore, we investigate the impact of the\nregister file cache and the number of register file read ports on both\nsimulation accuracy and performance.\n  By modeling all these new discovered microarchitectural details, we achieve\n18.24% lower mean absolute percentage error (MAPE) in execution cycles than\nprevious state-of-the-art simulators, resulting in an average of 13.98% MAPE\nwith respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that\nthis new model stands for other NVIDIA architectures, such as Turing. Finally,\nwe show that the software-based dependence management mechanism included in\nmodern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in\nterms of performance and area."
                },
                "authors": [
                    {
                        "name": "Rodrigo Huerta"
                    },
                    {
                        "name": "Mojtaba Abaie Shoushtary"
                    },
                    {
                        "name": "Jos-Lorenzo Cruz"
                    },
                    {
                        "name": "Antonio Gonzlez"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Gonzlez"
                },
                "author": "Antonio Gonzlez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v2",
                "updated": "2025-03-26T11:08:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    11,
                    8,
                    20,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20174v1",
                "updated": "2025-03-26T02:58:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "published": "2025-03-26T02:58:41Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    2,
                    58,
                    41,
                    2,
                    85,
                    0
                ],
                "title": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Devil is in the Uniformity: Exploring Diverse Learners within\n  Transformer for Image Restoration"
                },
                "summary": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based approaches have gained significant attention in image\nrestoration, where the core component, i.e, Multi-Head Attention (MHA), plays a\ncrucial role in capturing diverse features and recovering high-quality results.\nIn MHA, heads perform attention calculation independently from uniform split\nsubspaces, and a redundancy issue is triggered to hinder the model from\nachieving satisfactory outputs. In this paper, we propose to improve MHA by\nexploring diverse learners and introducing various interactions between heads,\nwhich results in a Hierarchical multI-head atteNtion driven Transformer model,\ntermed HINT, for image restoration. HINT contains two modules, i.e., the\nHierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating\n(QKCU) module, to address the redundancy problem that is rooted in vanilla MHA.\nSpecifically, HMHA extracts diverse contextual features by employing heads to\nlearn from subspaces of varying sizes and containing different information.\nMoreover, QKCU, comprising intra- and inter-layer schemes, further reduces the\nredundancy problem by facilitating enhanced interactions between attention\nheads within and across layers. Extensive experiments are conducted on 12\nbenchmarks across 5 image restoration tasks, including low-light enhancement,\ndehazing, desnowing, denoising, and deraining, to demonstrate the superiority\nof HINT. The source code is available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Shihao Zhou"
                    },
                    {
                        "name": "Dayu Li"
                    },
                    {
                        "name": "Jinshan Pan"
                    },
                    {
                        "name": "Juncheng Zhou"
                    },
                    {
                        "name": "Jinglei Shi"
                    },
                    {
                        "name": "Jufeng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jufeng Yang"
                },
                "author": "Jufeng Yang",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v2",
                "updated": "2025-03-26T01:58:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    26,
                    1,
                    58,
                    40,
                    2,
                    85,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\ninference requests for context lengths in the range of millions of tokens\npresents unique challenges. While existing techniques are effective for\ntraining, they fail to address the unique challenges of inference, such as\nvarying prefill and decode phases and their associated latency constraints --\nlike Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore,\nno long-context inference solutions address head-of-line blocking today.\n  We present Medha, a system for efficient long-context LLM inference that\nintroduces three key innovations: adaptive chunking with slack-aware scheduling\nto prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce\nTTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into\na novel 3D parallelism serving engine, Medha achieves unprecedented scale --\nsupporting contexts up to 10M tokens with production-grade latency. Our\nevaluation shows Medha reduces median latency by up to 30x compared to\nstate-of-the-art systems when serving a mix of short and long requests, while\nimproving throughput by upwards of 5x. This enables, for the first time,\nefficient long-context LLM inference at scale without compromising on shorter\nrequest latencies or system efficiency."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "igo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v2",
                "updated": "2025-03-25T17:56:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    56,
                    1,
                    1,
                    84,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visualizing the Invisible: A Generative AR System for Intuitive\n  Multi-Modal Sensor Data Presentation"
                },
                "summary": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be difficult for non-experts because of the\ncomplexity and different semantic meanings of sensor modalities. This leads to\na need for intuitive and effective methods to present sensor information.\nHowever, creating intuitive sensor data visualizations presents three key\nchallenges: the variability of sensor readings, gaps in domain comprehension,\nand the dynamic nature of sensor data. To address these issues, we propose\nVivar, a novel system that integrates multi-modal sensor data and presents 3D\nvolumetric content for AR visualization. In particular, we introduce a\ncross-modal embedding approach that maps sensor data into a pre-trained visual\nembedding space through barycentric interpolation. This approach accurately\nreflects value changes in multi-modal sensor information, ensuring that sensor\nvariations are properly shown in visualization outcomes. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation, demonstrating 11x latency reduction without compromising\nquality. A user study involving over 503 participants, including domain\nexperts, demonstrates Vivar's effectiveness in accuracy, consistency, and\nreal-world applicability, paving the way for more intuitive sensor data\nvisualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19950v1",
                "updated": "2025-03-25T16:24:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T16:24:45Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    16,
                    24,
                    45,
                    1,
                    84,
                    0
                ],
                "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior\n  Accuracy Preservation"
                },
                "summary": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV\nCache in large language model (LLM) inference, delivering substantial memory\nsavings while preserving superior performance. Previous methods either assume\nthat later tokens are more important or attempt to predict important tokens\nbased on earlier attention patterns. Both approaches, however, can result in\nperformance bottlenecks or frequent mispredictions.\n  LogQuant takes a different approach. By applying a log-based filtering\nmechanism, it selectively compresses the KV Cache across the entire context,\nachieving better performance with the same or even reduced memory footprint\ncompared to existing methods. In benchmark tests, it enhances throughput by 25%\nand boosts batch size by 60% without increasing memory consumption. For\nchallenging tasks such as Math and Code Completion, LogQuant improves accuracy\nby 40% to 200% at the same compression ratio, outperforming comparable\ntechniques.LogQuant integrates effortlessly with popular inference frameworks\nlike Python's transformers library. Implementation can be available in\nhttps://github.com/Concyclics/LogQuantKV."
                },
                "authors": [
                    {
                        "name": "Han Chen"
                    },
                    {
                        "name": "Zicong Jiang"
                    },
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Pingyi Luo"
                    },
                    {
                        "name": "Mian Lu"
                    },
                    {
                        "name": "Yuqiang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuqiang Chen"
                },
                "author": "Yuqiang Chen",
                "arxiv_comment": "Accepted by ICLR 2025 Workshop on Sparsity in LLMs (SLLM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19786v1",
                "updated": "2025-03-25T15:52:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T15:52:34Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    15,
                    52,
                    34,
                    1,
                    84,
                    0
                ],
                "title": "Gemma 3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gemma 3 Technical Report"
                },
                "summary": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Gemma 3, a multimodal addition to the Gemma family of\nlightweight open models, ranging in scale from 1 to 27 billion parameters. This\nversion introduces vision understanding abilities, a wider coverage of\nlanguages and longer context - at least 128K tokens. We also change the\narchitecture of the model to reduce the KV-cache memory that tends to explode\nwith long context. This is achieved by increasing the ratio of local to global\nattention layers, and keeping the span on local attention short. The Gemma 3\nmodels are trained with distillation and achieve superior performance to Gemma\n2 for both pre-trained and instruction finetuned versions. In particular, our\nnovel post-training recipe significantly improves the math, chat,\ninstruction-following and multilingual abilities, making Gemma3-4B-IT\ncompetitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro\nacross benchmarks. We release all our models to the community."
                },
                "authors": [
                    {
                        "name": "Gemma Team"
                    },
                    {
                        "name": "Aishwarya Kamath"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Nino Vieillard"
                    },
                    {
                        "name": "Ramona Merhej"
                    },
                    {
                        "name": "Sarah Perrin"
                    },
                    {
                        "name": "Tatiana Matejovicova"
                    },
                    {
                        "name": "Alexandre Ram"
                    },
                    {
                        "name": "Morgane Rivire"
                    },
                    {
                        "name": "Louis Rouillard"
                    },
                    {
                        "name": "Thomas Mesnard"
                    },
                    {
                        "name": "Geoffrey Cideron"
                    },
                    {
                        "name": "Jean-bastien Grill"
                    },
                    {
                        "name": "Sabela Ramos"
                    },
                    {
                        "name": "Edouard Yvinec"
                    },
                    {
                        "name": "Michelle Casbon"
                    },
                    {
                        "name": "Etienne Pot"
                    },
                    {
                        "name": "Ivo Penchev"
                    },
                    {
                        "name": "Gal Liu"
                    },
                    {
                        "name": "Francesco Visin"
                    },
                    {
                        "name": "Kathleen Kenealy"
                    },
                    {
                        "name": "Lucas Beyer"
                    },
                    {
                        "name": "Xiaohai Zhai"
                    },
                    {
                        "name": "Anton Tsitsulin"
                    },
                    {
                        "name": "Robert Busa-Fekete"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Benjamin Coleman"
                    },
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Basil Mustafa"
                    },
                    {
                        "name": "Iain Barr"
                    },
                    {
                        "name": "Emilio Parisotto"
                    },
                    {
                        "name": "David Tian"
                    },
                    {
                        "name": "Matan Eyal"
                    },
                    {
                        "name": "Colin Cherry"
                    },
                    {
                        "name": "Jan-Thorsten Peter"
                    },
                    {
                        "name": "Danila Sinopalnikov"
                    },
                    {
                        "name": "Surya Bhupatiraju"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Mehran Kazemi"
                    },
                    {
                        "name": "Dan Malkin"
                    },
                    {
                        "name": "Ravin Kumar"
                    },
                    {
                        "name": "David Vilar"
                    },
                    {
                        "name": "Idan Brusilovsky"
                    },
                    {
                        "name": "Jiaming Luo"
                    },
                    {
                        "name": "Andreas Steiner"
                    },
                    {
                        "name": "Abe Friesen"
                    },
                    {
                        "name": "Abhanshu Sharma"
                    },
                    {
                        "name": "Abheesht Sharma"
                    },
                    {
                        "name": "Adi Mayrav Gilady"
                    },
                    {
                        "name": "Adrian Goedeckemeyer"
                    },
                    {
                        "name": "Alaa Saade"
                    },
                    {
                        "name": "Alex Feng"
                    },
                    {
                        "name": "Alexander Kolesnikov"
                    },
                    {
                        "name": "Alexei Bendebury"
                    },
                    {
                        "name": "Alvin Abdagic"
                    },
                    {
                        "name": "Amit Vadi"
                    },
                    {
                        "name": "Andrs Gyrgy"
                    },
                    {
                        "name": "Andr Susano Pinto"
                    },
                    {
                        "name": "Anil Das"
                    },
                    {
                        "name": "Ankur Bapna"
                    },
                    {
                        "name": "Antoine Miech"
                    },
                    {
                        "name": "Antoine Yang"
                    },
                    {
                        "name": "Antonia Paterson"
                    },
                    {
                        "name": "Ashish Shenoy"
                    },
                    {
                        "name": "Ayan Chakrabarti"
                    },
                    {
                        "name": "Bilal Piot"
                    },
                    {
                        "name": "Bo Wu"
                    },
                    {
                        "name": "Bobak Shahriari"
                    },
                    {
                        "name": "Bryce Petrini"
                    },
                    {
                        "name": "Charlie Chen"
                    },
                    {
                        "name": "Charline Le Lan"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "CJ Carey"
                    },
                    {
                        "name": "Cormac Brick"
                    },
                    {
                        "name": "Daniel Deutsch"
                    },
                    {
                        "name": "Danielle Eisenbud"
                    },
                    {
                        "name": "Dee Cattle"
                    },
                    {
                        "name": "Derek Cheng"
                    },
                    {
                        "name": "Dimitris Paparas"
                    },
                    {
                        "name": "Divyashree Shivakumar Sreepathihalli"
                    },
                    {
                        "name": "Doug Reid"
                    },
                    {
                        "name": "Dustin Tran"
                    },
                    {
                        "name": "Dustin Zelle"
                    },
                    {
                        "name": "Eric Noland"
                    },
                    {
                        "name": "Erwin Huizenga"
                    },
                    {
                        "name": "Eugene Kharitonov"
                    },
                    {
                        "name": "Frederick Liu"
                    },
                    {
                        "name": "Gagik Amirkhanyan"
                    },
                    {
                        "name": "Glenn Cameron"
                    },
                    {
                        "name": "Hadi Hashemi"
                    },
                    {
                        "name": "Hanna Klimczak-Pluciska"
                    },
                    {
                        "name": "Harman Singh"
                    },
                    {
                        "name": "Harsh Mehta"
                    },
                    {
                        "name": "Harshal Tushar Lehri"
                    },
                    {
                        "name": "Hussein Hazimeh"
                    },
                    {
                        "name": "Ian Ballantyne"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Ivan Nardini"
                    },
                    {
                        "name": "Jean Pouget-Abadie"
                    },
                    {
                        "name": "Jetha Chan"
                    },
                    {
                        "name": "Joe Stanton"
                    },
                    {
                        "name": "John Wieting"
                    },
                    {
                        "name": "Jonathan Lai"
                    },
                    {
                        "name": "Jordi Orbay"
                    },
                    {
                        "name": "Joseph Fernandez"
                    },
                    {
                        "name": "Josh Newlan"
                    },
                    {
                        "name": "Ju-yeong Ji"
                    },
                    {
                        "name": "Jyotinder Singh"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Kathy Yu"
                    },
                    {
                        "name": "Kevin Hui"
                    },
                    {
                        "name": "Kiran Vodrahalli"
                    },
                    {
                        "name": "Klaus Greff"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Marcella Valentine"
                    },
                    {
                        "name": "Marina Coelho"
                    },
                    {
                        "name": "Marvin Ritter"
                    },
                    {
                        "name": "Matt Hoffman"
                    },
                    {
                        "name": "Matthew Watson"
                    },
                    {
                        "name": "Mayank Chaturvedi"
                    },
                    {
                        "name": "Michael Moynihan"
                    },
                    {
                        "name": "Min Ma"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Natasha Noy"
                    },
                    {
                        "name": "Nathan Byrd"
                    },
                    {
                        "name": "Nick Roy"
                    },
                    {
                        "name": "Nikola Momchev"
                    },
                    {
                        "name": "Nilay Chauhan"
                    },
                    {
                        "name": "Noveen Sachdeva"
                    },
                    {
                        "name": "Oskar Bunyan"
                    },
                    {
                        "name": "Pankil Botarda"
                    },
                    {
                        "name": "Paul Caron"
                    },
                    {
                        "name": "Paul Kishan Rubenstein"
                    },
                    {
                        "name": "Phil Culliton"
                    },
                    {
                        "name": "Philipp Schmid"
                    },
                    {
                        "name": "Pier Giuseppe Sessa"
                    },
                    {
                        "name": "Pingmei Xu"
                    },
                    {
                        "name": "Piotr Stanczyk"
                    },
                    {
                        "name": "Pouya Tafti"
                    },
                    {
                        "name": "Rakesh Shivanna"
                    },
                    {
                        "name": "Renjie Wu"
                    },
                    {
                        "name": "Renke Pan"
                    },
                    {
                        "name": "Reza Rokni"
                    },
                    {
                        "name": "Rob Willoughby"
                    },
                    {
                        "name": "Rohith Vallu"
                    },
                    {
                        "name": "Ryan Mullins"
                    },
                    {
                        "name": "Sammy Jerome"
                    },
                    {
                        "name": "Sara Smoot"
                    },
                    {
                        "name": "Sertan Girgin"
                    },
                    {
                        "name": "Shariq Iqbal"
                    },
                    {
                        "name": "Shashir Reddy"
                    },
                    {
                        "name": "Shruti Sheth"
                    },
                    {
                        "name": "Siim Pder"
                    },
                    {
                        "name": "Sijal Bhatnagar"
                    },
                    {
                        "name": "Sindhu Raghuram Panyam"
                    },
                    {
                        "name": "Sivan Eiger"
                    },
                    {
                        "name": "Susan Zhang"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Trevor Yacovone"
                    },
                    {
                        "name": "Tyler Liechty"
                    },
                    {
                        "name": "Uday Kalra"
                    },
                    {
                        "name": "Utku Evci"
                    },
                    {
                        "name": "Vedant Misra"
                    },
                    {
                        "name": "Vincent Roseberry"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Vlad Kolesnikov"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Yinlam Chow"
                    },
                    {
                        "name": "Yuvein Zhu"
                    },
                    {
                        "name": "Zichuan Wei"
                    },
                    {
                        "name": "Zoltan Egyed"
                    },
                    {
                        "name": "Victor Cotruta"
                    },
                    {
                        "name": "Minh Giang"
                    },
                    {
                        "name": "Phoebe Kirk"
                    },
                    {
                        "name": "Anand Rao"
                    },
                    {
                        "name": "Kat Black"
                    },
                    {
                        "name": "Nabila Babar"
                    },
                    {
                        "name": "Jessica Lo"
                    },
                    {
                        "name": "Erica Moreira"
                    },
                    {
                        "name": "Luiz Gustavo Martins"
                    },
                    {
                        "name": "Omar Sanseviero"
                    },
                    {
                        "name": "Lucas Gonzalez"
                    },
                    {
                        "name": "Zach Gleicher"
                    },
                    {
                        "name": "Tris Warkentin"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Evan Senter"
                    },
                    {
                        "name": "Eli Collins"
                    },
                    {
                        "name": "Joelle Barral"
                    },
                    {
                        "name": "Zoubin Ghahramani"
                    },
                    {
                        "name": "Raia Hadsell"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "D. Sculley"
                    },
                    {
                        "name": "Slav Petrov"
                    },
                    {
                        "name": "Noah Fiedel"
                    },
                    {
                        "name": "Noam Shazeer"
                    },
                    {
                        "name": "Oriol Vinyals"
                    },
                    {
                        "name": "Jeff Dean"
                    },
                    {
                        "name": "Demis Hassabis"
                    },
                    {
                        "name": "Koray Kavukcuoglu"
                    },
                    {
                        "name": "Clement Farabet"
                    },
                    {
                        "name": "Elena Buchatskaya"
                    },
                    {
                        "name": "Jean-Baptiste Alayrac"
                    },
                    {
                        "name": "Rohan Anil"
                    },
                    {
                        "name": "Dmitry"
                    },
                    {
                        "name": "Lepikhin"
                    },
                    {
                        "name": "Sebastian Borgeaud"
                    },
                    {
                        "name": "Olivier Bachem"
                    },
                    {
                        "name": "Armand Joulin"
                    },
                    {
                        "name": "Alek Andreev"
                    },
                    {
                        "name": "Cassidy Hardin"
                    },
                    {
                        "name": "Robert Dadashi"
                    },
                    {
                        "name": "Lonard Hussenot"
                    }
                ],
                "author_detail": {
                    "name": "Lonard Hussenot"
                },
                "author": "Lonard Hussenot",
                "arxiv_affiliation": "Dima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19390v1",
                "updated": "2025-03-25T06:45:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "published": "2025-03-25T06:45:13Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    6,
                    45,
                    13,
                    1,
                    84,
                    0
                ],
                "title": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Prefetcher Selection with Dynamic Request Allocation\n  Improves Prefetching Efficiency"
                },
                "summary": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware prefetching plays a critical role in hiding the off-chip DRAM\nlatency. The complexity of applications results in a wide variety of memory\naccess patterns, prompting the development of numerous cache-prefetching\nalgorithms. Consequently, commercial processors often employ a hybrid of these\nalgorithms to enhance the overall prefetching performance. Nonetheless, since\nthese prefetchers share hardware resources, conflicts arising from competing\nprefetching requests can negate the benefits of hardware prefetching. Under\nsuch circumstances, several prefetcher selection algorithms have been proposed\nto mitigate conflicts between prefetchers. However, these prior solutions\nsuffer from two limitations. First, the input demand request allocation is\ninaccurate. Second, the prefetcher selection criteria are coarse-grained.\n  In this paper, we address both limitations by introducing an efficient and\nwidely applicable prefetcher selection algorithm--Alecto, which tailors the\ndemand requests for each prefetcher. Every demand request is first sent to\nAlecto to identify suitable prefetchers before being routed to prefetchers for\ntraining and prefetching. Our analysis shows that Alecto is adept at not only\nharmonizing prefetching accuracy, coverage, and timeliness but also\nsignificantly enhancing the utilization of the prefetcher table, which is vital\nfor temporal prefetching. Alecto outperforms the state-of-the-art RL-based\nprefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in\neight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by\n5.25%. Alecto consistently delivers state-of-the-art performance in scheduling\nvarious types of cache prefetchers. In addition to the performance improvement,\nAlecto can reduce the energy consumption associated with accessing the\nprefetchers' table by 48%, while only adding less than 1 KB of storage\noverhead."
                },
                "authors": [
                    {
                        "name": "Mengming Li"
                    },
                    {
                        "name": "Qijun Zhang"
                    },
                    {
                        "name": "Yongqing Ren"
                    },
                    {
                        "name": "Zhiyao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyao Xie"
                },
                "author": "Zhiyao Xie",
                "arxiv_comment": "In 31th IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14882v2",
                "updated": "2025-03-24T23:47:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    23,
                    47,
                    51,
                    0,
                    83,
                    0
                ],
                "published": "2025-02-15T05:08:01Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    5,
                    8,
                    1,
                    5,
                    46,
                    0
                ],
                "title": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance across diverse applications. However, their computational overhead\nduring deployment remains a critical bottleneck. While Key-Value (KV) caching\neffectively trades memory for computation to enhance inference efficiency, the\ngrowing memory footprint from extensive KV caches significantly reduces\nthroughput and restricts prolonged deployment on memory-constrained GPU\ndevices. To address this challenge, we propose CalibQuant, a simple yet highly\neffective visual quantization strategy that drastically reduces both memory and\ncomputational overhead. Specifically, CalibQuant introduces an extreme 1-bit\nquantization scheme, complemented by novel post-scaling and calibration\ntechniques tailored to the intrinsic patterns of KV caches, thereby ensuring\nhigh efficiency without compromising model performance. Leveraging Triton for\nruntime optimization, we achieve a 10x throughput increase on InternVL models.\nOur method is designed to be plug-and-play, seamlessly integrating with various\nexisting MLLMs without requiring architectural changes. Extensive experiments\nconfirm that our approach significantly reduces memory usage while maintaining\ncomputational efficiency and preserving multimodal capabilities. Codes are\navailable at https://github.com/insuhan/calibquant."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Zeliang Zhang"
                    },
                    {
                        "name": "Zhiyuan Wang"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Jiani Liu"
                    },
                    {
                        "name": "Haiting Lin"
                    },
                    {
                        "name": "Mingjie Zhao"
                    },
                    {
                        "name": "Chenliang Xu"
                    },
                    {
                        "name": "Kun Wan"
                    },
                    {
                        "name": "Wentian Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wentian Zhao"
                },
                "author": "Wentian Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v2",
                "updated": "2025-03-24T21:27:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    27,
                    53,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra, particularly in comparison to that of general and\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. This\nwork examines the factorization of a skew-symmetric matrix $X$ into its\n$LTL^\\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is\ntridiagonal. This is also known as a triangular tridiagonalization. This\noperation is a means for computing the determinant of $X$ as the square of the\n(cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as\nwell as for solving systems of equations, across fields such as quantum\nelectronic structure and machine learning. Its application also often requires\npivoting in order to improve numerical stability. We compare and contrast\npreviously-published algorithms with those systematically derived using the\nFLAME methodology. Performant parallel CPU implementations are achieved by\nfusing operations at multiple levels in order to reduce memory traffic\noverhead. A key factor is the employment of new capabilities of the BLAS-like\nLibrary Instantiation Software (BLIS) framework, which now supports casting\nlevel-2 and level-3 BLAS-like operations by leveraging its gemm and other\nkernels, hierarchical parallelism, and cache blocking. A prototype, concise C++\nAPI facilitates the translation of correct-by-construction algorithms into\ncorrect code. Experiments verify that the resulting implementations greatly\nexceed the performance of previous work."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "Devin A. Matthews"
                    },
                    {
                        "name": "Maggie Myers"
                    },
                    {
                        "name": "Robert van de Geijn"
                    },
                    {
                        "name": "RuQing G. Xu"
                    }
                ],
                "author_detail": {
                    "name": "RuQing G. Xu"
                },
                "author": "RuQing G. Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19145v1",
                "updated": "2025-03-24T21:00:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T21:00:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    0,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Caching for Training-free Open-vocabulary Attribute\n  Detection"
                },
                "summary": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribute detection is crucial for many computer vision tasks, as it enables\nsystems to describe properties such as color, texture, and material. Current\napproaches often rely on labor-intensive annotation processes which are\ninherently limited: objects can be described at an arbitrary level of detail\n(e.g., color vs. color shades), leading to ambiguities when the annotators are\nnot instructed carefully. Furthermore, they operate within a predefined set of\nattributes, reducing scalability and adaptability to unforeseen downstream\napplications. We present Compositional Caching (ComCa), a training-free method\nfor open-vocabulary attribute detection that overcomes these constraints. ComCa\nrequires only the list of target attributes and objects as input, using them to\npopulate an auxiliary cache of images by leveraging web-scale databases and\nLarge Language Models to determine attribute-object compatibility. To account\nfor the compositional nature of attributes, cache images receive soft attribute\nlabels. Those are aggregated at inference time based on the similarity between\nthe input and cache images, refining the predictions of underlying\nVision-Language Models (VLMs). Importantly, our approach is model-agnostic,\ncompatible with various VLMs. Experiments on public datasets demonstrate that\nComCa significantly outperforms zero-shot and cache-based baselines, competing\nwith recent training-based methods, proving that a carefully designed\ntraining-free approach can successfully address open-vocabulary attribute\ndetection."
                },
                "authors": [
                    {
                        "name": "Marco Garosi"
                    },
                    {
                        "name": "Alessandro Conti"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Elisa Ricci"
                    },
                    {
                        "name": "Massimiliano Mancini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Mancini"
                },
                "author": "Massimiliano Mancini",
                "arxiv_comment": "CVPR 2025. Project website at https://comca-attributes.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13773v2",
                "updated": "2025-03-24T18:50:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    50,
                    9,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T23:38:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    23,
                    38,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating KV Cache Competition to Enhance User Experience in LLM\n  Inference"
                },
                "summary": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes\nhigh tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing\nuser experience, particularly in time-sensitive applications. However,\nsatisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To\naddress this, we propose a system, named CacheOPT for mitigating KV Cache\ncompetition, based on key insights from our measurements, incorporating novel\ncomponents. First, it estimates a request's output length, bounding the\ndeviation with a high specified probability, adjusted based on the request\narrival rate. Second, it allocates the estimated KVC demand to a request, and\nreuses other requests' allocated KVC to avoid preemptions while reducing\nwaiting time. Third, it proactively allocates KVC before instead of at the time\na request exhausts its allocation and reserves KVC globally to prevent\npreemptions. Fourth, it chooses a request that has long TBT SLO, long job\nremaining time and short preemption time to preempt. Fifth, it selects the\nshortest-latency strategy between swapping and recomputation for preemptions.\nExperiments show that CacheOPT achieves up to 3.29$\\times$ and 2.83$\\times$\nlower tail TBT and tail TTFT, 47\\% and 53\\% higher TTFT and TBT SLO\nattainments, and supports up to 1.58$\\times$ higher request arrival rate than\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Tanaka"
                },
                "author": "Masahiro Tanaka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v2",
                "updated": "2025-03-24T18:16:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    18,
                    16,
                    58,
                    0,
                    83,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EconoServe.\nEconoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EconoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EconoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18893v1",
                "updated": "2025-03-24T17:06:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T17:06:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    17,
                    6,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "xKV: Cross-Layer SVD for KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xKV: Cross-Layer SVD for KV-Cache Compression"
                },
                "summary": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with long context windows enable powerful\napplications but come at the cost of high memory consumption to store the Key\nand Value states (KV-Cache). Recent studies attempted to merge KV-cache from\nmultiple layers into shared representations, yet these approaches either\nrequire expensive pretraining or rely on assumptions of high per-token cosine\nsimilarity across layers which generally does not hold in practice. We find\nthat the dominant singular vectors are remarkably well-aligned across multiple\nlayers of the KV-Cache. Exploiting this insight, we propose xKV, a simple\npost-training method that applies Singular Value Decomposition (SVD) on the\nKV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers\ninto a shared low-rank subspace, significantly reducing KV-Cache sizes. Through\nextensive evaluations on the RULER long-context benchmark with widely-used LLMs\n(e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates\nthan state-of-the-art inter-layer technique while improving accuracy by 2.7%.\nMoreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA)\n(e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding\ntasks without performance degradation. These results highlight xKV's strong\ncapability and versatility in addressing memory bottlenecks for long-context\nLLM inference. Our code is publicly available at:\nhttps://github.com/abdelfattah-lab/xKV."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13064v2",
                "updated": "2025-03-24T16:47:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    47,
                    48,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-17T11:10:49Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    11,
                    10,
                    49,
                    0,
                    76,
                    0
                ],
                "title": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads"
                },
                "summary": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth of machine learning (ML) workloads has underscored the importance\nof efficient memory hierarchies to address bandwidth, latency, and scalability\nchallenges. HERMES focuses on optimizing memory subsystems for RISC-V\narchitectures to meet the computational needs of ML models such as CNNs, RNNs,\nand Transformers. This project explores state-of-the-art techniques such as\nadvanced prefetching, tensor-aware caching, and hybrid memory models. The\ncornerstone of HERMES is the integration of shared L3 caches with fine-grained\ncoherence protocols equipped with specialized pathways to deep-learning\naccelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used\nto evaluate baseline performance and scalability under representative ML\nworkloads. The findings of this study highlight the design choices, and the\nanticipated challenges, paving the way for low-latency scalable memory\noperations for ML applications."
                },
                "authors": [
                    {
                        "name": "Pranav Suryadevara"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Suryadevara"
                },
                "author": "Pranav Suryadevara",
                "arxiv_comment": "5 pages, 5 figures. Individual Project",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.2; C.1.3; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18862v1",
                "updated": "2025-03-24T16:38:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T16:38:31Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    38,
                    31,
                    0,
                    83,
                    0
                ],
                "title": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Integration of Key-Value Attention Into Pure and Hybrid\n  Transformers for Semantic Segmentation"
                },
                "summary": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While CNNs were long considered state of the art for image processing, the\nintroduction of Transformer architectures has challenged this position. While\nachieving excellent results in image classification and segmentation,\nTransformers remain inherently reliant on large training datasets and remain\ncomputationally expensive. A newly introduced Transformer derivative named KV\nTransformer shows promising results in synthetic, NLP, and image classification\ntasks, while reducing complexity and memory usage. This is especially conducive\nto use cases where local inference is required, such as medical screening\napplications. We endeavoured to further evaluate the merit of KV Transformers\non semantic segmentation tasks, specifically in the domain of medical imaging.\nBy directly comparing traditional and KV variants of the same base\narchitectures, we provide further insight into the practical tradeoffs of\nreduced model complexity. We observe a notable reduction in parameter count and\nmultiply accumulate operations, while achieving similar performance from most\nof the KV variant models when directly compared to their QKV implementation."
                },
                "authors": [
                    {
                        "name": "DeShin Hwa"
                    },
                    {
                        "name": "Tobias Holmes"
                    },
                    {
                        "name": "Klaus Drechsler"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Drechsler"
                },
                "author": "Klaus Drechsler",
                "arxiv_doi": "10.1007/978-3-658-47422-5_71",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-658-47422-5_71",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 3 figures, Preprint. Final version published in:\n  Bildverarbeitung f\\\"ur die Medizin 2025, Springer. DOI:\n  https://doi.org/10.1007/978-3-658-47422-5_71",
                "arxiv_journal_ref": "Bildverarbeitung f\\\"ur die Medizin 2025. BVM 2025. Informatik\n  aktuell. Springer Vieweg, Wiesbaden, pp 305-310",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v1",
                "updated": "2025-03-24T15:22:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with\n  Low-Bit KV Cache"
                },
                "summary": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of long-context Large Language Models (LLMs) has\nintroduced significant memory and computational challenges in autoregressive\ndecoding due to the expanding Key-Value (KV) cache. KV cache quantization has\nemerged as a promising solution, with prior work showing that 4-bit or even\n2-bit quantization can maintain model accuracy while reducing memory costs.\nHowever, despite these benefits, preliminary implementations for the low-bit KV\ncache struggle to deliver the expected speedup due to quantization and\ndequantization overheads and the lack of Tensor Cores utilization. In this\nwork, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor\nCores for efficient decoding with low-bit KV cache. Efficiently leveraging\nTensor Cores for low-bit KV cache is challenging due to the dynamic nature of\nKV cache generation at each decoding step. BitDecoding addresses these\nchallenges with a Tensor Cores-Centric BitFusion Scheme that ensures data\nlayout compatibility to enable high utilization of Tensor Cores. Additionally,\nBitDecoding incorporates a warp-efficient parallel decoding kernel and a\nfine-grained asynchronous pipeline, minimizing dequantization overhead and\nimproving computational efficiency. Experiments show that BitDecoding achieves\nup to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to\nFP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV\ncache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K\nsequence length, BitDecoding reduces single-batch decoding latency by 3x,\ndemonstrating its effectiveness in long-context generation scenarios. The code\nis available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v2",
                "updated": "2025-03-24T13:09:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    13,
                    9,
                    3,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v1",
                "updated": "2025-03-24T11:56:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_comment": "15 pages, 14 figures, and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17333v2",
                "updated": "2025-03-24T11:00:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    0,
                    35,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-21T17:33:03Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    33,
                    3,
                    4,
                    80,
                    0
                ],
                "title": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Register Dispersion: Reducing the Footprint of the Vector Register File\n  in Vector Engines of Low-Cost RISC-V CPUs"
                },
                "summary": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning (ML) applications at the edge on\nresource-constrained devices has accentuated the need for efficient ML\nprocessing on low-cost processors. While traditional CPUs provide programming\nflexibility, their general-purpose architecture often lacks the throughput\nrequired for complex ML models. The augmentation of a RISC-V processor with a\nvector unit can provide substantial data-level parallelism. However, increasing\nthe data-level parallelism supported by vector processing would make the Vector\nRegister File (VRF) a major area consumer in ultra low-cost processors, since\n32 vector registers are required for RISC-V Vector ISA compliance. This work\nleverages the insight that many ML vectorized kernels require a small number of\nactive vector registers, and proposes the use of a physically smaller VRF that\ndynamically caches only the vector registers currently accessed by the\napplication. This approach, called Register Dispersion, maps the architectural\nvector registers to a smaller set of physical registers. The proposed\nISA-compliant VRF is significantly smaller than a full-size VRF and operates\nlike a conventional cache, i.e., it only stores the most recently accessed\nvector registers. Essential registers remain readily accessible within the\ncompact VRF, while the others are offloaded to the cache/memory sub-system. The\ncompact VRF design is demonstrated to yield substantial area and power savings,\nas compared to using a full VRF, with no or minimal impact on performance. This\neffective trade-off renders the inclusion of vector units in low-cost\nprocessors feasible and practical."
                },
                "authors": [
                    {
                        "name": "Vasileios Titopoulos"
                    },
                    {
                        "name": "George Alexakis"
                    },
                    {
                        "name": "Kosmas Alexandridis"
                    },
                    {
                        "name": "Chrysostomos Nicopoulos"
                    },
                    {
                        "name": "Giorgos Dimitrakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Giorgos Dimitrakopoulos"
                },
                "author": "Giorgos Dimitrakopoulos",
                "arxiv_comment": "22nd ACM International Conference on Computing Frontiers (CF' 25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16653v2",
                "updated": "2025-03-24T03:18:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    3,
                    18,
                    49,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-20T19:10:37Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    19,
                    10,
                    37,
                    3,
                    79,
                    0
                ],
                "title": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iFlame: Interleaving Full and Linear Attention for Efficient Mesh\n  Generation"
                },
                "summary": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper propose iFlame, a novel transformer-based network architecture for\nmesh generation. While attention-based models have demonstrated remarkable\nperformance in mesh generation, their quadratic computational complexity limits\nscalability, particularly for high-resolution 3D data. Conversely, linear\nattention mechanisms offer lower computational costs but often struggle to\ncapture long-range dependencies, resulting in suboptimal outcomes. To address\nthis trade-off, we propose an interleaving autoregressive mesh generation\nframework that combines the efficiency of linear attention with the expressive\npower of full attention mechanisms. To further enhance efficiency and leverage\nthe inherent structure of mesh representations, we integrate this interleaving\napproach into an hourglass architecture, which significantly boosts efficiency.\nOur approach reduces training time while achieving performance comparable to\npure attention-based models. To improve inference efficiency, we implemented a\ncaching algorithm that almost doubles the speed and reduces the KV cache size\nby seven-eighths compared to the original Transformer. We evaluate our\nframework on ShapeNet and Objaverse, demonstrating its ability to generate\nhigh-quality 3D meshes efficiently. Our results indicate that the proposed\ninterleaving framework effectively balances computational efficiency and\ngenerative performance, making it a practical solution for mesh generation. The\ntraining takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces\non Objaverse."
                },
                "authors": [
                    {
                        "name": "Hanxiao Wang"
                    },
                    {
                        "name": "Biao Zhang"
                    },
                    {
                        "name": "Weize Quan"
                    },
                    {
                        "name": "Dong-Ming Yan"
                    },
                    {
                        "name": "Peter Wonka"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wonka"
                },
                "author": "Peter Wonka",
                "arxiv_comment": "Project website: https://wanghanxiao123.github.io/iFa/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18292v1",
                "updated": "2025-03-24T02:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T02:28:04Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    28,
                    4,
                    0,
                    83,
                    0
                ],
                "title": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jenga: Effective Memory Management for Serving LLM with Heterogeneity"
                },
                "summary": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used but expensive to run, especially\nas inference workloads grow. To lower costs, maximizing the request batch size\nby managing GPU memory efficiently is crucial. While PagedAttention has\nrecently been proposed to improve the efficiency of memory management, we find\nthat the growing heterogeneity in the embeddings dimensions, attention, and\naccess patterns of modern LLM architectures introduces new challenges for\nmemory allocation.\n  In this paper, we present Jenga, a novel memory allocation framework for\nheterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1)\nminimizing memory fragmentation when managing embeddings of different sizes,\nand (2) enabling flexible caching and eviction policies tailored to the\nspecific token-dependency patterns of various layers. Jenga employs a two-level\nmemory allocator, leveraging the least common multiple (LCM) of embedding sizes\nto optimize memory usage and providing APIs to express layer-specific caching\nlogic to enhance memory reuse.\n  We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and\nevaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations\nshow that Jenga improves GPU memory utilization by up to 79.6%, and increases\nserving throughput by up to 4.92x (1.80x on average)."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Woosuk Kwon"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Kaichao You"
                    },
                    {
                        "name": "Zhuohan Li"
                    },
                    {
                        "name": "Mingsheng Long"
                    },
                    {
                        "name": "Jidong Zhai"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_comment": "16 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v5",
                "updated": "2025-03-24T02:17:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    2,
                    17,
                    34,
                    0,
                    83,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have made significant strides in\nvideo understanding but struggle with long videos due to the limitations of\ntheir backbone LLMs. Existing solutions rely on length extrapolation, which is\nmemory-constrained, or visual token compression, which primarily leverages\nlow-level temporal redundancy while overlooking the more effective high-level\nknowledge redundancy. To address this, we propose $\\textbf{ReTaKe}$, a\ntraining-free method with two novel modules DPSelect and PivotKV, to jointly\nreduce both temporal visual redundancy and knowledge redundancy for video\ncompression. To align with the way of human temporal perception, DPSelect\nidentifies keyframes based on inter-frame distance peaks. To leverage LLMs'\nlearned prior knowledge, PivotKV marks the keyframes as pivots and compress\nnon-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe\nenables VideoLLMs to process 8 times longer frames (up to 2048), outperforming\nsimilar-sized models by 3-5% and even rivaling much larger ones on VideoMME,\nMLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression\noperations with prefilling, ReTaKe introduces only ~10% prefilling latency\noverhead while reducing decoding latency by ~20%. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench. Update metadata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18265v1",
                "updated": "2025-03-24T01:15:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "published": "2025-03-24T01:15:43Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    15,
                    43,
                    0,
                    83,
                    0
                ],
                "title": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk Management for Distributed Arbitrage Systems: Integrating\n  Artificial Intelligence"
                },
                "summary": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective risk management solutions become absolutely crucial when financial\nmarkets embrace distributed technology and decentralized financing (DeFi). This\nstudy offers a thorough survey and comparative analysis of the integration of\nartificial intelligence (AI) in risk management for distributed arbitrage\nsystems. We examine several modern caching techniques namely in memory caching,\ndistributed caching, and proxy caching and their functions in enhancing\nperformance in decentralized settings. Through literature review we examine the\nutilization of AI techniques for alleviating risks related to market\nvolatility, liquidity challenges, operational failures, regulatory compliance,\nand security threats. This comparison research evaluates various case studies\nfrom prominent DeFi technologies, emphasizing critical performance metrics like\nlatency reduction, load balancing, and system resilience. Additionally, we\nexamine the problems and trade offs associated with these technologies,\nemphasizing their effects on consistency, scalability, and fault tolerance. By\nmeticulously analyzing real world applications, specifically centering on the\nAave platform as our principal case study, we illustrate how the purposeful\namalgamation of AI with contemporary caching methodologies has revolutionized\nrisk management in distributed arbitrage systems."
                },
                "authors": [
                    {
                        "name": "Akaash Vishal Hazarika"
                    },
                    {
                        "name": "Mahak Shah"
                    },
                    {
                        "name": "Swapnil Patil"
                    },
                    {
                        "name": "Pradyumna Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Pradyumna Shukla"
                },
                "author": "Pradyumna Shukla",
                "arxiv_comment": "International Conference on AI and Financial Innovation AIFI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v1",
                "updated": "2025-03-23T20:18:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling the Write-Back Page Cache with Strong Consistency in\n  Distributed Userspace File Systems"
                },
                "summary": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale, multi-tenant nature of cloud computing requires distributed\nfile systems that offer stability, adaptability, and compatibility. FUSE-based\ndistributed file systems have emerged as a popular solution for the cloud,\noffering fast deployment, fault isolation, and POSIX compliance. However,\nFUSE's performance limitations, particularly its inability to reconcile page\ncaching with strong consistency in distributed environments, remain a\npersistent problem. Existing approaches either sacrifice consistency for\nperformance or rely on inefficient caching, limiting their practicality.\n  To this end, we present DistFUSE, the first FUSE-based distributed file\nsystem that relies on a write-back kernel-based page cache for performance and\nprovides strong consistency. DistFUSE achieves this by offloading userspace\nlock management to the kernel driver, allowing coordinated access to the\nkernel's page cache across nodes. This design eliminates blind local cache\nupdates and ensures cluster-wide consistency without compromising performance.\nOur evaluation shows DistFUSE improves throughput by up to 75% compared to\nbaseline approaches."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18030v1",
                "updated": "2025-03-23T11:07:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T11:07:24Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    11,
                    7,
                    24,
                    6,
                    82,
                    0
                ],
                "title": "Formal Verification of Parameterized Systems based on Induction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal Verification of Parameterized Systems based on Induction"
                },
                "summary": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized systems play a crucial role in the computer field, and their\nsecurity is of great significance. Formal verification of parameterized\nprotocols is especially challenging due to its \"parameterized\" feature, which\nbrings complexity and undecidability. Existing automated parameterized\nverification methods have limitations, such as facing difficulties in\nautomatically deriving parameterized invariants constrained by mixed Forall and\nExists quantifiers, or having challenges in completing the parameterized\nverification of large and complex protocols. This paper proposes a formal\nverification framework for parameterized systems based on induction, named\nwiseParaverifier. It starts from small concretizations of protocols, analyzes\ninductive counterexamples, and constructs counterexample formulas to guide the\nentire process of parameterized verification. It also presents a heuristic\nGeneralize method to quickly find auxiliary invariants, a method for promoting\ncomplex mixed quantifiers and merging parameterized invariants, and uses\nsymmetric reduction ideas to accelerate the verification process. Experimental\nresults show that wiseParaverifier can successfully complete automatic\ninductive verification on 7 cache coherence protocols and 10 distributed\nprotocols. It has strong verification capabilities and migration capabilities,\nand can provide concise and readable verification results, which is helpful for\nlearners to understand protocol behaviors."
                },
                "authors": [
                    {
                        "name": "Jiaqi Xiu"
                    },
                    {
                        "name": "Yongjian Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongjian Li"
                },
                "author": "Yongjian Li",
                "arxiv_comment": "9 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10425v2",
                "updated": "2025-03-23T06:14:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    6,
                    14,
                    35,
                    6,
                    82,
                    0
                ],
                "published": "2023-12-16T11:40:49Z",
                "published_parsed": [
                    2023,
                    12,
                    16,
                    11,
                    40,
                    49,
                    5,
                    350,
                    0
                ],
                "title": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Rumination for Client Utility Evaluation in Heterogeneous\n  Federated Learning"
                },
                "summary": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows several clients to cooperatively train machine\nlearning models without disclosing the raw data. In practical applications,\nasynchronous FL (AFL) can address the straggler effect compared to synchronous\nFL. However, Non-IID data and stale models pose significant challenges to AFL,\nas they can diminish the practicality of the global model and even lead to\ntraining failures. In this work, we propose a novel AFL framework called\nFederated Historical Learning (FedHist), which effectively addresses the\nchallenges posed by both Non-IID data and gradient staleness based on the\nconcept of knowledge rumination. FedHist enhances the stability of local\ngradients by performing weighted fusion with historical global gradients cached\non the server. Relying on hindsight, it assigns aggregation weights to each\nparticipant in a multi-dimensional manner during each communication round. To\nfurther enhance the efficiency and stability of the training process, we\nintroduce an intelligent $\\ell_2$-norm amplification scheme, which dynamically\nregulates the learning progress based on the $\\ell_2$-norms of the submitted\ngradients. Extensive experiments indicate FedHist outperforms state-of-the-art\nmethods in terms of convergence performance and test accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaorui Jiang"
                    },
                    {
                        "name": "Yu Gao"
                    },
                    {
                        "name": "Hengwei Xu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Yong Liao"
                    },
                    {
                        "name": "Pengyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pengyuan Zhou"
                },
                "author": "Pengyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17913v1",
                "updated": "2025-03-23T03:20:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:20:25Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    20,
                    25,
                    6,
                    82,
                    0
                ],
                "title": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aware Cooperative Multicast Beamforming in Dynamic\n  Satellite-Terrestrial Networks"
                },
                "summary": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the burgeoning demand for data-intensive services, satellite-terrestrial\nnetworks (STNs) face increasing backhaul link congestion, deteriorating user\nquality of service (QoS), and escalating power consumption. Cache-aided STNs\nare acknowledged as a promising paradigm for accelerating content delivery to\nusers and alleviating the load of backhaul links. However, the dynamic nature\nof low earth orbit (LEO) satellites and the complex interference among\nsatellite beams and terrestrial base stations pose challenges in effectively\nmanaging limited edge resources. To address these issues, this paper proposes a\nmethod for dynamically scheduling caching and communication resources, aiming\nto reduce network costs in terms of transmission power consumption and backhaul\ntraffic, while meeting user QoS demands and resource constraints. We formulate\na mixed timescale problem to jointly optimize cache placement, LEO satellite\nbeam direction, and cooperative multicast beamforming among satellite beams and\nbase stations. To tackle this intricate problem, we propose a two-stage\nsolution framework, where the primary problem is decoupled into a short-term\ncontent delivery subproblem and a long-term cache placement subproblem. The\nformer subproblem is solved by designing an alternating optimization approach\nwith whale optimization and successive convex approximation methods according\nto the cache placement state, while cache content in STNs is updated using an\niterative algorithm that utilizes historical information. Simulation results\ndemonstrate the effectiveness of our proposed algorithms, showcasing their\nconvergence and significantly reducing transmission power consumption and\nbackhaul traffic by up to 52%."
                },
                "authors": [
                    {
                        "name": "Shuo Yuan"
                    },
                    {
                        "name": "Yaohua Sun"
                    },
                    {
                        "name": "Mugen Peng"
                    }
                ],
                "author_detail": {
                    "name": "Mugen Peng"
                },
                "author": "Mugen Peng",
                "arxiv_doi": "10.1109/TVT.2024.3463548",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TVT.2024.3463548",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.17913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Vehicular Technology",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v1",
                "updated": "2025-03-23T03:16:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "16 pages, the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17895v1",
                "updated": "2025-03-23T01:17:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "published": "2025-03-23T01:17:08Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    1,
                    17,
                    8,
                    6,
                    82,
                    0
                ],
                "title": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orientation-Dependent \\b{eta}-Ga2O3 Heterojunction Diode with Atomic\n  Layer Deposition (ALD) Grown NiO"
                },
                "summary": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the demonstration of ALD-deposited NiO/\\b{eta}-Ga2O3\nheterojunction diodes (HJDs) on low doped drift layer and highly doped (001) &\n(100) n+ substrates with experimental observation of a parallel-plane junction\nelectric field as high as 7.5 MV/cm, revealing a crystal orientation dependence\nin \\b{eta}-Ga2O3. We use a novel metalorganic precursor\nbis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to\ndeposit NiO. The NiO/\\b{eta}-Ga2O3 HJD on 7.7 {\\mu}m-thick HVPE-grown drift\nregion exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2\nreverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of\n~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm\nparallel-plane junction electric field, with a noise floor reverse leakage\n(10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The\nNiO/\\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited\nbreakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted\ncritical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing\na substrate crystal orientation dependence on breakdown electric field for\n\\b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest\nparallel-plane junction electric fields reported in literature."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Shane M. W. Witsell"
                    },
                    {
                        "name": "John F. Conley"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17603v1",
                "updated": "2025-03-22T01:17:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:17:56Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    17,
                    56,
                    5,
                    81,
                    0
                ],
                "title": "A Generative Caching System for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Generative Caching System for Large Language Models"
                },
                "summary": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching has the potential to be of significant benefit for accessing large\nlanguage models (LLMs) due to their high latencies which typically range from a\nsmall number of seconds to well over a minute. Furthermore, many LLMs charge\nmoney for queries; caching thus has a clear monetary benefit. This paper\npresents a new caching system for improving user experiences with LLMs. In\naddition to reducing both latencies and monetary costs for accessing LLMs, our\nsystem also provides important features that go beyond the performance benefits\ntypically associated with caches. A key feature we provide is generative\ncaching, wherein multiple cached responses can be synthesized to provide\nanswers to queries which have never been seen before. Our generative caches\nfunction as repositories of valuable information which can be mined and\nanalyzed. We also improve upon past semantic caching techniques by tailoring\nthe caching algorithms to optimally balance cost and latency reduction with the\nquality of responses provided. Performance tests indicate that our caches are\nconsiderably faster than GPTcache."
                },
                "authors": [
                    {
                        "name": "Arun Iyengar"
                    },
                    {
                        "name": "Ashish Kundu"
                    },
                    {
                        "name": "Ramana Kompella"
                    },
                    {
                        "name": "Sai Nandan Mamidi"
                    }
                ],
                "author_detail": {
                    "name": "Sai Nandan Mamidi"
                },
                "author": "Sai Nandan Mamidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17602v1",
                "updated": "2025-03-22T01:16:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "published": "2025-03-22T01:16:24Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    1,
                    16,
                    24,
                    5,
                    81,
                    0
                ],
                "title": "Multiport Support for Vortex OpenGPU Memory Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiport Support for Vortex OpenGPU Memory Hierarchy"
                },
                "summary": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day applications have grown in size and require more computational\npower. The rise of machine learning and AI increased the need for parallel\ncomputation, which has increased the need for GPGPUs. With the increasing\ndemand for computational power, GPGPUs' SIMT architecture has solved this with\nan increase in the number of threads and the number of cores in a GPU,\nincreasing the throughput of these processors to match the demand of the\napplications. However, this created a larger demand for the memory, making the\nmemory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM)\nwith its increased number of memory ports offers a potential solution for the\nGPU to exploit its memory parallelism to increase the memory bandwidth.\nHowever, effectively leveraging HBM's memory parallelism to maximize bandwidth\npresents a unique and complex challenge for GPU architectures on how to\ndistribute those ports among the streaming multiprocessors in the GPGPU. In\nthis work, we extend the Vortex OpenGPU microarchitecture to incorporate a\nmultiport memory hierarchy, spanning from the L1 cache to the last-level cache\n(LLC). In addition, we propose various arbitration strategies to optimize\nmemory transfers across the cache hierarchy. The results have shown that an\nincrease in memory ports increases IPC, achieving an average speedup of 2.34x\nwith 8 memory ports in the tested configuration while showing relatively small\narea overhead."
                },
                "authors": [
                    {
                        "name": "Injae Shin"
                    },
                    {
                        "name": "Blaise Tine"
                    }
                ],
                "author_detail": {
                    "name": "Blaise Tine"
                },
                "author": "Blaise Tine",
                "arxiv_comment": "OSSMPIC2025, 1st workshop on Open Source Solutions for Massively\n  Parallel Integrated Circuits",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.03640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03640v1",
                "updated": "2025-04-04T17:59:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    59,
                    50,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:59:50Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    59,
                    50,
                    4,
                    94,
                    0
                ],
                "title": "Bonsai: Interpretable Tree-Adaptive Grounded Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bonsai: Interpretable Tree-Adaptive Grounded Reasoning"
                },
                "summary": "To develop general-purpose collaborative agents, humans need reliable AI\nsystems that can (1) adapt to new domains and (2) transparently reason with\nuncertainty to allow for verification and correction. Black-box models\ndemonstrate powerful data processing abilities but do not satisfy these\ncriteria due to their opaqueness, domain specificity, and lack of uncertainty\nawareness. We introduce Bonsai, a compositional and probabilistic reasoning\nsystem that generates adaptable inference trees by retrieving relevant\ngrounding evidence and using it to compute likelihoods of sub-claims derived\nfrom broader natural language inferences. Bonsai's reasoning power is tunable\nat test-time via evidence scaling and it demonstrates reliable handling of\nvaried domains including transcripts, photographs, videos, audio, and\ndatabases. Question-answering and human alignment experiments demonstrate that\nBonsai matches the performance of domain-specific black-box methods while\ngenerating interpretable, grounded, and uncertainty-aware reasoning traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To develop general-purpose collaborative agents, humans need reliable AI\nsystems that can (1) adapt to new domains and (2) transparently reason with\nuncertainty to allow for verification and correction. Black-box models\ndemonstrate powerful data processing abilities but do not satisfy these\ncriteria due to their opaqueness, domain specificity, and lack of uncertainty\nawareness. We introduce Bonsai, a compositional and probabilistic reasoning\nsystem that generates adaptable inference trees by retrieving relevant\ngrounding evidence and using it to compute likelihoods of sub-claims derived\nfrom broader natural language inferences. Bonsai's reasoning power is tunable\nat test-time via evidence scaling and it demonstrates reliable handling of\nvaried domains including transcripts, photographs, videos, audio, and\ndatabases. Question-answering and human alignment experiments demonstrate that\nBonsai matches the performance of domain-specific black-box methods while\ngenerating interpretable, grounded, and uncertainty-aware reasoning traces."
                },
                "authors": [
                    {
                        "name": "Kate Sanders"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "arxiv_comment": "9 pages, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T37",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12593v3",
                "updated": "2025-04-04T17:58:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    58,
                    8,
                    4,
                    94,
                    0
                ],
                "published": "2024-11-19T18:04:13Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    4,
                    13,
                    1,
                    324,
                    0
                ],
                "title": "AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive\n  Cross-Modality Memory Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive\n  Cross-Modality Memory Reduction"
                },
                "summary": "The advancements in large language models (LLMs) have propelled the\nimprovement of video understanding tasks by incorporating LLMs with visual\nmodels. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat)\nare constrained to processing short-duration videos. Recent attempts to\nunderstand long-term videos by extracting and compressing visual features into\na fixed memory size. Nevertheless, those methods leverage only visual modality\nto merge video tokens and overlook the correlation between visual and textual\nqueries, leading to difficulties in effectively handling complex\nquestion-answering tasks. To address the challenges of long videos and complex\nprompts, we propose AdaCM$^2$, which, for the first time, introduces an\nadaptive cross-modality memory reduction approach to video-text alignment in an\nauto-regressive manner on video streams. Our extensive experiments on various\nvideo understanding tasks, such as video captioning, video question answering,\nand video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art\nperformance across multiple datasets while significantly reducing memory usage.\nNotably, it achieves a 4.5% improvement across multiple tasks in the LVU\ndataset with a GPU memory consumption reduction of up to 65%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancements in large language models (LLMs) have propelled the\nimprovement of video understanding tasks by incorporating LLMs with visual\nmodels. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat)\nare constrained to processing short-duration videos. Recent attempts to\nunderstand long-term videos by extracting and compressing visual features into\na fixed memory size. Nevertheless, those methods leverage only visual modality\nto merge video tokens and overlook the correlation between visual and textual\nqueries, leading to difficulties in effectively handling complex\nquestion-answering tasks. To address the challenges of long videos and complex\nprompts, we propose AdaCM$^2$, which, for the first time, introduces an\nadaptive cross-modality memory reduction approach to video-text alignment in an\nauto-regressive manner on video streams. Our extensive experiments on various\nvideo understanding tasks, such as video captioning, video question answering,\nand video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art\nperformance across multiple datasets while significantly reducing memory usage.\nNotably, it achieves a 4.5% improvement across multiple tasks in the LVU\ndataset with a GPU memory consumption reduction of up to 65%."
                },
                "authors": [
                    {
                        "name": "Yuanbin Man"
                    },
                    {
                        "name": "Ying Huang"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Bingzhe Li"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Miao Yin"
                    }
                ],
                "author_detail": {
                    "name": "Miao Yin"
                },
                "author": "Miao Yin",
                "arxiv_comment": "CVPR 2025 Highlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03635v1",
                "updated": "2025-04-04T17:57:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    57,
                    22,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:57:22Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    57,
                    22,
                    4,
                    94,
                    0
                ],
                "title": "Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling\n  Law for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling\n  Law for Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Shawn Tan"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Rameswar Panda"
                    },
                    {
                        "name": "Yikang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yikang Shen"
                },
                "author": "Yikang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07700v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07700v2",
                "updated": "2025-04-04T17:52:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    52,
                    32,
                    4,
                    94,
                    0
                ],
                "published": "2024-08-14T17:57:18Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    17,
                    57,
                    18,
                    2,
                    227,
                    0
                ],
                "title": "Profile Likelihoods in Cosmology: When, Why and How illustrated with\n  $$CDM, Massive Neutrinos and Dark Energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Profile Likelihoods in Cosmology: When, Why and How illustrated with\n  $$CDM, Massive Neutrinos and Dark Energy"
                },
                "summary": "Frequentist parameter inference using profile likelihoods has received\nincreased attention in the cosmology literature recently since it can give\nimportant complementary information to Bayesian credible intervals. Here, we\ngive a pedagogical review of frequentist parameter inference in cosmology and\nfocus on when the graphical profile likelihood construction gives meaningful\nconstraints, i.e. confidence intervals with correct coverage. This construction\nrests on the assumption of the asymptotic limit of a large data set such as in\nWilks' theorem. We assess the validity of this assumption in the context of\nthree cosmological models with Planck 2018 Plik_lite data: While our tests for\nthe $\\Lambda$CDM model indicate that the profile likelihood method gives\ncorrect coverage, $\\Lambda$CDM with the sum of neutrino masses as a free\nparameter appears consistent with a Gaussian near a boundary motivating the use\nof the boundary-corrected or Feldman-Cousins graphical method; for $w_0$CDM\nwith the equation of state of dark energy, $w_0$, as a free parameter, we find\nindication of a violation of the assumptions. Finally, we compare frequentist\nand Bayesian constraints of these models. Our results motivate care when using\nthe graphical profile likelihood method in cosmology. Along with this paper, we\npublish our profile-likelihood code \"pinc\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequentist parameter inference using profile likelihoods has received\nincreased attention in the cosmology literature recently since it can give\nimportant complementary information to Bayesian credible intervals. Here, we\ngive a pedagogical review of frequentist parameter inference in cosmology and\nfocus on when the graphical profile likelihood construction gives meaningful\nconstraints, i.e. confidence intervals with correct coverage. This construction\nrests on the assumption of the asymptotic limit of a large data set such as in\nWilks' theorem. We assess the validity of this assumption in the context of\nthree cosmological models with Planck 2018 Plik_lite data: While our tests for\nthe $\\Lambda$CDM model indicate that the profile likelihood method gives\ncorrect coverage, $\\Lambda$CDM with the sum of neutrino masses as a free\nparameter appears consistent with a Gaussian near a boundary motivating the use\nof the boundary-corrected or Feldman-Cousins graphical method; for $w_0$CDM\nwith the equation of state of dark energy, $w_0$, as a free parameter, we find\nindication of a violation of the assumptions. Finally, we compare frequentist\nand Bayesian constraints of these models. Our results motivate care when using\nthe graphical profile likelihood method in cosmology. Along with this paper, we\npublish our profile-likelihood code \"pinc\"."
                },
                "authors": [
                    {
                        "name": "Laura Herold"
                    },
                    {
                        "name": "Elisa G. M. Ferreira"
                    },
                    {
                        "name": "Lukas Heinrich"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Heinrich"
                },
                "author": "Lukas Heinrich",
                "arxiv_doi": "10.1103/PhysRevD.111.083504",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.083504",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.07700v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07700v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Small changes to match the version published in PRD",
                "arxiv_journal_ref": "Phys.Rev.D 111 (2025) 8, 083504",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03624v1",
                "updated": "2025-04-04T17:41:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    41,
                    58,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:41:58Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    41,
                    58,
                    4,
                    94,
                    0
                ],
                "title": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer\n  Models"
                },
                "summary": "As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. All Nemotron-H models will be released, with\nsupport in Hugging Face, NeMo, and Megatron-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference-time scaling becomes critical for enhanced reasoning\ncapabilities, it is increasingly becoming important to build models that are\nefficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid\nMamba-Transformer models designed to reduce inference cost for a given accuracy\nlevel. To achieve this goal, we replace the majority of self-attention layers\nin the common Transformer model architecture with Mamba layers that perform\nconstant computation and require constant memory per generated token. We show\nthat Nemotron-H models offer either better or on-par accuracy compared to other\nsimilarly-sized state-of-the-art open-sourced Transformer models (e.g.,\nQwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at\ninference. To further increase inference speed and reduce the memory required\nat inference time, we created Nemotron-H-47B-Base from the 56B model using a\nnew compression via pruning and distillation technique called MiniPuzzle.\nNemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20%\nfaster to infer. In addition, we introduce an FP8-based training recipe and\nshow that it can achieve on par results with BF16-based training. This recipe\nis used to train the 56B model. All Nemotron-H models will be released, with\nsupport in Hugging Face, NeMo, and Megatron-LM."
                },
                "authors": [
                    {
                        "name": "NVIDIA"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Aaron Blakeman"
                    },
                    {
                        "name": "Aarti Basant"
                    },
                    {
                        "name": "Abhinav Khattar"
                    },
                    {
                        "name": "Adithya Renduchintala"
                    },
                    {
                        "name": "Akhiad Bercovich"
                    },
                    {
                        "name": "Aleksander Ficek"
                    },
                    {
                        "name": "Alexis Bjorlin"
                    },
                    {
                        "name": "Ali Taghibakhshi"
                    },
                    {
                        "name": "Amala Sanjay Deshmukh"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Andrew Tao"
                    },
                    {
                        "name": "Anna Shors"
                    },
                    {
                        "name": "Ashwath Aithal"
                    },
                    {
                        "name": "Ashwin Poojary"
                    },
                    {
                        "name": "Ayush Dattagupta"
                    },
                    {
                        "name": "Balaram Buddharaju"
                    },
                    {
                        "name": "Bobby Chen"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Boxin Wang"
                    },
                    {
                        "name": "Brandon Norick"
                    },
                    {
                        "name": "Brian Butterfield"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Carlo del Mundo"
                    },
                    {
                        "name": "Chengyu Dong"
                    },
                    {
                        "name": "Christine Harvey"
                    },
                    {
                        "name": "Christopher Parisien"
                    },
                    {
                        "name": "Dan Su"
                    },
                    {
                        "name": "Daniel Korzekwa"
                    },
                    {
                        "name": "Danny Yin"
                    },
                    {
                        "name": "Daria Gitman"
                    },
                    {
                        "name": "David Mosallanezhad"
                    },
                    {
                        "name": "Deepak Narayanan"
                    },
                    {
                        "name": "Denys Fridman"
                    },
                    {
                        "name": "Dima Rekesh"
                    },
                    {
                        "name": "Ding Ma"
                    },
                    {
                        "name": "Dmytro Pykhtar"
                    },
                    {
                        "name": "Dong Ahn"
                    },
                    {
                        "name": "Duncan Riach"
                    },
                    {
                        "name": "Dusan Stosic"
                    },
                    {
                        "name": "Eileen Long"
                    },
                    {
                        "name": "Elad Segal"
                    },
                    {
                        "name": "Ellie Evans"
                    },
                    {
                        "name": "Eric Chung"
                    },
                    {
                        "name": "Erick Galinkin"
                    },
                    {
                        "name": "Evelina Bakhturina"
                    },
                    {
                        "name": "Ewa Dobrowolska"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Fuxiao Liu"
                    },
                    {
                        "name": "Gargi Prasad"
                    },
                    {
                        "name": "Gerald Shen"
                    },
                    {
                        "name": "Guilin Liu"
                    },
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "Haifeng Qian"
                    },
                    {
                        "name": "Helen Ngo"
                    },
                    {
                        "name": "Hongbin Liu"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Igor Gitman"
                    },
                    {
                        "name": "Ilia Karmanov"
                    },
                    {
                        "name": "Ivan Moshkov"
                    },
                    {
                        "name": "Izik Golan"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Jane Polak Scowcroft"
                    },
                    {
                        "name": "Jared Casper"
                    },
                    {
                        "name": "Jarno Seppanen"
                    },
                    {
                        "name": "Jason Lu"
                    },
                    {
                        "name": "Jason Sewall"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Jimmy Zhang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Jining Huang"
                    },
                    {
                        "name": "Jinze Xue"
                    },
                    {
                        "name": "Jocelyn Huang"
                    },
                    {
                        "name": "Joey Conway"
                    },
                    {
                        "name": "John Kamalu"
                    },
                    {
                        "name": "Jon Barker"
                    },
                    {
                        "name": "Jonathan Cohen"
                    },
                    {
                        "name": "Joseph Jennings"
                    },
                    {
                        "name": "Jupinder Parmar"
                    },
                    {
                        "name": "Karan Sapra"
                    },
                    {
                        "name": "Kari Briski"
                    },
                    {
                        "name": "Kateryna Chumachenko"
                    },
                    {
                        "name": "Katherine Luna"
                    },
                    {
                        "name": "Keshav Santhanam"
                    },
                    {
                        "name": "Kezhi Kong"
                    },
                    {
                        "name": "Kirthi Sivamani"
                    },
                    {
                        "name": "Krzysztof Pawelec"
                    },
                    {
                        "name": "Kumar Anik"
                    },
                    {
                        "name": "Kunlun Li"
                    },
                    {
                        "name": "Lawrence McAfee"
                    },
                    {
                        "name": "Leon Derczynski"
                    },
                    {
                        "name": "Lindsey Pavao"
                    },
                    {
                        "name": "Luis Vega"
                    },
                    {
                        "name": "Lukas Voegtle"
                    },
                    {
                        "name": "Maciej Bala"
                    },
                    {
                        "name": "Maer Rodrigues de Melo"
                    },
                    {
                        "name": "Makesh Narsimhan Sreedhar"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Markus Kliegl"
                    },
                    {
                        "name": "Marta Stepniewska-Dziubinska"
                    },
                    {
                        "name": "Matthieu Le"
                    },
                    {
                        "name": "Matvei Novikov"
                    },
                    {
                        "name": "Mehrzad Samadi"
                    },
                    {
                        "name": "Michael Andersch"
                    },
                    {
                        "name": "Michael Evans"
                    },
                    {
                        "name": "Miguel Martinez"
                    },
                    {
                        "name": "Mike Chrzanowski"
                    },
                    {
                        "name": "Mike Ranzinger"
                    },
                    {
                        "name": "Mikolaj Blaz"
                    },
                    {
                        "name": "Misha Smelyanskiy"
                    },
                    {
                        "name": "Mohamed Fawzy"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Nayeon Lee"
                    },
                    {
                        "name": "Nima Tajbakhsh"
                    },
                    {
                        "name": "Ning Xu"
                    },
                    {
                        "name": "Oleg Rybakov"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Osvald Nitski"
                    },
                    {
                        "name": "Parth Chadha"
                    },
                    {
                        "name": "Pasha Shamis"
                    },
                    {
                        "name": "Paulius Micikevicius"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Peter Dykas"
                    },
                    {
                        "name": "Philipp Fischer"
                    },
                    {
                        "name": "Pierre-Yves Aquilanti"
                    },
                    {
                        "name": "Piotr Bialecki"
                    },
                    {
                        "name": "Prasoon Varshney"
                    },
                    {
                        "name": "Pritam Gundecha"
                    },
                    {
                        "name": "Przemek Tredak"
                    },
                    {
                        "name": "Rabeeh Karimi"
                    },
                    {
                        "name": "Rahul Kandu"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Ruoxi Zhang"
                    },
                    {
                        "name": "Sabrina Kavanaugh"
                    },
                    {
                        "name": "Sahil Jain"
                    },
                    {
                        "name": "Samuel Kriman"
                    },
                    {
                        "name": "Sangkug Lym"
                    },
                    {
                        "name": "Sanjeev Satheesh"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Sean Narenthiran"
                    },
                    {
                        "name": "Selvaraj Anandaraj"
                    },
                    {
                        "name": "Seonmyeong Bak"
                    },
                    {
                        "name": "Sergey Kashirsky"
                    },
                    {
                        "name": "Seungju Han"
                    },
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Shaona Ghosh"
                    },
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Sharon Clay"
                    },
                    {
                        "name": "Shelby Thomas"
                    },
                    {
                        "name": "Shrimai Prabhumoye"
                    },
                    {
                        "name": "Shubham Pachori"
                    },
                    {
                        "name": "Shubham Toshniwal"
                    },
                    {
                        "name": "Shyamala Prayaga"
                    },
                    {
                        "name": "Siddhartha Jain"
                    },
                    {
                        "name": "Sirshak Das"
                    },
                    {
                        "name": "Slawek Kierat"
                    },
                    {
                        "name": "Somshubra Majumdar"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Soumye Singhal"
                    },
                    {
                        "name": "Sriharsha Niverty"
                    },
                    {
                        "name": "Stefania Alborghetti"
                    },
                    {
                        "name": "Suseella Panguluri"
                    },
                    {
                        "name": "Swetha Bhendigeri"
                    },
                    {
                        "name": "Syeda Nahida Akter"
                    },
                    {
                        "name": "Szymon Migacz"
                    },
                    {
                        "name": "Tal Shiri"
                    },
                    {
                        "name": "Terry Kong"
                    },
                    {
                        "name": "Timo Roman"
                    },
                    {
                        "name": "Tomer Ronen"
                    },
                    {
                        "name": "Trisha Saar"
                    },
                    {
                        "name": "Tugrul Konuk"
                    },
                    {
                        "name": "Tuomas Rintamaki"
                    },
                    {
                        "name": "Tyler Poon"
                    },
                    {
                        "name": "Ushnish De"
                    },
                    {
                        "name": "Vahid Noroozi"
                    },
                    {
                        "name": "Varun Singh"
                    },
                    {
                        "name": "Vijay Korthikanti"
                    },
                    {
                        "name": "Vitaly Kurin"
                    },
                    {
                        "name": "Wasi Uddin Ahmad"
                    },
                    {
                        "name": "Wei Du"
                    },
                    {
                        "name": "Wei Ping"
                    },
                    {
                        "name": "Wenliang Dai"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Xiaowei Ren"
                    },
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Yian Zhang"
                    },
                    {
                        "name": "Ying Lin"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Zhiqi Li"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Zhongbo Zhu"
                    },
                    {
                        "name": "Zhuolin Yang"
                    },
                    {
                        "name": "Zijia Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zijia Chen"
                },
                "author": "Zijia Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03622v1",
                "updated": "2025-04-04T17:40:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    40,
                    4,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:40:04Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    40,
                    4,
                    4,
                    94,
                    0
                ],
                "title": "Align to Structure: Aligning Large Language Models with Structural\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align to Structure: Aligning Large Language Models with Structural\n  Information"
                },
                "summary": "Generating long, coherent text remains a challenge for large language models\n(LLMs), as they lack hierarchical planning and structured organization in\ndiscourse generation. We introduce Structural Alignment, a novel method that\naligns LLMs with human-like discourse structures to enhance long-form text\ngeneration. By integrating linguistically grounded discourse frameworks into\nreinforcement learning, our approach guides models to produce coherent and\nwell-organized outputs. We employ a dense reward scheme within a Proximal\nPolicy Optimization framework, assigning fine-grained, token-level rewards\nbased on the discourse distinctiveness relative to human writing. Two\ncomplementary reward models are evaluated: the first improves readability by\nscoring surface-level textual features to provide explicit structuring, while\nthe second reinforces deeper coherence and rhetorical sophistication by\nanalyzing global discourse patterns through hierarchical discourse motifs,\noutperforming both standard and RLHF-enhanced models in tasks such as essay\ngeneration and long-document summarization. All training data and code will be\npublicly shared at https://github.com/minnesotanlp/struct_align.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, coherent text remains a challenge for large language models\n(LLMs), as they lack hierarchical planning and structured organization in\ndiscourse generation. We introduce Structural Alignment, a novel method that\naligns LLMs with human-like discourse structures to enhance long-form text\ngeneration. By integrating linguistically grounded discourse frameworks into\nreinforcement learning, our approach guides models to produce coherent and\nwell-organized outputs. We employ a dense reward scheme within a Proximal\nPolicy Optimization framework, assigning fine-grained, token-level rewards\nbased on the discourse distinctiveness relative to human writing. Two\ncomplementary reward models are evaluated: the first improves readability by\nscoring surface-level textual features to provide explicit structuring, while\nthe second reinforces deeper coherence and rhetorical sophistication by\nanalyzing global discourse patterns through hierarchical discourse motifs,\noutperforming both standard and RLHF-enhanced models in tasks such as essay\ngeneration and long-document summarization. All training data and code will be\npublicly shared at https://github.com/minnesotanlp/struct_align."
                },
                "authors": [
                    {
                        "name": "Zae Myung Kim"
                    },
                    {
                        "name": "Anand Ramachandran"
                    },
                    {
                        "name": "Farideh Tavazoee"
                    },
                    {
                        "name": "Joo-Kyung Kim"
                    },
                    {
                        "name": "Oleg Rokhlenko"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03616v1",
                "updated": "2025-04-04T17:35:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    35,
                    43,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:35:43Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    35,
                    43,
                    4,
                    94,
                    0
                ],
                "title": "Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task"
                },
                "summary": "Retrieval-augmented generation (RAG) has become a cornerstone of contemporary\nNLP, enhancing large language models (LLMs) by allowing them to access richer\nfactual contexts through in-context retrieval. While effective in monolingual\nsettings, especially in English, its use in multilingual tasks remains\nunexplored. This paper investigates the effectiveness of RAG across multiple\nlanguages by proposing novel approaches for multilingual open-domain\nquestion-answering. We evaluate the performance of various multilingual RAG\nstrategies, including question-translation (tRAG), which translates questions\ninto English before retrieval, and Multilingual RAG (MultiRAG), where retrieval\noccurs directly across multiple languages. Our findings reveal that tRAG, while\nuseful, suffers from limited coverage. In contrast, MultiRAG improves\nefficiency by enabling multilingual retrieval but introduces inconsistencies\ndue to cross-lingual variations in the retrieved content. To address these\nissues, we propose Crosslingual RAG (CrossRAG), a method that translates\nretrieved documents into a common language (e.g., English) before generating\nthe response. Our experiments show that CrossRAG significantly enhances\nperformance on knowledge-intensive tasks, benefiting both high-resource and\nlow-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has become a cornerstone of contemporary\nNLP, enhancing large language models (LLMs) by allowing them to access richer\nfactual contexts through in-context retrieval. While effective in monolingual\nsettings, especially in English, its use in multilingual tasks remains\nunexplored. This paper investigates the effectiveness of RAG across multiple\nlanguages by proposing novel approaches for multilingual open-domain\nquestion-answering. We evaluate the performance of various multilingual RAG\nstrategies, including question-translation (tRAG), which translates questions\ninto English before retrieval, and Multilingual RAG (MultiRAG), where retrieval\noccurs directly across multiple languages. Our findings reveal that tRAG, while\nuseful, suffers from limited coverage. In contrast, MultiRAG improves\nefficiency by enabling multilingual retrieval but introduces inconsistencies\ndue to cross-lingual variations in the retrieved content. To address these\nissues, we propose Crosslingual RAG (CrossRAG), a method that translates\nretrieved documents into a common language (e.g., English) before generating\nthe response. Our experiments show that CrossRAG significantly enhances\nperformance on knowledge-intensive tasks, benefiting both high-resource and\nlow-resource languages."
                },
                "authors": [
                    {
                        "name": "Leonardo Ranaldi"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07958v2",
                "updated": "2025-04-04T17:33:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    33,
                    53,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-10T22:51:31Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    51,
                    31,
                    1,
                    345,
                    0
                ],
                "title": "PAFFA: Premeditated Actions For Fast Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAFFA: Premeditated Actions For Fast Agents"
                },
                "summary": "Modern AI assistants have made significant progress in natural language\nunderstanding and tool-use, with emerging efforts to interact with Web\ninterfaces. However, current approaches that heavily rely on repeated\nLLM-driven HTML parsing are computationally expensive and error-prone,\nparticularly when handling dynamic web interfaces and multi-step tasks. We\nintroduce PAFFA (Premeditated Actions For Fast Agents), a method that makes\nLLMs faster and more accurate in completing tasks on the internet using a novel\ninference-time technique that requires no task-specific training. PAFFA\nconstructs an 'Action Library', leveraging the parametric knowledge of the base\nLLM to pre-compute browser interaction patterns that generalize across tasks.\nBy strategically re-using LLM inference across tasks - either via 'Dist-Map'\nfor task-agnostic identification of key interactive web elements, or 'Unravel'\nfor first-encounter, stateful exploration of novel tasks/sites) - PAFFA\ndrastically reduces inference time tokens by 87% while maintaining robust\nperformance (achieving 0.57 vs. 0.50 step accuracy compared to baseline).\nFurther, Unravel's ability to update its action library based on explorations\nallows generalization and adaptation to unseen websites. In sum, this work\nexhibits that LLM reasoning sequences can generalize across prompts, offering a\nway to scale inference-time techniques for internet-scale data with sublinear\ntoken count.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI assistants have made significant progress in natural language\nunderstanding and tool-use, with emerging efforts to interact with Web\ninterfaces. However, current approaches that heavily rely on repeated\nLLM-driven HTML parsing are computationally expensive and error-prone,\nparticularly when handling dynamic web interfaces and multi-step tasks. We\nintroduce PAFFA (Premeditated Actions For Fast Agents), a method that makes\nLLMs faster and more accurate in completing tasks on the internet using a novel\ninference-time technique that requires no task-specific training. PAFFA\nconstructs an 'Action Library', leveraging the parametric knowledge of the base\nLLM to pre-compute browser interaction patterns that generalize across tasks.\nBy strategically re-using LLM inference across tasks - either via 'Dist-Map'\nfor task-agnostic identification of key interactive web elements, or 'Unravel'\nfor first-encounter, stateful exploration of novel tasks/sites) - PAFFA\ndrastically reduces inference time tokens by 87% while maintaining robust\nperformance (achieving 0.57 vs. 0.50 step accuracy compared to baseline).\nFurther, Unravel's ability to update its action library based on explorations\nallows generalization and adaptation to unseen websites. In sum, this work\nexhibits that LLM reasoning sequences can generalize across prompts, offering a\nway to scale inference-time techniques for internet-scale data with sublinear\ntoken count."
                },
                "authors": [
                    {
                        "name": "Shambhavi Krishna"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Yuan Ling"
                    },
                    {
                        "name": "Xiaojiang Huang"
                    },
                    {
                        "name": "Yingjie Li"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03612v1",
                "updated": "2025-04-04T17:33:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    33,
                    7,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:33:07Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    33,
                    7,
                    4,
                    94,
                    0
                ],
                "title": "AIR: A Systematic Analysis of Annotations, Instructions, and Response\n  Pairs in Preference Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIR: A Systematic Analysis of Annotations, Instructions, and Response\n  Pairs in Preference Dataset"
                },
                "summary": "Preference learning is critical for aligning large language models (LLMs)\nwith human values, yet its success hinges on high-quality datasets comprising\nthree core components: Preference \\textbf{A}nnotations, \\textbf{I}nstructions,\nand \\textbf{R}esponse Pairs. Current approaches conflate these components,\nobscuring their individual impacts and hindering systematic optimization. In\nthis work, we propose \\textbf{AIR}, a component-wise analysis framework that\nsystematically isolates and optimizes each component while evaluating their\nsynergistic effects. Through rigorous experimentation, AIR reveals actionable\nprinciples: annotation simplicity (point-wise generative scoring), instruction\ninference stability (variance-based filtering across LLMs), and response pair\nquality (moderate margins + high absolute scores). When combined, these\nprinciples yield +5.3 average gains over baseline method, even with only 14k\nhigh-quality pairs. Our work shifts preference dataset design from ad hoc\nscaling to component-aware optimization, offering a blueprint for efficient,\nreproducible alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference learning is critical for aligning large language models (LLMs)\nwith human values, yet its success hinges on high-quality datasets comprising\nthree core components: Preference \\textbf{A}nnotations, \\textbf{I}nstructions,\nand \\textbf{R}esponse Pairs. Current approaches conflate these components,\nobscuring their individual impacts and hindering systematic optimization. In\nthis work, we propose \\textbf{AIR}, a component-wise analysis framework that\nsystematically isolates and optimizes each component while evaluating their\nsynergistic effects. Through rigorous experimentation, AIR reveals actionable\nprinciples: annotation simplicity (point-wise generative scoring), instruction\ninference stability (variance-based filtering across LLMs), and response pair\nquality (moderate margins + high absolute scores). When combined, these\nprinciples yield +5.3 average gains over baseline method, even with only 14k\nhigh-quality pairs. Our work shifts preference dataset design from ad hoc\nscaling to component-aware optimization, offering a blueprint for efficient,\nreproducible alignment."
                },
                "authors": [
                    {
                        "name": "Bingxiang He"
                    },
                    {
                        "name": "Wenbin Zhang"
                    },
                    {
                        "name": "Jiaxi Song"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Zixuan Fu"
                    },
                    {
                        "name": "Bowen Sun"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Haiwen Hong"
                    },
                    {
                        "name": "Longtao Huang"
                    },
                    {
                        "name": "Hui Xue"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15556v2",
                "updated": "2025-04-04T17:13:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    13,
                    59,
                    4,
                    94,
                    0
                ],
                "published": "2025-03-18T20:23:51Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    23,
                    51,
                    1,
                    77,
                    0
                ],
                "title": "Fully Automated Generation of Combinatorial Optimisation Systems Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Automated Generation of Combinatorial Optimisation Systems Using\n  Large Language Models"
                },
                "summary": "Over the last few decades, researchers have made considerable efforts to make\ndecision support more accessible for small and medium enterprises by reducing\nthe cost of designing, developing and maintaining automated decision support\nsystems. However, due to the diversity of the underlying combinatorial\noptimisation problems, reusability of such systems has been limited; in most\ncases, expensive expertise has been required to implement bespoke software\ncomponents.\n  We explore the feasibility of fully automated generation of combinatorial\noptimisation systems using large language models (LLMs). An LLM will be\nresponsible for interpreting the user-provided problem description in natural\nlanguage and designing and implementing problem-specific software components.\nWe discuss the principles of fully automated LLM-based optimisation system\ngeneration, and evaluate several proof-of-concept generators, comparing their\nperformance on four optimisation problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the last few decades, researchers have made considerable efforts to make\ndecision support more accessible for small and medium enterprises by reducing\nthe cost of designing, developing and maintaining automated decision support\nsystems. However, due to the diversity of the underlying combinatorial\noptimisation problems, reusability of such systems has been limited; in most\ncases, expensive expertise has been required to implement bespoke software\ncomponents.\n  We explore the feasibility of fully automated generation of combinatorial\noptimisation systems using large language models (LLMs). An LLM will be\nresponsible for interpreting the user-provided problem description in natural\nlanguage and designing and implementing problem-specific software components.\nWe discuss the principles of fully automated LLM-based optimisation system\ngeneration, and evaluate several proof-of-concept generators, comparing their\nperformance on four optimisation problems."
                },
                "authors": [
                    {
                        "name": "Daniel Karapetyan"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Karapetyan"
                },
                "author": "Daniel Karapetyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03601v1",
                "updated": "2025-04-04T17:13:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    13,
                    57,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:13:57Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    13,
                    57,
                    4,
                    94,
                    0
                ],
                "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay"
                },
                "summary": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io"
                },
                "authors": [
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Tulika Awalgaonkar"
                    },
                    {
                        "name": "Haolin Chen"
                    },
                    {
                        "name": "Thai Hoang"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "12 pages plus references and appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03598v1",
                "updated": "2025-04-04T17:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    8,
                    46,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    8,
                    46,
                    4,
                    94,
                    0
                ],
                "title": "EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline"
                },
                "summary": "Existing information retrieval systems excel in cases where the language of\ntarget documents closely matches that of the user query. However, real-world\nretrieval systems are often required to implicitly reason whether a document is\nrelevant. For example, when retrieving technical texts or tables, their\nrelevance to the user query may be implied through a particular jargon or\nstructure, rather than explicitly expressed in their content. Large language\nmodels (LLMs) hold great potential in identifying such implied relevance by\nleveraging their reasoning skills. Nevertheless, current LLM-augmented\nretrieval is hindered by high latency and computation cost, as the LLM\ntypically computes the query-document relevance online, for every query anew.\nTo tackle this issue we introduce EnrichIndex, a retrieval approach which\ninstead uses the LLM offline to build semantically-enriched retrieval indices,\nby performing a single pass over all documents in the retrieval corpus once\nduring ingestion time. Furthermore, the semantically-enriched indices can\ncomplement existing online retrieval approaches, boosting the performance of\nLLM re-rankers. We evaluated EnrichIndex on five retrieval tasks, involving\npassages and tables, and found that it outperforms strong online LLM-based\nretrieval systems, with an average improvement of 11.7 points in recall @ 10\nand 10.6 points in NDCG @ 10 compared to strong baselines. In terms of online\ncalls to the LLM, it processes 293.3 times fewer tokens which greatly reduces\nthe online latency and cost. Overall, EnrichIndex is an effective way to build\nbetter retrieval indices offline by leveraging the strong reasoning skills of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing information retrieval systems excel in cases where the language of\ntarget documents closely matches that of the user query. However, real-world\nretrieval systems are often required to implicitly reason whether a document is\nrelevant. For example, when retrieving technical texts or tables, their\nrelevance to the user query may be implied through a particular jargon or\nstructure, rather than explicitly expressed in their content. Large language\nmodels (LLMs) hold great potential in identifying such implied relevance by\nleveraging their reasoning skills. Nevertheless, current LLM-augmented\nretrieval is hindered by high latency and computation cost, as the LLM\ntypically computes the query-document relevance online, for every query anew.\nTo tackle this issue we introduce EnrichIndex, a retrieval approach which\ninstead uses the LLM offline to build semantically-enriched retrieval indices,\nby performing a single pass over all documents in the retrieval corpus once\nduring ingestion time. Furthermore, the semantically-enriched indices can\ncomplement existing online retrieval approaches, boosting the performance of\nLLM re-rankers. We evaluated EnrichIndex on five retrieval tasks, involving\npassages and tables, and found that it outperforms strong online LLM-based\nretrieval systems, with an average improvement of 11.7 points in recall @ 10\nand 10.6 points in NDCG @ 10 compared to strong baselines. In terms of online\ncalls to the LLM, it processes 293.3 times fewer tokens which greatly reduces\nthe online latency and cost. Overall, EnrichIndex is an effective way to build\nbetter retrieval indices offline by leveraging the strong reasoning skills of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Peter Baile Chen"
                    },
                    {
                        "name": "Tomer Wolfson"
                    },
                    {
                        "name": "Michael Cafarella"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "arxiv_comment": "Dataset and code are available at\n  https://peterbaile.github.io/enrichindex/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v2",
                "updated": "2025-04-04T16:51:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    51,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03583v1",
                "updated": "2025-04-04T16:47:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    47,
                    30,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T16:47:30Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    47,
                    30,
                    4,
                    94,
                    0
                ],
                "title": "Scalable Hypergraph Structure Learning with Diverse Smoothness Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Hypergraph Structure Learning with Diverse Smoothness Priors"
                },
                "summary": "In graph signal processing, learning the weighted connections between nodes\nfrom a set of sample signals is a fundamental task when the underlying\nrelationships are not known a priori. This task is typically addressed by\nfinding a graph Laplacian on which the observed signals are smooth. With the\nextension of graphs to hypergraphs - where edges can connect more than two\nnodes - graph learning methods have similarly been generalized to hypergraphs.\nHowever, the absence of a unified framework for calculating total variation has\nled to divergent definitions of smoothness and, consequently, differing\napproaches to hyperedge recovery. We confront this challenge through\ngeneralization of several previously proposed hypergraph total variations,\nsubsequently allowing ease of substitution into a vector based optimization. To\nthis end, we propose a novel hypergraph learning method that recovers a\nhypergraph topology from time-series signals based on a smoothness prior. Our\napproach addresses key limitations in prior works, such as hyperedge selection\nand convergence issues, by formulating the problem as a convex optimization\nsolved via a forward-backward-forward algorithm, ensuring guaranteed\nconvergence. Additionally, we introduce a process that simultaneously limits\nthe span of the hyperedge search and maintains a valid hyperedge selection set.\nIn doing so, our method becomes scalable in increasingly complex network\nstructures. The experimental results demonstrate improved performance, in terms\nof accuracy, over other state-of-the-art hypergraph inference methods;\nfurthermore, we empirically show our method to be robust to total variation\nterms, biased towards global smoothness, and scalable to larger hypergraphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In graph signal processing, learning the weighted connections between nodes\nfrom a set of sample signals is a fundamental task when the underlying\nrelationships are not known a priori. This task is typically addressed by\nfinding a graph Laplacian on which the observed signals are smooth. With the\nextension of graphs to hypergraphs - where edges can connect more than two\nnodes - graph learning methods have similarly been generalized to hypergraphs.\nHowever, the absence of a unified framework for calculating total variation has\nled to divergent definitions of smoothness and, consequently, differing\napproaches to hyperedge recovery. We confront this challenge through\ngeneralization of several previously proposed hypergraph total variations,\nsubsequently allowing ease of substitution into a vector based optimization. To\nthis end, we propose a novel hypergraph learning method that recovers a\nhypergraph topology from time-series signals based on a smoothness prior. Our\napproach addresses key limitations in prior works, such as hyperedge selection\nand convergence issues, by formulating the problem as a convex optimization\nsolved via a forward-backward-forward algorithm, ensuring guaranteed\nconvergence. Additionally, we introduce a process that simultaneously limits\nthe span of the hyperedge search and maintains a valid hyperedge selection set.\nIn doing so, our method becomes scalable in increasingly complex network\nstructures. The experimental results demonstrate improved performance, in terms\nof accuracy, over other state-of-the-art hypergraph inference methods;\nfurthermore, we empirically show our method to be robust to total variation\nterms, biased towards global smoothness, and scalable to larger hypergraphs."
                },
                "authors": [
                    {
                        "name": "Benjamin T. Brown"
                    },
                    {
                        "name": "Haoxiang Zhang"
                    },
                    {
                        "name": "Daniel L. Lau"
                    },
                    {
                        "name": "Gonzalo R. Arce"
                    }
                ],
                "author_detail": {
                    "name": "Gonzalo R. Arce"
                },
                "author": "Gonzalo R. Arce",
                "arxiv_comment": "13 pages, 6 figures, submitted to IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03579v1",
                "updated": "2025-04-04T16:30:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    30,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T16:30:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    30,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Hallucination Detection on a Budget: Efficient Bayesian Estimation of\n  Semantic Entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination Detection on a Budget: Efficient Bayesian Estimation of\n  Semantic Entropy"
                },
                "summary": "Detecting whether an LLM hallucinates is an important research challenge. One\npromising way of doing so is to estimate the semantic entropy (Farquhar et al.,\n2024) of the distribution of generated sequences. We propose a new algorithm\nfor doing that, with two main advantages. First, due to us taking the Bayesian\napproach, we achieve a much better quality of semantic entropy estimates for a\ngiven budget of samples from the LLM. Second, we are able to tune the number of\nsamples adaptively so that `harder' contexts receive more samples. We\ndemonstrate empirically that our approach systematically beats the baselines,\nrequiring only 59% of samples used by Farquhar et al. (2024) to achieve the\nsame quality of hallucination detection as measured by AUROC. Moreover, quite\ncounterintuitively, our estimator is useful even with just one sample from the\nLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting whether an LLM hallucinates is an important research challenge. One\npromising way of doing so is to estimate the semantic entropy (Farquhar et al.,\n2024) of the distribution of generated sequences. We propose a new algorithm\nfor doing that, with two main advantages. First, due to us taking the Bayesian\napproach, we achieve a much better quality of semantic entropy estimates for a\ngiven budget of samples from the LLM. Second, we are able to tune the number of\nsamples adaptively so that `harder' contexts receive more samples. We\ndemonstrate empirically that our approach systematically beats the baselines,\nrequiring only 59% of samples used by Farquhar et al. (2024) to achieve the\nsame quality of hallucination detection as measured by AUROC. Moreover, quite\ncounterintuitively, our estimator is useful even with just one sample from the\nLLM."
                },
                "authors": [
                    {
                        "name": "Kamil Ciosek"
                    },
                    {
                        "name": "Nicol Felicioni"
                    },
                    {
                        "name": "Sina Ghiassian"
                    }
                ],
                "author_detail": {
                    "name": "Sina Ghiassian"
                },
                "author": "Sina Ghiassian",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03745v2",
                "updated": "2025-04-04T16:28:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    28,
                    33,
                    4,
                    94,
                    0
                ],
                "published": "2024-08-07T12:58:39Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    12,
                    58,
                    39,
                    2,
                    220,
                    0
                ],
                "title": "Intuitionistic Fuzzy Cognitive Maps for Interpretable Image\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intuitionistic Fuzzy Cognitive Maps for Interpretable Image\n  Classification"
                },
                "summary": "Several deep learning (DL) approaches have been proposed to deal with image\nclassification tasks. However, despite their effectiveness, they lack\ninterpretability, as they are unable to explain or justify their results. To\naddress the challenge of interpretable image classification, this paper\nintroduces a novel framework, named Interpretable Intuitionistic Fuzzy\nCognitive Maps (I2FCMs).Intuitionistic FCMs (iFCMs) have been proposed as an\nextension of FCMs offering a natural mechanism to assess the quality of their\noutput through the estimation of hesitancy, a concept resembling human\nhesitation in decision making. In the context of image classification,\nhesitancy is considered as a degree of unconfidence with which an image is\ncategorized to a class. To the best of our knowledge this is the first time\niFCMs are applied for image classification. Further novel contributions of the\nintroduced framework include the following: a) a feature extraction process\nfocusing on the most informative image regions; b) a learning algorithm for\nautomatic data-driven determination of the intuitionistic fuzzy\ninterconnections of the iFCM, thereby reducing human intervention in the\ndefinition of the graph structure; c) an inherently interpretable\nclassification approach based on image contents, providing understandable\nexplanations of its predictions, using linguistic terms. Furthermore, the\nproposed I2FCM framework can be applied to DL models, including Convolutional\nNeural Network (CNN), rendering them interpretable. The effectiveness of I2FCM\nis evaluated on publicly available datasets, and the results confirm that it\ncan provide enhanced classification performance, while providing interpretable\ninferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several deep learning (DL) approaches have been proposed to deal with image\nclassification tasks. However, despite their effectiveness, they lack\ninterpretability, as they are unable to explain or justify their results. To\naddress the challenge of interpretable image classification, this paper\nintroduces a novel framework, named Interpretable Intuitionistic Fuzzy\nCognitive Maps (I2FCMs).Intuitionistic FCMs (iFCMs) have been proposed as an\nextension of FCMs offering a natural mechanism to assess the quality of their\noutput through the estimation of hesitancy, a concept resembling human\nhesitation in decision making. In the context of image classification,\nhesitancy is considered as a degree of unconfidence with which an image is\ncategorized to a class. To the best of our knowledge this is the first time\niFCMs are applied for image classification. Further novel contributions of the\nintroduced framework include the following: a) a feature extraction process\nfocusing on the most informative image regions; b) a learning algorithm for\nautomatic data-driven determination of the intuitionistic fuzzy\ninterconnections of the iFCM, thereby reducing human intervention in the\ndefinition of the graph structure; c) an inherently interpretable\nclassification approach based on image contents, providing understandable\nexplanations of its predictions, using linguistic terms. Furthermore, the\nproposed I2FCM framework can be applied to DL models, including Convolutional\nNeural Network (CNN), rendering them interpretable. The effectiveness of I2FCM\nis evaluated on publicly available datasets, and the results confirm that it\ncan provide enhanced classification performance, while providing interpretable\ninferences."
                },
                "authors": [
                    {
                        "name": "Georgia Sovatzidi"
                    },
                    {
                        "name": "Michael D. Vasilakakis"
                    },
                    {
                        "name": "Dimitris K. Iakovidis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris K. Iakovidis"
                },
                "author": "Dimitris K. Iakovidis",
                "arxiv_comment": "This work has been submitted for possible journal publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03561v1",
                "updated": "2025-04-04T16:10:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    10,
                    57,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T16:10:57Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    10,
                    57,
                    4,
                    94,
                    0
                ],
                "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement"
                },
                "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld."
                },
                "authors": [
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Xiaobin Wang"
                    },
                    {
                        "name": "Yuan Liang"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zekun Xi"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.16091v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.16091v4",
                "updated": "2025-04-04T16:03:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    3,
                    42,
                    4,
                    94,
                    0
                ],
                "published": "2023-06-28T10:47:28Z",
                "published_parsed": [
                    2023,
                    6,
                    28,
                    10,
                    47,
                    28,
                    2,
                    179,
                    0
                ],
                "title": "Adaptive functional principal components analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive functional principal components analysis"
                },
                "summary": "Functional data analysis almost always involves smoothing discrete\nobservations into curves, because they are never observed in continuous time\nand rarely without error. Although smoothing parameters affect the subsequent\ninference, data-driven methods for selecting these parameters are not\nwell-developed, frustrated by the difficulty of using all the information\nshared by curves while being computationally efficient. On the one hand,\nsmoothing individual curves in an isolated, albeit sophisticated way, ignores\nuseful signals present in other curves. On the other hand, bandwidth selection\nby automatic procedures such as cross-validation after pooling all the curves\ntogether quickly become computationally unfeasible due to the large number of\ndata points. In this paper we propose a new data-driven, adaptive kernel\nsmoothing, specifically tailored for functional principal components analysis\nthrough the derivation of sharp, explicit risk bounds for the eigen-elements.\nThe minimization of these quadratic risk bounds provide refined, yet\ncomputationally efficient bandwidth rules for each eigen-element separately.\nBoth common and independent design cases are allowed. Rates of convergence for\nthe estimators are derived. An extensive simulation study, designed in a\nversatile manner to closely mimic the characteristics of real data sets\nsupports our methodological contribution. An illustration on a real data\napplication is provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional data analysis almost always involves smoothing discrete\nobservations into curves, because they are never observed in continuous time\nand rarely without error. Although smoothing parameters affect the subsequent\ninference, data-driven methods for selecting these parameters are not\nwell-developed, frustrated by the difficulty of using all the information\nshared by curves while being computationally efficient. On the one hand,\nsmoothing individual curves in an isolated, albeit sophisticated way, ignores\nuseful signals present in other curves. On the other hand, bandwidth selection\nby automatic procedures such as cross-validation after pooling all the curves\ntogether quickly become computationally unfeasible due to the large number of\ndata points. In this paper we propose a new data-driven, adaptive kernel\nsmoothing, specifically tailored for functional principal components analysis\nthrough the derivation of sharp, explicit risk bounds for the eigen-elements.\nThe minimization of these quadratic risk bounds provide refined, yet\ncomputationally efficient bandwidth rules for each eigen-element separately.\nBoth common and independent design cases are allowed. Rates of convergence for\nthe estimators are derived. An extensive simulation study, designed in a\nversatile manner to closely mimic the characteristics of real data sets\nsupports our methodological contribution. An illustration on a real data\napplication is provided."
                },
                "authors": [
                    {
                        "name": "Sunny G. W. Wang"
                    },
                    {
                        "name": "Valentin Patilea"
                    },
                    {
                        "name": "Nicolas Klutchnikoff"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Klutchnikoff"
                },
                "author": "Nicolas Klutchnikoff",
                "arxiv_comment": "Final version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.16091v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.16091v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62R10, 62G08, 62M99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03553v1",
                "updated": "2025-04-04T16:03:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    3,
                    38,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T16:03:38Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    3,
                    38,
                    4,
                    94,
                    0
                ],
                "title": "Agentic Knowledgeable Self-awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Knowledgeable Self-awareness"
                },
                "summary": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf."
                },
                "authors": [
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Zhisong Qiu"
                    },
                    {
                        "name": "Baochang Ren"
                    },
                    {
                        "name": "Xiaobin Wang"
                    },
                    {
                        "name": "Xiangyuan Ru"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03541v1",
                "updated": "2025-04-04T15:41:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    41,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T15:41:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    41,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Diverse In-Context Example Selection After Decomposing Programs and\n  Aligned Utterances Improves Semantic Parsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverse In-Context Example Selection After Decomposing Programs and\n  Aligned Utterances Improves Semantic Parsing"
                },
                "summary": "LLMs are increasingly used as seq2seq translators from natural language\nutterances to structured programs, a process called semantic interpretation.\nUnlike atomic labels or token sequences, programs are naturally represented as\nabstract syntax trees (ASTs). Such structured representation raises novel\nissues related to the design and selection of in-context examples (ICEs)\npresented to the LLM. We focus on decomposing the pool of available ICE trees\ninto fragments, some of which may be better suited to solving the test\ninstance. Next, we propose how to use (additional invocations of) an LLM with\nprompted syntax constraints to automatically map the fragments to corresponding\nutterances. Finally, we adapt and extend a recent method for diverse ICE\nselection to work with whole and fragmented ICE instances. We evaluate our\nsystem, SCUD4ICL, on popular diverse semantic parsing benchmarks, showing\nvisible accuracy gains from our proposed decomposed diverse demonstration\nmethod. Benefits are particularly notable for smaller LLMs, ICE pools having\nlarger labeled trees, and programs in lower resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly used as seq2seq translators from natural language\nutterances to structured programs, a process called semantic interpretation.\nUnlike atomic labels or token sequences, programs are naturally represented as\nabstract syntax trees (ASTs). Such structured representation raises novel\nissues related to the design and selection of in-context examples (ICEs)\npresented to the LLM. We focus on decomposing the pool of available ICE trees\ninto fragments, some of which may be better suited to solving the test\ninstance. Next, we propose how to use (additional invocations of) an LLM with\nprompted syntax constraints to automatically map the fragments to corresponding\nutterances. Finally, we adapt and extend a recent method for diverse ICE\nselection to work with whole and fragmented ICE instances. We evaluate our\nsystem, SCUD4ICL, on popular diverse semantic parsing benchmarks, showing\nvisible accuracy gains from our proposed decomposed diverse demonstration\nmethod. Benefits are particularly notable for smaller LLMs, ICE pools having\nlarger labeled trees, and programs in lower resource languages."
                },
                "authors": [
                    {
                        "name": "Mayank Kothyari"
                    },
                    {
                        "name": "Sunita Sarawagi"
                    },
                    {
                        "name": "Soumen Chakrabarti"
                    },
                    {
                        "name": "Gaurav Arora"
                    },
                    {
                        "name": "Srujana Merugu"
                    }
                ],
                "author_detail": {
                    "name": "Srujana Merugu"
                },
                "author": "Srujana Merugu",
                "arxiv_comment": "To appear at NAACL 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17703v2",
                "updated": "2025-04-04T15:38:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    38,
                    50,
                    4,
                    94,
                    0
                ],
                "published": "2025-03-22T09:03:31Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    9,
                    3,
                    31,
                    5,
                    81,
                    0
                ],
                "title": "RAIDER: Tool-Equipped Large Language Model Agent for Robotic Action\n  Issue Detection, Explanation and Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAIDER: Tool-Equipped Large Language Model Agent for Robotic Action\n  Issue Detection, Explanation and Recovery"
                },
                "summary": "As robots increasingly operate in dynamic human-centric environments,\nimproving their ability to detect, explain, and recover from action-related\nissues becomes crucial. Traditional model-based and data-driven techniques lack\nadaptability, while more flexible generative AI methods struggle with grounding\nextracted information to real-world constraints. We introduce RAIDER, a novel\nagent that integrates Large Language Models (LLMs) with grounded tools for\nadaptable and efficient issue detection and explanation. Using a unique\n\"Ground, Ask&Answer, Issue\" procedure, RAIDER dynamically generates\ncontext-aware precondition questions and selects appropriate tools for\nresolution, achieving targeted information gathering. Our results within a\nsimulated household environment surpass methods relying on predefined models,\nfull scene descriptions, or standalone trained models. Additionally, RAIDER's\nexplanations enhance recovery success, including cases requiring human\ninteraction. Its modular architecture, featuring self-correction mechanisms,\nenables straightforward adaptation to diverse scenarios, as demonstrated in a\nreal-world human-assistive task. This showcases RAIDER's potential as a\nversatile agentic AI solution for robotic issue detection and explanation,\nwhile addressing the problem of grounding generative AI for its effective\napplication in embodied agents. Project website:\nhttps://eurecat.github.io/raider-llmagent/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As robots increasingly operate in dynamic human-centric environments,\nimproving their ability to detect, explain, and recover from action-related\nissues becomes crucial. Traditional model-based and data-driven techniques lack\nadaptability, while more flexible generative AI methods struggle with grounding\nextracted information to real-world constraints. We introduce RAIDER, a novel\nagent that integrates Large Language Models (LLMs) with grounded tools for\nadaptable and efficient issue detection and explanation. Using a unique\n\"Ground, Ask&Answer, Issue\" procedure, RAIDER dynamically generates\ncontext-aware precondition questions and selects appropriate tools for\nresolution, achieving targeted information gathering. Our results within a\nsimulated household environment surpass methods relying on predefined models,\nfull scene descriptions, or standalone trained models. Additionally, RAIDER's\nexplanations enhance recovery success, including cases requiring human\ninteraction. Its modular architecture, featuring self-correction mechanisms,\nenables straightforward adaptation to diverse scenarios, as demonstrated in a\nreal-world human-assistive task. This showcases RAIDER's potential as a\nversatile agentic AI solution for robotic issue detection and explanation,\nwhile addressing the problem of grounding generative AI for its effective\napplication in embodied agents. Project website:\nhttps://eurecat.github.io/raider-llmagent/"
                },
                "authors": [
                    {
                        "name": "Silvia Izquierdo-Badiola"
                    },
                    {
                        "name": "Carlos Rizzo"
                    },
                    {
                        "name": "Guillem Aleny"
                    }
                ],
                "author_detail": {
                    "name": "Guillem Aleny"
                },
                "author": "Guillem Aleny",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03537v1",
                "updated": "2025-04-04T15:35:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    35,
                    57,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T15:35:57Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    35,
                    57,
                    4,
                    94,
                    0
                ],
                "title": "Coupling hydrodynamics with comoving frame radiative transfer. III. The\n  wind regime of early-type B hypergiants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coupling hydrodynamics with comoving frame radiative transfer. III. The\n  wind regime of early-type B hypergiants"
                },
                "summary": "Context. B hypergiants (BHGs) are important for understanding high-mass\nstellar evolution. While they are in a similar parameter space of B supergiants\n(BSGs), some BHGs are known to be luminous blue variables (LBVs). Their spectra\nwith absorption and emission features resemble Of/WNh stars. Yet, their wind\nphysics and their evolutionary connections are not clear. Aims. In this study,\nwe aim to understand (i) the atmospheric and wind structure, (ii) the\nwind-launching and -driving mechanisms, and (iii) the spectrum formation of\nearly-type BHGs. As an observational prototype, we use zet1 Sco (B1.5Ia+).\nMethods. Using the atmosphere code PoWRhd, we calculated the first\nhydrodynamically consistent models at the BHG domain. They give insights into\nthe radiative driving of the calculated wind regimes and enable us to study the\ninfluence of clumping and X-rays on the resulting wind structure. Results. Our\nconsistent model reproduces the main spectral features of zet1 Sco. The\nobtained mass-loss rate is higher than that of similar spectral type BSGs.\nHowever, the wind optical depth of BHGs is way below unity, making them less of\na transition type. To reproduce zet1 Sco's spectrum, we needed low clumping\nwith subsonic onset. The wind has a shallow-gradient velocity profile,\ndeviating from the beta law, and is mainly driven by Fe III opacity.\nConclusions. Our study suggests that despite more mass loss, early-type\nGalactic BHGs have winds relatively similar to BSGs. Their winds are not thick\nenough to characterize them as \"transition-type\" stars, unlike Of/WNh, implying\nthat emission features arise more easily in cooler than in hotter stars. The\nspectral BHG appearance is likely connected to atmospheric inhomogeneities\nbelow the sonic point. To reach an appearance similar to LBVs, BHGs need to be\neither closer to the Eddington limit or have higher wind clumping than inferred\nfor zeta1 Sco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. B hypergiants (BHGs) are important for understanding high-mass\nstellar evolution. While they are in a similar parameter space of B supergiants\n(BSGs), some BHGs are known to be luminous blue variables (LBVs). Their spectra\nwith absorption and emission features resemble Of/WNh stars. Yet, their wind\nphysics and their evolutionary connections are not clear. Aims. In this study,\nwe aim to understand (i) the atmospheric and wind structure, (ii) the\nwind-launching and -driving mechanisms, and (iii) the spectrum formation of\nearly-type BHGs. As an observational prototype, we use zet1 Sco (B1.5Ia+).\nMethods. Using the atmosphere code PoWRhd, we calculated the first\nhydrodynamically consistent models at the BHG domain. They give insights into\nthe radiative driving of the calculated wind regimes and enable us to study the\ninfluence of clumping and X-rays on the resulting wind structure. Results. Our\nconsistent model reproduces the main spectral features of zet1 Sco. The\nobtained mass-loss rate is higher than that of similar spectral type BSGs.\nHowever, the wind optical depth of BHGs is way below unity, making them less of\na transition type. To reproduce zet1 Sco's spectrum, we needed low clumping\nwith subsonic onset. The wind has a shallow-gradient velocity profile,\ndeviating from the beta law, and is mainly driven by Fe III opacity.\nConclusions. Our study suggests that despite more mass loss, early-type\nGalactic BHGs have winds relatively similar to BSGs. Their winds are not thick\nenough to characterize them as \"transition-type\" stars, unlike Of/WNh, implying\nthat emission features arise more easily in cooler than in hotter stars. The\nspectral BHG appearance is likely connected to atmospheric inhomogeneities\nbelow the sonic point. To reach an appearance similar to LBVs, BHGs need to be\neither closer to the Eddington limit or have higher wind clumping than inferred\nfor zeta1 Sco."
                },
                "authors": [
                    {
                        "name": "M. Bernini-Peron"
                    },
                    {
                        "name": "A. A. C. Sander"
                    },
                    {
                        "name": "F. Najarro"
                    },
                    {
                        "name": "G. N. Sabhahit"
                    },
                    {
                        "name": "D. Pauli"
                    },
                    {
                        "name": "R. R. Lefever"
                    },
                    {
                        "name": "J. S. Vink"
                    },
                    {
                        "name": "V. Ramachandran"
                    },
                    {
                        "name": "L. M. Oskinova"
                    },
                    {
                        "name": "G. Gonzlez-Tor"
                    },
                    {
                        "name": "E. C. Schsser"
                    }
                ],
                "author_detail": {
                    "name": "E. C. Schsser"
                },
                "author": "E. C. Schsser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15429v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15429v3",
                "updated": "2025-04-04T15:21:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    21,
                    3,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-21T12:54:56Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    54,
                    56,
                    4,
                    52,
                    0
                ],
                "title": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable\n  Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable\n  Explanations"
                },
                "summary": "A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Shuojie Fu"
                    },
                    {
                        "name": "Gabriel Freedman"
                    },
                    {
                        "name": "Cemre Zor"
                    },
                    {
                        "name": "Guy Martin"
                    },
                    {
                        "name": "James Kinross"
                    },
                    {
                        "name": "Uddhav Vaghela"
                    },
                    {
                        "name": "Ovidiu Serban"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "long paper under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15429v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15429v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03520v1",
                "updated": "2025-04-04T15:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    17,
                    53,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T15:17:53Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    17,
                    53,
                    4,
                    94,
                    0
                ],
                "title": "Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles"
                },
                "summary": "Bias in news reporting significantly impacts public perception, particularly\nregarding crime, politics, and societal issues. Traditional bias detection\nmethods, predominantly reliant on human moderation, suffer from subjective\ninterpretations and scalability constraints. Here, we introduce an AI-driven\nframework leveraging advanced large language models (LLMs), specifically\nGPT-4o, GPT-4o Mini, Gemini Pro, Gemini Flash, Llama 8B, and Llama 3B, to\nsystematically identify and mitigate biases in news articles. To this end, we\ncollect an extensive dataset consisting of over 30,000 crime-related articles\nfrom five politically diverse news sources spanning a decade (2013-2023). Our\napproach employs a two-stage methodology: (1) bias detection, where each LLM\nscores and justifies biased content at the paragraph level, validated through\nhuman evaluation for ground truth establishment, and (2) iterative debiasing\nusing GPT-4o Mini, verified by both automated reassessment and human reviewers.\nEmpirical results indicate GPT-4o Mini's superior accuracy in bias detection\nand effectiveness in debiasing. Furthermore, our analysis reveals temporal and\ngeographical variations in media bias correlating with socio-political dynamics\nand real-world events. This study contributes to scalable computational\nmethodologies for bias mitigation, promoting fairness and accountability in\nnews reporting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in news reporting significantly impacts public perception, particularly\nregarding crime, politics, and societal issues. Traditional bias detection\nmethods, predominantly reliant on human moderation, suffer from subjective\ninterpretations and scalability constraints. Here, we introduce an AI-driven\nframework leveraging advanced large language models (LLMs), specifically\nGPT-4o, GPT-4o Mini, Gemini Pro, Gemini Flash, Llama 8B, and Llama 3B, to\nsystematically identify and mitigate biases in news articles. To this end, we\ncollect an extensive dataset consisting of over 30,000 crime-related articles\nfrom five politically diverse news sources spanning a decade (2013-2023). Our\napproach employs a two-stage methodology: (1) bias detection, where each LLM\nscores and justifies biased content at the paragraph level, validated through\nhuman evaluation for ground truth establishment, and (2) iterative debiasing\nusing GPT-4o Mini, verified by both automated reassessment and human reviewers.\nEmpirical results indicate GPT-4o Mini's superior accuracy in bias detection\nand effectiveness in debiasing. Furthermore, our analysis reveals temporal and\ngeographical variations in media bias correlating with socio-political dynamics\nand real-world events. This study contributes to scalable computational\nmethodologies for bias mitigation, promoting fairness and accountability in\nnews reporting."
                },
                "authors": [
                    {
                        "name": "Chen Wei Kuo"
                    },
                    {
                        "name": "Kevin Chu"
                    },
                    {
                        "name": "Nouar AlDahoul"
                    },
                    {
                        "name": "Hazem Ibrahim"
                    },
                    {
                        "name": "Talal Rahwan"
                    },
                    {
                        "name": "Yasir Zaki"
                    }
                ],
                "author_detail": {
                    "name": "Yasir Zaki"
                },
                "author": "Yasir Zaki",
                "arxiv_comment": "23 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03518v1",
                "updated": "2025-04-04T15:17:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    17,
                    26,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T15:17:26Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    17,
                    26,
                    4,
                    94,
                    0
                ],
                "title": "Intracluster light is a biased tracer of the dark matter distribution in\n  clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intracluster light is a biased tracer of the dark matter distribution in\n  clusters"
                },
                "summary": "The diffuse stellar component of galaxy clusters known as intracluster light\n(ICL) has been proposed as an observable tracer of the cluster's dark matter\n(DM) halo. Assessing its reliability as a DM tracer requires understanding how\nthe intracluster stars are energetically linked to the underlying DM\ndistribution, which we investigate at $z\\approx0$ in 12 galaxy clusters with\n$M_{178} = 1.18 - 3.71 \\times 10^{14}\\,\\textrm{M}_\\odot$ from the Horizon-AGN\nsimulation. We quantify the orbital energies of these components by their mean\nspecific energies ${\\langle \\varepsilon \\rangle}$, and find that this quantity\nis $\\approx$ 25 per cent lower for the intracluster stars than the DM, whilst\nthe energetics of the satellite galaxies (a standard DM tracer) are only\nmarginally ($\\approx$ 5 per cent) higher than the DM. Importantly, the lower\n${\\langle \\varepsilon \\rangle}$ of the intracluster stars compared to the DM is\nrobust against the precise separation between the brightest cluster galaxy\n(BCG) and the ICL. The specific energy distribution of ICL stars is\nconcentrated towards lower energies and poorly samples the higher energies,\nwhere much of the DM resides. Consequently, the intracluster stars have\nvelocity distributions with lower typical speeds and a more\ncentrally-concentrated density profile than the DM. We also find that\nintracluster stars have more radially-biased orbits than the DM, indicating\nthese components have distinct orbital distributions. This study demonstrates\nthat although the morphology of the ICL may match the DM halo, the ICL is a\nbiased tracer of DM, and these biases must be understood in order to infer\nproperties of the DM from the ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The diffuse stellar component of galaxy clusters known as intracluster light\n(ICL) has been proposed as an observable tracer of the cluster's dark matter\n(DM) halo. Assessing its reliability as a DM tracer requires understanding how\nthe intracluster stars are energetically linked to the underlying DM\ndistribution, which we investigate at $z\\approx0$ in 12 galaxy clusters with\n$M_{178} = 1.18 - 3.71 \\times 10^{14}\\,\\textrm{M}_\\odot$ from the Horizon-AGN\nsimulation. We quantify the orbital energies of these components by their mean\nspecific energies ${\\langle \\varepsilon \\rangle}$, and find that this quantity\nis $\\approx$ 25 per cent lower for the intracluster stars than the DM, whilst\nthe energetics of the satellite galaxies (a standard DM tracer) are only\nmarginally ($\\approx$ 5 per cent) higher than the DM. Importantly, the lower\n${\\langle \\varepsilon \\rangle}$ of the intracluster stars compared to the DM is\nrobust against the precise separation between the brightest cluster galaxy\n(BCG) and the ICL. The specific energy distribution of ICL stars is\nconcentrated towards lower energies and poorly samples the higher energies,\nwhere much of the DM resides. Consequently, the intracluster stars have\nvelocity distributions with lower typical speeds and a more\ncentrally-concentrated density profile than the DM. We also find that\nintracluster stars have more radially-biased orbits than the DM, indicating\nthese components have distinct orbital distributions. This study demonstrates\nthat although the morphology of the ICL may match the DM halo, the ICL is a\nbiased tracer of DM, and these biases must be understood in order to infer\nproperties of the DM from the ICL."
                },
                "authors": [
                    {
                        "name": "J. Butler"
                    },
                    {
                        "name": "G. Martin"
                    },
                    {
                        "name": "N. A. Hatch"
                    },
                    {
                        "name": "F. Pearce"
                    },
                    {
                        "name": "S. Brough"
                    },
                    {
                        "name": "Y. Dubois"
                    }
                ],
                "author_detail": {
                    "name": "Y. Dubois"
                },
                "author": "Y. Dubois",
                "arxiv_comment": "14 pages, 8 figures, resubmitted to MNRAS following minor revisions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03517v1",
                "updated": "2025-04-04T15:17:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    17,
                    9,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T15:17:09Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    17,
                    9,
                    4,
                    94,
                    0
                ],
                "title": "A framework for computing upper bounds in passive learning settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A framework for computing upper bounds in passive learning settings"
                },
                "summary": "The task of inferring logical formulas from examples has garnered significant\nattention as a means to assist engineers in creating formal specifications used\nin the design, synthesis, and verification of computing systems. Among various\napproaches, enumeration algorithms have emerged as some of the most effective\ntechniques for this task. These algorithms employ advanced strategies to\nsystematically enumerate candidate formulas while minimizing redundancies by\navoiding the generation of syntactically different but semantically equivalent\nformulas. However, a notable drawback is that these algorithms typically do not\nprovide guarantees of termination, which poses challenges for their use in\nreal-world applications.\n  This paper develops an abstract framework to bound the size of possible\nsolutions for a logic inference task, thereby providing a termination guarantee\nfor enumeration algorithms through the introduction of a sufficient stopping\ncriterion. The proposed framework is designed with flexibility in mind and is\napplicable to a broad spectrum of practically relevant logical formalisms,\nincluding Modal Logic, Linear Temporal Logic, Computation Tree Logic,\nAlternating-time Temporal Logic, and even selected inference task for finite\nautomata. In addition, our approach enabled us to develop a new class of\nalgorithms that enumerate over the semantics of formulas rather than their\nsyntactic representations, offering new possibilities for reducing redundancy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The task of inferring logical formulas from examples has garnered significant\nattention as a means to assist engineers in creating formal specifications used\nin the design, synthesis, and verification of computing systems. Among various\napproaches, enumeration algorithms have emerged as some of the most effective\ntechniques for this task. These algorithms employ advanced strategies to\nsystematically enumerate candidate formulas while minimizing redundancies by\navoiding the generation of syntactically different but semantically equivalent\nformulas. However, a notable drawback is that these algorithms typically do not\nprovide guarantees of termination, which poses challenges for their use in\nreal-world applications.\n  This paper develops an abstract framework to bound the size of possible\nsolutions for a logic inference task, thereby providing a termination guarantee\nfor enumeration algorithms through the introduction of a sufficient stopping\ncriterion. The proposed framework is designed with flexibility in mind and is\napplicable to a broad spectrum of practically relevant logical formalisms,\nincluding Modal Logic, Linear Temporal Logic, Computation Tree Logic,\nAlternating-time Temporal Logic, and even selected inference task for finite\nautomata. In addition, our approach enabled us to develop a new class of\nalgorithms that enumerate over the semantics of formulas rather than their\nsyntactic representations, offering new possibilities for reducing redundancy."
                },
                "authors": [
                    {
                        "name": "Benjamin Bordais"
                    },
                    {
                        "name": "Daniel Neider"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Neider"
                },
                "author": "Daniel Neider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03508v1",
                "updated": "2025-04-04T15:11:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    11,
                    23,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T15:11:23Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    11,
                    23,
                    4,
                    94,
                    0
                ],
                "title": "Low rock mass fraction within trans-Neptunian objects inferred from the\n  spin-orbit evolution of Orcus-Vanth and Salacia-Actaea",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low rock mass fraction within trans-Neptunian objects inferred from the\n  spin-orbit evolution of Orcus-Vanth and Salacia-Actaea"
                },
                "summary": "Satellites play a crucial role in understanding the formation and evolution\nof trans-Neptunian objects (TNOs). The spin--orbit evolution of satellite\nsystems depends on their thermal histories, allowing us to constrain the rock\nmass fraction within TNOs based on their current spin--orbit states. In this\nstudy, we perform coupled thermal--orbital evolution calculations for two\nsatellite systems around undifferentiated TNOs: Orcus--Vanth and\nSalacia--Actaea. Our results demonstrate that the current spin--orbit states of\nthese systems are consistent with a rock mass fraction of approximately\n20--30%. Additionally, we estimate the organic mass fraction within the TNOs\nand find that it is comparable to the rock mass fraction. These findings\nsuggest that the chemical composition of TNOs closely resembles that of comets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satellites play a crucial role in understanding the formation and evolution\nof trans-Neptunian objects (TNOs). The spin--orbit evolution of satellite\nsystems depends on their thermal histories, allowing us to constrain the rock\nmass fraction within TNOs based on their current spin--orbit states. In this\nstudy, we perform coupled thermal--orbital evolution calculations for two\nsatellite systems around undifferentiated TNOs: Orcus--Vanth and\nSalacia--Actaea. Our results demonstrate that the current spin--orbit states of\nthese systems are consistent with a rock mass fraction of approximately\n20--30%. Additionally, we estimate the organic mass fraction within the TNOs\nand find that it is comparable to the rock mass fraction. These findings\nsuggest that the chemical composition of TNOs closely resembles that of comets."
                },
                "authors": [
                    {
                        "name": "Sota Arakawa"
                    },
                    {
                        "name": "Shunichi Kamata"
                    },
                    {
                        "name": "Hidenori Genda"
                    }
                ],
                "author_detail": {
                    "name": "Hidenori Genda"
                },
                "author": "Hidenori Genda",
                "arxiv_comment": "Accepted for publication in JGR: Planets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03486v1",
                "updated": "2025-04-04T14:41:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    41,
                    50,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:41:50Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    41,
                    50,
                    4,
                    94,
                    0
                ],
                "title": "Structured Legal Document Generation in India: A Model-Agnostic Wrapper\n  Approach with VidhikDastaavej",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Legal Document Generation in India: A Model-Agnostic Wrapper\n  Approach with VidhikDastaavej"
                },
                "summary": "Automating legal document drafting can significantly enhance efficiency,\nreduce manual effort, and streamline legal workflows. While prior research has\nexplored tasks such as judgment prediction and case summarization, the\nstructured generation of private legal documents in the Indian legal domain\nremains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej,\na novel, anonymized dataset of private legal documents, and develop NyayaShilp,\na fine-tuned legal document generation model specifically adapted to Indian\nlegal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework\nthat first generates structured section titles and then iteratively produces\ncontent while leveraging retrieval-based mechanisms to ensure coherence and\nfactual accuracy. We benchmark multiple open-source LLMs, including\ninstruction-tuned and domain-adapted versions, alongside proprietary models for\ncomparison. Our findings indicate that while direct fine-tuning on small\ndatasets does not always yield improvements, our structured wrapper\nsignificantly enhances coherence, factual adherence, and overall document\nquality while mitigating hallucinations. To ensure real-world applicability, we\ndeveloped a Human-in-the-Loop (HITL) Document Generation System, an interactive\nuser interface that enables users to specify document types, refine section\ndetails, and generate structured legal drafts. This tool allows legal\nprofessionals and researchers to generate, validate, and refine AI-generated\nlegal documents efficiently. Extensive evaluations, including expert\nassessments, confirm that our framework achieves high reliability in structured\nlegal drafting. This research establishes a scalable and adaptable foundation\nfor AI-assisted legal drafting in India, offering an effective approach to\nstructured legal document generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating legal document drafting can significantly enhance efficiency,\nreduce manual effort, and streamline legal workflows. While prior research has\nexplored tasks such as judgment prediction and case summarization, the\nstructured generation of private legal documents in the Indian legal domain\nremains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej,\na novel, anonymized dataset of private legal documents, and develop NyayaShilp,\na fine-tuned legal document generation model specifically adapted to Indian\nlegal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework\nthat first generates structured section titles and then iteratively produces\ncontent while leveraging retrieval-based mechanisms to ensure coherence and\nfactual accuracy. We benchmark multiple open-source LLMs, including\ninstruction-tuned and domain-adapted versions, alongside proprietary models for\ncomparison. Our findings indicate that while direct fine-tuning on small\ndatasets does not always yield improvements, our structured wrapper\nsignificantly enhances coherence, factual adherence, and overall document\nquality while mitigating hallucinations. To ensure real-world applicability, we\ndeveloped a Human-in-the-Loop (HITL) Document Generation System, an interactive\nuser interface that enables users to specify document types, refine section\ndetails, and generate structured legal drafts. This tool allows legal\nprofessionals and researchers to generate, validate, and refine AI-generated\nlegal documents efficiently. Extensive evaluations, including expert\nassessments, confirm that our framework achieves high reliability in structured\nlegal drafting. This research establishes a scalable and adaptable foundation\nfor AI-assisted legal drafting in India, offering an effective approach to\nstructured legal document generation."
                },
                "authors": [
                    {
                        "name": "Shubham Kumar Nigam"
                    },
                    {
                        "name": "Balaramamahanthi Deepak Patnaik"
                    },
                    {
                        "name": "Ajay Varghese Thomas"
                    },
                    {
                        "name": "Noel Shallum"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03485v1",
                "updated": "2025-04-04T14:41:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    41,
                    41,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:41:41Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    41,
                    41,
                    4,
                    94,
                    0
                ],
                "title": "Gaussian Process Tilted Nonparametric Density Estimation using Fisher\n  Divergence Score Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Process Tilted Nonparametric Density Estimation using Fisher\n  Divergence Score Matching"
                },
                "summary": "We present three Fisher divergence (FD) minimization algorithms for learning\nGaussian process (GP) based score models for lower dimensional density\nestimation problems. The density is formed by multiplying a base multivariate\nnormal distribution with an exponentiated GP refinement, and so we refer to it\nas a GP-tilted nonparametric density. By representing the GP part of the score\nas a linear function using the random Fourier feature (RFF) approximation, we\nshow that all learning problems can be solved in closed form. This includes the\nbasic and noise conditional versions of the Fisher divergence, as well as a\nnovel alternative to noise conditional FD models based on variational inference\n(VI). Here, we propose using an ELBO-like optimization of the approximate\nposterior with which we derive a Fisher variational predictive distribution.\nThe RFF representation of the GP, which is functionally equivalent to a single\nlayer neural network score model with cosine activation, provides a unique\nlinear form for which all expectations are in closed form. The Gaussian base\nalso helps with tractability of the VI approximation. We demonstrate our three\nlearning algorithms, as well as a MAP baseline algorithm, on several low\ndimensional density estimation problems. The closed-form nature of the learning\nproblem removes the reliance on iterative algorithms, making this technique\nparticularly well-suited to large data sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present three Fisher divergence (FD) minimization algorithms for learning\nGaussian process (GP) based score models for lower dimensional density\nestimation problems. The density is formed by multiplying a base multivariate\nnormal distribution with an exponentiated GP refinement, and so we refer to it\nas a GP-tilted nonparametric density. By representing the GP part of the score\nas a linear function using the random Fourier feature (RFF) approximation, we\nshow that all learning problems can be solved in closed form. This includes the\nbasic and noise conditional versions of the Fisher divergence, as well as a\nnovel alternative to noise conditional FD models based on variational inference\n(VI). Here, we propose using an ELBO-like optimization of the approximate\nposterior with which we derive a Fisher variational predictive distribution.\nThe RFF representation of the GP, which is functionally equivalent to a single\nlayer neural network score model with cosine activation, provides a unique\nlinear form for which all expectations are in closed form. The Gaussian base\nalso helps with tractability of the VI approximation. We demonstrate our three\nlearning algorithms, as well as a MAP baseline algorithm, on several low\ndimensional density estimation problems. The closed-form nature of the learning\nproblem removes the reliance on iterative algorithms, making this technique\nparticularly well-suited to large data sets."
                },
                "authors": [
                    {
                        "name": "John Paisley"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Brian Barr"
                    }
                ],
                "author_detail": {
                    "name": "Brian Barr"
                },
                "author": "Brian Barr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19665v2",
                "updated": "2025-04-04T14:40:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    40,
                    40,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-27T14:23:31Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    23,
                    31,
                    4,
                    362,
                    0
                ],
                "title": "Standardizing reverberation-mapped H$$ and H$$ active\n  galactic nuclei using radius--luminosity relations involving monochromatic\n  and broad H$$ luminosities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standardizing reverberation-mapped H$$ and H$$ active\n  galactic nuclei using radius--luminosity relations involving monochromatic\n  and broad H$$ luminosities"
                },
                "summary": "We test the standardizability of a homogeneous sample of 41 lower-redshift\n($0.00415\\leq z \\leq 0.474$) active galactic nuclei (AGNs) reverberation-mapped\n(RM) using the broad H$\\alpha$ and H$\\beta$ emission lines. We find that these\nsources can be standardized using four radius$-$luminosity ($R-L$) relations\nincorporating H$\\alpha$ and H$\\beta$ time delays and monochromatic and broad\nH$\\alpha$ luminosities. Although the $R-L$ relation parameters are well\nconstrained and independent of the six cosmological models considered, the\nresulting cosmological constraints are weak. The measured $R-L$ relations\nexhibit slightly steeper slopes than predicted by a simple photoionization\nmodel and steeper than those from previous higher-redshift H$\\beta$ analyses\nbased on larger datasets. These differences likely reflect the absence of\nhigh-accreting sources in our smaller, lower-redshift sample, which primarily\ncomprises lower-accreting AGNs. The inferred cosmological parameters are\nconsistent within 2$\\sigma$ (or better) with those from better-established\ncosmological probes. This contrasts with our earlier findings using a larger,\nheterogeneous sample of 118 H$\\beta$ AGNs, which yielded cosmological\nconstraints differing by $\\gtrsim 2\\sigma$ from better-established cosmological\nprobes. Our analysis demonstrates that sample homogeneity$-$specifically, the\nuse of a consistent time-lag determination method$-$is crucial for developing\nRM AGNs as a cosmological probe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We test the standardizability of a homogeneous sample of 41 lower-redshift\n($0.00415\\leq z \\leq 0.474$) active galactic nuclei (AGNs) reverberation-mapped\n(RM) using the broad H$\\alpha$ and H$\\beta$ emission lines. We find that these\nsources can be standardized using four radius$-$luminosity ($R-L$) relations\nincorporating H$\\alpha$ and H$\\beta$ time delays and monochromatic and broad\nH$\\alpha$ luminosities. Although the $R-L$ relation parameters are well\nconstrained and independent of the six cosmological models considered, the\nresulting cosmological constraints are weak. The measured $R-L$ relations\nexhibit slightly steeper slopes than predicted by a simple photoionization\nmodel and steeper than those from previous higher-redshift H$\\beta$ analyses\nbased on larger datasets. These differences likely reflect the absence of\nhigh-accreting sources in our smaller, lower-redshift sample, which primarily\ncomprises lower-accreting AGNs. The inferred cosmological parameters are\nconsistent within 2$\\sigma$ (or better) with those from better-established\ncosmological probes. This contrasts with our earlier findings using a larger,\nheterogeneous sample of 118 H$\\beta$ AGNs, which yielded cosmological\nconstraints differing by $\\gtrsim 2\\sigma$ from better-established cosmological\nprobes. Our analysis demonstrates that sample homogeneity$-$specifically, the\nuse of a consistent time-lag determination method$-$is crucial for developing\nRM AGNs as a cosmological probe."
                },
                "authors": [
                    {
                        "name": "Shulei Cao"
                    },
                    {
                        "name": "Amit Kumar Mandal"
                    },
                    {
                        "name": "Michal Zajaek"
                    },
                    {
                        "name": "Boena Czerny"
                    },
                    {
                        "name": "Bharat Ratra"
                    }
                ],
                "author_detail": {
                    "name": "Bharat Ratra"
                },
                "author": "Bharat Ratra",
                "arxiv_comment": "34 pages, 18 figures, 9 tables, accepted for publication in PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03480v1",
                "updated": "2025-04-04T14:38:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    38,
                    54,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:38:54Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    38,
                    54,
                    4,
                    94,
                    0
                ],
                "title": "Multivariate Causal Effects: a Bayesian Causal Regression Factor Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate Causal Effects: a Bayesian Causal Regression Factor Model"
                },
                "summary": "The impact of wildfire smoke on air quality is a growing concern,\ncontributing to air pollution through a complex mixture of chemical species\nwith important implications for public health. While previous studies have\nprimarily focused on its association with total particulate matter (PM2.5), the\ncausal relationship between wildfire smoke and the chemical composition of\nPM2.5 remains largely unexplored. Exposure to these chemical mixtures plays a\ncritical role in shaping public health, yet capturing their relationships\nrequires advanced statistical methods capable of modeling the complex\ndependencies among chemical species. To fill this gap, we propose a Bayesian\ncausal regression factor model that estimates the multivariate causal effects\nof wildfire smoke on the concentration of 27 chemical species in PM2.5 across\nthe United States. Our approach introduces two key innovations: (i) a causal\ninference framework for multivariate potential outcomes, and (ii) a novel\nBayesian factor model that employs a probit stick-breaking process as prior for\ntreatment-specific factor scores. By focusing on factor scores, our method\naddresses the missing data challenge common in causal inference and enables a\nflexible, data-driven characterization of the latent factor structure, which is\ncrucial to capture the complex correlation among multivariate outcomes. Through\nMonte Carlo simulations, we show the model's accuracy in estimating the causal\neffects in multivariate outcomes and characterizing the treatment-specific\nlatent structure. Finally, we apply our method to US air quality data,\nestimating the causal effect of wildfire smoke on 27 chemical species in PM2.5,\nproviding a deeper understanding of their interdependencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impact of wildfire smoke on air quality is a growing concern,\ncontributing to air pollution through a complex mixture of chemical species\nwith important implications for public health. While previous studies have\nprimarily focused on its association with total particulate matter (PM2.5), the\ncausal relationship between wildfire smoke and the chemical composition of\nPM2.5 remains largely unexplored. Exposure to these chemical mixtures plays a\ncritical role in shaping public health, yet capturing their relationships\nrequires advanced statistical methods capable of modeling the complex\ndependencies among chemical species. To fill this gap, we propose a Bayesian\ncausal regression factor model that estimates the multivariate causal effects\nof wildfire smoke on the concentration of 27 chemical species in PM2.5 across\nthe United States. Our approach introduces two key innovations: (i) a causal\ninference framework for multivariate potential outcomes, and (ii) a novel\nBayesian factor model that employs a probit stick-breaking process as prior for\ntreatment-specific factor scores. By focusing on factor scores, our method\naddresses the missing data challenge common in causal inference and enables a\nflexible, data-driven characterization of the latent factor structure, which is\ncrucial to capture the complex correlation among multivariate outcomes. Through\nMonte Carlo simulations, we show the model's accuracy in estimating the causal\neffects in multivariate outcomes and characterizing the treatment-specific\nlatent structure. Finally, we apply our method to US air quality data,\nestimating the causal effect of wildfire smoke on 27 chemical species in PM2.5,\nproviding a deeper understanding of their interdependencies."
                },
                "authors": [
                    {
                        "name": "Dafne Zorzetto"
                    },
                    {
                        "name": "Jenna Landy"
                    },
                    {
                        "name": "Corwin Zigler"
                    },
                    {
                        "name": "Giovanni Parmigiani"
                    },
                    {
                        "name": "Roberta De Vito"
                    }
                ],
                "author_detail": {
                    "name": "Roberta De Vito"
                },
                "author": "Roberta De Vito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23185v2",
                "updated": "2025-04-04T14:29:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    29,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2025-03-29T18:48:46Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    18,
                    48,
                    46,
                    5,
                    88,
                    0
                ],
                "title": "Real-time Video Prediction With Fast Video Interpolation Model and\n  Prediction Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Video Prediction With Fast Video Interpolation Model and\n  Prediction Training"
                },
                "summary": "Transmission latency significantly affects users' quality of experience in\nreal-time interaction and actuation. As latency is principally inevitable,\nvideo prediction can be utilized to mitigate the latency and ultimately enable\nzero-latency transmission. However, most of the existing video prediction\nmethods are computationally expensive and impractical for real-time\napplications. In this work, we therefore propose real-time video prediction\ntowards the zero-latency interaction over networks, called IFRVP (Intermediate\nFeature Refinement Video Prediction). Firstly, we propose three training\nmethods for video prediction that extend frame interpolation models, where we\nutilize a simple convolution-only frame interpolation network based on IFRNet.\nSecondly, we introduce ELAN-based residual blocks into the prediction models to\nimprove both inference speed and accuracy. Our evaluations show that our\nproposed models perform efficiently and achieve the best trade-off between\nprediction accuracy and computational speed among the existing video prediction\nmethods. A demonstration movie is also provided at http://bit.ly/IFRVPDemo. The\ncode will be released at https://github.com/FykAikawa/IFRVP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transmission latency significantly affects users' quality of experience in\nreal-time interaction and actuation. As latency is principally inevitable,\nvideo prediction can be utilized to mitigate the latency and ultimately enable\nzero-latency transmission. However, most of the existing video prediction\nmethods are computationally expensive and impractical for real-time\napplications. In this work, we therefore propose real-time video prediction\ntowards the zero-latency interaction over networks, called IFRVP (Intermediate\nFeature Refinement Video Prediction). Firstly, we propose three training\nmethods for video prediction that extend frame interpolation models, where we\nutilize a simple convolution-only frame interpolation network based on IFRNet.\nSecondly, we introduce ELAN-based residual blocks into the prediction models to\nimprove both inference speed and accuracy. Our evaluations show that our\nproposed models perform efficiently and achieve the best trade-off between\nprediction accuracy and computational speed among the existing video prediction\nmethods. A demonstration movie is also provided at http://bit.ly/IFRVPDemo. The\ncode will be released at https://github.com/FykAikawa/IFRVP."
                },
                "authors": [
                    {
                        "name": "Shota Hirose"
                    },
                    {
                        "name": "Kazuki Kotoyori"
                    },
                    {
                        "name": "Kasidis Arunruangsirilert"
                    },
                    {
                        "name": "Fangzheng Lin"
                    },
                    {
                        "name": "Heming Sun"
                    },
                    {
                        "name": "Jiro Katto"
                    }
                ],
                "author_detail": {
                    "name": "Jiro Katto"
                },
                "author": "Jiro Katto",
                "arxiv_comment": "ICIP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03471v1",
                "updated": "2025-04-04T14:23:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    23,
                    30,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:23:30Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    23,
                    30,
                    4,
                    94,
                    0
                ],
                "title": "Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis"
                },
                "summary": "Traditional diffusion models typically employ a U-Net architecture. Previous\nstudies have unveiled the roles of attention blocks in the U-Net. However, they\noverlook the dynamic evolution of their importance during the inference\nprocess, which hinders their further exploitation to improve image\napplications. In this study, we first theoretically proved that, re-weighting\nthe outputs of the Transformer blocks within the U-Net is a \"free lunch\" for\nimproving the signal-to-noise ratio during the sampling process. Next, we\nproposed Importance Probe to uncover and quantify the dynamic shifts in\nimportance of the Transformer blocks throughout the denoising process. Finally,\nwe design an adaptive importance-based re-weighting schedule tailored to\nspecific image generation and editing tasks. Experimental results demonstrate\nthat, our approach significantly improves the efficiency of the inference\nprocess, and enhances the aesthetic quality of the samples with identity\nconsistency. Our method can be seamlessly integrated into any U-Net-based\narchitecture. Code: https://github.com/Hytidel/UNetReweighting",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional diffusion models typically employ a U-Net architecture. Previous\nstudies have unveiled the roles of attention blocks in the U-Net. However, they\noverlook the dynamic evolution of their importance during the inference\nprocess, which hinders their further exploitation to improve image\napplications. In this study, we first theoretically proved that, re-weighting\nthe outputs of the Transformer blocks within the U-Net is a \"free lunch\" for\nimproving the signal-to-noise ratio during the sampling process. Next, we\nproposed Importance Probe to uncover and quantify the dynamic shifts in\nimportance of the Transformer blocks throughout the denoising process. Finally,\nwe design an adaptive importance-based re-weighting schedule tailored to\nspecific image generation and editing tasks. Experimental results demonstrate\nthat, our approach significantly improves the efficiency of the inference\nprocess, and enhances the aesthetic quality of the samples with identity\nconsistency. Our method can be seamlessly integrated into any U-Net-based\narchitecture. Code: https://github.com/Hytidel/UNetReweighting"
                },
                "authors": [
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Ziqi He"
                    },
                    {
                        "name": "Yang Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhou"
                },
                "author": "Yang Zhou",
                "arxiv_comment": "Accepted to ICME 2025. Appendix & Code:\n  https://github.com/Hytidel/UNetReweighting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03464v1",
                "updated": "2025-04-04T14:14:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    14,
                    6,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:14:06Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    14,
                    6,
                    4,
                    94,
                    0
                ],
                "title": "Spatiotemporal causal inference with arbitrary spillover and carryover\n  effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatiotemporal causal inference with arbitrary spillover and carryover\n  effects"
                },
                "summary": "Micro-level data with granular spatial and temporal information are becoming\nincreasingly available to social scientists. Most researchers aggregate such\ndata into a convenient panel data format and apply standard causal inference\nmethods. This approach, however, has two limitations. First, data aggregation\nresults in the loss of detailed geo-location and temporal information, leading\nto potential biases. Second, most panel data methods either ignore spatial\nspillover and temporal carryover effects or impose restrictive assumptions on\ntheir structure. We introduce a general methodological framework for\nspatiotemporal causal inference with arbitrary spillover and carryover effects.\nUnder this general framework, we demonstrate how to define and estimate causal\nquantities of interest, explore heterogeneous treatment effects, investigate\ncausal mechanisms, and visualize the results to facilitate their\ninterpretation. We illustrate the proposed methodology through an analysis of\nairstrikes and insurgent attacks in Iraq. The open-source software package\ngeocausal implements all of our methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Micro-level data with granular spatial and temporal information are becoming\nincreasingly available to social scientists. Most researchers aggregate such\ndata into a convenient panel data format and apply standard causal inference\nmethods. This approach, however, has two limitations. First, data aggregation\nresults in the loss of detailed geo-location and temporal information, leading\nto potential biases. Second, most panel data methods either ignore spatial\nspillover and temporal carryover effects or impose restrictive assumptions on\ntheir structure. We introduce a general methodological framework for\nspatiotemporal causal inference with arbitrary spillover and carryover effects.\nUnder this general framework, we demonstrate how to define and estimate causal\nquantities of interest, explore heterogeneous treatment effects, investigate\ncausal mechanisms, and visualize the results to facilitate their\ninterpretation. We illustrate the proposed methodology through an analysis of\nairstrikes and insurgent attacks in Iraq. The open-source software package\ngeocausal implements all of our methods."
                },
                "authors": [
                    {
                        "name": "Mitsuru Mukaigawara"
                    },
                    {
                        "name": "Kosuke Imai"
                    },
                    {
                        "name": "Jason Lyall"
                    },
                    {
                        "name": "Georgia Papadogeorgou"
                    }
                ],
                "author_detail": {
                    "name": "Georgia Papadogeorgou"
                },
                "author": "Georgia Papadogeorgou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15888v2",
                "updated": "2025-04-04T14:12:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    12,
                    54,
                    4,
                    94,
                    0
                ],
                "published": "2024-06-22T16:37:51Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    16,
                    37,
                    51,
                    5,
                    174,
                    0
                ],
                "title": "Real-time Speech Summarization for Medical Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Speech Summarization for Medical Conversations"
                },
                "summary": "In doctor-patient conversations, identifying medically relevant information\nis crucial, posing the need for conversation summarization. In this work, we\npropose the first deployable real-time speech summarization system for\nreal-world applications in industry, which generates a local summary after\nevery N speech utterances within a conversation and a global summary after the\nend of a conversation. Our system could enhance user experience from a business\nstandpoint, while also reducing computational costs from a technical\nperspective. Secondly, we present VietMed-Sum which, to our knowledge, is the\nfirst speech summarization dataset for medical conversations. Thirdly, we are\nthe first to utilize LLM and human annotators collaboratively to create gold\nstandard and synthetic summaries for medical conversation summarization.\nFinally, we present baseline results of state-of-the-art models on VietMed-Sum.\nAll code, data (English-translated and Vietnamese) and models are available\nonline: https://github.com/leduckhai/MultiMed/tree/master/VietMed-Sum",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In doctor-patient conversations, identifying medically relevant information\nis crucial, posing the need for conversation summarization. In this work, we\npropose the first deployable real-time speech summarization system for\nreal-world applications in industry, which generates a local summary after\nevery N speech utterances within a conversation and a global summary after the\nend of a conversation. Our system could enhance user experience from a business\nstandpoint, while also reducing computational costs from a technical\nperspective. Secondly, we present VietMed-Sum which, to our knowledge, is the\nfirst speech summarization dataset for medical conversations. Thirdly, we are\nthe first to utilize LLM and human annotators collaboratively to create gold\nstandard and synthetic summaries for medical conversation summarization.\nFinally, we present baseline results of state-of-the-art models on VietMed-Sum.\nAll code, data (English-translated and Vietnamese) and models are available\nonline: https://github.com/leduckhai/MultiMed/tree/master/VietMed-Sum"
                },
                "authors": [
                    {
                        "name": "Khai Le-Duc"
                    },
                    {
                        "name": "Khai-Nguyen Nguyen"
                    },
                    {
                        "name": "Long Vo-Dang"
                    },
                    {
                        "name": "Truong-Son Hy"
                    }
                ],
                "author_detail": {
                    "name": "Truong-Son Hy"
                },
                "author": "Truong-Son Hy",
                "arxiv_comment": "Interspeech 2024 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03463v1",
                "updated": "2025-04-04T14:12:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    12,
                    53,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:12:53Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    12,
                    53,
                    4,
                    94,
                    0
                ],
                "title": "Generating ensembles of spatially-coherent in-situ forecasts using flow\n  matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ensembles of spatially-coherent in-situ forecasts using flow\n  matching"
                },
                "summary": "We propose a machine-learning-based methodology for in-situ weather forecast\npostprocessing that is both spatially coherent and multivariate. Compared to\nprevious work, our Flow MAtching Postprocessing (FMAP) better represents the\ncorrelation structures of the observations distribution, while also improving\nmarginal performance at the stations. FMAP generates forecasts that are not\nbound to what is already modeled by the underlying gridded prediction and can\ninfer new correlation structures from data. The resulting model can generate an\narbitrary number of forecasts from a limited number of numerical simulations,\nallowing for low-cost forecasting systems. A single training is sufficient to\nperform postprocessing at multiple lead times, in contrast with other methods\nwhich use multiple trained networks at generation time. This work details our\nmethodology, including a spatial attention transformer backbone trained within\na flow matching generative modeling framework. FMAP shows promising performance\nin experiments on the EUPPBench dataset, forecasting surface temperature and\nwind gust values at station locations in western Europe up to five-day lead\ntimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a machine-learning-based methodology for in-situ weather forecast\npostprocessing that is both spatially coherent and multivariate. Compared to\nprevious work, our Flow MAtching Postprocessing (FMAP) better represents the\ncorrelation structures of the observations distribution, while also improving\nmarginal performance at the stations. FMAP generates forecasts that are not\nbound to what is already modeled by the underlying gridded prediction and can\ninfer new correlation structures from data. The resulting model can generate an\narbitrary number of forecasts from a limited number of numerical simulations,\nallowing for low-cost forecasting systems. A single training is sufficient to\nperform postprocessing at multiple lead times, in contrast with other methods\nwhich use multiple trained networks at generation time. This work details our\nmethodology, including a spatial attention transformer backbone trained within\na flow matching generative modeling framework. FMAP shows promising performance\nin experiments on the EUPPBench dataset, forecasting surface temperature and\nwind gust values at station locations in western Europe up to five-day lead\ntimes."
                },
                "authors": [
                    {
                        "name": "David Landry"
                    },
                    {
                        "name": "Claire Monteleoni"
                    },
                    {
                        "name": "Anastase Charantonis"
                    }
                ],
                "author_detail": {
                    "name": "Anastase Charantonis"
                },
                "author": "Anastase Charantonis",
                "arxiv_comment": "16 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03454v1",
                "updated": "2025-04-04T13:58:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    58,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T13:58:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    58,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "SpectR: Dynamically Composing LM Experts with Spectral Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpectR: Dynamically Composing LM Experts with Spectral Routing"
                },
                "summary": "Training large, general-purpose language models poses significant challenges.\nThe growing availability of specialized expert models, fine-tuned from\npretrained models for specific tasks or domains, offers a promising\nalternative. Leveraging the potential of these existing expert models in\nreal-world applications requires effective methods to select or merge the\nmodels best suited for a given task. This paper introduces SPECTR, an approach\nfor dynamically composing expert models at each time step during inference.\nNotably, our method requires no additional training and enables flexible,\ntoken- and layer-wise model combinations. Our experimental results demonstrate\nthat SPECTR improves routing accuracy over alternative training-free methods,\nincreasing task performance across expert domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large, general-purpose language models poses significant challenges.\nThe growing availability of specialized expert models, fine-tuned from\npretrained models for specific tasks or domains, offers a promising\nalternative. Leveraging the potential of these existing expert models in\nreal-world applications requires effective methods to select or merge the\nmodels best suited for a given task. This paper introduces SPECTR, an approach\nfor dynamically composing expert models at each time step during inference.\nNotably, our method requires no additional training and enables flexible,\ntoken- and layer-wise model combinations. Our experimental results demonstrate\nthat SPECTR improves routing accuracy over alternative training-free methods,\nincreasing task performance across expert domains."
                },
                "authors": [
                    {
                        "name": "William Fleshman"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03444v1",
                "updated": "2025-04-04T13:37:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    37,
                    29,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T13:37:29Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    37,
                    29,
                    4,
                    94,
                    0
                ],
                "title": "LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM\n  Applications"
                },
                "summary": "Developing compound Large Language Model (LLM) applications is becoming an\nincreasingly prevalent approach to solving real-world problems. In these\napplications, an LLM collaborates with various external modules, including APIs\nand even other LLMs, to realize complex intelligent services. However, we\nreveal that the intrinsic duration and structural uncertainty in compound LLM\napplications pose great challenges for LLM service providers in serving and\nscheduling them efficiently. In this paper, we propose LLMSched, an\nuncertainty-aware scheduling framework for emerging compound LLM applications.\nIn LLMSched, we first design a novel DAG-based model to describe the uncertain\ncompound LLM applications. Then, we adopt the Bayesian network to\ncomprehensively profile compound LLM applications and identify\nuncertainty-reducing stages, along with an entropy-based mechanism to quantify\ntheir uncertainty reduction. Combining an uncertainty reduction strategy and a\njob completion time (JCT)-efficient scheme, we further propose an efficient\nscheduler to reduce the average JCT. Evaluation of both simulation and testbed\nexperiments on various representative compound LLM applications shows that\ncompared to existing state-of-the-art scheduling schemes, LLMSched can reduce\nthe average JCT by 14~79%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing compound Large Language Model (LLM) applications is becoming an\nincreasingly prevalent approach to solving real-world problems. In these\napplications, an LLM collaborates with various external modules, including APIs\nand even other LLMs, to realize complex intelligent services. However, we\nreveal that the intrinsic duration and structural uncertainty in compound LLM\napplications pose great challenges for LLM service providers in serving and\nscheduling them efficiently. In this paper, we propose LLMSched, an\nuncertainty-aware scheduling framework for emerging compound LLM applications.\nIn LLMSched, we first design a novel DAG-based model to describe the uncertain\ncompound LLM applications. Then, we adopt the Bayesian network to\ncomprehensively profile compound LLM applications and identify\nuncertainty-reducing stages, along with an entropy-based mechanism to quantify\ntheir uncertainty reduction. Combining an uncertainty reduction strategy and a\njob completion time (JCT)-efficient scheme, we further propose an efficient\nscheduler to reduce the average JCT. Evaluation of both simulation and testbed\nexperiments on various representative compound LLM applications shows that\ncompared to existing state-of-the-art scheduling schemes, LLMSched can reduce\nthe average JCT by 14~79%."
                },
                "authors": [
                    {
                        "name": "Botao Zhu"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xiaoyi Fan"
                    },
                    {
                        "name": "Yifei Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Zhu"
                },
                "author": "Yifei Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17743v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17743v5",
                "updated": "2025-04-04T13:31:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    31,
                    38,
                    4,
                    94,
                    0
                ],
                "published": "2024-05-28T01:55:35Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    1,
                    55,
                    35,
                    1,
                    149,
                    0
                ],
                "title": "ORLM: A Customizable Framework in Training Large Models for Automated\n  Optimization Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORLM: A Customizable Framework in Training Large Models for Automated\n  Optimization Modeling"
                },
                "summary": "Optimization modeling plays a critical role in the application of Operations\nResearch (OR) tools to address real-world problems, yet they pose challenges\nand require extensive expertise from OR experts. With the advent of large\nlanguage models (LLMs), new opportunities have emerged to streamline and\nautomate such task. However, current research predominantly relies on\nclosed-source LLMs such as GPT-4, along with extensive prompt engineering\ntechniques. This reliance stems from the scarcity of high-quality training\ndatasets for optimization modeling, resulting in elevated costs, prolonged\nprocessing times, and privacy concerns. To address these challenges, our work\nis the first to propose a viable path for training open-source LLMs that are\ncapable of optimization modeling and developing solver codes, eventually\nleading to a superior ability for automating optimization modeling and solving.\nParticularly, we design the {\\sc OR-Instruct}, a semi-automated data synthesis\nframework for optimization modeling that enables customizable enhancements for\nspecific scenarios or model types. This work also introduces IndustryOR, the\nfirst industrial benchmark for evaluating LLMs in solving practical OR\nproblems. We train several 7B-scale open-source LLMs using synthesized data\n(dubbed ORLMs{https://github.com/Cardinal-Operations/ORLM}), which exhibit\nsignificantly enhanced optimization modeling capabilities, achieving\ncompetitive performance across the NL4OPT, MAMO, and IndustryOR benchmarks.\nAdditionally, our experiments highlight the potential of scaling law and\nreinforcement learning to further enhance the performance of ORLMs. The\nworkflows and human-machine interaction paradigms of ORLMs in practical\nindustrial applications are also discussed in the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization modeling plays a critical role in the application of Operations\nResearch (OR) tools to address real-world problems, yet they pose challenges\nand require extensive expertise from OR experts. With the advent of large\nlanguage models (LLMs), new opportunities have emerged to streamline and\nautomate such task. However, current research predominantly relies on\nclosed-source LLMs such as GPT-4, along with extensive prompt engineering\ntechniques. This reliance stems from the scarcity of high-quality training\ndatasets for optimization modeling, resulting in elevated costs, prolonged\nprocessing times, and privacy concerns. To address these challenges, our work\nis the first to propose a viable path for training open-source LLMs that are\ncapable of optimization modeling and developing solver codes, eventually\nleading to a superior ability for automating optimization modeling and solving.\nParticularly, we design the {\\sc OR-Instruct}, a semi-automated data synthesis\nframework for optimization modeling that enables customizable enhancements for\nspecific scenarios or model types. This work also introduces IndustryOR, the\nfirst industrial benchmark for evaluating LLMs in solving practical OR\nproblems. We train several 7B-scale open-source LLMs using synthesized data\n(dubbed ORLMs{https://github.com/Cardinal-Operations/ORLM}), which exhibit\nsignificantly enhanced optimization modeling capabilities, achieving\ncompetitive performance across the NL4OPT, MAMO, and IndustryOR benchmarks.\nAdditionally, our experiments highlight the potential of scaling law and\nreinforcement learning to further enhance the performance of ORLMs. The\nworkflows and human-machine interaction paradigms of ORLMs in practical\nindustrial applications are also discussed in the paper."
                },
                "authors": [
                    {
                        "name": "Chenyu Huang"
                    },
                    {
                        "name": "Zhengyang Tang"
                    },
                    {
                        "name": "Shixi Hu"
                    },
                    {
                        "name": "Ruoqing Jiang"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Dongdong Ge"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Zizhuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zizhuo Wang"
                },
                "author": "Zizhuo Wang",
                "arxiv_comment": "accepted by Operations Research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17743v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17743v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03440v1",
                "updated": "2025-04-04T13:31:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    31,
                    8,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T13:31:08Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    31,
                    8,
                    4,
                    94,
                    0
                ],
                "title": "Know What You do Not Know: Verbalized Uncertainty Estimation Robustness\n  on Corrupted Images in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know What You do Not Know: Verbalized Uncertainty Estimation Robustness\n  on Corrupted Images in Vision-Language Models"
                },
                "summary": "To leverage the full potential of Large Language Models (LLMs) it is crucial\nto have some information on their answers' uncertainty. This means that the\nmodel has to be able to quantify how certain it is in the correctness of a\ngiven response. Bad uncertainty estimates can lead to overconfident wrong\nanswers undermining trust in these models. Quite a lot of research has been\ndone on language models that work with text inputs and provide text outputs.\nStill, since the visual capabilities have been added to these models recently,\nthere has not been much progress on the uncertainty of Visual Language Models\n(VLMs). We tested three state-of-the-art VLMs on corrupted image data. We found\nthat the severity of the corruption negatively impacted the models' ability to\nestimate their uncertainty and the models also showed overconfidence in most of\nthe experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To leverage the full potential of Large Language Models (LLMs) it is crucial\nto have some information on their answers' uncertainty. This means that the\nmodel has to be able to quantify how certain it is in the correctness of a\ngiven response. Bad uncertainty estimates can lead to overconfident wrong\nanswers undermining trust in these models. Quite a lot of research has been\ndone on language models that work with text inputs and provide text outputs.\nStill, since the visual capabilities have been added to these models recently,\nthere has not been much progress on the uncertainty of Visual Language Models\n(VLMs). We tested three state-of-the-art VLMs on corrupted image data. We found\nthat the severity of the corruption negatively impacted the models' ability to\nestimate their uncertainty and the models also showed overconfidence in most of\nthe experiments."
                },
                "authors": [
                    {
                        "name": "Mirko Borszukovszki"
                    },
                    {
                        "name": "Ivo Pascal de Jong"
                    },
                    {
                        "name": "Matias Valdenegro-Toro"
                    }
                ],
                "author_detail": {
                    "name": "Matias Valdenegro-Toro"
                },
                "author": "Matias Valdenegro-Toro",
                "arxiv_comment": "10 pages, 11 figures, TrustNLP Workshop @ NAACL 2025 Camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03438v1",
                "updated": "2025-04-04T13:29:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    29,
                    32,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T13:29:32Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    29,
                    32,
                    4,
                    94,
                    0
                ],
                "title": "ZFusion: An Effective Fuser of Camera and 4D Radar for 3D Object\n  Perception in Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZFusion: An Effective Fuser of Camera and 4D Radar for 3D Object\n  Perception in Autonomous Driving"
                },
                "summary": "Reliable 3D object perception is essential in autonomous driving. Owing to\nits sensing capabilities in all weather conditions, 4D radar has recently\nreceived much attention. However, compared to LiDAR, 4D radar provides much\nsparser point cloud. In this paper, we propose a 3D object detection method,\ntermed ZFusion, which fuses 4D radar and vision modality. As the core of\nZFusion, our proposed FP-DDCA (Feature Pyramid-Double Deformable Cross\nAttention) fuser complements the (sparse) radar information and (dense) vision\ninformation, effectively. Specifically, with a feature-pyramid structure, the\nFP-DDCA fuser packs Transformer blocks to interactively fuse multi-modal\nfeatures at different scales, thus enhancing perception accuracy. In addition,\nwe utilize the Depth-Context-Split view transformation module due to the\nphysical properties of 4D radar. Considering that 4D radar has a much lower\ncost than LiDAR, ZFusion is an attractive alternative to LiDAR-based methods.\nIn typical traffic scenarios like the VoD (View-of-Delft) dataset, experiments\nshow that with reasonable inference speed, ZFusion achieved the\nstate-of-the-art mAP (mean average precision) in the region of interest, while\nhaving competitive mAP in the entire area compared to the baseline methods,\nwhich demonstrates performance close to LiDAR and greatly outperforms those\ncamera-only methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable 3D object perception is essential in autonomous driving. Owing to\nits sensing capabilities in all weather conditions, 4D radar has recently\nreceived much attention. However, compared to LiDAR, 4D radar provides much\nsparser point cloud. In this paper, we propose a 3D object detection method,\ntermed ZFusion, which fuses 4D radar and vision modality. As the core of\nZFusion, our proposed FP-DDCA (Feature Pyramid-Double Deformable Cross\nAttention) fuser complements the (sparse) radar information and (dense) vision\ninformation, effectively. Specifically, with a feature-pyramid structure, the\nFP-DDCA fuser packs Transformer blocks to interactively fuse multi-modal\nfeatures at different scales, thus enhancing perception accuracy. In addition,\nwe utilize the Depth-Context-Split view transformation module due to the\nphysical properties of 4D radar. Considering that 4D radar has a much lower\ncost than LiDAR, ZFusion is an attractive alternative to LiDAR-based methods.\nIn typical traffic scenarios like the VoD (View-of-Delft) dataset, experiments\nshow that with reasonable inference speed, ZFusion achieved the\nstate-of-the-art mAP (mean average precision) in the region of interest, while\nhaving competitive mAP in the entire area compared to the baseline methods,\nwhich demonstrates performance close to LiDAR and greatly outperforms those\ncamera-only methods."
                },
                "authors": [
                    {
                        "name": "Sheng Yang"
                    },
                    {
                        "name": "Tong Zhan"
                    },
                    {
                        "name": "Shichen Qiao"
                    },
                    {
                        "name": "Jicheng Gong"
                    },
                    {
                        "name": "Qing Yang"
                    },
                    {
                        "name": "Yanfeng Lu"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "arxiv_comment": "CVPR 2025 WDFM-AD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03434v1",
                "updated": "2025-04-04T13:25:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    25,
                    32,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T13:25:32Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    25,
                    32,
                    4,
                    94,
                    0
                ],
                "title": "Locations of Characters in Narratives: Andersen and Persuasion Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locations of Characters in Narratives: Andersen and Persuasion Datasets"
                },
                "summary": "The ability of machines to grasp spatial understanding within narrative\ncontexts is an intriguing aspect of reading comprehension that continues to be\nstudied. Motivated by the goal to test the AI's competence in understanding the\nrelationship between characters and their respective locations in narratives,\nwe introduce two new datasets: Andersen and Persuasion. For the Andersen\ndataset, we selected fifteen children's stories from \"Andersen's Fairy Tales\"\nby Hans Christian Andersen and manually annotated the characters and their\nrespective locations throughout each story. Similarly, for the Persuasion\ndataset, characters and their locations in the novel \"Persuasion\" by Jane\nAusten were also manually annotated. We used these datasets to prompt Large\nLanguage Models (LLMs). The prompts are created by extracting excerpts from the\nstories or the novel and combining them with a question asking the location of\na character mentioned in that excerpt. Out of the five LLMs we tested, the\nbest-performing one for the Andersen dataset accurately identified the location\nin 61.85% of the examples, while for the Persuasion dataset, the\nbest-performing one did so in 56.06% of the cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of machines to grasp spatial understanding within narrative\ncontexts is an intriguing aspect of reading comprehension that continues to be\nstudied. Motivated by the goal to test the AI's competence in understanding the\nrelationship between characters and their respective locations in narratives,\nwe introduce two new datasets: Andersen and Persuasion. For the Andersen\ndataset, we selected fifteen children's stories from \"Andersen's Fairy Tales\"\nby Hans Christian Andersen and manually annotated the characters and their\nrespective locations throughout each story. Similarly, for the Persuasion\ndataset, characters and their locations in the novel \"Persuasion\" by Jane\nAusten were also manually annotated. We used these datasets to prompt Large\nLanguage Models (LLMs). The prompts are created by extracting excerpts from the\nstories or the novel and combining them with a question asking the location of\na character mentioned in that excerpt. Out of the five LLMs we tested, the\nbest-performing one for the Andersen dataset accurately identified the location\nin 61.85% of the examples, while for the Persuasion dataset, the\nbest-performing one did so in 56.06% of the cases."
                },
                "authors": [
                    {
                        "name": "Batuhan Ozyurt"
                    },
                    {
                        "name": "Roya Arkhmammadova"
                    },
                    {
                        "name": "Deniz Yuret"
                    }
                ],
                "author_detail": {
                    "name": "Deniz Yuret"
                },
                "author": "Deniz Yuret",
                "arxiv_comment": "14 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00623v2",
                "updated": "2025-04-04T13:16:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    16,
                    35,
                    4,
                    94,
                    0
                ],
                "published": "2024-11-01T14:28:39Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    28,
                    39,
                    4,
                    306,
                    0
                ],
                "title": "Dual Low-Rank Adaptation for Continual Learning with Pre-Trained Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Low-Rank Adaptation for Continual Learning with Pre-Trained Models"
                },
                "summary": "In the era of foundation models, we revisit continual learning~(CL), which\naims to enable vision transformers (ViTs) to learn new tasks over time.\nHowever, as the scale of these models increases, catastrophic forgetting\nremains a persistent challenge, particularly in the presence of significant\ndomain shifts across tasks. Recent studies highlight a crossover between CL\ntechniques and parameter-efficient fine-tuning (PEFT), which focuses on\nfine-tuning only a small set of trainable parameters to adapt to downstream\ntasks, such as low-rank adaptation (LoRA). While LoRA achieves faster\nconvergence and requires fewer trainable parameters, it has seldom been\nexplored in the context of continual learning. To address this gap, we propose\na novel PEFT-CL method called Dual Low-Rank Adaptation (DualLoRA), which\nintroduces both an orthogonal LoRA adapter and a residual LoRA adapter parallel\nto pre-trained weights in each layer. These components are orchestrated by a\ndynamic memory mechanism to strike a balance between stability and plasticity.\nThe orthogonal LoRA adapter's parameters are updated in an orthogonal subspace\nof previous tasks to mitigate catastrophic forgetting, while the residual LoRA\nadapter's parameters are updated in the residual subspace spanned by\ntask-specific bases without interaction across tasks, offering complementary\ncapabilities for fine-tuning new tasks. On ViT-based models, we demonstrate\nthat DualLoRA offers significant advantages in accuracy, inference speed, and\nmemory efficiency over existing CL methods across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of foundation models, we revisit continual learning~(CL), which\naims to enable vision transformers (ViTs) to learn new tasks over time.\nHowever, as the scale of these models increases, catastrophic forgetting\nremains a persistent challenge, particularly in the presence of significant\ndomain shifts across tasks. Recent studies highlight a crossover between CL\ntechniques and parameter-efficient fine-tuning (PEFT), which focuses on\nfine-tuning only a small set of trainable parameters to adapt to downstream\ntasks, such as low-rank adaptation (LoRA). While LoRA achieves faster\nconvergence and requires fewer trainable parameters, it has seldom been\nexplored in the context of continual learning. To address this gap, we propose\na novel PEFT-CL method called Dual Low-Rank Adaptation (DualLoRA), which\nintroduces both an orthogonal LoRA adapter and a residual LoRA adapter parallel\nto pre-trained weights in each layer. These components are orchestrated by a\ndynamic memory mechanism to strike a balance between stability and plasticity.\nThe orthogonal LoRA adapter's parameters are updated in an orthogonal subspace\nof previous tasks to mitigate catastrophic forgetting, while the residual LoRA\nadapter's parameters are updated in the residual subspace spanned by\ntask-specific bases without interaction across tasks, offering complementary\ncapabilities for fine-tuning new tasks. On ViT-based models, we demonstrate\nthat DualLoRA offers significant advantages in accuracy, inference speed, and\nmemory efficiency over existing CL methods across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Huancheng Chen"
                    },
                    {
                        "name": "Jingtao Li"
                    },
                    {
                        "name": "Nidham Gazagnadou"
                    },
                    {
                        "name": "Weiming Zhuang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lingjuan Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Lingjuan Lyu"
                },
                "author": "Lingjuan Lyu",
                "arxiv_comment": "There is a major deduction error in Section 4.1 of the paper, and the\n  relevant results in Table 1 and Table 2 need to be corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06978v2",
                "updated": "2025-04-04T12:48:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    12,
                    48,
                    16,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-09T20:38:44Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    20,
                    38,
                    44,
                    0,
                    344,
                    0
                ],
                "title": "Edge-SD-SR: Low Latency and Parameter Efficient On-device\n  Super-Resolution with Stable Diffusion via Bidirectional Conditioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-SD-SR: Low Latency and Parameter Efficient On-device\n  Super-Resolution with Stable Diffusion via Bidirectional Conditioning"
                },
                "summary": "There has been immense progress recently in the visual quality of Stable\nDiffusion-based Super Resolution (SD-SR). However, deploying large diffusion\nmodels on computationally restricted devices such as mobile phones remains\nimpractical due to the large model size and high latency. This is compounded\nfor SR as it often operates at high res (e.g. 4Kx3K). In this work, we\nintroduce Edge-SD-SR, the first parameter efficient and low latency diffusion\nmodel for image super-resolution. Edge-SD-SR consists of ~169M parameters,\nincluding UNet, encoder and decoder, and has a complexity of only ~142 GFLOPs.\nTo maintain a high visual quality on such low compute budget, we introduce a\nnumber of training strategies: (i) A novel conditioning mechanism on the low\nresolution input, coined bidirectional conditioning, which tailors the SD model\nfor the SR task. (ii) Joint training of the UNet and encoder, while decoupling\nthe encodings of the HR and LR images and using a dedicated schedule. (iii)\nFinetuning the decoder using the UNet's output to directly tailor the decoder\nto the latents obtained at inference time. Edge-SD-SR runs efficiently on\ndevice, e.g. it can upscale a 128x128 patch to 512x512 in 38 msec while running\non a Samsung S24 DSP, and of a 512x512 to 2048x2048 (requiring 25 model\nevaluations) in just ~1.1 sec. Furthermore, we show that Edge-SD-SR matches or\neven outperforms state-of-the-art SR approaches on the most established SR\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been immense progress recently in the visual quality of Stable\nDiffusion-based Super Resolution (SD-SR). However, deploying large diffusion\nmodels on computationally restricted devices such as mobile phones remains\nimpractical due to the large model size and high latency. This is compounded\nfor SR as it often operates at high res (e.g. 4Kx3K). In this work, we\nintroduce Edge-SD-SR, the first parameter efficient and low latency diffusion\nmodel for image super-resolution. Edge-SD-SR consists of ~169M parameters,\nincluding UNet, encoder and decoder, and has a complexity of only ~142 GFLOPs.\nTo maintain a high visual quality on such low compute budget, we introduce a\nnumber of training strategies: (i) A novel conditioning mechanism on the low\nresolution input, coined bidirectional conditioning, which tailors the SD model\nfor the SR task. (ii) Joint training of the UNet and encoder, while decoupling\nthe encodings of the HR and LR images and using a dedicated schedule. (iii)\nFinetuning the decoder using the UNet's output to directly tailor the decoder\nto the latents obtained at inference time. Edge-SD-SR runs efficiently on\ndevice, e.g. it can upscale a 128x128 patch to 512x512 in 38 msec while running\non a Samsung S24 DSP, and of a 512x512 to 2048x2048 (requiring 25 model\nevaluations) in just ~1.1 sec. Furthermore, we show that Edge-SD-SR matches or\neven outperforms state-of-the-art SR approaches on the most established SR\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Mehdi Noroozi"
                    },
                    {
                        "name": "Isma Hadji"
                    },
                    {
                        "name": "Victor Escorcia"
                    },
                    {
                        "name": "Anestis Zaganidis"
                    },
                    {
                        "name": "Brais Martinez"
                    },
                    {
                        "name": "Georgios Tzimiropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Tzimiropoulos"
                },
                "author": "Georgios Tzimiropoulos",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01848v2",
                "updated": "2025-04-04T12:44:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    12,
                    44,
                    57,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-02T15:55:24Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    55,
                    24,
                    2,
                    92,
                    0
                ],
                "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaperBench: Evaluating AI's Ability to Replicate AI Research"
                },
                "summary": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\n\\href{https://github.com/openai/preparedness}{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\n\\href{https://github.com/openai/preparedness}{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents."
                },
                "authors": [
                    {
                        "name": "Giulio Starace"
                    },
                    {
                        "name": "Oliver Jaffe"
                    },
                    {
                        "name": "Dane Sherburn"
                    },
                    {
                        "name": "James Aung"
                    },
                    {
                        "name": "Jun Shern Chan"
                    },
                    {
                        "name": "Leon Maksin"
                    },
                    {
                        "name": "Rachel Dias"
                    },
                    {
                        "name": "Evan Mays"
                    },
                    {
                        "name": "Benjamin Kinsella"
                    },
                    {
                        "name": "Wyatt Thompson"
                    },
                    {
                        "name": "Johannes Heidecke"
                    },
                    {
                        "name": "Amelia Glaese"
                    },
                    {
                        "name": "Tejal Patwardhan"
                    }
                ],
                "author_detail": {
                    "name": "Tejal Patwardhan"
                },
                "author": "Tejal Patwardhan",
                "arxiv_comment": "30 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03390v1",
                "updated": "2025-04-04T12:03:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    12,
                    3,
                    11,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T12:03:11Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    12,
                    3,
                    11,
                    4,
                    94,
                    0
                ],
                "title": "Eigen-inference by Marchenko-Pastur inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen-inference by Marchenko-Pastur inversion"
                },
                "summary": "A new formula for Marchenko-Pastur inversion is derived and used for\ninference of population linear spectral statistics. The formula allows for\nestimation of the Stieltjes transform of the population spectral distribution\n$s_H(z)$, when $z$ is sufficiently far from the support of the population\nspectral distribution $H$. If the dimension $d$ and the sample size $n$ go to\ninfinity simultaneously such that $\\frac{d}{n} \\rightarrow c>0$, the estimation\nerror is shown to be asymptotically less than $\\frac{n^{\\varepsilon}}{n}$ for\narbitrary $\\varepsilon > 0$. By integrating along a curve around the support of\n$H$, estimators for population linear spectral statistics are constructed,\nwhich benefit from this convergence speed of $\\frac{n^{\\varepsilon}}{n}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new formula for Marchenko-Pastur inversion is derived and used for\ninference of population linear spectral statistics. The formula allows for\nestimation of the Stieltjes transform of the population spectral distribution\n$s_H(z)$, when $z$ is sufficiently far from the support of the population\nspectral distribution $H$. If the dimension $d$ and the sample size $n$ go to\ninfinity simultaneously such that $\\frac{d}{n} \\rightarrow c>0$, the estimation\nerror is shown to be asymptotically less than $\\frac{n^{\\varepsilon}}{n}$ for\narbitrary $\\varepsilon > 0$. By integrating along a curve around the support of\n$H$, estimators for population linear spectral statistics are constructed,\nwhich benefit from this convergence speed of $\\frac{n^{\\varepsilon}}{n}$."
                },
                "authors": [
                    {
                        "name": "Ben Deitmar"
                    }
                ],
                "author_detail": {
                    "name": "Ben Deitmar"
                },
                "author": "Ben Deitmar",
                "arxiv_comment": "34 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05925v2",
                "updated": "2025-04-04T11:59:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    59,
                    49,
                    4,
                    94,
                    0
                ],
                "published": "2024-09-09T08:29:39Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    29,
                    39,
                    0,
                    253,
                    0
                ],
                "title": "Assessing SPARQL capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing SPARQL capabilities of Large Language Models"
                },
                "summary": "The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs)\noffers significant synergistic potential for knowledge-driven applications. One\npossible integration is the interpretation and generation of formal languages,\nsuch as those used in the Semantic Web, with SPARQL being a core technology for\naccessing KGs. In this paper, we focus on measuring out-of-the box capabilities\nof LLMs to work with SPARQL and more specifically with SPARQL SELECT queries\napplying a quantitative approach.\n  We implemented various benchmarking tasks in the LLM-KG-Bench framework for\nautomated execution and evaluation with several LLMs. The tasks assess\ncapabilities along the dimensions of syntax, semantic read, semantic create,\nand the role of knowledge graph prompt inclusion.\n  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini,\nand Claude models. Our findings indicate that working with SPARQL SELECT\nqueries is still challenging for LLMs and heavily depends on the specific LLM\nas well as the complexity of the task. While fixing basic syntax errors seems\nto pose no problems for the best of the current LLMs evaluated, creating\nsemantically correct SPARQL SELECT queries is difficult in several cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs)\noffers significant synergistic potential for knowledge-driven applications. One\npossible integration is the interpretation and generation of formal languages,\nsuch as those used in the Semantic Web, with SPARQL being a core technology for\naccessing KGs. In this paper, we focus on measuring out-of-the box capabilities\nof LLMs to work with SPARQL and more specifically with SPARQL SELECT queries\napplying a quantitative approach.\n  We implemented various benchmarking tasks in the LLM-KG-Bench framework for\nautomated execution and evaluation with several LLMs. The tasks assess\ncapabilities along the dimensions of syntax, semantic read, semantic create,\nand the role of knowledge graph prompt inclusion.\n  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini,\nand Claude models. Our findings indicate that working with SPARQL SELECT\nqueries is still challenging for LLMs and heavily depends on the specific LLM\nas well as the complexity of the task. While fixing basic syntax errors seems\nto pose no problems for the best of the current LLMs evaluated, creating\nsemantically correct SPARQL SELECT queries is difficult in several cases."
                },
                "authors": [
                    {
                        "name": "Lars-Peter Meyer"
                    },
                    {
                        "name": "Johannes Frey"
                    },
                    {
                        "name": "Felix Brei"
                    },
                    {
                        "name": "Natanael Arndt"
                    }
                ],
                "author_detail": {
                    "name": "Natanael Arndt"
                },
                "author": "Natanael Arndt",
                "arxiv_comment": "Peer reviewed and published at NLP4KGc @ Semantics 2024, see original\n  publication at https://ceur-ws.org/Vol-3874/paper3.pdf . Updated Metadata",
                "arxiv_journal_ref": "CEUR-WS Vol.3874 (12/2024) 35-53",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17477v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17477v4",
                "updated": "2025-04-04T11:55:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    55,
                    58,
                    4,
                    94,
                    0
                ],
                "published": "2024-10-22T23:24:15Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    23,
                    24,
                    15,
                    1,
                    296,
                    0
                ],
                "title": "Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of\n  Architectural Inductive Biases on Hallucination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of\n  Architectural Inductive Biases on Hallucination"
                },
                "summary": "The growth in prominence of large language models (LLMs) in everyday life can\nbe largely attributed to their generative abilities, yet some of this is also\nowed to the risks and costs associated with their use. On one front is their\ntendency to hallucinate false or misleading information, limiting their\nreliability. On another is the increasing focus on the computational\nlimitations associated with traditional self-attention based LLMs, which has\nbrought about new alternatives, in particular recurrent models, meant to\novercome them. Yet it remains uncommon to consider these two concerns\nsimultaneously. Do changes in architecture exacerbate/alleviate existing\nconcerns about hallucinations? Do they affect how and where they occur? Through\nan extensive evaluation, we study how these architecture-based inductive biases\naffect the propensity to hallucinate. While hallucination remains a general\nphenomenon not limited to specific architectures, the situations in which they\noccur and the ease with which specific types of hallucinations can be induced\ncan significantly differ based on the model architecture. These findings\nhighlight the need for better understanding both these problems in conjunction\nwith each other, as well as consider how to design more universal techniques\nfor handling hallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth in prominence of large language models (LLMs) in everyday life can\nbe largely attributed to their generative abilities, yet some of this is also\nowed to the risks and costs associated with their use. On one front is their\ntendency to hallucinate false or misleading information, limiting their\nreliability. On another is the increasing focus on the computational\nlimitations associated with traditional self-attention based LLMs, which has\nbrought about new alternatives, in particular recurrent models, meant to\novercome them. Yet it remains uncommon to consider these two concerns\nsimultaneously. Do changes in architecture exacerbate/alleviate existing\nconcerns about hallucinations? Do they affect how and where they occur? Through\nan extensive evaluation, we study how these architecture-based inductive biases\naffect the propensity to hallucinate. While hallucination remains a general\nphenomenon not limited to specific architectures, the situations in which they\noccur and the ease with which specific types of hallucinations can be induced\ncan significantly differ based on the model architecture. These findings\nhighlight the need for better understanding both these problems in conjunction\nwith each other, as well as consider how to design more universal techniques\nfor handling hallucinations."
                },
                "authors": [
                    {
                        "name": "Jerry Huang"
                    },
                    {
                        "name": "Prasanna Parthasarathi"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Sarath Chandar"
                    }
                ],
                "author_detail": {
                    "name": "Sarath Chandar"
                },
                "author": "Sarath Chandar",
                "arxiv_comment": "22 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17477v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17477v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03384v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03384v1",
                "updated": "2025-04-04T11:55:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    55,
                    26,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T11:55:26Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    55,
                    26,
                    4,
                    94,
                    0
                ],
                "title": "Capturing Small-Scale Reionization Physics: A Sub-Grid Model for Photon\n  Sinks with SCRIPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing Small-Scale Reionization Physics: A Sub-Grid Model for Photon\n  Sinks with SCRIPT"
                },
                "summary": "The epoch of reionization represents a major phase transition in cosmic\nhistory, during which the first luminous sources ionized the intergalactic\nmedium (IGM). However, the small-scale physics governing ionizing photon sinks\n- particularly the interplay between recombinations, photon propagation, and\nself-shielded regions - remains poorly understood. Accurately modeling these\nprocesses requires a framework that self-consistently links ionizing\nemissivity, the clumping factor, mean free path, and photoionization rate. In\nthis work, we extend the photon-conserving semi-numerical framework, SCRIPT, by\nintroducing a self-consistent sub-grid model that dynamically connects these\nquantities to the underlying density field, enabling a more realistic treatment\nof inhomogeneous recombinations and photon sinks. We validate our model against\na comprehensive set of observational constraints, including the UV luminosity\nfunction from HST and JWST, CMB optical depth from Planck, and Lyman-$\\alpha$\nforest measurements of the IGM temperature, photoionization rate, and mean free\npath. Our fiducial model also successfully reproduces Lyman-$\\alpha$ opacity\nfluctuations, reinforcing its ability to capture large-scale inhomogeneities in\nthe reionization process. Notably, we demonstrate that traditionally\nindependent parameters, such as the clumping factor and mean free path, are\nstrongly correlated, with implications for the timing, morphology, and thermal\nevolution of reionization. Looking ahead, we will extend this framework to\ninclude machine learning-based parameter inference. With upcoming 21cm\nexperiments poised to provide unprecedented insights, SCRIPT offers a powerful\ncomputational tool for interpreting high-redshift observations and refining our\nunderstanding of the last major phase transition in the universe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The epoch of reionization represents a major phase transition in cosmic\nhistory, during which the first luminous sources ionized the intergalactic\nmedium (IGM). However, the small-scale physics governing ionizing photon sinks\n- particularly the interplay between recombinations, photon propagation, and\nself-shielded regions - remains poorly understood. Accurately modeling these\nprocesses requires a framework that self-consistently links ionizing\nemissivity, the clumping factor, mean free path, and photoionization rate. In\nthis work, we extend the photon-conserving semi-numerical framework, SCRIPT, by\nintroducing a self-consistent sub-grid model that dynamically connects these\nquantities to the underlying density field, enabling a more realistic treatment\nof inhomogeneous recombinations and photon sinks. We validate our model against\na comprehensive set of observational constraints, including the UV luminosity\nfunction from HST and JWST, CMB optical depth from Planck, and Lyman-$\\alpha$\nforest measurements of the IGM temperature, photoionization rate, and mean free\npath. Our fiducial model also successfully reproduces Lyman-$\\alpha$ opacity\nfluctuations, reinforcing its ability to capture large-scale inhomogeneities in\nthe reionization process. Notably, we demonstrate that traditionally\nindependent parameters, such as the clumping factor and mean free path, are\nstrongly correlated, with implications for the timing, morphology, and thermal\nevolution of reionization. Looking ahead, we will extend this framework to\ninclude machine learning-based parameter inference. With upcoming 21cm\nexperiments poised to provide unprecedented insights, SCRIPT offers a powerful\ncomputational tool for interpreting high-redshift observations and refining our\nunderstanding of the last major phase transition in the universe."
                },
                "authors": [
                    {
                        "name": "Tirthankar Roy Choudhury"
                    },
                    {
                        "name": "Anirban Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Chakraborty"
                },
                "author": "Anirban Chakraborty",
                "arxiv_comment": "To be submitted to JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03384v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03384v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03380v1",
                "updated": "2025-04-04T11:52:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    52,
                    5,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T11:52:05Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    52,
                    5,
                    4,
                    94,
                    0
                ],
                "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Difficulty Filtering for Reasoning Oriented Reinforcement\n  Learning"
                },
                "summary": "Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning\nability of Large Language Models (LLMs). However, due to the sparsity of\nrewards in RORL, effective training is highly dependent on the selection of\nproblems of appropriate difficulty. Although curriculum learning attempts to\naddress this by adjusting difficulty, it often relies on static schedules, and\neven recent online filtering methods lack theoretical grounding and a\nsystematic understanding of their effectiveness. In this work, we theoretically\nand empirically show that curating the batch with the problems that the\ntraining model achieves intermediate accuracy on the fly can maximize the\neffectiveness of RORL training, namely balanced online difficulty filtering. We\nfirst derive that the lower bound of the KL divergence between the initial and\nthe optimal policy can be expressed with the variance of the sampled accuracy.\nBuilding on those insights, we show that balanced filtering can maximize the\nlower bound, leading to better performance. Experimental results across five\nchallenging math reasoning benchmarks show that balanced online filtering\nyields an additional 10% in AIME and 4% improvements in average over plain\nGRPO. Moreover, further analysis shows the gains in sample efficiency and\ntraining time efficiency, exceeding the maximum reward of plain GRPO within 60%\ntraining time and the volume of the training set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning\nability of Large Language Models (LLMs). However, due to the sparsity of\nrewards in RORL, effective training is highly dependent on the selection of\nproblems of appropriate difficulty. Although curriculum learning attempts to\naddress this by adjusting difficulty, it often relies on static schedules, and\neven recent online filtering methods lack theoretical grounding and a\nsystematic understanding of their effectiveness. In this work, we theoretically\nand empirically show that curating the batch with the problems that the\ntraining model achieves intermediate accuracy on the fly can maximize the\neffectiveness of RORL training, namely balanced online difficulty filtering. We\nfirst derive that the lower bound of the KL divergence between the initial and\nthe optimal policy can be expressed with the variance of the sampled accuracy.\nBuilding on those insights, we show that balanced filtering can maximize the\nlower bound, leading to better performance. Experimental results across five\nchallenging math reasoning benchmarks show that balanced online filtering\nyields an additional 10% in AIME and 4% improvements in average over plain\nGRPO. Moreover, further analysis shows the gains in sample efficiency and\ntraining time efficiency, exceeding the maximum reward of plain GRPO within 60%\ntraining time and the volume of the training set."
                },
                "authors": [
                    {
                        "name": "Sanghwan Bae"
                    },
                    {
                        "name": "Jiwoo Hong"
                    },
                    {
                        "name": "Min Young Lee"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "JeongYeon Nam"
                    },
                    {
                        "name": "Donghyun Kwak"
                    }
                ],
                "author_detail": {
                    "name": "Donghyun Kwak"
                },
                "author": "Donghyun Kwak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09893v2",
                "updated": "2025-04-04T11:45:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    45,
                    2,
                    4,
                    94,
                    0
                ],
                "published": "2024-10-13T16:06:54Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    16,
                    6,
                    54,
                    6,
                    287,
                    0
                ],
                "title": "RMB: Comprehensively Benchmarking Reward Models in LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMB: Comprehensively Benchmarking Reward Models in LLM Alignment"
                },
                "summary": "Reward models (RMs) guide the alignment of large language models (LLMs),\nsteering them toward behaviors preferred by humans. Evaluating RMs is the key\nto better aligning LLMs. However, the current evaluation of RMs may not\ndirectly correspond to their alignment performance due to the limited\ndistribution of evaluation data and evaluation methods that are not closely\nrelated to alignment objectives. To address these limitations, we propose RMB,\na comprehensive RM benchmark that covers over 49 real-world scenarios and\nincludes both pairwise and Best-of-N (BoN) evaluations to better reflect the\neffectiveness of RMs in guiding alignment optimization. We demonstrate a\npositive correlation between our benchmark and the downstream alignment task\nperformance. Based on our benchmark, we conduct extensive analysis on the\nstate-of-the-art RMs, revealing their generalization defects that were not\ndiscovered by previous benchmarks, and highlighting the potential of generative\nRMs. Furthermore, we delve into open questions in reward models, specifically\nexamining the effectiveness of majority voting for the evaluation of reward\nmodels and analyzing the impact factors of generative RMs, including the\ninfluence of evaluation criteria and instructing methods. Our evaluation code\nand datasets are available at\nhttps://github.com/Zhou-Zoey/RMB-Reward-Model-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) guide the alignment of large language models (LLMs),\nsteering them toward behaviors preferred by humans. Evaluating RMs is the key\nto better aligning LLMs. However, the current evaluation of RMs may not\ndirectly correspond to their alignment performance due to the limited\ndistribution of evaluation data and evaluation methods that are not closely\nrelated to alignment objectives. To address these limitations, we propose RMB,\na comprehensive RM benchmark that covers over 49 real-world scenarios and\nincludes both pairwise and Best-of-N (BoN) evaluations to better reflect the\neffectiveness of RMs in guiding alignment optimization. We demonstrate a\npositive correlation between our benchmark and the downstream alignment task\nperformance. Based on our benchmark, we conduct extensive analysis on the\nstate-of-the-art RMs, revealing their generalization defects that were not\ndiscovered by previous benchmarks, and highlighting the potential of generative\nRMs. Furthermore, we delve into open questions in reward models, specifically\nexamining the effectiveness of majority voting for the evaluation of reward\nmodels and analyzing the impact factors of generative RMs, including the\ninfluence of evaluation criteria and instructing methods. Our evaluation code\nand datasets are available at\nhttps://github.com/Zhou-Zoey/RMB-Reward-Model-Benchmark."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Guodong Zheng"
                    },
                    {
                        "name": "Binghai Wang"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Rong Bao"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Limao Xiong"
                    },
                    {
                        "name": "Jessica Fan"
                    },
                    {
                        "name": "Yurong Mou"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Accepted by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10801v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10801v3",
                "updated": "2025-04-04T11:43:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    43,
                    47,
                    4,
                    94,
                    0
                ],
                "published": "2024-11-16T13:35:05Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    35,
                    5,
                    5,
                    321,
                    0
                ],
                "title": "Mixing Samples to Address Weak Overlap in Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixing Samples to Address Weak Overlap in Causal Inference"
                },
                "summary": "In observational studies, the assumption of sufficient overlap (positivity)\nis fundamental for the identification and estimation of causal effects. Failing\nto account for this assumption yields inaccurate and potentially infeasible\nestimators. To address this issue, we introduce a simple yet novel approach,\n\\textit{mixing}, which mitigates overlap violations by constructing a synthetic\ntreated group that combines treated and control units. Our strategy offers\nthree key advantages. First, it improves the accuracy of the estimator by\npreserving unbiasedness while reducing variance. The benefit is particularly\nsignificant in settings with weak overlap, though the method remains effective\nregardless of the overlap level. This phenomenon results from the shrinkage of\npropensity scores in the mixed sample, which enhances robustness to poor\noverlap. Second, it enables direct estimation of the target estimand without\ndiscarding extreme observations or modifying the target population, thus\nfacilitating a straightforward interpretation of the results. Third, the mixing\napproach is highly adaptable to various weighting schemes, including\ncontemporary methods such as entropy balancing. The estimation of the Mixed IPW\n(MIPW) estimator is done via M-estimation, and the method extends to a broader\nclass of weighting estimators through a resampling algorithm. We illustrate the\nmixing approach through extensive simulation studies and provide practical\nguidance with a real-data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In observational studies, the assumption of sufficient overlap (positivity)\nis fundamental for the identification and estimation of causal effects. Failing\nto account for this assumption yields inaccurate and potentially infeasible\nestimators. To address this issue, we introduce a simple yet novel approach,\n\\textit{mixing}, which mitigates overlap violations by constructing a synthetic\ntreated group that combines treated and control units. Our strategy offers\nthree key advantages. First, it improves the accuracy of the estimator by\npreserving unbiasedness while reducing variance. The benefit is particularly\nsignificant in settings with weak overlap, though the method remains effective\nregardless of the overlap level. This phenomenon results from the shrinkage of\npropensity scores in the mixed sample, which enhances robustness to poor\noverlap. Second, it enables direct estimation of the target estimand without\ndiscarding extreme observations or modifying the target population, thus\nfacilitating a straightforward interpretation of the results. Third, the mixing\napproach is highly adaptable to various weighting schemes, including\ncontemporary methods such as entropy balancing. The estimation of the Mixed IPW\n(MIPW) estimator is done via M-estimation, and the method extends to a broader\nclass of weighting estimators through a resampling algorithm. We illustrate the\nmixing approach through extensive simulation studies and provide practical\nguidance with a real-data analysis."
                },
                "authors": [
                    {
                        "name": "Jaehyuk Jang"
                    },
                    {
                        "name": "Suehyun Kim"
                    },
                    {
                        "name": "Kwonsang Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kwonsang Lee"
                },
                "author": "Kwonsang Lee",
                "arxiv_comment": "37 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10801v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10801v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20344v2",
                "updated": "2025-04-04T11:38:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    38,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2024-10-27T05:28:44Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    5,
                    28,
                    44,
                    6,
                    301,
                    0
                ],
                "title": "Deep Learning-Assisted Jamming Mitigation with Movable Antenna Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Assisted Jamming Mitigation with Movable Antenna Array"
                },
                "summary": "This paper reveals the potential of movable antennas in enhancing\nanti-jamming communication. We consider a legitimate communication link in the\npresence of multiple jammers and propose deploying a movable antenna array at\nthe receiver to combat jamming attacks. We formulate the problem as a\nsignal-to-interference-plus-noise ratio maximization, by jointly optimizing the\nreceive beamforming and antenna element positioning. Due to the non-convexity\nand multi-fold difficulties from an optimization perspective, we develop a deep\nlearning-based framework where beamforming is tackled as a Rayleigh quotient\nproblem, while antenna positioning is addressed through multi-layer perceptron\ntraining. The neural network parameters are optimized using stochastic gradient\ndescent to achieve effective jamming mitigation strategy, featuring offline\ntraining with marginal complexity for online inference. Numerical results\ndemonstrate that the proposed approach achieves near-optimal anti-jamming\nperformance thereby significantly improving the efficiency in strategy\ndetermination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper reveals the potential of movable antennas in enhancing\nanti-jamming communication. We consider a legitimate communication link in the\npresence of multiple jammers and propose deploying a movable antenna array at\nthe receiver to combat jamming attacks. We formulate the problem as a\nsignal-to-interference-plus-noise ratio maximization, by jointly optimizing the\nreceive beamforming and antenna element positioning. Due to the non-convexity\nand multi-fold difficulties from an optimization perspective, we develop a deep\nlearning-based framework where beamforming is tackled as a Rayleigh quotient\nproblem, while antenna positioning is addressed through multi-layer perceptron\ntraining. The neural network parameters are optimized using stochastic gradient\ndescent to achieve effective jamming mitigation strategy, featuring offline\ntraining with marginal complexity for online inference. Numerical results\ndemonstrate that the proposed approach achieves near-optimal anti-jamming\nperformance thereby significantly improving the efficiency in strategy\ndetermination."
                },
                "authors": [
                    {
                        "name": "Xiao Tang"
                    },
                    {
                        "name": "Yudan Jiang"
                    },
                    {
                        "name": "Jinxin Liu"
                    },
                    {
                        "name": "Qinghe Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Zhu Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhu Han"
                },
                "author": "Zhu Han",
                "arxiv_comment": "Accepted @ IEEE TVT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03360v1",
                "updated": "2025-04-04T11:29:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    29,
                    30,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T11:29:30Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    29,
                    30,
                    4,
                    94,
                    0
                ],
                "title": "Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for\n  Energy Efficiency, Output Accuracy, and Inference Latency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for\n  Energy Efficiency, Output Accuracy, and Inference Latency"
                },
                "summary": "Deploying Large Language Models (LLMs) on edge devices presents significant\nchallenges due to computational constraints, memory limitations, inference\nspeed, and energy consumption. Model quantization has emerged as a key\ntechnique to enable efficient LLM inference by reducing model size and\ncomputational overhead. In this study, we conduct a comprehensive analysis of\n28 quantized LLMs from the Ollama library, which applies by default\nPost-Training Quantization (PTQ) and weight-only quantization techniques,\ndeployed on an edge device (Raspberry Pi 4 with 4GB RAM). We evaluate energy\nefficiency, inference performance, and output accuracy across multiple\nquantization levels and task types. Models are benchmarked on five standardized\ndatasets (CommonsenseQA, BIG-Bench Hard, TruthfulQA, GSM8K, and HumanEval), and\nwe employ a high-resolution, hardware-based energy measurement tool to capture\nreal-world power consumption. Our findings reveal the trade-offs between energy\nefficiency, inference speed, and accuracy in different quantization settings,\nhighlighting configurations that optimize LLM deployment for\nresource-constrained environments. By integrating hardware-level energy\nprofiling with LLM benchmarking, this study provides actionable insights for\nsustainable AI, bridging a critical gap in existing research on energy-aware\nLLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Large Language Models (LLMs) on edge devices presents significant\nchallenges due to computational constraints, memory limitations, inference\nspeed, and energy consumption. Model quantization has emerged as a key\ntechnique to enable efficient LLM inference by reducing model size and\ncomputational overhead. In this study, we conduct a comprehensive analysis of\n28 quantized LLMs from the Ollama library, which applies by default\nPost-Training Quantization (PTQ) and weight-only quantization techniques,\ndeployed on an edge device (Raspberry Pi 4 with 4GB RAM). We evaluate energy\nefficiency, inference performance, and output accuracy across multiple\nquantization levels and task types. Models are benchmarked on five standardized\ndatasets (CommonsenseQA, BIG-Bench Hard, TruthfulQA, GSM8K, and HumanEval), and\nwe employ a high-resolution, hardware-based energy measurement tool to capture\nreal-world power consumption. Our findings reveal the trade-offs between energy\nefficiency, inference speed, and accuracy in different quantization settings,\nhighlighting configurations that optimize LLM deployment for\nresource-constrained environments. By integrating hardware-level energy\nprofiling with LLM benchmarking, this study provides actionable insights for\nsustainable AI, bridging a critical gap in existing research on energy-aware\nLLM deployment."
                },
                "authors": [
                    {
                        "name": "Erik Johannes Husom"
                    },
                    {
                        "name": "Arda Goknil"
                    },
                    {
                        "name": "Merve Astekin"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "Andre Ksen"
                    },
                    {
                        "name": "Sagar Sen"
                    },
                    {
                        "name": "Benedikt Andreas Mithassel"
                    },
                    {
                        "name": "Ahmet Soylu"
                    }
                ],
                "author_detail": {
                    "name": "Ahmet Soylu"
                },
                "author": "Ahmet Soylu",
                "arxiv_comment": "30 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10679v2",
                "updated": "2025-04-04T11:17:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    17,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2025-03-11T14:09:04Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    9,
                    4,
                    1,
                    70,
                    0
                ],
                "title": "End-to-end Learning of Sparse Interventions on Activations to Steer\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end Learning of Sparse Interventions on Activations to Steer\n  Generation"
                },
                "summary": "The growing use of generative models in daily life calls for efficient\nmechanisms to control their generation, to e.g., produce safe content or\nprovide users with tools to explore style changes. Ideally, such mechanisms\nshould be cheap, both at train and inference time, while preserving output\nquality. Recent research has shown that such mechanisms can be obtained by\nintervening exclusively on model activations, with the goal of correcting\ndistributional differences between activations seen when using prompts from a\nsource vs. a target set (e.g., toxic and non-toxic sentences). While cheap,\nthese fast methods are inherently crude: their maps are tuned locally, not\naccounting for their impact on downstream layers, resulting in interventions\nthat cause unintended shifts when used out-of-sample. We propose in this work\nlinear end-to-end activation steering (LinEAS), an approach trained with a\nglobal loss that accounts simultaneously for all layerwise distributional\nshifts. In addition to being more robust, the loss used to train LinEAS can be\nregularized with sparsifying norms, which can automatically carry out neuron\nand layer selection. Empirically, LinEAS only requires a handful of samples to\nbe effective, and beats similar baselines on toxicity mitigation, while\nperforming on par with far more involved finetuning approaches. We show that\nLinEAS interventions can be composed, study the impact of sparsity on their\nperformance, and showcase applications in text-to-image diffusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing use of generative models in daily life calls for efficient\nmechanisms to control their generation, to e.g., produce safe content or\nprovide users with tools to explore style changes. Ideally, such mechanisms\nshould be cheap, both at train and inference time, while preserving output\nquality. Recent research has shown that such mechanisms can be obtained by\nintervening exclusively on model activations, with the goal of correcting\ndistributional differences between activations seen when using prompts from a\nsource vs. a target set (e.g., toxic and non-toxic sentences). While cheap,\nthese fast methods are inherently crude: their maps are tuned locally, not\naccounting for their impact on downstream layers, resulting in interventions\nthat cause unintended shifts when used out-of-sample. We propose in this work\nlinear end-to-end activation steering (LinEAS), an approach trained with a\nglobal loss that accounts simultaneously for all layerwise distributional\nshifts. In addition to being more robust, the loss used to train LinEAS can be\nregularized with sparsifying norms, which can automatically carry out neuron\nand layer selection. Empirically, LinEAS only requires a handful of samples to\nbe effective, and beats similar baselines on toxicity mitigation, while\nperforming on par with far more involved finetuning approaches. We show that\nLinEAS interventions can be composed, study the impact of sparsity on their\nperformance, and showcase applications in text-to-image diffusions."
                },
                "authors": [
                    {
                        "name": "Pau Rodriguez"
                    },
                    {
                        "name": "Michal Klein"
                    },
                    {
                        "name": "Eleonora Gualdoni"
                    },
                    {
                        "name": "Arno Blaas"
                    },
                    {
                        "name": "Luca Zappella"
                    },
                    {
                        "name": "Marco Cuturi"
                    },
                    {
                        "name": "Xavier Suau"
                    }
                ],
                "author_detail": {
                    "name": "Xavier Suau"
                },
                "author": "Xavier Suau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03352v1",
                "updated": "2025-04-04T11:14:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    14,
                    38,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T11:14:38Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    14,
                    38,
                    4,
                    94,
                    0
                ],
                "title": "Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social\n  Psychological Underpinnings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social\n  Psychological Underpinnings"
                },
                "summary": "Stereotypes are known to be highly pernicious, making their detection\ncritically important. However, current research predominantly focuses on\ndetecting and evaluating stereotypical biases in LLMs, leaving the study of\nstereotypes in its early stages. Many studies have failed to clearly\ndistinguish between stereotypes and stereotypical biases, which has\nsignificantly slowed progress in advancing research in this area. Stereotype\nand anti-stereotype detection is a problem that requires knowledge of society;\nhence, it is one of the most difficult areas in Responsible AI. This work\ninvestigates this task, where we propose a four-tuple definition and provide\nprecise terminology distinguishing stereotype, anti-stereotype, stereotypical\nbias, and bias, offering valuable insights into their various aspects. In this\npaper, we propose StereoDetect, a high-quality benchmarking dataset curated for\nthis task by optimally utilizing current datasets such as StereoSet and\nWinoQueer, involving a manual verification process and the transfer of semantic\ninformation. We demonstrate that language models for reasoning with fewer than\n10B parameters often get confused when detecting anti-stereotypes. We also\ndemonstrate the critical importance of well-curated datasets by comparing our\nmodel with other current models for stereotype detection. The dataset and code\nis available at https://github.com/KaustubhShejole/StereoDetect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotypes are known to be highly pernicious, making their detection\ncritically important. However, current research predominantly focuses on\ndetecting and evaluating stereotypical biases in LLMs, leaving the study of\nstereotypes in its early stages. Many studies have failed to clearly\ndistinguish between stereotypes and stereotypical biases, which has\nsignificantly slowed progress in advancing research in this area. Stereotype\nand anti-stereotype detection is a problem that requires knowledge of society;\nhence, it is one of the most difficult areas in Responsible AI. This work\ninvestigates this task, where we propose a four-tuple definition and provide\nprecise terminology distinguishing stereotype, anti-stereotype, stereotypical\nbias, and bias, offering valuable insights into their various aspects. In this\npaper, we propose StereoDetect, a high-quality benchmarking dataset curated for\nthis task by optimally utilizing current datasets such as StereoSet and\nWinoQueer, involving a manual verification process and the transfer of semantic\ninformation. We demonstrate that language models for reasoning with fewer than\n10B parameters often get confused when detecting anti-stereotypes. We also\ndemonstrate the critical importance of well-curated datasets by comparing our\nmodel with other current models for stereotype detection. The dataset and code\nis available at https://github.com/KaustubhShejole/StereoDetect."
                },
                "authors": [
                    {
                        "name": "Kaustubh Shivshankar Shejole"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03343v1",
                "updated": "2025-04-04T10:58:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    58,
                    57,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T10:58:57Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    58,
                    57,
                    4,
                    94,
                    0
                ],
                "title": "Talk2X -- An Open-Source Toolkit Facilitating Deployment of LLM-Powered\n  Chatbots on the Web",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talk2X -- An Open-Source Toolkit Facilitating Deployment of LLM-Powered\n  Chatbots on the Web"
                },
                "summary": "Integrated into websites, LLM-powered chatbots offer alternative means of\nnavigation and information retrieval, leading to a shift in how users access\ninformation on the web. Yet, predominantly closed-sourced solutions limit\nproliferation among web hosts and suffer from a lack of transparency with\nregard to implementation details and energy efficiency. In this work, we\npropose our openly available agent Talk2X leveraging an adapted\nretrieval-augmented generation approach (RAG) combined with an automatically\ngenerated vector database, benefiting energy efficiency. Talk2X's architecture\nis generalizable to arbitrary websites offering developers a ready to use tool\nfor integration. Using a mixed-methods approach, we evaluated Talk2X's\nusability by tasking users to acquire specific assets from an open science\nrepository. Talk2X significantly improved task completion time, correctness,\nand user experience supporting users in quickly pinpointing specific\ninformation as compared to standard user-website interaction. Our findings\ncontribute technical advancements to an ongoing paradigm shift of how we access\ninformation on the web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated into websites, LLM-powered chatbots offer alternative means of\nnavigation and information retrieval, leading to a shift in how users access\ninformation on the web. Yet, predominantly closed-sourced solutions limit\nproliferation among web hosts and suffer from a lack of transparency with\nregard to implementation details and energy efficiency. In this work, we\npropose our openly available agent Talk2X leveraging an adapted\nretrieval-augmented generation approach (RAG) combined with an automatically\ngenerated vector database, benefiting energy efficiency. Talk2X's architecture\nis generalizable to arbitrary websites offering developers a ready to use tool\nfor integration. Using a mixed-methods approach, we evaluated Talk2X's\nusability by tasking users to acquire specific assets from an open science\nrepository. Talk2X significantly improved task completion time, correctness,\nand user experience supporting users in quickly pinpointing specific\ninformation as compared to standard user-website interaction. Our findings\ncontribute technical advancements to an ongoing paradigm shift of how we access\ninformation on the web."
                },
                "authors": [
                    {
                        "name": "Lars Krupp"
                    },
                    {
                        "name": "Daniel Geiler"
                    },
                    {
                        "name": "Peter Hevesi"
                    },
                    {
                        "name": "Marco Hirsch"
                    },
                    {
                        "name": "Paul Lukowicz"
                    },
                    {
                        "name": "Jakob Karolus"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Karolus"
                },
                "author": "Jakob Karolus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00159v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00159v3",
                "updated": "2025-04-04T10:58:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    58,
                    40,
                    4,
                    94,
                    0
                ],
                "published": "2024-08-30T15:04:11Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    4,
                    11,
                    4,
                    243,
                    0
                ],
                "title": "LLMs Prompted for Graphs: Hallucinations and Generative Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Prompted for Graphs: Hallucinations and Generative Capabilities"
                },
                "summary": "Large Language Models (LLMs) are nowadays prompted for a wide variety of\ntasks. In this article, we investigate their ability in reciting and generating\ngraphs. We first study the ability of LLMs to regurgitate well known graphs\nfrom the literature (e.g. Karate club or the graph atlas)4. Secondly, we\nquestion the generative capabilities of LLMs by asking for Erdos-Renyi random\ngraphs. As opposed to the possibility that they could memorize some Erdos-Renyi\ngraphs included in their scraped training set, this second investigation aims\nat studying a possible emergent property of LLMs. For both tasks, we propose a\nmetric to assess their errors with the lens of hallucination (i.e. incorrect\ninformation returned as facts). We most notably find that the amplitude of\ngraph hallucinations can characterize the superiority of some LLMs. Indeed, for\nthe recitation task, we observe that graph hallucinations correlate with the\nHallucination Leaderboard, a hallucination rank that leverages 10, 000 times\nmore prompts to obtain its ranking. For the generation task, we find\nsurprisingly good and reproducible results in most of LLMs. We believe this to\nconstitute a starting point for more in-depth studies of this emergent\ncapability and a challenging benchmark for their improvements. Altogether,\nthese two aspects of LLMs capabilities bridge a gap between the network science\nand machine learning communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are nowadays prompted for a wide variety of\ntasks. In this article, we investigate their ability in reciting and generating\ngraphs. We first study the ability of LLMs to regurgitate well known graphs\nfrom the literature (e.g. Karate club or the graph atlas)4. Secondly, we\nquestion the generative capabilities of LLMs by asking for Erdos-Renyi random\ngraphs. As opposed to the possibility that they could memorize some Erdos-Renyi\ngraphs included in their scraped training set, this second investigation aims\nat studying a possible emergent property of LLMs. For both tasks, we propose a\nmetric to assess their errors with the lens of hallucination (i.e. incorrect\ninformation returned as facts). We most notably find that the amplitude of\ngraph hallucinations can characterize the superiority of some LLMs. Indeed, for\nthe recitation task, we observe that graph hallucinations correlate with the\nHallucination Leaderboard, a hallucination rank that leverages 10, 000 times\nmore prompts to obtain its ranking. For the generation task, we find\nsurprisingly good and reproducible results in most of LLMs. We believe this to\nconstitute a starting point for more in-depth studies of this emergent\ncapability and a challenging benchmark for their improvements. Altogether,\nthese two aspects of LLMs capabilities bridge a gap between the network science\nand machine learning communities."
                },
                "authors": [
                    {
                        "name": "Gurvan Richardeau"
                    },
                    {
                        "name": "Samy Chali"
                    },
                    {
                        "name": "Erwan Le Merrer"
                    },
                    {
                        "name": "Camilla Penzo"
                    },
                    {
                        "name": "Gilles Tredan"
                    }
                ],
                "author_detail": {
                    "name": "Gilles Tredan"
                },
                "author": "Gilles Tredan",
                "arxiv_comment": "A preliminary version of this work appeared in the Complex Networks\n  2024 conference, under the title \"LLMs hallucinate graphs too: a structural\n  perspective\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00159v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00159v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03338v1",
                "updated": "2025-04-04T10:42:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    42,
                    56,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T10:42:56Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    42,
                    56,
                    4,
                    94,
                    0
                ],
                "title": "BabyLM's First Words: Word Segmentation as a Phonological Probing Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BabyLM's First Words: Word Segmentation as a Phonological Probing Task"
                },
                "summary": "Language models provide a key framework for studying linguistic theories\nbased on prediction, but phonological analysis using large language models\n(LLMs) is difficult; there are few phonological benchmarks beyond English and\nthe standard input representation used in LLMs (subwords of graphemes) is not\nsuitable for analyzing the representation of phonemes. In this work, we\ndemonstrate how word segmentation can be used as a phonological probing task,\nallowing us to study the representations learned by phoneme-based language\nmodels trained on child-directed speech across 31 languages. Following\ncomputational models of word segmentation, we present unsupervised methods for\nextracting word boundaries from a trained model using the observation that\nprediction-error peaks at the start of words. We also use linear probes to\nidentify that these models implicitly track word boundaries, even when they do\nnot appear in training. This cross-lingual work corroborates statistical\nlearning theories of acquisition and empirically motivates new methods for\ntraining subword tokenizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models provide a key framework for studying linguistic theories\nbased on prediction, but phonological analysis using large language models\n(LLMs) is difficult; there are few phonological benchmarks beyond English and\nthe standard input representation used in LLMs (subwords of graphemes) is not\nsuitable for analyzing the representation of phonemes. In this work, we\ndemonstrate how word segmentation can be used as a phonological probing task,\nallowing us to study the representations learned by phoneme-based language\nmodels trained on child-directed speech across 31 languages. Following\ncomputational models of word segmentation, we present unsupervised methods for\nextracting word boundaries from a trained model using the observation that\nprediction-error peaks at the start of words. We also use linear probes to\nidentify that these models implicitly track word boundaries, even when they do\nnot appear in training. This cross-lingual work corroborates statistical\nlearning theories of acquisition and empirically motivates new methods for\ntraining subword tokenizers."
                },
                "authors": [
                    {
                        "name": "Zbulon Goriely"
                    }
                ],
                "author_detail": {
                    "name": "Zbulon Goriely"
                },
                "author": "Zbulon Goriely",
                "arxiv_comment": "17 pages, 10 figures, submitted to CoNLL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17034v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17034v4",
                "updated": "2025-04-04T10:42:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    42,
                    39,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-24T10:34:35Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    10,
                    34,
                    35,
                    0,
                    55,
                    0
                ],
                "title": "Evolution 6.0: Evolving Robotic Capabilities Through Generative Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolution 6.0: Evolving Robotic Capabilities Through Generative Design"
                },
                "summary": "We propose a new concept, Evolution 6.0, which represents the evolution of\nrobotics driven by Generative AI. When a robot lacks the necessary tools to\naccomplish a task requested by a human, it autonomously designs the required\ninstruments and learns how to use them to achieve the goal. Evolution 6.0 is an\nautonomous robotic system powered by Vision-Language Models (VLMs),\nVision-Language Action (VLA) models, and Text-to-3D generative models for tool\ndesign and task execution. The system comprises two key modules: the Tool\nGeneration Module, which fabricates task-specific tools from visual and textual\ndata, and the Action Generation Module, which converts natural language\ninstructions into robotic actions. It integrates QwenVLM for environmental\nunderstanding, OpenVLA for task execution, and Llama-Mesh for 3D tool\ngeneration. Evaluation results demonstrate a 90% success rate for tool\ngeneration with a 10-second inference time, and action generation achieving\n83.5% in physical and visual generalization, 70% in motion generalization, and\n37% in semantic generalization. Future improvements will focus on bimanual\nmanipulation, expanded task capabilities, and enhanced environmental\ninterpretation to improve real-world adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a new concept, Evolution 6.0, which represents the evolution of\nrobotics driven by Generative AI. When a robot lacks the necessary tools to\naccomplish a task requested by a human, it autonomously designs the required\ninstruments and learns how to use them to achieve the goal. Evolution 6.0 is an\nautonomous robotic system powered by Vision-Language Models (VLMs),\nVision-Language Action (VLA) models, and Text-to-3D generative models for tool\ndesign and task execution. The system comprises two key modules: the Tool\nGeneration Module, which fabricates task-specific tools from visual and textual\ndata, and the Action Generation Module, which converts natural language\ninstructions into robotic actions. It integrates QwenVLM for environmental\nunderstanding, OpenVLA for task execution, and Llama-Mesh for 3D tool\ngeneration. Evaluation results demonstrate a 90% success rate for tool\ngeneration with a 10-second inference time, and action generation achieving\n83.5% in physical and visual generalization, 70% in motion generalization, and\n37% in semantic generalization. Future improvements will focus on bimanual\nmanipulation, expanded task capabilities, and enhanced environmental\ninterpretation to improve real-world adaptability."
                },
                "authors": [
                    {
                        "name": "Muhammad Haris Khan"
                    },
                    {
                        "name": "Artyom Myshlyaev"
                    },
                    {
                        "name": "Artem Lykov"
                    },
                    {
                        "name": "Miguel Altamirano Cabrera"
                    },
                    {
                        "name": "Dzmitry Tsetserukou"
                    }
                ],
                "author_detail": {
                    "name": "Dzmitry Tsetserukou"
                },
                "author": "Dzmitry Tsetserukou",
                "arxiv_comment": "Submitted to IROS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17034v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17034v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03337v1",
                "updated": "2025-04-04T10:38:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    38,
                    28,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T10:38:28Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    38,
                    28,
                    4,
                    94,
                    0
                ],
                "title": "QIRL: Boosting Visual Question Answering via Optimized Question-Image\n  Relation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QIRL: Boosting Visual Question Answering via Optimized Question-Image\n  Relation Learning"
                },
                "summary": "Existing debiasing approaches in Visual Question Answering (VQA) primarily\nfocus on enhancing visual learning, integrating auxiliary models, or employing\ndata augmentation strategies. However, these methods exhibit two major\ndrawbacks. First, current debiasing techniques fail to capture the superior\nrelation between images and texts because prevalent learning frameworks do not\nenable models to extract deeper correlations from highly contrasting samples.\nSecond, they do not assess the relevance between the input question and image\nduring inference, as no prior work has examined the degree of input relevance\nin debiasing studies. Motivated by these limitations, we propose a novel\nframework, Optimized Question-Image Relation Learning (QIRL), which employs a\ngeneration-based self-supervised learning strategy. Specifically, two modules\nare introduced to address the aforementioned issues. The Negative Image\nGeneration (NIG) module automatically produces highly irrelevant question-image\npairs during training to enhance correlation learning, while the Irrelevant\nSample Identification (ISI) module improves model robustness by detecting and\nfiltering irrelevant inputs, thereby reducing prediction errors. Furthermore,\nto validate our concept of reducing output errors through filtering unrelated\nquestion-image inputs, we propose a specialized metric to evaluate the\nperformance of the ISI module. Notably, our approach is model-agnostic and can\nbe integrated with various VQA models. Extensive experiments on VQA-CPv2 and\nVQA-v2 demonstrate the effectiveness and generalization ability of our method.\nAmong data augmentation strategies, our approach achieves state-of-the-art\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing debiasing approaches in Visual Question Answering (VQA) primarily\nfocus on enhancing visual learning, integrating auxiliary models, or employing\ndata augmentation strategies. However, these methods exhibit two major\ndrawbacks. First, current debiasing techniques fail to capture the superior\nrelation between images and texts because prevalent learning frameworks do not\nenable models to extract deeper correlations from highly contrasting samples.\nSecond, they do not assess the relevance between the input question and image\nduring inference, as no prior work has examined the degree of input relevance\nin debiasing studies. Motivated by these limitations, we propose a novel\nframework, Optimized Question-Image Relation Learning (QIRL), which employs a\ngeneration-based self-supervised learning strategy. Specifically, two modules\nare introduced to address the aforementioned issues. The Negative Image\nGeneration (NIG) module automatically produces highly irrelevant question-image\npairs during training to enhance correlation learning, while the Irrelevant\nSample Identification (ISI) module improves model robustness by detecting and\nfiltering irrelevant inputs, thereby reducing prediction errors. Furthermore,\nto validate our concept of reducing output errors through filtering unrelated\nquestion-image inputs, we propose a specialized metric to evaluate the\nperformance of the ISI module. Notably, our approach is model-agnostic and can\nbe integrated with various VQA models. Extensive experiments on VQA-CPv2 and\nVQA-v2 demonstrate the effectiveness and generalization ability of our method.\nAmong data augmentation strategies, our approach achieves state-of-the-art\nresults."
                },
                "authors": [
                    {
                        "name": "Quanxing Xu"
                    },
                    {
                        "name": "Ling Zhou"
                    },
                    {
                        "name": "Xian Zhong"
                    },
                    {
                        "name": "Feifei Zhang"
                    },
                    {
                        "name": "Rubing Huang"
                    },
                    {
                        "name": "Chia-Wen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chia-Wen Lin"
                },
                "author": "Chia-Wen Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07838v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07838v3",
                "updated": "2025-04-04T10:37:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    37,
                    36,
                    4,
                    94,
                    0
                ],
                "published": "2024-10-10T11:56:09Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    56,
                    9,
                    3,
                    284,
                    0
                ],
                "title": "Minority-Focused Text-to-Image Generation via Prompt Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minority-Focused Text-to-Image Generation via Prompt Optimization"
                },
                "summary": "We investigate the generation of minority samples using pretrained\ntext-to-image (T2I) latent diffusion models. Minority instances, in the context\nof T2I generation, can be defined as ones living on low-density regions of\ntext-conditional data distributions. They are valuable for various applications\nof modern T2I generators, such as data augmentation and creative AI.\nUnfortunately, existing pretrained T2I diffusion models primarily focus on\nhigh-density regions, largely due to the influence of guided samplers (like\nCFG) that are essential for high-quality generation. To address this, we\npresent a novel framework to counter the high-density-focus of T2I diffusion\nmodels. Specifically, we first develop an online prompt optimization framework\nthat encourages emergence of desired properties during inference while\npreserving semantic contents of user-provided prompts. We subsequently tailor\nthis generic prompt optimizer into a specialized solver that promotes\ngeneration of minority features by incorporating a carefully-crafted likelihood\nobjective. Extensive experiments conducted across various types of T2I models\ndemonstrate that our approach significantly enhances the capability to produce\nhigh-quality minority instances compared to existing samplers. Code is\navailable at https://github.com/soobin-um/MinorityPrompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the generation of minority samples using pretrained\ntext-to-image (T2I) latent diffusion models. Minority instances, in the context\nof T2I generation, can be defined as ones living on low-density regions of\ntext-conditional data distributions. They are valuable for various applications\nof modern T2I generators, such as data augmentation and creative AI.\nUnfortunately, existing pretrained T2I diffusion models primarily focus on\nhigh-density regions, largely due to the influence of guided samplers (like\nCFG) that are essential for high-quality generation. To address this, we\npresent a novel framework to counter the high-density-focus of T2I diffusion\nmodels. Specifically, we first develop an online prompt optimization framework\nthat encourages emergence of desired properties during inference while\npreserving semantic contents of user-provided prompts. We subsequently tailor\nthis generic prompt optimizer into a specialized solver that promotes\ngeneration of minority features by incorporating a carefully-crafted likelihood\nobjective. Extensive experiments conducted across various types of T2I models\ndemonstrate that our approach significantly enhances the capability to produce\nhigh-quality minority instances compared to existing samplers. Code is\navailable at https://github.com/soobin-um/MinorityPrompt."
                },
                "authors": [
                    {
                        "name": "Soobin Um"
                    },
                    {
                        "name": "Jong Chul Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jong Chul Ye"
                },
                "author": "Jong Chul Ye",
                "arxiv_comment": "CVPR 2025 (Oral), 21 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07838v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07838v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03333v1",
                "updated": "2025-04-04T10:29:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    29,
                    6,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T10:29:06Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    29,
                    6,
                    4,
                    94,
                    0
                ],
                "title": "NucleiML: A machine learning framework of ground-state properties of\n  finite nuclei for accelerated Bayesian exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NucleiML: A machine learning framework of ground-state properties of\n  finite nuclei for accelerated Bayesian exploration"
                },
                "summary": "The global behavior of the nuclear equation of state (EoS) is usually\ninvestigated using finite nuclei (FN) data, along with constraints from\nheavy-ion collisions and astrophysical observations of neutron star (NS)\nproperties. The FN constraints explicitly imposed through the binding energies\nand charge radii of selected nuclei are found to significantly affect the EoS\nacross different densities. However, the high computational cost of these\nconstraints makes it challenging to extend the analysis to a broader set of\nnuclei, particularly when the objective is not merely to obtain a single\noptimized model but to systematically explore uncertainties in global modeling.\nTo overcome this challenge, we introduce NucleiML (NML), a machine learning\nframework trained on ground-state FN properties obtained from mean-field\nmodels. Integrated into a Bayesian inference approach, NML demonstrates high\naccuracy and strong consistency with the underlying mean-field model. The NML\nachieves around ten-fold computational speed-up, from $\\sim 4.5$ hours to 30\nminutes. Its predictive performance improves further as the number of nuclei in\nthe training data increases, which we plan to employ in extensive future\nexplorations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The global behavior of the nuclear equation of state (EoS) is usually\ninvestigated using finite nuclei (FN) data, along with constraints from\nheavy-ion collisions and astrophysical observations of neutron star (NS)\nproperties. The FN constraints explicitly imposed through the binding energies\nand charge radii of selected nuclei are found to significantly affect the EoS\nacross different densities. However, the high computational cost of these\nconstraints makes it challenging to extend the analysis to a broader set of\nnuclei, particularly when the objective is not merely to obtain a single\noptimized model but to systematically explore uncertainties in global modeling.\nTo overcome this challenge, we introduce NucleiML (NML), a machine learning\nframework trained on ground-state FN properties obtained from mean-field\nmodels. Integrated into a Bayesian inference approach, NML demonstrates high\naccuracy and strong consistency with the underlying mean-field model. The NML\nachieves around ten-fold computational speed-up, from $\\sim 4.5$ hours to 30\nminutes. Its predictive performance improves further as the number of nuclei in\nthe training data increases, which we plan to employ in extensive future\nexplorations."
                },
                "authors": [
                    {
                        "name": "Anagh Venneti"
                    },
                    {
                        "name": "Chiranjib Mondal"
                    },
                    {
                        "name": "Sk Md Adil Imam"
                    },
                    {
                        "name": "Sarmistha Banik"
                    },
                    {
                        "name": "Bijay K. Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Bijay K. Agrawal"
                },
                "author": "Bijay K. Agrawal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03331v1",
                "updated": "2025-04-04T10:23:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    23,
                    22,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T10:23:22Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    23,
                    22,
                    4,
                    94,
                    0
                ],
                "title": "The Galactic-Centre Arms inferred from ACES (ALMA CMZ Exploration\n  Survey)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Galactic-Centre Arms inferred from ACES (ALMA CMZ Exploration\n  Survey)"
                },
                "summary": "Analyzing longitude-velocity diagrams (LVDs) in the CS(J=2-1) and\nH13CN(J=1-0) molecular lines from the internal release data of the ALMA\nCentral-Molecular-Zone Exploration Survey (ACES) and in the 13CO (J=1-0) line\nfrom the Nobeyama Galactic-Centre (GC) survey, we identify six GC Arms as\nprominent straight LV ridges. In addition to the currently known Arms I to IV,\nwe identify a new inner arm, Arm V, and further highlight the circum-nuclear\ndisc (CND) as Arm VI. Integrated intensity maps of the Arms on the sky suggest\nthat most of the Arms compose ring-like structures inclined from the Galactic\nplane. We determine the radii (curvatures) of the Arms using the\nvelocity-gradient ($dv/dl$) method, assuming that the arms are rotating on\ncircular orbits at a constant velocity of $\\sim 150$ km/s. We show that Arms I\nand II compose the main ring structure of the CMZ with radii $\\sim 100$--120\npc; Arm III is a dense arm 42 pc from the GC; Arm IV is a clear and narrow arm\n20 pc from the GC; and Arm V is a faint, long arm of 8.2 pc radius. We show\nthat the circum-nuclear disc (CND) composes the sixth arm, Arm VI, of radius\n$\\sim 2.3$ pc associated with bifurcated spiral fins. We also discuss the\nassociation of the 20- and 50-km/s clouds with these Arms. The radii of the\narms fall on an empirical relation $R\\sim 630 (2/5)^N$ for $N=1$ (Arm I) to 6\n(VI), suggesting either discrete rings or a logarithmic spiral with pitch angle\n$\\sim 22^\\circ$. The vertical full extent of the arm increases with radius and\nis represented by $z\\sim 0.7 (R/1 {\\rm pc})^{0.7}$ pc. The tilt angle of the\narms from the Galactic plane, or the warping, increases rapidly toward the GC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing longitude-velocity diagrams (LVDs) in the CS(J=2-1) and\nH13CN(J=1-0) molecular lines from the internal release data of the ALMA\nCentral-Molecular-Zone Exploration Survey (ACES) and in the 13CO (J=1-0) line\nfrom the Nobeyama Galactic-Centre (GC) survey, we identify six GC Arms as\nprominent straight LV ridges. In addition to the currently known Arms I to IV,\nwe identify a new inner arm, Arm V, and further highlight the circum-nuclear\ndisc (CND) as Arm VI. Integrated intensity maps of the Arms on the sky suggest\nthat most of the Arms compose ring-like structures inclined from the Galactic\nplane. We determine the radii (curvatures) of the Arms using the\nvelocity-gradient ($dv/dl$) method, assuming that the arms are rotating on\ncircular orbits at a constant velocity of $\\sim 150$ km/s. We show that Arms I\nand II compose the main ring structure of the CMZ with radii $\\sim 100$--120\npc; Arm III is a dense arm 42 pc from the GC; Arm IV is a clear and narrow arm\n20 pc from the GC; and Arm V is a faint, long arm of 8.2 pc radius. We show\nthat the circum-nuclear disc (CND) composes the sixth arm, Arm VI, of radius\n$\\sim 2.3$ pc associated with bifurcated spiral fins. We also discuss the\nassociation of the 20- and 50-km/s clouds with these Arms. The radii of the\narms fall on an empirical relation $R\\sim 630 (2/5)^N$ for $N=1$ (Arm I) to 6\n(VI), suggesting either discrete rings or a logarithmic spiral with pitch angle\n$\\sim 22^\\circ$. The vertical full extent of the arm increases with radius and\nis represented by $z\\sim 0.7 (R/1 {\\rm pc})^{0.7}$ pc. The tilt angle of the\narms from the Galactic plane, or the warping, increases rapidly toward the GC."
                },
                "authors": [
                    {
                        "name": "Y. Sofue"
                    },
                    {
                        "name": "Tomo. Oka"
                    },
                    {
                        "name": "S. N. Longmore"
                    },
                    {
                        "name": "D. Walker"
                    },
                    {
                        "name": "A. Ginsburg"
                    },
                    {
                        "name": "J. D. Henshaw"
                    },
                    {
                        "name": "J. Bally"
                    },
                    {
                        "name": "A. T. Barne"
                    },
                    {
                        "name": "C. Battersby"
                    },
                    {
                        "name": "L. Colzi"
                    },
                    {
                        "name": "P. Ho"
                    },
                    {
                        "name": "I. Jimenez-Serra"
                    },
                    {
                        "name": "J. M. D. Kruijssen"
                    },
                    {
                        "name": "E. Mills"
                    },
                    {
                        "name": "M. A. Petkova"
                    },
                    {
                        "name": "M. C. Sormani"
                    },
                    {
                        "name": "J. Wallace"
                    },
                    {
                        "name": "J. Armijos-Abendano"
                    },
                    {
                        "name": "K. M. Dutkowska"
                    },
                    {
                        "name": "R. Enokiya"
                    },
                    {
                        "name": "Y. Fukui"
                    },
                    {
                        "name": "P. Garcia"
                    },
                    {
                        "name": "A. Guzman"
                    },
                    {
                        "name": "C. Henkel"
                    },
                    {
                        "name": "P. -Y. Hsieh"
                    },
                    {
                        "name": "Y. Hu"
                    },
                    {
                        "name": "K. Immer"
                    },
                    {
                        "name": "D. Jeff"
                    },
                    {
                        "name": "R. S. Klessen"
                    },
                    {
                        "name": "K. Kohno"
                    },
                    {
                        "name": "M. R. Krumholz"
                    },
                    {
                        "name": "D. Lipman"
                    },
                    {
                        "name": "S. Martin"
                    },
                    {
                        "name": "M. R. Morris"
                    },
                    {
                        "name": "F. Nogueras-Lara"
                    },
                    {
                        "name": "M. Nonhebel"
                    },
                    {
                        "name": "J. Otto"
                    },
                    {
                        "name": "J. E. Pineda"
                    },
                    {
                        "name": "M. A. Requena-Torres"
                    },
                    {
                        "name": "V. M. Rivilla"
                    },
                    {
                        "name": "D. Riquelme-Vasquez"
                    },
                    {
                        "name": "A. Sanchez-Monge"
                    },
                    {
                        "name": "M. G. Santa-Maria"
                    },
                    {
                        "name": "H. A. Smith"
                    },
                    {
                        "name": "T. S. Tanvir"
                    },
                    {
                        "name": "V. Tolls"
                    },
                    {
                        "name": "Q. D. Wang"
                    }
                ],
                "author_detail": {
                    "name": "Q. D. Wang"
                },
                "author": "Q. D. Wang",
                "arxiv_comment": "Accepted for PASJ, 20 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03312v1",
                "updated": "2025-04-04T09:47:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    9,
                    47,
                    58,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T09:47:58Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    9,
                    47,
                    58,
                    4,
                    94,
                    0
                ],
                "title": "Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User\n  Devices"
                },
                "summary": "Large Language Models have significantly advanced natural language\nprocessing, achieving remarkable performance in tasks such as language\ngeneration, translation, and reasoning. However, their substantial\ncomputational requirements restrict deployment to high-end systems, limiting\naccessibility on consumer-grade devices. This challenge is especially\npronounced for under-resourced languages like those spoken in the Iberian\nPeninsula, where relatively limited linguistic resources and benchmarks hinder\neffective evaluation. This work presents a comprehensive evaluation of compact\nstate-of-the-art LLMs across several essential NLP tasks tailored for Iberian\nlanguages. The results reveal that while some models consistently excel in\ncertain tasks, significant performance gaps remain, particularly for languages\nsuch as Basque. These findings highlight the need for further research on\nbalancing model compactness with robust multilingual performance",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have significantly advanced natural language\nprocessing, achieving remarkable performance in tasks such as language\ngeneration, translation, and reasoning. However, their substantial\ncomputational requirements restrict deployment to high-end systems, limiting\naccessibility on consumer-grade devices. This challenge is especially\npronounced for under-resourced languages like those spoken in the Iberian\nPeninsula, where relatively limited linguistic resources and benchmarks hinder\neffective evaluation. This work presents a comprehensive evaluation of compact\nstate-of-the-art LLMs across several essential NLP tasks tailored for Iberian\nlanguages. The results reveal that while some models consistently excel in\ncertain tasks, significant performance gaps remain, particularly for languages\nsuch as Basque. These findings highlight the need for further research on\nbalancing model compactness with robust multilingual performance"
                },
                "authors": [
                    {
                        "name": "Lus Couto Seller"
                    },
                    {
                        "name": "igo Sanz Torres"
                    },
                    {
                        "name": "Adrin Vogel-Fernndez"
                    },
                    {
                        "name": "Carlos Gonzlez Carballo"
                    },
                    {
                        "name": "Pedro Miguel Snchez Snchez"
                    },
                    {
                        "name": "Adrin Carruana Martn"
                    },
                    {
                        "name": "Enrique de Miguel Ambite"
                    }
                ],
                "author_detail": {
                    "name": "Enrique de Miguel Ambite"
                },
                "author": "Enrique de Miguel Ambite",
                "arxiv_comment": "Under Revision al SEPLN conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03302v1",
                "updated": "2025-04-04T09:27:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    9,
                    27,
                    19,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T09:27:19Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    9,
                    27,
                    19,
                    4,
                    94,
                    0
                ],
                "title": "Noise Augmented Fine Tuning for Mitigating Hallucinations in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise Augmented Fine Tuning for Mitigating Hallucinations in Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) often produce inaccurate or misleading\ncontent-hallucinations. To address this challenge, we introduce Noise-Augmented\nFine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise\ninjection based on the signal-to-noise ratio (SNR) to enhance model robustness.\nIn particular, NoiseFiT selectively perturbs layers identified as either\nhigh-SNR (more robust) or low-SNR (potentially under-regularized) using a\ndynamically scaled Gaussian noise. We further propose a hybrid loss that\ncombines standard cross-entropy, soft cross-entropy, and consistency\nregularization to ensure stable and accurate outputs under noisy training\nconditions. Our theoretical analysis shows that adaptive noise injection is\nboth unbiased and variance-preserving, providing strong guarantees for\nconvergence in expectation. Empirical results on multiple test and benchmark\ndatasets demonstrate that NoiseFiT significantly reduces hallucination rates,\noften improving or matching baseline performance in key tasks. These findings\nhighlight the promise of noise-driven strategies for achieving robust,\ntrustworthy language modeling without incurring prohibitive computational\noverhead. Given the comprehensive and detailed nature of our experiments, we\nhave publicly released the fine-tuning logs, benchmark evaluation artifacts,\nand source code online at W&B, Hugging Face, and GitHub, respectively, to\nfoster further research, accessibility and reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often produce inaccurate or misleading\ncontent-hallucinations. To address this challenge, we introduce Noise-Augmented\nFine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise\ninjection based on the signal-to-noise ratio (SNR) to enhance model robustness.\nIn particular, NoiseFiT selectively perturbs layers identified as either\nhigh-SNR (more robust) or low-SNR (potentially under-regularized) using a\ndynamically scaled Gaussian noise. We further propose a hybrid loss that\ncombines standard cross-entropy, soft cross-entropy, and consistency\nregularization to ensure stable and accurate outputs under noisy training\nconditions. Our theoretical analysis shows that adaptive noise injection is\nboth unbiased and variance-preserving, providing strong guarantees for\nconvergence in expectation. Empirical results on multiple test and benchmark\ndatasets demonstrate that NoiseFiT significantly reduces hallucination rates,\noften improving or matching baseline performance in key tasks. These findings\nhighlight the promise of noise-driven strategies for achieving robust,\ntrustworthy language modeling without incurring prohibitive computational\noverhead. Given the comprehensive and detailed nature of our experiments, we\nhave publicly released the fine-tuning logs, benchmark evaluation artifacts,\nand source code online at W&B, Hugging Face, and GitHub, respectively, to\nfoster further research, accessibility and reproducibility."
                },
                "authors": [
                    {
                        "name": "Afshin Khadangi"
                    },
                    {
                        "name": "Amir Sartipi"
                    },
                    {
                        "name": "Igor Tchappi"
                    },
                    {
                        "name": "Ramin Bahmani"
                    }
                ],
                "author_detail": {
                    "name": "Ramin Bahmani"
                },
                "author": "Ramin Bahmani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03295v1",
                "updated": "2025-04-04T09:20:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    9,
                    20,
                    19,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T09:20:19Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    9,
                    20,
                    19,
                    4,
                    94,
                    0
                ],
                "title": "Stance-Driven Multimodal Controlled Statement Generation: New Dataset\n  and Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance-Driven Multimodal Controlled Statement Generation: New Dataset\n  and Task"
                },
                "summary": "Formulating statements that support diverse or controversial stances on\nspecific topics is vital for platforms that enable user expression, reshape\npolitical discourse, and drive social critique and information dissemination.\nWith the rise of Large Language Models (LLMs), controllable text generation\ntowards specific stances has become a promising research area with applications\nin shaping public opinion and commercial marketing. However, current datasets\noften focus solely on pure texts, lacking multimodal content and effective\ncontext, particularly in the context of stance detection. In this paper, we\nformally define and study the new problem of stance-driven controllable content\ngeneration for tweets with text and images, where given a multimodal post (text\nand image/video), a model generates a stance-controlled response. To this end,\nwe create the Multimodal Stance Generation Dataset (StanceGen2024), the first\nresource explicitly designed for multimodal stance-controllable text generation\nin political discourse. It includes posts and user comments from the 2024 U.S.\npresidential election, featuring text, images, videos, and stance annotations\nto explore how multimodal political content shapes stance expression.\nFurthermore, we propose a Stance-Driven Multimodal Generation (SDMG) framework\nthat integrates weighted fusion of multimodal features and stance guidance to\nimprove semantic consistency and stance control. We release the dataset and\ncode (https://anonymous.4open.science/r/StanceGen-BE9D) for public use and\nfurther research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formulating statements that support diverse or controversial stances on\nspecific topics is vital for platforms that enable user expression, reshape\npolitical discourse, and drive social critique and information dissemination.\nWith the rise of Large Language Models (LLMs), controllable text generation\ntowards specific stances has become a promising research area with applications\nin shaping public opinion and commercial marketing. However, current datasets\noften focus solely on pure texts, lacking multimodal content and effective\ncontext, particularly in the context of stance detection. In this paper, we\nformally define and study the new problem of stance-driven controllable content\ngeneration for tweets with text and images, where given a multimodal post (text\nand image/video), a model generates a stance-controlled response. To this end,\nwe create the Multimodal Stance Generation Dataset (StanceGen2024), the first\nresource explicitly designed for multimodal stance-controllable text generation\nin political discourse. It includes posts and user comments from the 2024 U.S.\npresidential election, featuring text, images, videos, and stance annotations\nto explore how multimodal political content shapes stance expression.\nFurthermore, we propose a Stance-Driven Multimodal Generation (SDMG) framework\nthat integrates weighted fusion of multimodal features and stance guidance to\nimprove semantic consistency and stance control. We release the dataset and\ncode (https://anonymous.4open.science/r/StanceGen-BE9D) for public use and\nfurther research."
                },
                "authors": [
                    {
                        "name": "Bingqian Wang"
                    },
                    {
                        "name": "Quan Fang"
                    },
                    {
                        "name": "Jiachen Sun"
                    },
                    {
                        "name": "Xiaoxiao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxiao Ma"
                },
                "author": "Xiaoxiao Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03274v1",
                "updated": "2025-04-04T08:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    8,
                    48,
                    43,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T08:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    8,
                    48,
                    43,
                    4,
                    94,
                    0
                ],
                "title": "Do Large Language Models Solve the Problems of Agent-Based Modeling? A\n  Critical Review of Generative Social Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Solve the Problems of Agent-Based Modeling? A\n  Critical Review of Generative Social Simulations"
                },
                "summary": "Recent advancements in AI have reinvigorated Agent-Based Models (ABMs), as\nthe integration of Large Language Models (LLMs) has led to the emergence of\n``generative ABMs'' as a novel approach to simulating social systems. While\nABMs offer means to bridge micro-level interactions with macro-level patterns,\nthey have long faced criticisms from social scientists, pointing to e.g., lack\nof realism, computational complexity, and challenges of calibrating and\nvalidating against empirical data. This paper reviews the generative ABM\nliterature to assess how this new approach adequately addresses these\nlong-standing criticisms. Our findings show that studies show limited awareness\nof historical debates. Validation remains poorly addressed, with many studies\nrelying solely on subjective assessments of model `believability', and even the\nmost rigorous validation failing to adequately evidence operational validity.\nWe argue that there are reasons to believe that LLMs will exacerbate rather\nthan resolve the long-standing challenges of ABMs. The black-box nature of LLMs\nmoreover limit their usefulness for disentangling complex emergent causal\nmechanisms. While generative ABMs are still in a stage of early\nexperimentation, these findings question of whether and how the field can\ntransition to the type of rigorous modeling needed to contribute to social\nscientific theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in AI have reinvigorated Agent-Based Models (ABMs), as\nthe integration of Large Language Models (LLMs) has led to the emergence of\n``generative ABMs'' as a novel approach to simulating social systems. While\nABMs offer means to bridge micro-level interactions with macro-level patterns,\nthey have long faced criticisms from social scientists, pointing to e.g., lack\nof realism, computational complexity, and challenges of calibrating and\nvalidating against empirical data. This paper reviews the generative ABM\nliterature to assess how this new approach adequately addresses these\nlong-standing criticisms. Our findings show that studies show limited awareness\nof historical debates. Validation remains poorly addressed, with many studies\nrelying solely on subjective assessments of model `believability', and even the\nmost rigorous validation failing to adequately evidence operational validity.\nWe argue that there are reasons to believe that LLMs will exacerbate rather\nthan resolve the long-standing challenges of ABMs. The black-box nature of LLMs\nmoreover limit their usefulness for disentangling complex emergent causal\nmechanisms. While generative ABMs are still in a stage of early\nexperimentation, these findings question of whether and how the field can\ntransition to the type of rigorous modeling needed to contribute to social\nscientific theory."
                },
                "authors": [
                    {
                        "name": "Maik Larooij"
                    },
                    {
                        "name": "Petter Trnberg"
                    }
                ],
                "author_detail": {
                    "name": "Petter Trnberg"
                },
                "author": "Petter Trnberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15316v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15316v3",
                "updated": "2025-04-04T08:29:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    8,
                    29,
                    19,
                    4,
                    94,
                    0
                ],
                "published": "2024-10-20T07:03:49Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    7,
                    3,
                    49,
                    6,
                    294,
                    0
                ],
                "title": "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their application to speech-based tasks remains challenging due to the\ncomplexities of integrating audio and text modalities. This paper introduces\nIchigo, a mixed-modal model that seamlessly processes interleaved sequences of\nspeech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes\nspeech into discrete tokens and employs a uniform transformer-based\narchitecture for both speech and text modalities. This method enables joint\nreasoning and generation across modalities without the need for separate\nadapters. We present a comprehensive training methodology, including\npre-training on multilingual speech recognition datasets and fine-tuning on a\ncurated instruction dataset. Ichigo demonstrates state-of-the-art performance\non speech question-answering benchmarks, outperforming existing open-source\nspeech language models and achieving comparable results to cascaded systems.\nNotably, Ichigo exhibits a latency of just 111 ms to first token generation,\nsignificantly lower than current models. Our approach not only advances the\nfield of multimodal AI but also provides a framework for smaller research teams\nto contribute effectively to open-source speech-language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their application to speech-based tasks remains challenging due to the\ncomplexities of integrating audio and text modalities. This paper introduces\nIchigo, a mixed-modal model that seamlessly processes interleaved sequences of\nspeech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes\nspeech into discrete tokens and employs a uniform transformer-based\narchitecture for both speech and text modalities. This method enables joint\nreasoning and generation across modalities without the need for separate\nadapters. We present a comprehensive training methodology, including\npre-training on multilingual speech recognition datasets and fine-tuning on a\ncurated instruction dataset. Ichigo demonstrates state-of-the-art performance\non speech question-answering benchmarks, outperforming existing open-source\nspeech language models and achieving comparable results to cascaded systems.\nNotably, Ichigo exhibits a latency of just 111 ms to first token generation,\nsignificantly lower than current models. Our approach not only advances the\nfield of multimodal AI but also provides a framework for smaller research teams\nto contribute effectively to open-source speech-language models."
                },
                "authors": [
                    {
                        "name": "Alan Dao"
                    },
                    {
                        "name": "Dinh Bach Vu"
                    },
                    {
                        "name": "Huy Hoang Ha"
                    }
                ],
                "author_detail": {
                    "name": "Huy Hoang Ha"
                },
                "arxiv_affiliation": "Gia Tuan Dao",
                "author": "Huy Hoang Ha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15316v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15316v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03258v1",
                "updated": "2025-04-04T08:18:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    8,
                    18,
                    48,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T08:18:48Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    8,
                    18,
                    48,
                    4,
                    94,
                    0
                ],
                "title": "TQD-Track: Temporal Query Denoising for 3D Multi-Object Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TQD-Track: Temporal Query Denoising for 3D Multi-Object Tracking"
                },
                "summary": "Query denoising has become a standard training strategy for DETR-based\ndetectors by addressing the slow convergence issue. Besides that, query\ndenoising can be used to increase the diversity of training samples for\nmodeling complex scenarios which is critical for Multi-Object Tracking (MOT),\nshowing its potential in MOT application. Existing approaches integrate query\ndenoising within the tracking-by-attention paradigm. However, as the denoising\nprocess only happens within the single frame, it cannot benefit the tracker to\nlearn temporal-related information. In addition, the attention mask in query\ndenoising prevents information exchange between denoising and object queries,\nlimiting its potential in improving association using self-attention. To\naddress these issues, we propose TQD-Track, which introduces Temporal Query\nDenoising (TQD) tailored for MOT, enabling denoising queries to carry temporal\ninformation and instance-specific feature representation. We introduce diverse\nnoise types onto denoising queries that simulate real-world challenges in MOT.\nWe analyze our proposed TQD for different tracking paradigms, and find out the\nparadigm with explicit learned data association module, e.g.\ntracking-by-detection or alternating detection and association, benefit from\nTQD by a larger margin. For these paradigms, we further design an association\nmask in the association module to ensure the consistent interaction between\ntrack and detection queries as during inference. Extensive experiments on the\nnuScenes dataset demonstrate that our approach consistently enhances different\ntracking methods by only changing the training process, especially the\nparadigms with explicit association module.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query denoising has become a standard training strategy for DETR-based\ndetectors by addressing the slow convergence issue. Besides that, query\ndenoising can be used to increase the diversity of training samples for\nmodeling complex scenarios which is critical for Multi-Object Tracking (MOT),\nshowing its potential in MOT application. Existing approaches integrate query\ndenoising within the tracking-by-attention paradigm. However, as the denoising\nprocess only happens within the single frame, it cannot benefit the tracker to\nlearn temporal-related information. In addition, the attention mask in query\ndenoising prevents information exchange between denoising and object queries,\nlimiting its potential in improving association using self-attention. To\naddress these issues, we propose TQD-Track, which introduces Temporal Query\nDenoising (TQD) tailored for MOT, enabling denoising queries to carry temporal\ninformation and instance-specific feature representation. We introduce diverse\nnoise types onto denoising queries that simulate real-world challenges in MOT.\nWe analyze our proposed TQD for different tracking paradigms, and find out the\nparadigm with explicit learned data association module, e.g.\ntracking-by-detection or alternating detection and association, benefit from\nTQD by a larger margin. For these paradigms, we further design an association\nmask in the association module to ensure the consistent interaction between\ntrack and detection queries as during inference. Extensive experiments on the\nnuScenes dataset demonstrate that our approach consistently enhances different\ntracking methods by only changing the training process, especially the\nparadigms with explicit association module."
                },
                "authors": [
                    {
                        "name": "Shuxiao Ding"
                    },
                    {
                        "name": "Yutong Yang"
                    },
                    {
                        "name": "Julian Wiederer"
                    },
                    {
                        "name": "Markus Braun"
                    },
                    {
                        "name": "Peizheng Li"
                    },
                    {
                        "name": "Juergen Gall"
                    },
                    {
                        "name": "Bin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Yang"
                },
                "author": "Bin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19207v2",
                "updated": "2025-04-04T08:17:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    8,
                    17,
                    8,
                    4,
                    94,
                    0
                ],
                "published": "2025-03-24T23:20:47Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    23,
                    20,
                    47,
                    0,
                    83,
                    0
                ],
                "title": "FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from\n  Few Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from\n  Few Images"
                },
                "summary": "We present a novel method for reconstructing personalized 3D human avatars\nwith realistic animation from only a few images. Due to the large variations in\nbody shapes, poses, and cloth types, existing methods mostly require hours of\nper-subject optimization during inference, which limits their practical\napplications. In contrast, we learn a universal prior from over a thousand\nclothed humans to achieve instant feedforward generation and zero-shot\ngeneralization. Specifically, instead of rigging the avatar with shared\nskinning weights, we jointly infer personalized avatar shape, skinning weights,\nand pose-dependent deformations, which effectively improves overall geometric\nfidelity and reduces deformation artifacts. Moreover, to normalize pose\nvariations and resolve coupled ambiguity between canonical shapes and skinning\nweights, we design a 3D canonicalization process to produce pixel-aligned\ninitial conditions, which helps to reconstruct fine-grained geometric details.\nWe then propose a multi-frame feature aggregation to robustly reduce artifacts\nintroduced in canonicalization and fuse a plausible avatar preserving\nperson-specific identities. Finally, we train the model in an end-to-end\nframework on a large-scale capture dataset, which contains diverse human\nsubjects paired with high-quality 3D scans. Extensive experiments show that our\nmethod generates more authentic reconstruction and animation than\nstate-of-the-arts, and can be directly generalized to inputs from casually\ntaken phone photos. Project page and code is available at\nhttps://github.com/rongakowang/FRESA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel method for reconstructing personalized 3D human avatars\nwith realistic animation from only a few images. Due to the large variations in\nbody shapes, poses, and cloth types, existing methods mostly require hours of\nper-subject optimization during inference, which limits their practical\napplications. In contrast, we learn a universal prior from over a thousand\nclothed humans to achieve instant feedforward generation and zero-shot\ngeneralization. Specifically, instead of rigging the avatar with shared\nskinning weights, we jointly infer personalized avatar shape, skinning weights,\nand pose-dependent deformations, which effectively improves overall geometric\nfidelity and reduces deformation artifacts. Moreover, to normalize pose\nvariations and resolve coupled ambiguity between canonical shapes and skinning\nweights, we design a 3D canonicalization process to produce pixel-aligned\ninitial conditions, which helps to reconstruct fine-grained geometric details.\nWe then propose a multi-frame feature aggregation to robustly reduce artifacts\nintroduced in canonicalization and fuse a plausible avatar preserving\nperson-specific identities. Finally, we train the model in an end-to-end\nframework on a large-scale capture dataset, which contains diverse human\nsubjects paired with high-quality 3D scans. Extensive experiments show that our\nmethod generates more authentic reconstruction and animation than\nstate-of-the-arts, and can be directly generalized to inputs from casually\ntaken phone photos. Project page and code is available at\nhttps://github.com/rongakowang/FRESA."
                },
                "authors": [
                    {
                        "name": "Rong Wang"
                    },
                    {
                        "name": "Fabian Prada"
                    },
                    {
                        "name": "Ziyan Wang"
                    },
                    {
                        "name": "Zhongshi Jiang"
                    },
                    {
                        "name": "Chengxiang Yin"
                    },
                    {
                        "name": "Junxuan Li"
                    },
                    {
                        "name": "Shunsuke Saito"
                    },
                    {
                        "name": "Igor Santesteban"
                    },
                    {
                        "name": "Javier Romero"
                    },
                    {
                        "name": "Rohan Joshi"
                    },
                    {
                        "name": "Hongdong Li"
                    },
                    {
                        "name": "Jason Saragih"
                    },
                    {
                        "name": "Yaser Sheikh"
                    }
                ],
                "author_detail": {
                    "name": "Yaser Sheikh"
                },
                "author": "Yaser Sheikh",
                "arxiv_comment": "Published in CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03255v1",
                "updated": "2025-04-04T08:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    8,
                    10,
                    2,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T08:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    8,
                    10,
                    2,
                    4,
                    94,
                    0
                ],
                "title": "Inherent and emergent liability issues in LLM-based agentic systems: a\n  principal-agent perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inherent and emergent liability issues in LLM-based agentic systems: a\n  principal-agent perspective"
                },
                "summary": "Agentic systems powered by large language models (LLMs) are becoming\nprogressively more complex and capable. Their increasing agency and expanding\ndeployment settings attract growing attention over effective governance\npolicies, monitoring and control protocols. Based on emerging landscapes of the\nagentic market, we analyze the potential liability issues stemming from\ndelegated use of LLM agents and their extended systems from a principal-agent\nperspective. Our analysis complements existing risk-based studies on artificial\nagency and covers the spectrum of important aspects of the principal-agent\nrelationship and their potential consequences at deployment. Furthermore, we\nmotivate method developments for technical governance along the directions of\ninterpretability and behavior evaluations, reward and conflict management, and\nthe mitigation of misalignment and misconduct through principled engineering of\ndetection and fail-safe mechanisms. By illustrating the outstanding issues in\nAI liability for LLM-based agentic systems, we aim to inform the system design,\nauditing and monitoring approaches to enhancing transparency and\naccountability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic systems powered by large language models (LLMs) are becoming\nprogressively more complex and capable. Their increasing agency and expanding\ndeployment settings attract growing attention over effective governance\npolicies, monitoring and control protocols. Based on emerging landscapes of the\nagentic market, we analyze the potential liability issues stemming from\ndelegated use of LLM agents and their extended systems from a principal-agent\nperspective. Our analysis complements existing risk-based studies on artificial\nagency and covers the spectrum of important aspects of the principal-agent\nrelationship and their potential consequences at deployment. Furthermore, we\nmotivate method developments for technical governance along the directions of\ninterpretability and behavior evaluations, reward and conflict management, and\nthe mitigation of misalignment and misconduct through principled engineering of\ndetection and fail-safe mechanisms. By illustrating the outstanding issues in\nAI liability for LLM-based agentic systems, we aim to inform the system design,\nauditing and monitoring approaches to enhancing transparency and\naccountability."
                },
                "authors": [
                    {
                        "name": "Garry A. Gabison"
                    },
                    {
                        "name": "R. Patrick Xian"
                    }
                ],
                "author_detail": {
                    "name": "R. Patrick Xian"
                },
                "author": "R. Patrick Xian",
                "arxiv_comment": "12 pages content (incl. appendix) + 12 pages references, comments\n  welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06786v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06786v3",
                "updated": "2025-04-04T07:48:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    7,
                    48,
                    19,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-09T18:59:46Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    59,
                    46,
                    0,
                    344,
                    0
                ],
                "title": "Retrieving Semantics from the Deep: an RAG Solution for Gesture\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieving Semantics from the Deep: an RAG Solution for Gesture\n  Synthesis"
                },
                "summary": "Non-verbal communication often comprises of semantically rich gestures that\nhelp convey the meaning of an utterance. Producing such semantic co-speech\ngestures has been a major challenge for the existing neural systems that can\ngenerate rhythmic beat gestures, but struggle to produce semantically\nmeaningful gestures. Therefore, we present RAG-Gesture, a diffusion-based\ngesture generation approach that leverages Retrieval Augmented Generation (RAG)\nto produce natural-looking and semantically rich gestures. Our neuro-explicit\ngesture generation approach is designed to produce semantic gestures grounded\nin interpretable linguistic knowledge. We achieve this by using explicit domain\nknowledge to retrieve exemplar motions from a database of co-speech gestures.\nOnce retrieved, we then inject these semantic exemplar gestures into our\ndiffusion-based gesture generation pipeline using DDIM inversion and retrieval\nguidance at the inference time without any need of training. Further, we\npropose a control paradigm for guidance, that allows the users to modulate the\namount of influence each retrieval insertion has over the generated sequence.\nOur comparative evaluations demonstrate the validity of our approach against\nrecent gesture generation approaches. The reader is urged to explore the\nresults on our project page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-verbal communication often comprises of semantically rich gestures that\nhelp convey the meaning of an utterance. Producing such semantic co-speech\ngestures has been a major challenge for the existing neural systems that can\ngenerate rhythmic beat gestures, but struggle to produce semantically\nmeaningful gestures. Therefore, we present RAG-Gesture, a diffusion-based\ngesture generation approach that leverages Retrieval Augmented Generation (RAG)\nto produce natural-looking and semantically rich gestures. Our neuro-explicit\ngesture generation approach is designed to produce semantic gestures grounded\nin interpretable linguistic knowledge. We achieve this by using explicit domain\nknowledge to retrieve exemplar motions from a database of co-speech gestures.\nOnce retrieved, we then inject these semantic exemplar gestures into our\ndiffusion-based gesture generation pipeline using DDIM inversion and retrieval\nguidance at the inference time without any need of training. Further, we\npropose a control paradigm for guidance, that allows the users to modulate the\namount of influence each retrieval insertion has over the generated sequence.\nOur comparative evaluations demonstrate the validity of our approach against\nrecent gesture generation approaches. The reader is urged to explore the\nresults on our project page."
                },
                "authors": [
                    {
                        "name": "M. Hamza Mughal"
                    },
                    {
                        "name": "Rishabh Dabral"
                    },
                    {
                        "name": "Merel C. J. Scholman"
                    },
                    {
                        "name": "Vera Demberg"
                    },
                    {
                        "name": "Christian Theobalt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Theobalt"
                },
                "author": "Christian Theobalt",
                "arxiv_comment": "CVPR 2025. Project page:\n  https://vcai.mpi-inf.mpg.de/projects/RAG-Gesture/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06786v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06786v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02732v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02732v2",
                "updated": "2025-04-04T07:41:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    7,
                    41,
                    19,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-03T16:17:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    17,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Why do LLMs attend to the first token?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why do LLMs attend to the first token?"
                },
                "summary": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training."
                },
                "authors": [
                    {
                        "name": "Federico Barbero"
                    },
                    {
                        "name": "lvaro Arroyo"
                    },
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Christos Perivolaropoulos"
                    },
                    {
                        "name": "Michael Bronstein"
                    },
                    {
                        "name": "Petar Velikovi"
                    },
                    {
                        "name": "Razvan Pascanu"
                    }
                ],
                "author_detail": {
                    "name": "Razvan Pascanu"
                },
                "author": "Razvan Pascanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02732v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02732v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23368v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23368v3",
                "updated": "2025-04-04T07:23:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    7,
                    23,
                    21,
                    4,
                    94,
                    0
                ],
                "published": "2025-03-30T09:03:09Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    3,
                    9,
                    6,
                    89,
                    0
                ],
                "title": "VLIPP: Towards Physically Plausible Video Generation with Vision and\n  Language Informed Physical Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLIPP: Towards Physically Plausible Video Generation with Vision and\n  Language Informed Physical Prior"
                },
                "summary": "Video diffusion models (VDMs) have advanced significantly in recent years,\nenabling the generation of highly realistic videos and drawing the attention of\nthe community in their potential as world simulators. However, despite their\ncapabilities, VDMs often fail to produce physically plausible videos due to an\ninherent lack of understanding of physics, resulting in incorrect dynamics and\nevent sequences. To address this limitation, we propose a novel two-stage\nimage-to-video generation framework that explicitly incorporates physics with\nvision and language informed physical prior. In the first stage, we employ a\nVision Language Model (VLM) as a coarse-grained motion planner, integrating\nchain-of-thought and physics-aware reasoning to predict a rough motion\ntrajectories/changes that approximate real-world physical dynamics while\nensuring the inter-frame consistency. In the second stage, we use the predicted\nmotion trajectories/changes to guide the video generation of a VDM. As the\npredicted motion trajectories/changes are rough, noise is added during\ninference to provide freedom to the VDM in generating motion with more fine\ndetails. Extensive experimental results demonstrate that our framework can\nproduce physically plausible motion, and comparative evaluations highlight the\nnotable superiority of our approach over existing methods. More video results\nare available on our Project Page:\nhttps://madaoer.github.io/projects/physically_plausible_video_generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion models (VDMs) have advanced significantly in recent years,\nenabling the generation of highly realistic videos and drawing the attention of\nthe community in their potential as world simulators. However, despite their\ncapabilities, VDMs often fail to produce physically plausible videos due to an\ninherent lack of understanding of physics, resulting in incorrect dynamics and\nevent sequences. To address this limitation, we propose a novel two-stage\nimage-to-video generation framework that explicitly incorporates physics with\nvision and language informed physical prior. In the first stage, we employ a\nVision Language Model (VLM) as a coarse-grained motion planner, integrating\nchain-of-thought and physics-aware reasoning to predict a rough motion\ntrajectories/changes that approximate real-world physical dynamics while\nensuring the inter-frame consistency. In the second stage, we use the predicted\nmotion trajectories/changes to guide the video generation of a VDM. As the\npredicted motion trajectories/changes are rough, noise is added during\ninference to provide freedom to the VDM in generating motion with more fine\ndetails. Extensive experimental results demonstrate that our framework can\nproduce physically plausible motion, and comparative evaluations highlight the\nnotable superiority of our approach over existing methods. More video results\nare available on our Project Page:\nhttps://madaoer.github.io/projects/physically_plausible_video_generation."
                },
                "authors": [
                    {
                        "name": "Xindi Yang"
                    },
                    {
                        "name": "Baolu Li"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Liqian Ma"
                    },
                    {
                        "name": "Zhiyong Wang"
                    },
                    {
                        "name": "Jianfei Cai"
                    },
                    {
                        "name": "Tien-Tsin Wong"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Xu Jia"
                    }
                ],
                "author_detail": {
                    "name": "Xu Jia"
                },
                "author": "Xu Jia",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23368v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23368v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03207v1",
                "updated": "2025-04-04T06:40:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    40,
                    3,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T06:40:03Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    40,
                    3,
                    4,
                    94,
                    0
                ],
                "title": "Augmenting Human Cognition With Generative AI: Lessons From AI-Assisted\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting Human Cognition With Generative AI: Lessons From AI-Assisted\n  Decision-Making"
                },
                "summary": "How can we use generative AI to design tools that augment rather than replace\nhuman cognition? In this position paper, we review our own research on\nAI-assisted decision-making for lessons to learn. We observe that in both\nAI-assisted decision-making and generative AI, a popular approach is to suggest\nAI-generated end-to-end solutions to users, which users can then accept,\nreject, or edit. Alternatively, AI tools could offer more incremental support\nto help users solve tasks themselves, which we call process-oriented support.\nWe describe findings on the challenges of end-to-end solutions, and how\nprocess-oriented support can address them. We also discuss the applicability of\nthese findings to generative AI based on a recent study in which we compared\nboth approaches to assist users in a complex decision-making task with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we use generative AI to design tools that augment rather than replace\nhuman cognition? In this position paper, we review our own research on\nAI-assisted decision-making for lessons to learn. We observe that in both\nAI-assisted decision-making and generative AI, a popular approach is to suggest\nAI-generated end-to-end solutions to users, which users can then accept,\nreject, or edit. Alternatively, AI tools could offer more incremental support\nto help users solve tasks themselves, which we call process-oriented support.\nWe describe findings on the challenges of end-to-end solutions, and how\nprocess-oriented support can address them. We also discuss the applicability of\nthese findings to generative AI based on a recent study in which we compared\nboth approaches to assist users in a complex decision-making task with LLMs."
                },
                "authors": [
                    {
                        "name": "Zelun Tony Zhang"
                    },
                    {
                        "name": "Leon Reicherts"
                    }
                ],
                "author_detail": {
                    "name": "Leon Reicherts"
                },
                "author": "Leon Reicherts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03206v1",
                "updated": "2025-04-04T06:35:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    35,
                    2,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T06:35:02Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    35,
                    2,
                    4,
                    94,
                    0
                ],
                "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward"
                },
                "summary": "Effective conversational agents must be able to personalize their behavior to\nsuit a user's preferences, personality, and attributes, whether they are\nassisting with writing tasks or operating in domains like education or\nhealthcare. Current training methods like Reinforcement Learning from Human\nFeedback (RLHF) prioritize helpfulness and safety but fall short in fostering\ntruly empathetic, adaptive, and personalized interactions. Traditional\napproaches to personalization often rely on extensive user history, limiting\ntheir effectiveness for new or context-limited users. To overcome these\nlimitations, we propose to incorporate an intrinsic motivation to improve the\nconversational agents's model of the user as an additional reward alongside\nmulti-turn RLHF. This reward mechanism encourages the agent to actively elicit\nuser traits by optimizing conversations to increase the accuracy of its user\nmodel. Consequently, the policy agent can deliver more personalized\ninteractions through obtaining more information about the user. We applied our\nmethod both education and fitness settings, where LLMs teach concepts or\nrecommend personalized strategies based on users' hidden learning style or\nlifestyle attributes. Using LLM-simulated users, our approach outperformed a\nmulti-turn RLHF baseline in revealing information about the users' preferences,\nand adapting to them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective conversational agents must be able to personalize their behavior to\nsuit a user's preferences, personality, and attributes, whether they are\nassisting with writing tasks or operating in domains like education or\nhealthcare. Current training methods like Reinforcement Learning from Human\nFeedback (RLHF) prioritize helpfulness and safety but fall short in fostering\ntruly empathetic, adaptive, and personalized interactions. Traditional\napproaches to personalization often rely on extensive user history, limiting\ntheir effectiveness for new or context-limited users. To overcome these\nlimitations, we propose to incorporate an intrinsic motivation to improve the\nconversational agents's model of the user as an additional reward alongside\nmulti-turn RLHF. This reward mechanism encourages the agent to actively elicit\nuser traits by optimizing conversations to increase the accuracy of its user\nmodel. Consequently, the policy agent can deliver more personalized\ninteractions through obtaining more information about the user. We applied our\nmethod both education and fitness settings, where LLMs teach concepts or\nrecommend personalized strategies based on users' hidden learning style or\nlifestyle attributes. Using LLM-simulated users, our approach outperformed a\nmulti-turn RLHF baseline in revealing information about the users' preferences,\nand adapting to them."
                },
                "authors": [
                    {
                        "name": "Yanming Wan"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Marwa Abdulhai"
                    },
                    {
                        "name": "Lior Shani"
                    },
                    {
                        "name": "Natasha Jaques"
                    }
                ],
                "author_detail": {
                    "name": "Natasha Jaques"
                },
                "author": "Natasha Jaques",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17796v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17796v2",
                "updated": "2025-04-04T06:30:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    30,
                    27,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-25T02:57:45Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    2,
                    57,
                    45,
                    1,
                    56,
                    0
                ],
                "title": "LAM: Large Avatar Model for One-shot Animatable Gaussian Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAM: Large Avatar Model for One-shot Animatable Gaussian Head"
                },
                "summary": "We present LAM, an innovative Large Avatar Model for animatable Gaussian head\nreconstruction from a single image. Unlike previous methods that require\nextensive training on captured video sequences or rely on auxiliary neural\nnetworks for animation and rendering during inference, our approach generates\nGaussian heads that are immediately animatable and renderable. Specifically,\nLAM creates an animatable Gaussian head in a single forward pass, enabling\nreenactment and rendering without additional networks or post-processing steps.\nThis capability allows for seamless integration into existing rendering\npipelines, ensuring real-time animation and rendering across a wide range of\nplatforms, including mobile phones. The centerpiece of our framework is the\ncanonical Gaussian attributes generator, which utilizes FLAME canonical points\nas queries. These points interact with multi-scale image features through a\nTransformer to accurately predict Gaussian attributes in the canonical space.\nThe reconstructed canonical Gaussian avatar can then be animated utilizing\nstandard linear blend skinning (LBS) with corrective blendshapes as the FLAME\nmodel did and rendered in real-time on various platforms. Our experimental\nresults demonstrate that LAM outperforms state-of-the-art methods on existing\nbenchmarks. Our code and video are available at\nhttps://aigc3d.github.io/projects/LAM/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LAM, an innovative Large Avatar Model for animatable Gaussian head\nreconstruction from a single image. Unlike previous methods that require\nextensive training on captured video sequences or rely on auxiliary neural\nnetworks for animation and rendering during inference, our approach generates\nGaussian heads that are immediately animatable and renderable. Specifically,\nLAM creates an animatable Gaussian head in a single forward pass, enabling\nreenactment and rendering without additional networks or post-processing steps.\nThis capability allows for seamless integration into existing rendering\npipelines, ensuring real-time animation and rendering across a wide range of\nplatforms, including mobile phones. The centerpiece of our framework is the\ncanonical Gaussian attributes generator, which utilizes FLAME canonical points\nas queries. These points interact with multi-scale image features through a\nTransformer to accurately predict Gaussian attributes in the canonical space.\nThe reconstructed canonical Gaussian avatar can then be animated utilizing\nstandard linear blend skinning (LBS) with corrective blendshapes as the FLAME\nmodel did and rendered in real-time on various platforms. Our experimental\nresults demonstrate that LAM outperforms state-of-the-art methods on existing\nbenchmarks. Our code and video are available at\nhttps://aigc3d.github.io/projects/LAM/"
                },
                "authors": [
                    {
                        "name": "Yisheng He"
                    },
                    {
                        "name": "Xiaodong Gu"
                    },
                    {
                        "name": "Xiaodan Ye"
                    },
                    {
                        "name": "Chao Xu"
                    },
                    {
                        "name": "Zhengyi Zhao"
                    },
                    {
                        "name": "Yuan Dong"
                    },
                    {
                        "name": "Weihao Yuan"
                    },
                    {
                        "name": "Zilong Dong"
                    },
                    {
                        "name": "Liefeng Bo"
                    }
                ],
                "author_detail": {
                    "name": "Liefeng Bo"
                },
                "author": "Liefeng Bo",
                "arxiv_comment": "Project Page: https://aigc3d.github.io/projects/LAM/ Source code:\n  https://github.com/aigc3d/LAM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17796v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00785v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00785v3",
                "updated": "2025-04-04T06:14:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    14,
                    36,
                    4,
                    94,
                    0
                ],
                "published": "2025-01-01T09:48:16Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    9,
                    48,
                    16,
                    2,
                    1,
                    0
                ],
                "title": "Natural Multimodal Fusion-Based Human-Robot Interaction: Application\n  With Voice and Deictic Posture via Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Multimodal Fusion-Based Human-Robot Interaction: Application\n  With Voice and Deictic Posture via Large Language Model"
                },
                "summary": "Translating human intent into robot commands is crucial for the future of\nservice robots in an aging society. Existing Human-Robot Interaction (HRI)\nsystems relying on gestures or verbal commands are impractical for the elderly\ndue to difficulties with complex syntax or sign language. To address the\nchallenge, this paper introduces a multi-modal interaction framework that\ncombines voice and deictic posture information to create a more natural HRI\nsystem. The visual cues are first processed by the object detection model to\ngain a global understanding of the environment, and then bounding boxes are\nestimated based on depth information. By using a large language model (LLM)\nwith voice-to-text commands and temporally aligned selected bounding boxes,\nrobot action sequences can be generated, while key control syntax constraints\nare applied to avoid potential LLM hallucination issues. The system is\nevaluated on real-world tasks with varying levels of complexity using a\nUniversal Robots UR3e manipulator. Our method demonstrates significantly better\nperformance in HRI in terms of accuracy and robustness. To benefit the research\ncommunity and the general public, we will make our code and design open-source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating human intent into robot commands is crucial for the future of\nservice robots in an aging society. Existing Human-Robot Interaction (HRI)\nsystems relying on gestures or verbal commands are impractical for the elderly\ndue to difficulties with complex syntax or sign language. To address the\nchallenge, this paper introduces a multi-modal interaction framework that\ncombines voice and deictic posture information to create a more natural HRI\nsystem. The visual cues are first processed by the object detection model to\ngain a global understanding of the environment, and then bounding boxes are\nestimated based on depth information. By using a large language model (LLM)\nwith voice-to-text commands and temporally aligned selected bounding boxes,\nrobot action sequences can be generated, while key control syntax constraints\nare applied to avoid potential LLM hallucination issues. The system is\nevaluated on real-world tasks with varying levels of complexity using a\nUniversal Robots UR3e manipulator. Our method demonstrates significantly better\nperformance in HRI in terms of accuracy and robustness. To benefit the research\ncommunity and the general public, we will make our code and design open-source."
                },
                "authors": [
                    {
                        "name": "Yuzhi Lai"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Youssef Nassar"
                    },
                    {
                        "name": "Mingyu Fan"
                    },
                    {
                        "name": "Atmaraaj Gopal"
                    },
                    {
                        "name": "Arihiro Yorita"
                    },
                    {
                        "name": "Naoyuki Kubota"
                    },
                    {
                        "name": "Matthias Rtsch"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Rtsch"
                },
                "author": "Matthias Rtsch",
                "arxiv_doi": "10.1109/MRA.2025.3543957",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MRA.2025.3543957",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.00785v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00785v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication by IEEE Robotics & Automation Magazine",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15291v3",
                "updated": "2025-04-04T06:11:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    11,
                    55,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-19T07:10:51Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    7,
                    10,
                    51,
                    3,
                    354,
                    0
                ],
                "title": "A Large-Scale Simulation on Large Language Models for Decision-Making in\n  Political Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large-Scale Simulation on Large Language Models for Decision-Making in\n  Political Science"
                },
                "summary": "While LLMs have demonstrated remarkable capabilities in text generation and\nreasoning, their ability to simulate human decision-making -- particularly in\npolitical contexts -- remains an open question. However, modeling voter\nbehavior presents unique challenges due to limited voter-level data, evolving\npolitical landscapes, and the complexity of human reasoning. In this study, we\ndevelop a theory-driven, multi-step reasoning framework that integrates\ndemographic, temporal and ideological factors to simulate voter decision-making\nat scale. Using synthetic personas calibrated to real-world voter data, we\nconduct large-scale simulations of recent U.S. presidential elections. Our\nmethod significantly improves simulation accuracy while mitigating model\nbiases. We examine its robustness by comparing performance across different\nLLMs. We further investigate the challenges and constraints that arise from\nLLM-based political simulations. Our work provides both a scalable framework\nfor modeling political decision-making behavior and insights into the promise\nand limitations of using LLMs in political science research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs have demonstrated remarkable capabilities in text generation and\nreasoning, their ability to simulate human decision-making -- particularly in\npolitical contexts -- remains an open question. However, modeling voter\nbehavior presents unique challenges due to limited voter-level data, evolving\npolitical landscapes, and the complexity of human reasoning. In this study, we\ndevelop a theory-driven, multi-step reasoning framework that integrates\ndemographic, temporal and ideological factors to simulate voter decision-making\nat scale. Using synthetic personas calibrated to real-world voter data, we\nconduct large-scale simulations of recent U.S. presidential elections. Our\nmethod significantly improves simulation accuracy while mitigating model\nbiases. We examine its robustness by comparing performance across different\nLLMs. We further investigate the challenges and constraints that arise from\nLLM-based political simulations. Our work provides both a scalable framework\nfor modeling political decision-making behavior and insights into the promise\nand limitations of using LLMs in political science research."
                },
                "authors": [
                    {
                        "name": "Chenxiao Yu"
                    },
                    {
                        "name": "Jinyi Ye"
                    },
                    {
                        "name": "Yuangang Li"
                    },
                    {
                        "name": "Zhaotian Weng"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Emilio Ferrara"
                    },
                    {
                        "name": "Xiyang Hu"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2411.03321 This\n  version adds a new model to our experimental setup, modifies the paper's main\n  discussion, and updates the authorship list",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16915v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16915v2",
                "updated": "2025-04-04T06:07:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    7,
                    56,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-22T08:19:22Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    8,
                    19,
                    22,
                    6,
                    357,
                    0
                ],
                "title": "FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG\n  Distillation"
                },
                "summary": "Diffusion-based audio-driven talking avatar methods have recently gained\nattention for their high-fidelity, vivid, and expressive results. However,\ntheir slow inference speed limits practical applications. Despite the\ndevelopment of various distillation techniques for diffusion models, we found\nthat naive diffusion distillation methods do not yield satisfactory results.\nDistilled models exhibit reduced robustness with open-set input images and a\ndecreased correlation between audio and video compared to teacher models,\nundermining the advantages of diffusion models. To address this, we propose\nFADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG\nDistillation). We first designed a mixed-supervised loss to leverage data of\nvarying quality and enhance the overall model capability as well as robustness.\nAdditionally, we propose a multi-CFG distillation with learnable tokens to\nutilize the correlation between audio and reference image conditions, reducing\nthe threefold inference runs caused by multi-CFG with acceptable quality\ndegradation. Extensive experiments across multiple datasets show that FADA\ngenerates vivid videos comparable to recent diffusion model-based methods while\nachieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage\nhttp://fadavatar.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based audio-driven talking avatar methods have recently gained\nattention for their high-fidelity, vivid, and expressive results. However,\ntheir slow inference speed limits practical applications. Despite the\ndevelopment of various distillation techniques for diffusion models, we found\nthat naive diffusion distillation methods do not yield satisfactory results.\nDistilled models exhibit reduced robustness with open-set input images and a\ndecreased correlation between audio and video compared to teacher models,\nundermining the advantages of diffusion models. To address this, we propose\nFADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG\nDistillation). We first designed a mixed-supervised loss to leverage data of\nvarying quality and enhance the overall model capability as well as robustness.\nAdditionally, we propose a multi-CFG distillation with learnable tokens to\nutilize the correlation between audio and reference image conditions, reducing\nthe threefold inference runs caused by multi-CFG with acceptable quality\ndegradation. Extensive experiments across multiple datasets show that FADA\ngenerates vivid videos comparable to recent diffusion model-based methods while\nachieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage\nhttp://fadavatar.github.io."
                },
                "authors": [
                    {
                        "name": "Tianyun Zhong"
                    },
                    {
                        "name": "Chao Liang"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Gaojie Lin"
                    },
                    {
                        "name": "Jiaqi Yang"
                    },
                    {
                        "name": "Zhou Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Zhao"
                },
                "author": "Zhou Zhao",
                "arxiv_comment": "CVPR 2025, Homepage https://fadavatar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16915v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16915v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03197v1",
                "updated": "2025-04-04T06:03:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    3,
                    13,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T06:03:13Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    3,
                    13,
                    4,
                    94,
                    0
                ],
                "title": "Explain with Visual Keypoints Like a Real Mentor! A Benchmark for\n  Multimodal Solution Explanation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explain with Visual Keypoints Like a Real Mentor! A Benchmark for\n  Multimodal Solution Explanation"
                },
                "summary": "With the rapid advancement of mathematical reasoning capabilities in large\nlanguage models (LLMs), AI systems are increasingly being adopted in\neducational settings to support students' comprehension of problem-solving\nprocesses. However, a critical component remains underexplored in current\nLLM-generated explanations: visual explanation. In real-world instructional\ncontexts, human tutors routinely employ visual aids-such as diagrams, markings,\nand highlights-to enhance conceptual clarity. To bridge this gap, we introduce\na novel task of visual solution explanation, which requires not only solving\nproblems but also generating explanations that incorporate newly introduced\nvisual elements essential for understanding (e.g., auxiliary lines,\nannotations, or geometric constructions). To evaluate model performance on this\ntask, we propose MathExplain, a multimodal benchmark consisting of 997 math\nproblems annotated with visual keypoints and corresponding explanatory text\nthat references those elements. Our empirical results show that while some\nclosed-source models demonstrate promising capabilities on visual\nsolution-explaining, current open-source general-purpose models perform\ninconsistently, particularly in identifying relevant visual components and\nproducing coherent keypoint-based explanations. We expect that visual\nsolution-explaining and the MathExplain dataset will catalyze further research\non multimodal LLMs in education and advance their deployment as effective,\nexplanation-oriented AI tutors. Code and data will be released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of mathematical reasoning capabilities in large\nlanguage models (LLMs), AI systems are increasingly being adopted in\neducational settings to support students' comprehension of problem-solving\nprocesses. However, a critical component remains underexplored in current\nLLM-generated explanations: visual explanation. In real-world instructional\ncontexts, human tutors routinely employ visual aids-such as diagrams, markings,\nand highlights-to enhance conceptual clarity. To bridge this gap, we introduce\na novel task of visual solution explanation, which requires not only solving\nproblems but also generating explanations that incorporate newly introduced\nvisual elements essential for understanding (e.g., auxiliary lines,\nannotations, or geometric constructions). To evaluate model performance on this\ntask, we propose MathExplain, a multimodal benchmark consisting of 997 math\nproblems annotated with visual keypoints and corresponding explanatory text\nthat references those elements. Our empirical results show that while some\nclosed-source models demonstrate promising capabilities on visual\nsolution-explaining, current open-source general-purpose models perform\ninconsistently, particularly in identifying relevant visual components and\nproducing coherent keypoint-based explanations. We expect that visual\nsolution-explaining and the MathExplain dataset will catalyze further research\non multimodal LLMs in education and advance their deployment as effective,\nexplanation-oriented AI tutors. Code and data will be released publicly."
                },
                "authors": [
                    {
                        "name": "Jaewoo Park"
                    },
                    {
                        "name": "Jungyang Park"
                    },
                    {
                        "name": "Dongju Jang"
                    },
                    {
                        "name": "Jiwan Chung"
                    },
                    {
                        "name": "Byungwoo Yoo"
                    },
                    {
                        "name": "Jaewoo Shin"
                    },
                    {
                        "name": "Seonjoon Park"
                    },
                    {
                        "name": "Taehyeong Kim"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03544v2",
                "updated": "2025-04-04T05:56:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    56,
                    4,
                    4,
                    94,
                    0
                ],
                "published": "2025-01-07T05:39:21Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    39,
                    21,
                    1,
                    7,
                    0
                ],
                "title": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for\n  Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for\n  Text-to-Image Models"
                },
                "summary": "Text-to-image (T2I) models have been shown to be vulnerable to misuse,\nparticularly in generating not-safe-for-work (NSFW) content, raising serious\nethical concerns. In this work, we present PromptGuard, a novel content\nmoderation technique that draws inspiration from the system prompt mechanism in\nlarge language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack\na direct interface for enforcing behavioral guidelines. Our key idea is to\noptimize a safety soft prompt that functions as an implicit system prompt\nwithin the T2I model's textual embedding space. This universal soft prompt (P*)\ndirectly moderates NSFW inputs, enabling safe yet realistic image generation\nwithout altering the inference efficiency or requiring proxy models. Extensive\nexperiments across three datasets demonstrate that PromptGuard effectively\nmitigates NSFW content generation while preserving high-quality benign outputs.\nPromptGuard achieves 7.8 times faster than prior content moderation methods,\nsurpassing eight state-of-the-art defenses with an optimal unsafe ratio down to\n5.84%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) models have been shown to be vulnerable to misuse,\nparticularly in generating not-safe-for-work (NSFW) content, raising serious\nethical concerns. In this work, we present PromptGuard, a novel content\nmoderation technique that draws inspiration from the system prompt mechanism in\nlarge language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack\na direct interface for enforcing behavioral guidelines. Our key idea is to\noptimize a safety soft prompt that functions as an implicit system prompt\nwithin the T2I model's textual embedding space. This universal soft prompt (P*)\ndirectly moderates NSFW inputs, enabling safe yet realistic image generation\nwithout altering the inference efficiency or requiring proxy models. Extensive\nexperiments across three datasets demonstrate that PromptGuard effectively\nmitigates NSFW content generation while preserving high-quality benign outputs.\nPromptGuard achieves 7.8 times faster than prior content moderation methods,\nsurpassing eight state-of-the-art defenses with an optimal unsafe ratio down to\n5.84%."
                },
                "authors": [
                    {
                        "name": "Lingzhi Yuan"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "16 pages, 8 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03194v1",
                "updated": "2025-04-04T05:50:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    50,
                    24,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T05:50:24Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    50,
                    24,
                    4,
                    94,
                    0
                ],
                "title": "Semi-empirical versus Theoretical Stellar Population Models: a\n  comparison with Star Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-empirical versus Theoretical Stellar Population Models: a\n  comparison with Star Clusters"
                },
                "summary": "Stellar population synthesis (SPS) models are a key tool for deriving the\nage, metallicity, radial velocity and reddening of star clusters from their\nintegrated spectra. Using a sample of 129 star clusters with high-quality\nspectra, we analyze the uncertainties associated with selecting an empirical\nversus a theoretical stellar spectral library in the SPS models. We find that\nthe fits from the different models agree on the goodness of fit metrics and\ninferred reddening. However, the derived age and metallicity can be affected by\nthe choice of the stellar library, with synthetic libraries tending to give\nlower age and metallicity, especially for spectra with low SNR. Ages and\nreddening values from SSP-equivalent fits are consistent with the\nmulti-population fits, however, SSP-equivalent metallicities are affected by\nthe coarse coverage of the SPS grid in [Fe/H]. When comparing the spectral\nfitting results with the literature, we find that (1) all models underestimate\nage for old and metal-poor systems; (2) on average, SPS models based on\nsynthetic stellar libraries better match the isochrone ages and metallicities\nfrom high-resolution stellar spectroscopy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stellar population synthesis (SPS) models are a key tool for deriving the\nage, metallicity, radial velocity and reddening of star clusters from their\nintegrated spectra. Using a sample of 129 star clusters with high-quality\nspectra, we analyze the uncertainties associated with selecting an empirical\nversus a theoretical stellar spectral library in the SPS models. We find that\nthe fits from the different models agree on the goodness of fit metrics and\ninferred reddening. However, the derived age and metallicity can be affected by\nthe choice of the stellar library, with synthetic libraries tending to give\nlower age and metallicity, especially for spectra with low SNR. Ages and\nreddening values from SSP-equivalent fits are consistent with the\nmulti-population fits, however, SSP-equivalent metallicities are affected by\nthe coarse coverage of the SPS grid in [Fe/H]. When comparing the spectral\nfitting results with the literature, we find that (1) all models underestimate\nage for old and metal-poor systems; (2) on average, SPS models based on\nsynthetic stellar libraries better match the isochrone ages and metallicities\nfrom high-resolution stellar spectroscopy."
                },
                "authors": [
                    {
                        "name": "Randa Asad"
                    },
                    {
                        "name": "Paula R. T. Coelho"
                    },
                    {
                        "name": "Johina M. John"
                    },
                    {
                        "name": "Igor Chilingarian"
                    },
                    {
                        "name": "Gustavo Bruzual"
                    },
                    {
                        "name": "Stephane Charlot"
                    }
                ],
                "author_detail": {
                    "name": "Stephane Charlot"
                },
                "author": "Stephane Charlot",
                "arxiv_comment": "13 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00967v2",
                "updated": "2025-04-04T05:49:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    49,
                    11,
                    4,
                    94,
                    0
                ],
                "published": "2025-03-02T17:16:32Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    17,
                    16,
                    32,
                    6,
                    61,
                    0
                ],
                "title": "How Do Teachers Create Pedagogical Chatbots?: Current Practices and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do Teachers Create Pedagogical Chatbots?: Current Practices and\n  Challenges"
                },
                "summary": "AI chatbots have emerged as promising educational tools for personalized\nlearning experiences, with advances in large language models (LLMs) enabling\nteachers to create and customize these chatbots for their specific classroom\nneeds. However, there is a limited understanding of how teachers create\npedagogical chatbots and integrate them into their lessons. Through\nsemi-structured interviews with seven K-12 teachers, we examined their\npractices and challenges when designing, implementing, and deploying chatbots.\nOur findings revealed that teachers prioritize developing task-specific\nchatbots aligned with their lessons. Teachers engaged in various creation\npractices and had different challenges; novices in chatbot creation struggled\nmainly with initial design and technical implementation, while experienced\nteachers faced challenges with technical aspects and analyzing conversational\ndata. Based on these insights, we explore approaches to supporting teachers'\nchatbot development and opportunities for designing future chatbot creation\nsystems. This work provides foundational insights from teachers that can\nempower teacher-created chatbots, facilitating AI-augmented teaching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI chatbots have emerged as promising educational tools for personalized\nlearning experiences, with advances in large language models (LLMs) enabling\nteachers to create and customize these chatbots for their specific classroom\nneeds. However, there is a limited understanding of how teachers create\npedagogical chatbots and integrate them into their lessons. Through\nsemi-structured interviews with seven K-12 teachers, we examined their\npractices and challenges when designing, implementing, and deploying chatbots.\nOur findings revealed that teachers prioritize developing task-specific\nchatbots aligned with their lessons. Teachers engaged in various creation\npractices and had different challenges; novices in chatbot creation struggled\nmainly with initial design and technical implementation, while experienced\nteachers faced challenges with technical aspects and analyzing conversational\ndata. Based on these insights, we explore approaches to supporting teachers'\nchatbot development and opportunities for designing future chatbot creation\nsystems. This work provides foundational insights from teachers that can\nempower teacher-created chatbots, facilitating AI-augmented teaching."
                },
                "authors": [
                    {
                        "name": "Minju Yoo"
                    },
                    {
                        "name": "Hyoungwook Jin"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "arxiv_comment": "CHI 2025 Workshop on Augmented Educators and AI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03189v1",
                "updated": "2025-04-04T05:33:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    33,
                    23,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T05:33:23Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    33,
                    23,
                    4,
                    94,
                    0
                ],
                "title": "An ADMM Algorithm for Structure Learning in Equilibrium Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An ADMM Algorithm for Structure Learning in Equilibrium Networks"
                },
                "summary": "Learning the edge connectivity structure of networked systems from limited\ndata is a fundamental challenge in many critical infrastructure domains,\nincluding power, traffic, and finance. Such systems obey steady-state\nconservation laws: $x(t) = Ly(t)$, where $x(t) \\in \\mathbb{R}^p$ and $y(t) \\in\n\\mathbb{R}^p$ represent injected flows (inputs) and potentials (outputs),\nrespectively. The sparsity pattern of the $p\\times p$ Laplacian L encodes the\nunderlying edge structure. In a stochastic setting, the goal is to infer this\nsparsity pattern from zero-mean i.i.d. samples of y(t).\n  Recent work by Rayas et al. [1] has established statistical consistency\nresults for this learning problem by considering an $\\ell_1$-regularized\nmaximum likelihood estimator. However, their approach did not focus on\ndeveloping a scalable algorithm but relies on solving a convex program via the\nCVX package in Python. To address this gap, we propose an alternating direction\nmethod of multipliers (ADMM) algorithm. Our approach is simple, transparent,\nand computationally fast. A key contribution is demonstrating the role of a\nnon-symmetric algebraic Riccati equation in the primal step of ADMM. Numerical\nexperiments on a host of synthetic and benchmark networks, including power and\nwater systems, show that our method achieves high recovery accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning the edge connectivity structure of networked systems from limited\ndata is a fundamental challenge in many critical infrastructure domains,\nincluding power, traffic, and finance. Such systems obey steady-state\nconservation laws: $x(t) = Ly(t)$, where $x(t) \\in \\mathbb{R}^p$ and $y(t) \\in\n\\mathbb{R}^p$ represent injected flows (inputs) and potentials (outputs),\nrespectively. The sparsity pattern of the $p\\times p$ Laplacian L encodes the\nunderlying edge structure. In a stochastic setting, the goal is to infer this\nsparsity pattern from zero-mean i.i.d. samples of y(t).\n  Recent work by Rayas et al. [1] has established statistical consistency\nresults for this learning problem by considering an $\\ell_1$-regularized\nmaximum likelihood estimator. However, their approach did not focus on\ndeveloping a scalable algorithm but relies on solving a convex program via the\nCVX package in Python. To address this gap, we propose an alternating direction\nmethod of multipliers (ADMM) algorithm. Our approach is simple, transparent,\nand computationally fast. A key contribution is demonstrating the role of a\nnon-symmetric algebraic Riccati equation in the primal step of ADMM. Numerical\nexperiments on a host of synthetic and benchmark networks, including power and\nwater systems, show that our method achieves high recovery accuracy."
                },
                "authors": [
                    {
                        "name": "Rohith Reddy Mada"
                    },
                    {
                        "name": "Rajasekhar Anguluri"
                    }
                ],
                "author_detail": {
                    "name": "Rajasekhar Anguluri"
                },
                "author": "Rajasekhar Anguluri",
                "arxiv_comment": "7 pages, 5 figures, Submitted to IEEE Conference on Decision and\n  Control 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03185v1",
                "updated": "2025-04-04T05:26:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    26,
                    28,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T05:26:28Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    26,
                    28,
                    4,
                    94,
                    0
                ],
                "title": "Learning Natural Language Constraints for Safe Reinforcement Learning of\n  Language Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Natural Language Constraints for Safe Reinforcement Learning of\n  Language Agents"
                },
                "summary": "Generalizable alignment is a core challenge for deploying Large Language\nModels (LLMs) safely in real-world NLP applications. Current alignment methods,\nincluding Reinforcement Learning from Human Feedback (RLHF), often fail to\nguarantee constraint satisfaction outside their training distribution due to\ntheir reliance on implicit, post-hoc preferences. Inspired by a paradigm shift\nto first curate data before tuning, we introduce a new framework for safe\nlanguage alignment that learns natural language constraints from positive and\nnegative demonstrations as a primary step. From inferring both a task-specific\nreward function and latent constraint functions, our approach fosters\nadaptation to novel safety requirements and robust generalization under domain\nshifts and adversarial inputs. We formalize the framework within a Constrained\nMarkov Decision Process (CMDP) and validate it via a text-based navigation\nenvironment, demonstrating safe adaptation to changing danger zones. Our\nexperiments show fewer violations upon domain shift when following a safe\nnavigation path, and we achieve zero violations by applying learned constraints\nto a distilled BERT model as a fine-tuning technique. This work offers a\npromising path toward building safety-critical and more generalizable LLMs for\npractical NLP settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizable alignment is a core challenge for deploying Large Language\nModels (LLMs) safely in real-world NLP applications. Current alignment methods,\nincluding Reinforcement Learning from Human Feedback (RLHF), often fail to\nguarantee constraint satisfaction outside their training distribution due to\ntheir reliance on implicit, post-hoc preferences. Inspired by a paradigm shift\nto first curate data before tuning, we introduce a new framework for safe\nlanguage alignment that learns natural language constraints from positive and\nnegative demonstrations as a primary step. From inferring both a task-specific\nreward function and latent constraint functions, our approach fosters\nadaptation to novel safety requirements and robust generalization under domain\nshifts and adversarial inputs. We formalize the framework within a Constrained\nMarkov Decision Process (CMDP) and validate it via a text-based navigation\nenvironment, demonstrating safe adaptation to changing danger zones. Our\nexperiments show fewer violations upon domain shift when following a safe\nnavigation path, and we achieve zero violations by applying learned constraints\nto a distilled BERT model as a fine-tuning technique. This work offers a\npromising path toward building safety-critical and more generalizable LLMs for\npractical NLP settings."
                },
                "authors": [
                    {
                        "name": "Jaymari Chua"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.4; I.2.6; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02634v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02634v3",
                "updated": "2025-04-04T05:13:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    13,
                    40,
                    4,
                    94,
                    0
                ],
                "published": "2024-09-04T11:55:14Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    55,
                    14,
                    2,
                    248,
                    0
                ],
                "title": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion\n  Dependency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion\n  Dependency"
                },
                "summary": "With the introduction of diffusion-based video generation techniques,\naudio-conditioned human video generation has recently achieved significant\nbreakthroughs in both the naturalness of motion and the synthesis of portrait\ndetails. Due to the limited control of audio signals in driving human motion,\nexisting methods often add auxiliary spatial signals to stabilize movements,\nwhich may compromise the naturalness and freedom of motion. In this paper, we\npropose an end-to-end audio-only conditioned video diffusion model named Loopy.\nSpecifically, we designed an inter- and intra-clip temporal module and an\naudio-to-latents module, enabling the model to leverage long-term motion\ninformation from the data to learn natural motion patterns and improving\naudio-portrait movement correlation. This method removes the need for manually\nspecified spatial motion templates used in existing methods to constrain motion\nduring inference. Extensive experiments show that Loopy outperforms recent\naudio-driven portrait diffusion models, delivering more lifelike and\nhigh-quality results across various scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the introduction of diffusion-based video generation techniques,\naudio-conditioned human video generation has recently achieved significant\nbreakthroughs in both the naturalness of motion and the synthesis of portrait\ndetails. Due to the limited control of audio signals in driving human motion,\nexisting methods often add auxiliary spatial signals to stabilize movements,\nwhich may compromise the naturalness and freedom of motion. In this paper, we\npropose an end-to-end audio-only conditioned video diffusion model named Loopy.\nSpecifically, we designed an inter- and intra-clip temporal module and an\naudio-to-latents module, enabling the model to leverage long-term motion\ninformation from the data to learn natural motion patterns and improving\naudio-portrait movement correlation. This method removes the need for manually\nspecified spatial motion templates used in existing methods to constrain motion\nduring inference. Extensive experiments show that Loopy outperforms recent\naudio-driven portrait diffusion models, delivering more lifelike and\nhigh-quality results across various scenarios."
                },
                "authors": [
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Chao Liang"
                    },
                    {
                        "name": "Jiaqi Yang"
                    },
                    {
                        "name": "Gaojie Lin"
                    },
                    {
                        "name": "Tianyun Zhong"
                    },
                    {
                        "name": "Yanbo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yanbo Zheng"
                },
                "author": "Yanbo Zheng",
                "arxiv_comment": "ICLR 2025 (Oral), Homepage: https://loopyavatar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02634v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02634v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03174v1",
                "updated": "2025-04-04T05:06:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    6,
                    12,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T05:06:12Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    6,
                    12,
                    4,
                    94,
                    0
                ],
                "title": "Multi-lingual Multi-turn Automated Red Teaming for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-lingual Multi-turn Automated Red Teaming for LLMs"
                },
                "summary": "Language Model Models (LLMs) have improved dramatically in the past few\nyears, increasing their adoption and the scope of their capabilities over time.\nA significant amount of work is dedicated to ``model alignment'', i.e.,\npreventing LLMs to generate unsafe responses when deployed into customer-facing\napplications. One popular method to evaluate safety risks is\n\\textit{red-teaming}, where agents attempt to bypass alignment by crafting\nelaborate prompts that trigger unsafe responses from a model. Standard\nhuman-driven red-teaming is costly, time-consuming and rarely covers all the\nrecent features (e.g., multi-lingual, multi-modal aspects), while proposed\nautomation methods only cover a small subset of LLMs capabilities (i.e.,\nEnglish or single-turn). We present Multi-lingual Multi-turn Automated Red\nTeaming (\\textbf{MM-ART}), a method to fully automate conversational,\nmulti-lingual red-teaming operations and quickly identify prompts leading to\nunsafe responses. Through extensive experiments on different languages, we show\nthe studied LLMs are on average 71\\% more vulnerable after a 5-turn\nconversation in English than after the initial turn. For conversations in\nnon-English languages, models display up to 195\\% more safety vulnerabilities\nthan the standard single-turn English approach, confirming the need for\nautomated red-teaming methods matching LLMs capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model Models (LLMs) have improved dramatically in the past few\nyears, increasing their adoption and the scope of their capabilities over time.\nA significant amount of work is dedicated to ``model alignment'', i.e.,\npreventing LLMs to generate unsafe responses when deployed into customer-facing\napplications. One popular method to evaluate safety risks is\n\\textit{red-teaming}, where agents attempt to bypass alignment by crafting\nelaborate prompts that trigger unsafe responses from a model. Standard\nhuman-driven red-teaming is costly, time-consuming and rarely covers all the\nrecent features (e.g., multi-lingual, multi-modal aspects), while proposed\nautomation methods only cover a small subset of LLMs capabilities (i.e.,\nEnglish or single-turn). We present Multi-lingual Multi-turn Automated Red\nTeaming (\\textbf{MM-ART}), a method to fully automate conversational,\nmulti-lingual red-teaming operations and quickly identify prompts leading to\nunsafe responses. Through extensive experiments on different languages, we show\nthe studied LLMs are on average 71\\% more vulnerable after a 5-turn\nconversation in English than after the initial turn. For conversations in\nnon-English languages, models display up to 195\\% more safety vulnerabilities\nthan the standard single-turn English approach, confirming the need for\nautomated red-teaming methods matching LLMs capabilities."
                },
                "authors": [
                    {
                        "name": "Abhishek Singhania"
                    },
                    {
                        "name": "Christophe Dupuy"
                    },
                    {
                        "name": "Shivam Mangale"
                    },
                    {
                        "name": "Amani Namboori"
                    }
                ],
                "author_detail": {
                    "name": "Amani Namboori"
                },
                "author": "Amani Namboori",
                "arxiv_comment": "Accepted at TrustNLP@NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03158v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03158v2",
                "updated": "2025-04-04T05:01:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    1,
                    8,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-05T13:31:38Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    31,
                    38,
                    2,
                    36,
                    0
                ],
                "title": "Strategizing with AI: Insights from a Beauty Contest Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategizing with AI: Insights from a Beauty Contest Experiment"
                },
                "summary": "A beauty contest is a wide class of games of guessing the most popular\nstrategy among other players. In particular, guessing a fraction of a mean of\nnumbers chosen by all players is a classic behavioral experiment designed to\ntest iterative reasoning patterns among various groups of people. The previous\nliterature reveals that the level of sophistication of the opponents is an\nimportant factor affecting the outcome of the game. Smarter decision makers\nchoose strategies that are closer to theoretical Nash equilibrium and\ndemonstrate faster convergence to equilibrium in iterated contests with\ninformation revelation. We replicate a series of classic experiments by running\nvirtual experiments with modern large language models (LLMs) who play against\nvarious groups of virtual players. We test how advanced the LLMs' behavior is\ncompared to the behavior of human players. We show that LLMs typically take\ninto account the opponents' level of sophistication and adapt by changing the\nstrategy. In various settings, most LLMs (with the exception of Llama) are more\nsophisticated and play lower numbers compared to human players. Our results\nsuggest that LLMs (except Llama) are rather successful in identifying the\nunderlying strategic environment and adopting the strategies to the changing\nset of parameters of the game in the same way that human players do. All LLMs\nstill fail to play dominant strategies in a two-player game. Our results\ncontribute to the discussion on the accuracy of modeling human economic agents\nby artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A beauty contest is a wide class of games of guessing the most popular\nstrategy among other players. In particular, guessing a fraction of a mean of\nnumbers chosen by all players is a classic behavioral experiment designed to\ntest iterative reasoning patterns among various groups of people. The previous\nliterature reveals that the level of sophistication of the opponents is an\nimportant factor affecting the outcome of the game. Smarter decision makers\nchoose strategies that are closer to theoretical Nash equilibrium and\ndemonstrate faster convergence to equilibrium in iterated contests with\ninformation revelation. We replicate a series of classic experiments by running\nvirtual experiments with modern large language models (LLMs) who play against\nvarious groups of virtual players. We test how advanced the LLMs' behavior is\ncompared to the behavior of human players. We show that LLMs typically take\ninto account the opponents' level of sophistication and adapt by changing the\nstrategy. In various settings, most LLMs (with the exception of Llama) are more\nsophisticated and play lower numbers compared to human players. Our results\nsuggest that LLMs (except Llama) are rather successful in identifying the\nunderlying strategic environment and adopting the strategies to the changing\nset of parameters of the game in the same way that human players do. All LLMs\nstill fail to play dominant strategies in a two-player game. Our results\ncontribute to the discussion on the accuracy of modeling human economic agents\nby artificial intelligence."
                },
                "authors": [
                    {
                        "name": "Iuliia Alekseenko"
                    },
                    {
                        "name": "Dmitry Dagaev"
                    },
                    {
                        "name": "Sofia Paklina"
                    },
                    {
                        "name": "Petr Parshakov"
                    }
                ],
                "author_detail": {
                    "name": "Petr Parshakov"
                },
                "author": "Petr Parshakov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03158v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03158v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03165v1",
                "updated": "2025-04-04T04:43:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    43,
                    13,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T04:43:13Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    43,
                    13,
                    4,
                    94,
                    0
                ],
                "title": "Efficient Dynamic Clustering-Based Document Compression for\n  Retrieval-Augmented-Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Dynamic Clustering-Based Document Compression for\n  Retrieval-Augmented-Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge integration during large language model (LLM) inference in recent\nyears. However, current RAG implementations face challenges in effectively\naddressing noise, repetition and redundancy in retrieved content, primarily due\nto their limited ability to exploit fine-grained inter-document relationships.\nTo address these limitations, we propose an \\textbf{E}fficient \\textbf{D}ynamic\n\\textbf{C}lustering-based document \\textbf{C}ompression framework\n(\\textbf{EDC\\textsuperscript{2}-RAG}) that effectively utilizes latent\ninter-document relationships while simultaneously removing irrelevant\ninformation and redundant content. We validate our approach, built upon\nGPT-3.5, on widely used knowledge-QA and hallucination-detected datasets. The\nresults show that this method achieves consistent performance improvements\nacross various scenarios and experimental settings, demonstrating strong\nrobustness and applicability. Our code and datasets can be found at\nhttps://github.com/Tsinghua-dhy/EDC-2-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge integration during large language model (LLM) inference in recent\nyears. However, current RAG implementations face challenges in effectively\naddressing noise, repetition and redundancy in retrieved content, primarily due\nto their limited ability to exploit fine-grained inter-document relationships.\nTo address these limitations, we propose an \\textbf{E}fficient \\textbf{D}ynamic\n\\textbf{C}lustering-based document \\textbf{C}ompression framework\n(\\textbf{EDC\\textsuperscript{2}-RAG}) that effectively utilizes latent\ninter-document relationships while simultaneously removing irrelevant\ninformation and redundant content. We validate our approach, built upon\nGPT-3.5, on widely used knowledge-QA and hallucination-detected datasets. The\nresults show that this method achieves consistent performance improvements\nacross various scenarios and experimental settings, demonstrating strong\nrobustness and applicability. Our code and datasets can be found at\nhttps://github.com/Tsinghua-dhy/EDC-2-RAG."
                },
                "authors": [
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Kaiming Liu"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Xuanyu Lei"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03161v1",
                "updated": "2025-04-04T04:42:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    42,
                    36,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T04:42:36Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    42,
                    36,
                    4,
                    94,
                    0
                ],
                "title": "Modified Tests of Linear Hypotheses Under Heteroscedasticity for\n  Multivariate Functional Data with Finite Sample Sizes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modified Tests of Linear Hypotheses Under Heteroscedasticity for\n  Multivariate Functional Data with Finite Sample Sizes"
                },
                "summary": "As big data continues to grow, statistical inference for multivariate\nfunctional data (MFD) has become crucial. Although recent advancements have\nbeen made in testing the equality of mean functions, research on testing linear\nhypotheses for mean functions remains limited. Current methods primarily\nconsist of permutation-based tests or asymptotic tests. However,\npermutation-based tests are known to be time-consuming, while asymptotic tests\ntypically require larger sample sizes to maintain an accurate Type I error\nrate. This paper introduces three finite-sample tests that modify traditional\nMANOVA methods to tackle the general linear hypothesis testing problem for MFD.\nThe test statistics rely on two symmetric, nonnegative-definite matrices,\napproximated by Wishart distributions, with degrees of freedom estimated via a\nU-statistics-based method. The proposed tests are affine-invariant,\ncomputationally more efficient than permutation-based tests, and better at\ncontrolling significance levels in small samples compared to asymptotic tests.\nA real-data example further showcases their practical utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As big data continues to grow, statistical inference for multivariate\nfunctional data (MFD) has become crucial. Although recent advancements have\nbeen made in testing the equality of mean functions, research on testing linear\nhypotheses for mean functions remains limited. Current methods primarily\nconsist of permutation-based tests or asymptotic tests. However,\npermutation-based tests are known to be time-consuming, while asymptotic tests\ntypically require larger sample sizes to maintain an accurate Type I error\nrate. This paper introduces three finite-sample tests that modify traditional\nMANOVA methods to tackle the general linear hypothesis testing problem for MFD.\nThe test statistics rely on two symmetric, nonnegative-definite matrices,\napproximated by Wishart distributions, with degrees of freedom estimated via a\nU-statistics-based method. The proposed tests are affine-invariant,\ncomputationally more efficient than permutation-based tests, and better at\ncontrolling significance levels in small samples compared to asymptotic tests.\nA real-data example further showcases their practical utility."
                },
                "authors": [
                    {
                        "name": "Tianming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Tianming Zhu"
                },
                "author": "Tianming Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03160v1",
                "updated": "2025-04-04T04:41:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    41,
                    28,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T04:41:28Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    41,
                    28,
                    4,
                    94,
                    0
                ],
                "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in\n  Real-world Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in\n  Real-world Environments"
                },
                "summary": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zheng"
                    },
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Xiangkun Hu"
                    },
                    {
                        "name": "Xiaojie Cai"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Pengrui Lu"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20279v2",
                "updated": "2025-04-04T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    36,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2025-03-26T07:08:15Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    7,
                    8,
                    15,
                    2,
                    85,
                    0
                ],
                "title": "sudo rm -rf agentic_security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sudo rm -rf agentic_security"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed as computer-use\nagents, autonomously performing tasks within real desktop or web environments.\nWhile this evolution greatly expands practical use cases for humans, it also\ncreates serious security exposures. We present SUDO (Screen-based Universal\nDetox2Tox Offense), a novel attack framework that systematically bypasses\nrefusal trained safeguards in commercial computer-use agents, such as Claude\nComputer Use. The core mechanism, Detox2Tox, transforms harmful requests (that\nagents initially reject) into seemingly benign requests via detoxification,\nsecures detailed instructions from advanced vision language models (VLMs), and\nthen reintroduces malicious content via toxification just before execution.\nUnlike conventional jailbreaks, SUDO iteratively refines its attacks based on a\nbuilt-in refusal feedback, making it increasingly effective against robust\npolicy filters. In extensive tests spanning 50 real-world tasks and multiple\nstate-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with\nno refinement), and up to 41% (by its iterative refinement) in Claude Computer\nUse. By revealing these vulnerabilities and demonstrating the ease with which\nthey can be exploited in real-world computing environments, this paper\nhighlights an immediate need for robust, context-aware safeguards. WARNING:\nThis paper includes harmful or offensive model outputs Our code is available\nat: https://github.com/AIM-Intelligence/SUDO.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed as computer-use\nagents, autonomously performing tasks within real desktop or web environments.\nWhile this evolution greatly expands practical use cases for humans, it also\ncreates serious security exposures. We present SUDO (Screen-based Universal\nDetox2Tox Offense), a novel attack framework that systematically bypasses\nrefusal trained safeguards in commercial computer-use agents, such as Claude\nComputer Use. The core mechanism, Detox2Tox, transforms harmful requests (that\nagents initially reject) into seemingly benign requests via detoxification,\nsecures detailed instructions from advanced vision language models (VLMs), and\nthen reintroduces malicious content via toxification just before execution.\nUnlike conventional jailbreaks, SUDO iteratively refines its attacks based on a\nbuilt-in refusal feedback, making it increasingly effective against robust\npolicy filters. In extensive tests spanning 50 real-world tasks and multiple\nstate-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with\nno refinement), and up to 41% (by its iterative refinement) in Claude Computer\nUse. By revealing these vulnerabilities and demonstrating the ease with which\nthey can be exploited in real-world computing environments, this paper\nhighlights an immediate need for robust, context-aware safeguards. WARNING:\nThis paper includes harmful or offensive model outputs Our code is available\nat: https://github.com/AIM-Intelligence/SUDO.git"
                },
                "authors": [
                    {
                        "name": "Sejin Lee"
                    },
                    {
                        "name": "Jian Kim"
                    },
                    {
                        "name": "Haon Park"
                    },
                    {
                        "name": "Ashkan Yousefpour"
                    },
                    {
                        "name": "Sangyoon Yu"
                    },
                    {
                        "name": "Min Song"
                    }
                ],
                "author_detail": {
                    "name": "Min Song"
                },
                "author": "Min Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03158v1",
                "updated": "2025-04-04T04:31:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    31,
                    19,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T04:31:19Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    31,
                    19,
                    4,
                    94,
                    0
                ],
                "title": "Accelerating Particle-based Energetic Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Particle-based Energetic Variational Inference"
                },
                "summary": "In this work, we propose a novel particle-based variational inference (ParVI)\nmethod that accelerates the EVI-Im. Inspired by energy quadratization (EQ) and\noperator splitting techniques for gradient flows, our approach efficiently\ndrives particles towards the target distribution. Unlike EVI-Im, which employs\nthe implicit Euler method to solve variational-preserving particle dynamics for\nminimizing the KL divergence, derived using a \"discretize-then-variational\"\napproach, the proposed algorithm avoids repeated evaluation of inter-particle\ninteraction terms, significantly reducing computational cost. The framework is\nalso extensible to other gradient-based sampling techniques. Through several\nnumerical experiments, we demonstrate that our method outperforms existing\nParVI approaches in efficiency, robustness, and accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a novel particle-based variational inference (ParVI)\nmethod that accelerates the EVI-Im. Inspired by energy quadratization (EQ) and\noperator splitting techniques for gradient flows, our approach efficiently\ndrives particles towards the target distribution. Unlike EVI-Im, which employs\nthe implicit Euler method to solve variational-preserving particle dynamics for\nminimizing the KL divergence, derived using a \"discretize-then-variational\"\napproach, the proposed algorithm avoids repeated evaluation of inter-particle\ninteraction terms, significantly reducing computational cost. The framework is\nalso extensible to other gradient-based sampling techniques. Through several\nnumerical experiments, we demonstrate that our method outperforms existing\nParVI approaches in efficiency, robustness, and accuracy."
                },
                "authors": [
                    {
                        "name": "Xuelian Bao"
                    },
                    {
                        "name": "Lulu Kang"
                    },
                    {
                        "name": "Chun Liu"
                    },
                    {
                        "name": "Yiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwei Wang"
                },
                "author": "Yiwei Wang",
                "arxiv_comment": "21 pages, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G05, 65K10, 65L05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00407v2",
                "updated": "2025-04-04T04:28:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    28,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-01T04:08:37Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    4,
                    8,
                    37,
                    1,
                    91,
                    0
                ],
                "title": "AMP4EC: Adaptive Model Partitioning Framework for Efficient Deep\n  Learning Inference in Edge Computing Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMP4EC: Adaptive Model Partitioning Framework for Efficient Deep\n  Learning Inference in Edge Computing Environments"
                },
                "summary": "Edge computing facilitates deep learning in resource-constrained\nenvironments, but challenges such as resource heterogeneity and dynamic\nconstraints persist. This paper introduces AMP4EC, an Adaptive Model\nPartitioning framework designed to optimize deep learning inference in edge\nenvironments through real-time resource monitoring, dynamic model partitioning,\nand adaptive task scheduling. AMP4EC features a resource-aware model\npartitioner that splits deep learning models based on device capabilities, a\ntask scheduler that ensures efficient load balancing using a weighted scoring\nmechanism, and a Docker-based deployment environment for validation.\nExperimental results show up to a 78% reduction in latency and a 414%\nimprovement in throughput compared to baseline methods. The framework achieves\nconsistent performance with low scheduling overhead across varying resource\nprofiles, demonstrating adaptability in high-resource (1 CPU, 1GB RAM) and\nlow-resource (0.4 CPU, 512MB RAM) scenarios. These results highlight AMP4EC's\nscalability, efficiency, and robustness for real-world edge deployments,\naddressing the critical need for efficient distributed inference in dynamic,\nresource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge computing facilitates deep learning in resource-constrained\nenvironments, but challenges such as resource heterogeneity and dynamic\nconstraints persist. This paper introduces AMP4EC, an Adaptive Model\nPartitioning framework designed to optimize deep learning inference in edge\nenvironments through real-time resource monitoring, dynamic model partitioning,\nand adaptive task scheduling. AMP4EC features a resource-aware model\npartitioner that splits deep learning models based on device capabilities, a\ntask scheduler that ensures efficient load balancing using a weighted scoring\nmechanism, and a Docker-based deployment environment for validation.\nExperimental results show up to a 78% reduction in latency and a 414%\nimprovement in throughput compared to baseline methods. The framework achieves\nconsistent performance with low scheduling overhead across varying resource\nprofiles, demonstrating adaptability in high-resource (1 CPU, 1GB RAM) and\nlow-resource (0.4 CPU, 512MB RAM) scenarios. These results highlight AMP4EC's\nscalability, efficiency, and robustness for real-world edge deployments,\naddressing the critical need for efficient distributed inference in dynamic,\nresource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Guilin Zhang"
                    },
                    {
                        "name": "Wulan Guo"
                    },
                    {
                        "name": "Ziqi Tan"
                    },
                    {
                        "name": "Hailong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hailong Jiang"
                },
                "author": "Hailong Jiang",
                "arxiv_comment": "8 pages, accepted for oral presentation at FMEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.6; C.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03155v1",
                "updated": "2025-04-04T04:25:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    25,
                    14,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T04:25:14Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    25,
                    14,
                    4,
                    94,
                    0
                ],
                "title": "Synthesizing Optimal Object Selection Predicates for Image Editing using\n  Lattices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Optimal Object Selection Predicates for Image Editing using\n  Lattices"
                },
                "summary": "Image editing is a common task across a wide range of domains, from personal\nuse to professional applications. Despite advances in computer vision, current\ntools still demand significant manual effort for editing tasks that require\nrepetitive operations on images with many objects. In this paper, we present a\nnovel approach to automating the image editing process using program synthesis.\nWe propose a new algorithm based on lattice structures to automatically\nsynthesize object selection predicates for image editing from positive and\nnegative examples. By leveraging the algebraic properties of lattices, our\nalgorithm efficiently synthesizes an optimal object selection predicate among\nmultiple correct solutions. We have implemented our technique and evaluated it\non 100 tasks over 20 images. The evaluation result demonstrates our tool is\neffective and efficient, which outperforms state-of-the-art synthesizers and\nLLM-based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image editing is a common task across a wide range of domains, from personal\nuse to professional applications. Despite advances in computer vision, current\ntools still demand significant manual effort for editing tasks that require\nrepetitive operations on images with many objects. In this paper, we present a\nnovel approach to automating the image editing process using program synthesis.\nWe propose a new algorithm based on lattice structures to automatically\nsynthesize object selection predicates for image editing from positive and\nnegative examples. By leveraging the algebraic properties of lattices, our\nalgorithm efficiently synthesizes an optimal object selection predicate among\nmultiple correct solutions. We have implemented our technique and evaluated it\non 100 tasks over 20 images. The evaluation result demonstrates our tool is\neffective and efficient, which outperforms state-of-the-art synthesizers and\nLLM-based approaches."
                },
                "authors": [
                    {
                        "name": "Yang He"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Yuepeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuepeng Wang"
                },
                "author": "Yuepeng Wang",
                "arxiv_comment": "29 pages, 10 tables, 9 figures, PLDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03154v1",
                "updated": "2025-04-04T04:24:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    24,
                    29,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T04:24:29Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    24,
                    29,
                    4,
                    94,
                    0
                ],
                "title": "TokenFLEX: Unified VLM Training for Flexible Visual Tokens Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenFLEX: Unified VLM Training for Flexible Visual Tokens Inference"
                },
                "summary": "Conventional Vision-Language Models(VLMs) typically utilize a fixed number of\nvision tokens, regardless of task complexity. This one-size-fits-all strategy\nintroduces notable inefficiencies: using excessive tokens leads to unnecessary\ncomputational overhead in simpler tasks, whereas insufficient tokens compromise\nfine-grained visual comprehension in more complex contexts. To overcome these\nlimitations, we present TokenFLEX, an innovative and adaptable vision-language\nframework that encodes images into a variable number of tokens for efficient\nintegration with a Large Language Model (LLM). Our approach is underpinned by\ntwo pivotal innovations. Firstly, we present a novel training paradigm that\nenhances performance across varying numbers of vision tokens by stochastically\nmodulating token counts during training. Secondly, we design a lightweight\nvision token projector incorporating an adaptive pooling layer and SwiGLU,\nallowing for flexible downsampling of vision tokens and adaptive selection of\nfeatures tailored to specific token counts. Comprehensive experiments reveal\nthat TokenFLEX consistently outperforms its fixed-token counterparts, achieving\nnotable performance gains across various token counts enhancements of 1.6%,\n1.0%, and 0.4% with 64, 144, and 256 tokens, respectively averaged over eight\nvision-language benchmarks. These results underscore TokenFLEX's remarkable\nflexibility while maintaining high-performance vision-language understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional Vision-Language Models(VLMs) typically utilize a fixed number of\nvision tokens, regardless of task complexity. This one-size-fits-all strategy\nintroduces notable inefficiencies: using excessive tokens leads to unnecessary\ncomputational overhead in simpler tasks, whereas insufficient tokens compromise\nfine-grained visual comprehension in more complex contexts. To overcome these\nlimitations, we present TokenFLEX, an innovative and adaptable vision-language\nframework that encodes images into a variable number of tokens for efficient\nintegration with a Large Language Model (LLM). Our approach is underpinned by\ntwo pivotal innovations. Firstly, we present a novel training paradigm that\nenhances performance across varying numbers of vision tokens by stochastically\nmodulating token counts during training. Secondly, we design a lightweight\nvision token projector incorporating an adaptive pooling layer and SwiGLU,\nallowing for flexible downsampling of vision tokens and adaptive selection of\nfeatures tailored to specific token counts. Comprehensive experiments reveal\nthat TokenFLEX consistently outperforms its fixed-token counterparts, achieving\nnotable performance gains across various token counts enhancements of 1.6%,\n1.0%, and 0.4% with 64, 144, and 256 tokens, respectively averaged over eight\nvision-language benchmarks. These results underscore TokenFLEX's remarkable\nflexibility while maintaining high-performance vision-language understanding."
                },
                "authors": [
                    {
                        "name": "Junshan Hu"
                    },
                    {
                        "name": "Jialiang Mao"
                    },
                    {
                        "name": "Zhikang Liu"
                    },
                    {
                        "name": "Zhongpu Xia"
                    },
                    {
                        "name": "Peng Jia"
                    },
                    {
                        "name": "Xianpeng Lang"
                    }
                ],
                "author_detail": {
                    "name": "Xianpeng Lang"
                },
                "author": "Xianpeng Lang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05625v3",
                "updated": "2025-04-04T04:07:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    7,
                    39,
                    4,
                    94,
                    0
                ],
                "published": "2024-07-08T05:35:54Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    5,
                    35,
                    54,
                    0,
                    190,
                    0
                ],
                "title": "New User Event Prediction Through the Lens of Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New User Event Prediction Through the Lens of Causal Inference"
                },
                "summary": "Modeling and analysis for event series generated by users of heterogeneous\nbehavioral patterns are closely involved in our daily lives, including credit\ncard fraud detection, online platform user recommendation, and social network\nanalysis. The most commonly adopted approach to this task is to assign users to\nbehavior-based categories and analyze each of them separately. However, this\nrequires extensive data to fully understand the user behavior, presenting\nchallenges in modeling newcomers without significant historical knowledge. In\nthis work, we propose a novel discrete event prediction framework for new users\nwith limited history, without needing to know the user's category. We treat the\nuser event history as the \"treatment\" for future events and the user category\nas the key confounder. Thus, the prediction problem can be framed as\ncounterfactual outcome estimation, where each event is re-weighted by its\ninverse propensity score. We demonstrate the improved performance of the\nproposed framework with a numerical simulation study and two real-world\napplications, including Netflix rating prediction and seller contact prediction\nfor customer support at Amazon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and analysis for event series generated by users of heterogeneous\nbehavioral patterns are closely involved in our daily lives, including credit\ncard fraud detection, online platform user recommendation, and social network\nanalysis. The most commonly adopted approach to this task is to assign users to\nbehavior-based categories and analyze each of them separately. However, this\nrequires extensive data to fully understand the user behavior, presenting\nchallenges in modeling newcomers without significant historical knowledge. In\nthis work, we propose a novel discrete event prediction framework for new users\nwith limited history, without needing to know the user's category. We treat the\nuser event history as the \"treatment\" for future events and the user category\nas the key confounder. Thus, the prediction problem can be framed as\ncounterfactual outcome estimation, where each event is re-weighted by its\ninverse propensity score. We demonstrate the improved performance of the\nproposed framework with a numerical simulation study and two real-world\napplications, including Netflix rating prediction and seller contact prediction\nfor customer support at Amazon."
                },
                "authors": [
                    {
                        "name": "Henry Shaowu Yuchi"
                    },
                    {
                        "name": "Shixiang Zhu"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Yigit M. Arisoy"
                    },
                    {
                        "name": "Matthew C. Spencer"
                    }
                ],
                "author_detail": {
                    "name": "Matthew C. Spencer"
                },
                "author": "Matthew C. Spencer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03151v1",
                "updated": "2025-04-04T04:04:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    4,
                    56,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T04:04:56Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    4,
                    56,
                    4,
                    94,
                    0
                ],
                "title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning\n  (v1)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning\n  (v1)"
                },
                "summary": "Reasoning is central to human intelligence, enabling structured\nproblem-solving across diverse tasks. Recent advances in large language models\n(LLMs) have greatly enhanced their reasoning abilities in arithmetic,\ncommonsense, and symbolic domains. However, effectively extending these\ncapabilities into multimodal contexts-where models must integrate both visual\nand textual inputs-continues to be a significant challenge. Multimodal\nreasoning introduces complexities, such as handling conflicting information\nacross modalities, which require models to adopt advanced interpretative\nstrategies. Addressing these challenges involves not only sophisticated\nalgorithms but also robust methodologies for evaluating reasoning accuracy and\ncoherence. This paper offers a concise yet insightful overview of reasoning\ntechniques in both textual and multimodal LLMs. Through a thorough and\nup-to-date comparison, we clearly formulate core reasoning challenges and\nopportunities, highlighting practical methods for post-training optimization\nand test-time inference. Our work provides valuable insights and guidance,\nbridging theoretical frameworks and practical implementations, and sets clear\ndirections for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is central to human intelligence, enabling structured\nproblem-solving across diverse tasks. Recent advances in large language models\n(LLMs) have greatly enhanced their reasoning abilities in arithmetic,\ncommonsense, and symbolic domains. However, effectively extending these\ncapabilities into multimodal contexts-where models must integrate both visual\nand textual inputs-continues to be a significant challenge. Multimodal\nreasoning introduces complexities, such as handling conflicting information\nacross modalities, which require models to adopt advanced interpretative\nstrategies. Addressing these challenges involves not only sophisticated\nalgorithms but also robust methodologies for evaluating reasoning accuracy and\ncoherence. This paper offers a concise yet insightful overview of reasoning\ntechniques in both textual and multimodal LLMs. Through a thorough and\nup-to-date comparison, we clearly formulate core reasoning challenges and\nopportunities, highlighting practical methods for post-training optimization\nand test-time inference. Our work provides valuable insights and guidance,\nbridging theoretical frameworks and practical implementations, and sets clear\ndirections for future research."
                },
                "authors": [
                    {
                        "name": "Jing Bi"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Xiaofei Zhou"
                    },
                    {
                        "name": "Pinxin Liu"
                    },
                    {
                        "name": "Junjia Guo"
                    },
                    {
                        "name": "Yunlong Tang"
                    },
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Jinxi He"
                    },
                    {
                        "name": "Jiarui Wu"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Daoan Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lianggong Bruce Wen"
                    },
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.12593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12593v3",
                "updated": "2025-04-04T17:58:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    58,
                    8,
                    4,
                    94,
                    0
                ],
                "published": "2024-11-19T18:04:13Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    4,
                    13,
                    1,
                    324,
                    0
                ],
                "title": "AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive\n  Cross-Modality Memory Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive\n  Cross-Modality Memory Reduction"
                },
                "summary": "The advancements in large language models (LLMs) have propelled the\nimprovement of video understanding tasks by incorporating LLMs with visual\nmodels. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat)\nare constrained to processing short-duration videos. Recent attempts to\nunderstand long-term videos by extracting and compressing visual features into\na fixed memory size. Nevertheless, those methods leverage only visual modality\nto merge video tokens and overlook the correlation between visual and textual\nqueries, leading to difficulties in effectively handling complex\nquestion-answering tasks. To address the challenges of long videos and complex\nprompts, we propose AdaCM$^2$, which, for the first time, introduces an\nadaptive cross-modality memory reduction approach to video-text alignment in an\nauto-regressive manner on video streams. Our extensive experiments on various\nvideo understanding tasks, such as video captioning, video question answering,\nand video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art\nperformance across multiple datasets while significantly reducing memory usage.\nNotably, it achieves a 4.5% improvement across multiple tasks in the LVU\ndataset with a GPU memory consumption reduction of up to 65%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancements in large language models (LLMs) have propelled the\nimprovement of video understanding tasks by incorporating LLMs with visual\nmodels. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat)\nare constrained to processing short-duration videos. Recent attempts to\nunderstand long-term videos by extracting and compressing visual features into\na fixed memory size. Nevertheless, those methods leverage only visual modality\nto merge video tokens and overlook the correlation between visual and textual\nqueries, leading to difficulties in effectively handling complex\nquestion-answering tasks. To address the challenges of long videos and complex\nprompts, we propose AdaCM$^2$, which, for the first time, introduces an\nadaptive cross-modality memory reduction approach to video-text alignment in an\nauto-regressive manner on video streams. Our extensive experiments on various\nvideo understanding tasks, such as video captioning, video question answering,\nand video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art\nperformance across multiple datasets while significantly reducing memory usage.\nNotably, it achieves a 4.5% improvement across multiple tasks in the LVU\ndataset with a GPU memory consumption reduction of up to 65%."
                },
                "authors": [
                    {
                        "name": "Yuanbin Man"
                    },
                    {
                        "name": "Ying Huang"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Bingzhe Li"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Miao Yin"
                    }
                ],
                "author_detail": {
                    "name": "Miao Yin"
                },
                "author": "Miao Yin",
                "arxiv_comment": "CVPR 2025 Highlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03635v1",
                "updated": "2025-04-04T17:57:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    57,
                    22,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:57:22Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    57,
                    22,
                    4,
                    94,
                    0
                ],
                "title": "Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling\n  Law for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling\n  Law for Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Shawn Tan"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Rameswar Panda"
                    },
                    {
                        "name": "Yikang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yikang Shen"
                },
                "author": "Yikang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03622v1",
                "updated": "2025-04-04T17:40:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    40,
                    4,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:40:04Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    40,
                    4,
                    4,
                    94,
                    0
                ],
                "title": "Align to Structure: Aligning Large Language Models with Structural\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align to Structure: Aligning Large Language Models with Structural\n  Information"
                },
                "summary": "Generating long, coherent text remains a challenge for large language models\n(LLMs), as they lack hierarchical planning and structured organization in\ndiscourse generation. We introduce Structural Alignment, a novel method that\naligns LLMs with human-like discourse structures to enhance long-form text\ngeneration. By integrating linguistically grounded discourse frameworks into\nreinforcement learning, our approach guides models to produce coherent and\nwell-organized outputs. We employ a dense reward scheme within a Proximal\nPolicy Optimization framework, assigning fine-grained, token-level rewards\nbased on the discourse distinctiveness relative to human writing. Two\ncomplementary reward models are evaluated: the first improves readability by\nscoring surface-level textual features to provide explicit structuring, while\nthe second reinforces deeper coherence and rhetorical sophistication by\nanalyzing global discourse patterns through hierarchical discourse motifs,\noutperforming both standard and RLHF-enhanced models in tasks such as essay\ngeneration and long-document summarization. All training data and code will be\npublicly shared at https://github.com/minnesotanlp/struct_align.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, coherent text remains a challenge for large language models\n(LLMs), as they lack hierarchical planning and structured organization in\ndiscourse generation. We introduce Structural Alignment, a novel method that\naligns LLMs with human-like discourse structures to enhance long-form text\ngeneration. By integrating linguistically grounded discourse frameworks into\nreinforcement learning, our approach guides models to produce coherent and\nwell-organized outputs. We employ a dense reward scheme within a Proximal\nPolicy Optimization framework, assigning fine-grained, token-level rewards\nbased on the discourse distinctiveness relative to human writing. Two\ncomplementary reward models are evaluated: the first improves readability by\nscoring surface-level textual features to provide explicit structuring, while\nthe second reinforces deeper coherence and rhetorical sophistication by\nanalyzing global discourse patterns through hierarchical discourse motifs,\noutperforming both standard and RLHF-enhanced models in tasks such as essay\ngeneration and long-document summarization. All training data and code will be\npublicly shared at https://github.com/minnesotanlp/struct_align."
                },
                "authors": [
                    {
                        "name": "Zae Myung Kim"
                    },
                    {
                        "name": "Anand Ramachandran"
                    },
                    {
                        "name": "Farideh Tavazoee"
                    },
                    {
                        "name": "Joo-Kyung Kim"
                    },
                    {
                        "name": "Oleg Rokhlenko"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03621v1",
                "updated": "2025-04-04T17:39:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    39,
                    53,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:39:53Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    39,
                    53,
                    4,
                    94,
                    0
                ],
                "title": "VISTA-OCR: Towards generative and interactive end to end OCR models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISTA-OCR: Towards generative and interactive end to end OCR models"
                },
                "summary": "We introduce \\textbf{VISTA-OCR} (Vision and Spatially-aware Text Analysis\nOCR), a lightweight architecture that unifies text detection and recognition\nwithin a single generative model. Unlike conventional methods that require\nseparate branches with dedicated parameters for text recognition and detection,\nour approach leverages a Transformer decoder to sequentially generate text\ntranscriptions and their spatial coordinates in a unified branch. Built on an\nencoder-decoder architecture, VISTA-OCR is progressively trained, starting with\nthe visual feature extraction phase, followed by multitask learning with\nmultimodal token generation. To address the increasing demand for versatile OCR\nsystems capable of advanced tasks, such as content-based text localization\n\\ref{content_based_localization}, we introduce new prompt-controllable OCR\ntasks during pre-training.To enhance the model's capabilities, we built a new\ndataset composed of real-world examples enriched with bounding box annotations\nand synthetic samples. Although recent Vision Large Language Models (VLLMs) can\nefficiently perform these tasks, their high computational cost remains a\nbarrier for practical deployment. In contrast, our VISTA$_{\\text{omni}}$\nvariant processes both handwritten and printed documents with only 150M\nparameters, interactively, by prompting. Extensive experiments on multiple\ndatasets demonstrate that VISTA-OCR achieves better performance compared to\nstate-of-the-art specialized models on standard OCR tasks while showing strong\npotential for more sophisticated OCR applications, addressing the growing need\nfor interactive OCR systems. All code and annotations for VISTA-OCR will be\nmade publicly available upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce \\textbf{VISTA-OCR} (Vision and Spatially-aware Text Analysis\nOCR), a lightweight architecture that unifies text detection and recognition\nwithin a single generative model. Unlike conventional methods that require\nseparate branches with dedicated parameters for text recognition and detection,\nour approach leverages a Transformer decoder to sequentially generate text\ntranscriptions and their spatial coordinates in a unified branch. Built on an\nencoder-decoder architecture, VISTA-OCR is progressively trained, starting with\nthe visual feature extraction phase, followed by multitask learning with\nmultimodal token generation. To address the increasing demand for versatile OCR\nsystems capable of advanced tasks, such as content-based text localization\n\\ref{content_based_localization}, we introduce new prompt-controllable OCR\ntasks during pre-training.To enhance the model's capabilities, we built a new\ndataset composed of real-world examples enriched with bounding box annotations\nand synthetic samples. Although recent Vision Large Language Models (VLLMs) can\nefficiently perform these tasks, their high computational cost remains a\nbarrier for practical deployment. In contrast, our VISTA$_{\\text{omni}}$\nvariant processes both handwritten and printed documents with only 150M\nparameters, interactively, by prompting. Extensive experiments on multiple\ndatasets demonstrate that VISTA-OCR achieves better performance compared to\nstate-of-the-art specialized models on standard OCR tasks while showing strong\npotential for more sophisticated OCR applications, addressing the growing need\nfor interactive OCR systems. All code and annotations for VISTA-OCR will be\nmade publicly available upon acceptance."
                },
                "authors": [
                    {
                        "name": "Laziz Hamdi"
                    },
                    {
                        "name": "Amine Tamasna"
                    },
                    {
                        "name": "Pascal Boisson"
                    },
                    {
                        "name": "Thierry Paquet"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Paquet"
                },
                "author": "Thierry Paquet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03616v1",
                "updated": "2025-04-04T17:35:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    35,
                    43,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:35:43Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    35,
                    43,
                    4,
                    94,
                    0
                ],
                "title": "Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task"
                },
                "summary": "Retrieval-augmented generation (RAG) has become a cornerstone of contemporary\nNLP, enhancing large language models (LLMs) by allowing them to access richer\nfactual contexts through in-context retrieval. While effective in monolingual\nsettings, especially in English, its use in multilingual tasks remains\nunexplored. This paper investigates the effectiveness of RAG across multiple\nlanguages by proposing novel approaches for multilingual open-domain\nquestion-answering. We evaluate the performance of various multilingual RAG\nstrategies, including question-translation (tRAG), which translates questions\ninto English before retrieval, and Multilingual RAG (MultiRAG), where retrieval\noccurs directly across multiple languages. Our findings reveal that tRAG, while\nuseful, suffers from limited coverage. In contrast, MultiRAG improves\nefficiency by enabling multilingual retrieval but introduces inconsistencies\ndue to cross-lingual variations in the retrieved content. To address these\nissues, we propose Crosslingual RAG (CrossRAG), a method that translates\nretrieved documents into a common language (e.g., English) before generating\nthe response. Our experiments show that CrossRAG significantly enhances\nperformance on knowledge-intensive tasks, benefiting both high-resource and\nlow-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has become a cornerstone of contemporary\nNLP, enhancing large language models (LLMs) by allowing them to access richer\nfactual contexts through in-context retrieval. While effective in monolingual\nsettings, especially in English, its use in multilingual tasks remains\nunexplored. This paper investigates the effectiveness of RAG across multiple\nlanguages by proposing novel approaches for multilingual open-domain\nquestion-answering. We evaluate the performance of various multilingual RAG\nstrategies, including question-translation (tRAG), which translates questions\ninto English before retrieval, and Multilingual RAG (MultiRAG), where retrieval\noccurs directly across multiple languages. Our findings reveal that tRAG, while\nuseful, suffers from limited coverage. In contrast, MultiRAG improves\nefficiency by enabling multilingual retrieval but introduces inconsistencies\ndue to cross-lingual variations in the retrieved content. To address these\nissues, we propose Crosslingual RAG (CrossRAG), a method that translates\nretrieved documents into a common language (e.g., English) before generating\nthe response. Our experiments show that CrossRAG significantly enhances\nperformance on knowledge-intensive tasks, benefiting both high-resource and\nlow-resource languages."
                },
                "authors": [
                    {
                        "name": "Leonardo Ranaldi"
                    },
                    {
                        "name": "Barry Haddow"
                    },
                    {
                        "name": "Alexandra Birch"
                    }
                ],
                "author_detail": {
                    "name": "Alexandra Birch"
                },
                "author": "Alexandra Birch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07958v2",
                "updated": "2025-04-04T17:33:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    33,
                    53,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-10T22:51:31Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    51,
                    31,
                    1,
                    345,
                    0
                ],
                "title": "PAFFA: Premeditated Actions For Fast Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAFFA: Premeditated Actions For Fast Agents"
                },
                "summary": "Modern AI assistants have made significant progress in natural language\nunderstanding and tool-use, with emerging efforts to interact with Web\ninterfaces. However, current approaches that heavily rely on repeated\nLLM-driven HTML parsing are computationally expensive and error-prone,\nparticularly when handling dynamic web interfaces and multi-step tasks. We\nintroduce PAFFA (Premeditated Actions For Fast Agents), a method that makes\nLLMs faster and more accurate in completing tasks on the internet using a novel\ninference-time technique that requires no task-specific training. PAFFA\nconstructs an 'Action Library', leveraging the parametric knowledge of the base\nLLM to pre-compute browser interaction patterns that generalize across tasks.\nBy strategically re-using LLM inference across tasks - either via 'Dist-Map'\nfor task-agnostic identification of key interactive web elements, or 'Unravel'\nfor first-encounter, stateful exploration of novel tasks/sites) - PAFFA\ndrastically reduces inference time tokens by 87% while maintaining robust\nperformance (achieving 0.57 vs. 0.50 step accuracy compared to baseline).\nFurther, Unravel's ability to update its action library based on explorations\nallows generalization and adaptation to unseen websites. In sum, this work\nexhibits that LLM reasoning sequences can generalize across prompts, offering a\nway to scale inference-time techniques for internet-scale data with sublinear\ntoken count.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI assistants have made significant progress in natural language\nunderstanding and tool-use, with emerging efforts to interact with Web\ninterfaces. However, current approaches that heavily rely on repeated\nLLM-driven HTML parsing are computationally expensive and error-prone,\nparticularly when handling dynamic web interfaces and multi-step tasks. We\nintroduce PAFFA (Premeditated Actions For Fast Agents), a method that makes\nLLMs faster and more accurate in completing tasks on the internet using a novel\ninference-time technique that requires no task-specific training. PAFFA\nconstructs an 'Action Library', leveraging the parametric knowledge of the base\nLLM to pre-compute browser interaction patterns that generalize across tasks.\nBy strategically re-using LLM inference across tasks - either via 'Dist-Map'\nfor task-agnostic identification of key interactive web elements, or 'Unravel'\nfor first-encounter, stateful exploration of novel tasks/sites) - PAFFA\ndrastically reduces inference time tokens by 87% while maintaining robust\nperformance (achieving 0.57 vs. 0.50 step accuracy compared to baseline).\nFurther, Unravel's ability to update its action library based on explorations\nallows generalization and adaptation to unseen websites. In sum, this work\nexhibits that LLM reasoning sequences can generalize across prompts, offering a\nway to scale inference-time techniques for internet-scale data with sublinear\ntoken count."
                },
                "authors": [
                    {
                        "name": "Shambhavi Krishna"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Yuan Ling"
                    },
                    {
                        "name": "Xiaojiang Huang"
                    },
                    {
                        "name": "Yingjie Li"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Xiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Li"
                },
                "author": "Xiang Li",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03612v1",
                "updated": "2025-04-04T17:33:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    33,
                    7,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:33:07Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    33,
                    7,
                    4,
                    94,
                    0
                ],
                "title": "AIR: A Systematic Analysis of Annotations, Instructions, and Response\n  Pairs in Preference Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIR: A Systematic Analysis of Annotations, Instructions, and Response\n  Pairs in Preference Dataset"
                },
                "summary": "Preference learning is critical for aligning large language models (LLMs)\nwith human values, yet its success hinges on high-quality datasets comprising\nthree core components: Preference \\textbf{A}nnotations, \\textbf{I}nstructions,\nand \\textbf{R}esponse Pairs. Current approaches conflate these components,\nobscuring their individual impacts and hindering systematic optimization. In\nthis work, we propose \\textbf{AIR}, a component-wise analysis framework that\nsystematically isolates and optimizes each component while evaluating their\nsynergistic effects. Through rigorous experimentation, AIR reveals actionable\nprinciples: annotation simplicity (point-wise generative scoring), instruction\ninference stability (variance-based filtering across LLMs), and response pair\nquality (moderate margins + high absolute scores). When combined, these\nprinciples yield +5.3 average gains over baseline method, even with only 14k\nhigh-quality pairs. Our work shifts preference dataset design from ad hoc\nscaling to component-aware optimization, offering a blueprint for efficient,\nreproducible alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference learning is critical for aligning large language models (LLMs)\nwith human values, yet its success hinges on high-quality datasets comprising\nthree core components: Preference \\textbf{A}nnotations, \\textbf{I}nstructions,\nand \\textbf{R}esponse Pairs. Current approaches conflate these components,\nobscuring their individual impacts and hindering systematic optimization. In\nthis work, we propose \\textbf{AIR}, a component-wise analysis framework that\nsystematically isolates and optimizes each component while evaluating their\nsynergistic effects. Through rigorous experimentation, AIR reveals actionable\nprinciples: annotation simplicity (point-wise generative scoring), instruction\ninference stability (variance-based filtering across LLMs), and response pair\nquality (moderate margins + high absolute scores). When combined, these\nprinciples yield +5.3 average gains over baseline method, even with only 14k\nhigh-quality pairs. Our work shifts preference dataset design from ad hoc\nscaling to component-aware optimization, offering a blueprint for efficient,\nreproducible alignment."
                },
                "authors": [
                    {
                        "name": "Bingxiang He"
                    },
                    {
                        "name": "Wenbin Zhang"
                    },
                    {
                        "name": "Jiaxi Song"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Zixuan Fu"
                    },
                    {
                        "name": "Bowen Sun"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Haiwen Hong"
                    },
                    {
                        "name": "Longtao Huang"
                    },
                    {
                        "name": "Hui Xue"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03603v1",
                "updated": "2025-04-04T17:20:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    20,
                    5,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:20:05Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    20,
                    5,
                    4,
                    94,
                    0
                ],
                "title": "Towards deployment-centric multimodal AI beyond vision and language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards deployment-centric multimodal AI beyond vision and language"
                },
                "summary": "Multimodal artificial intelligence (AI) integrates diverse types of data via\nmachine learning to improve understanding, prediction, and decision-making\nacross disciplines such as healthcare, science, and engineering. However, most\nmultimodal AI advances focus on models for vision and language data, while\ntheir deployability remains a key challenge. We advocate a deployment-centric\nworkflow that incorporates deployment constraints early to reduce the\nlikelihood of undeployable solutions, complementing data-centric and\nmodel-centric approaches. We also emphasise deeper integration across multiple\nlevels of multimodality and multidisciplinary collaboration to significantly\nbroaden the research scope beyond vision and language. To facilitate this\napproach, we identify common multimodal-AI-specific challenges shared across\ndisciplines and examine three real-world use cases: pandemic response,\nself-driving car design, and climate change adaptation, drawing expertise from\nhealthcare, social science, engineering, science, sustainability, and finance.\nBy fostering multidisciplinary dialogue and open research practices, our\ncommunity can accelerate deployment-centric development for broad societal\nimpact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal artificial intelligence (AI) integrates diverse types of data via\nmachine learning to improve understanding, prediction, and decision-making\nacross disciplines such as healthcare, science, and engineering. However, most\nmultimodal AI advances focus on models for vision and language data, while\ntheir deployability remains a key challenge. We advocate a deployment-centric\nworkflow that incorporates deployment constraints early to reduce the\nlikelihood of undeployable solutions, complementing data-centric and\nmodel-centric approaches. We also emphasise deeper integration across multiple\nlevels of multimodality and multidisciplinary collaboration to significantly\nbroaden the research scope beyond vision and language. To facilitate this\napproach, we identify common multimodal-AI-specific challenges shared across\ndisciplines and examine three real-world use cases: pandemic response,\nself-driving car design, and climate change adaptation, drawing expertise from\nhealthcare, social science, engineering, science, sustainability, and finance.\nBy fostering multidisciplinary dialogue and open research practices, our\ncommunity can accelerate deployment-centric development for broad societal\nimpact."
                },
                "authors": [
                    {
                        "name": "Xianyuan Liu"
                    },
                    {
                        "name": "Jiayang Zhang"
                    },
                    {
                        "name": "Shuo Zhou"
                    },
                    {
                        "name": "Thijs L. van der Plas"
                    },
                    {
                        "name": "Avish Vijayaraghavan"
                    },
                    {
                        "name": "Anastasiia Grishina"
                    },
                    {
                        "name": "Mengdie Zhuang"
                    },
                    {
                        "name": "Daniel Schofield"
                    },
                    {
                        "name": "Christopher Tomlinson"
                    },
                    {
                        "name": "Yuhan Wang"
                    },
                    {
                        "name": "Ruizhe Li"
                    },
                    {
                        "name": "Louisa van Zeeland"
                    },
                    {
                        "name": "Sina Tabakhi"
                    },
                    {
                        "name": "Cyndie Demeocq"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Arunav Das"
                    },
                    {
                        "name": "Orlando Timmerman"
                    },
                    {
                        "name": "Thomas Baldwin-McDonald"
                    },
                    {
                        "name": "Jinge Wu"
                    },
                    {
                        "name": "Peizhen Bai"
                    },
                    {
                        "name": "Zahraa Al Sahili"
                    },
                    {
                        "name": "Omnia Alwazzan"
                    },
                    {
                        "name": "Thao N. Do"
                    },
                    {
                        "name": "Mohammod N. I. Suvon"
                    },
                    {
                        "name": "Angeline Wang"
                    },
                    {
                        "name": "Lucia Cipolina-Kun"
                    },
                    {
                        "name": "Luigi A. Moretti"
                    },
                    {
                        "name": "Lucas Farndale"
                    },
                    {
                        "name": "Nitisha Jain"
                    },
                    {
                        "name": "Natalia Efremova"
                    },
                    {
                        "name": "Yan Ge"
                    },
                    {
                        "name": "Marta Varela"
                    },
                    {
                        "name": "Hak-Keung Lam"
                    },
                    {
                        "name": "Oya Celiktutan"
                    },
                    {
                        "name": "Ben R. Evans"
                    },
                    {
                        "name": "Alejandro Coca-Castro"
                    },
                    {
                        "name": "Honghan Wu"
                    },
                    {
                        "name": "Zahraa S. Abdallah"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Valentin Danchev"
                    },
                    {
                        "name": "Nataliya Tkachenko"
                    },
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Tingting Zhu"
                    },
                    {
                        "name": "Gregory G. Slabaugh"
                    },
                    {
                        "name": "Roger K. Moore"
                    },
                    {
                        "name": "William K. Cheung"
                    },
                    {
                        "name": "Peter H. Charlton"
                    },
                    {
                        "name": "Haiping Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haiping Lu"
                },
                "author": "Haiping Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15556v2",
                "updated": "2025-04-04T17:13:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    13,
                    59,
                    4,
                    94,
                    0
                ],
                "published": "2025-03-18T20:23:51Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    20,
                    23,
                    51,
                    1,
                    77,
                    0
                ],
                "title": "Fully Automated Generation of Combinatorial Optimisation Systems Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Automated Generation of Combinatorial Optimisation Systems Using\n  Large Language Models"
                },
                "summary": "Over the last few decades, researchers have made considerable efforts to make\ndecision support more accessible for small and medium enterprises by reducing\nthe cost of designing, developing and maintaining automated decision support\nsystems. However, due to the diversity of the underlying combinatorial\noptimisation problems, reusability of such systems has been limited; in most\ncases, expensive expertise has been required to implement bespoke software\ncomponents.\n  We explore the feasibility of fully automated generation of combinatorial\noptimisation systems using large language models (LLMs). An LLM will be\nresponsible for interpreting the user-provided problem description in natural\nlanguage and designing and implementing problem-specific software components.\nWe discuss the principles of fully automated LLM-based optimisation system\ngeneration, and evaluate several proof-of-concept generators, comparing their\nperformance on four optimisation problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the last few decades, researchers have made considerable efforts to make\ndecision support more accessible for small and medium enterprises by reducing\nthe cost of designing, developing and maintaining automated decision support\nsystems. However, due to the diversity of the underlying combinatorial\noptimisation problems, reusability of such systems has been limited; in most\ncases, expensive expertise has been required to implement bespoke software\ncomponents.\n  We explore the feasibility of fully automated generation of combinatorial\noptimisation systems using large language models (LLMs). An LLM will be\nresponsible for interpreting the user-provided problem description in natural\nlanguage and designing and implementing problem-specific software components.\nWe discuss the principles of fully automated LLM-based optimisation system\ngeneration, and evaluate several proof-of-concept generators, comparing their\nperformance on four optimisation problems."
                },
                "authors": [
                    {
                        "name": "Daniel Karapetyan"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Karapetyan"
                },
                "author": "Daniel Karapetyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03601v1",
                "updated": "2025-04-04T17:13:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    13,
                    57,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:13:57Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    13,
                    57,
                    4,
                    94,
                    0
                ],
                "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated\n  Agent-Human Interplay"
                },
                "summary": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training effective AI agents for multi-turn interactions requires\nhigh-quality data that captures realistic human-agent dynamics, yet such data\nis scarce and expensive to collect manually. We introduce APIGen-MT, a\ntwo-phase framework that generates verifiable and diverse multi-turn agent\ndata. In the first phase, our agentic pipeline produces detailed task\nblueprints with ground-truth actions, leveraging a committee of LLM reviewers\nand iterative feedback loops. These blueprints are then transformed into\ncomplete interaction trajectories through simulated human-agent interplay. We\ntrain a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B\nto 70B parameters. Our models outperform frontier models such as GPT-4o and\nClaude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models\nsurpassing their larger counterparts, particularly in multi-turn settings,\nwhile maintaining superior consistency across multiple trials. Comprehensive\nexperiments demonstrate that our verified blueprint-to-details approach yields\nhigh-quality training data, enabling the development of more reliable,\nefficient, and capable agents. We open-source both the synthetic data collected\nand the trained xLAM-2-fc-r models to advance research in AI agents. Models are\navailable on HuggingFace at\nhttps://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4\nand project website is https://apigen-mt.github.io"
                },
                "authors": [
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Tulika Awalgaonkar"
                    },
                    {
                        "name": "Haolin Chen"
                    },
                    {
                        "name": "Thai Hoang"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "12 pages plus references and appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03600v1",
                "updated": "2025-04-04T17:13:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    13,
                    37,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:13:37Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    13,
                    37,
                    4,
                    94,
                    0
                ],
                "title": "MedSAM2: Segment Anything in 3D Medical Images and Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedSAM2: Segment Anything in 3D Medical Images and Videos"
                },
                "summary": "Medical image and video segmentation is a critical task for precision\nmedicine, which has witnessed considerable progress in developing task or\nmodality-specific and generalist models for 2D images. However, there have been\nlimited studies on building general-purpose models for 3D images and videos\nwith comprehensive user studies. Here, we present MedSAM2, a promptable\nsegmentation foundation model for 3D image and video segmentation. The model is\ndeveloped by fine-tuning the Segment Anything Model 2 on a large medical\ndataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming\nprevious models across a wide range of organs, lesions, and imaging modalities.\nFurthermore, we implement a human-in-the-loop pipeline to facilitate the\ncreation of large-scale datasets resulting in, to the best of our knowledge,\nthe most extensive user study to date, involving the annotation of 5,000 CT\nlesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames,\ndemonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is\nalso integrated into widely used platforms with user-friendly interfaces for\nlocal and cloud deployment, making it a practical tool for supporting\nefficient, scalable, and high-quality segmentation in both research and\nhealthcare environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical image and video segmentation is a critical task for precision\nmedicine, which has witnessed considerable progress in developing task or\nmodality-specific and generalist models for 2D images. However, there have been\nlimited studies on building general-purpose models for 3D images and videos\nwith comprehensive user studies. Here, we present MedSAM2, a promptable\nsegmentation foundation model for 3D image and video segmentation. The model is\ndeveloped by fine-tuning the Segment Anything Model 2 on a large medical\ndataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming\nprevious models across a wide range of organs, lesions, and imaging modalities.\nFurthermore, we implement a human-in-the-loop pipeline to facilitate the\ncreation of large-scale datasets resulting in, to the best of our knowledge,\nthe most extensive user study to date, involving the annotation of 5,000 CT\nlesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames,\ndemonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is\nalso integrated into widely used platforms with user-friendly interfaces for\nlocal and cloud deployment, making it a practical tool for supporting\nefficient, scalable, and high-quality segmentation in both research and\nhealthcare environments."
                },
                "authors": [
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Zongxin Yang"
                    },
                    {
                        "name": "Sumin Kim"
                    },
                    {
                        "name": "Bihui Chen"
                    },
                    {
                        "name": "Mohammed Baharoon"
                    },
                    {
                        "name": "Adibvafa Fallahpour"
                    },
                    {
                        "name": "Reza Asakereh"
                    },
                    {
                        "name": "Hongwei Lyu"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "arxiv_comment": "https://medsam2.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03598v1",
                "updated": "2025-04-04T17:08:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    8,
                    46,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:08:46Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    8,
                    46,
                    4,
                    94,
                    0
                ],
                "title": "EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline"
                },
                "summary": "Existing information retrieval systems excel in cases where the language of\ntarget documents closely matches that of the user query. However, real-world\nretrieval systems are often required to implicitly reason whether a document is\nrelevant. For example, when retrieving technical texts or tables, their\nrelevance to the user query may be implied through a particular jargon or\nstructure, rather than explicitly expressed in their content. Large language\nmodels (LLMs) hold great potential in identifying such implied relevance by\nleveraging their reasoning skills. Nevertheless, current LLM-augmented\nretrieval is hindered by high latency and computation cost, as the LLM\ntypically computes the query-document relevance online, for every query anew.\nTo tackle this issue we introduce EnrichIndex, a retrieval approach which\ninstead uses the LLM offline to build semantically-enriched retrieval indices,\nby performing a single pass over all documents in the retrieval corpus once\nduring ingestion time. Furthermore, the semantically-enriched indices can\ncomplement existing online retrieval approaches, boosting the performance of\nLLM re-rankers. We evaluated EnrichIndex on five retrieval tasks, involving\npassages and tables, and found that it outperforms strong online LLM-based\nretrieval systems, with an average improvement of 11.7 points in recall @ 10\nand 10.6 points in NDCG @ 10 compared to strong baselines. In terms of online\ncalls to the LLM, it processes 293.3 times fewer tokens which greatly reduces\nthe online latency and cost. Overall, EnrichIndex is an effective way to build\nbetter retrieval indices offline by leveraging the strong reasoning skills of\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing information retrieval systems excel in cases where the language of\ntarget documents closely matches that of the user query. However, real-world\nretrieval systems are often required to implicitly reason whether a document is\nrelevant. For example, when retrieving technical texts or tables, their\nrelevance to the user query may be implied through a particular jargon or\nstructure, rather than explicitly expressed in their content. Large language\nmodels (LLMs) hold great potential in identifying such implied relevance by\nleveraging their reasoning skills. Nevertheless, current LLM-augmented\nretrieval is hindered by high latency and computation cost, as the LLM\ntypically computes the query-document relevance online, for every query anew.\nTo tackle this issue we introduce EnrichIndex, a retrieval approach which\ninstead uses the LLM offline to build semantically-enriched retrieval indices,\nby performing a single pass over all documents in the retrieval corpus once\nduring ingestion time. Furthermore, the semantically-enriched indices can\ncomplement existing online retrieval approaches, boosting the performance of\nLLM re-rankers. We evaluated EnrichIndex on five retrieval tasks, involving\npassages and tables, and found that it outperforms strong online LLM-based\nretrieval systems, with an average improvement of 11.7 points in recall @ 10\nand 10.6 points in NDCG @ 10 compared to strong baselines. In terms of online\ncalls to the LLM, it processes 293.3 times fewer tokens which greatly reduces\nthe online latency and cost. Overall, EnrichIndex is an effective way to build\nbetter retrieval indices offline by leveraging the strong reasoning skills of\nLLMs."
                },
                "authors": [
                    {
                        "name": "Peter Baile Chen"
                    },
                    {
                        "name": "Tomer Wolfson"
                    },
                    {
                        "name": "Michael Cafarella"
                    },
                    {
                        "name": "Dan Roth"
                    }
                ],
                "author_detail": {
                    "name": "Dan Roth"
                },
                "author": "Dan Roth",
                "arxiv_comment": "Dataset and code are available at\n  https://peterbaile.github.io/enrichindex/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03597v1",
                "updated": "2025-04-04T17:05:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    5,
                    56,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:05:56Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    5,
                    56,
                    4,
                    94,
                    0
                ],
                "title": "Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin\n  for Real-World Robot Policy Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin\n  for Real-World Robot Policy Evaluation"
                },
                "summary": "Recent advancements in behavior cloning have enabled robots to perform\ncomplex manipulation tasks. However, accurately assessing training performance\nremains challenging, particularly for real-world applications, as behavior\ncloning losses often correlate poorly with actual task success. Consequently,\nresearchers resort to success rate metrics derived from costly and\ntime-consuming real-world evaluations, making the identification of optimal\npolicies and detection of overfitting or underfitting impractical. To address\nthese issues, we propose real-is-sim, a novel behavior cloning framework that\nincorporates a dynamic digital twin (based on Embodied Gaussians) throughout\nthe entire policy development pipeline: data collection, training, and\ndeployment. By continuously aligning the simulated world with the physical\nworld, demonstrations can be collected in the real world with states extracted\nfrom the simulator. The simulator enables flexible state representations by\nrendering image inputs from any viewpoint or extracting low-level state\ninformation from objects embodied within the scene. During training, policies\ncan be directly evaluated within the simulator in an offline and highly\nparallelizable manner. Finally, during deployment, policies are run within the\nsimulator where the real robot directly tracks the simulated robot's joints,\neffectively decoupling policy execution from real hardware and mitigating\ntraditional domain-transfer challenges. We validate real-is-sim on the PushT\nmanipulation task, demonstrating strong correlation between success rates\nobtained in the simulator and real-world evaluations. Videos of our system can\nbe found at https://realissim.rai-inst.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in behavior cloning have enabled robots to perform\ncomplex manipulation tasks. However, accurately assessing training performance\nremains challenging, particularly for real-world applications, as behavior\ncloning losses often correlate poorly with actual task success. Consequently,\nresearchers resort to success rate metrics derived from costly and\ntime-consuming real-world evaluations, making the identification of optimal\npolicies and detection of overfitting or underfitting impractical. To address\nthese issues, we propose real-is-sim, a novel behavior cloning framework that\nincorporates a dynamic digital twin (based on Embodied Gaussians) throughout\nthe entire policy development pipeline: data collection, training, and\ndeployment. By continuously aligning the simulated world with the physical\nworld, demonstrations can be collected in the real world with states extracted\nfrom the simulator. The simulator enables flexible state representations by\nrendering image inputs from any viewpoint or extracting low-level state\ninformation from objects embodied within the scene. During training, policies\ncan be directly evaluated within the simulator in an offline and highly\nparallelizable manner. Finally, during deployment, policies are run within the\nsimulator where the real robot directly tracks the simulated robot's joints,\neffectively decoupling policy execution from real hardware and mitigating\ntraditional domain-transfer challenges. We validate real-is-sim on the PushT\nmanipulation task, demonstrating strong correlation between success rates\nobtained in the simulator and real-world evaluations. Videos of our system can\nbe found at https://realissim.rai-inst.com."
                },
                "authors": [
                    {
                        "name": "Jad Abou-Chakra"
                    },
                    {
                        "name": "Lingfeng Sun"
                    },
                    {
                        "name": "Krishan Rana"
                    },
                    {
                        "name": "Brandon May"
                    },
                    {
                        "name": "Karl Schmeckpeper"
                    },
                    {
                        "name": "Maria Vittoria Minniti"
                    },
                    {
                        "name": "Laura Herlant"
                    }
                ],
                "author_detail": {
                    "name": "Laura Herlant"
                },
                "author": "Laura Herlant",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v2",
                "updated": "2025-04-04T16:51:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    51,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03579v1",
                "updated": "2025-04-04T16:30:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    30,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T16:30:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    30,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Hallucination Detection on a Budget: Efficient Bayesian Estimation of\n  Semantic Entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination Detection on a Budget: Efficient Bayesian Estimation of\n  Semantic Entropy"
                },
                "summary": "Detecting whether an LLM hallucinates is an important research challenge. One\npromising way of doing so is to estimate the semantic entropy (Farquhar et al.,\n2024) of the distribution of generated sequences. We propose a new algorithm\nfor doing that, with two main advantages. First, due to us taking the Bayesian\napproach, we achieve a much better quality of semantic entropy estimates for a\ngiven budget of samples from the LLM. Second, we are able to tune the number of\nsamples adaptively so that `harder' contexts receive more samples. We\ndemonstrate empirically that our approach systematically beats the baselines,\nrequiring only 59% of samples used by Farquhar et al. (2024) to achieve the\nsame quality of hallucination detection as measured by AUROC. Moreover, quite\ncounterintuitively, our estimator is useful even with just one sample from the\nLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting whether an LLM hallucinates is an important research challenge. One\npromising way of doing so is to estimate the semantic entropy (Farquhar et al.,\n2024) of the distribution of generated sequences. We propose a new algorithm\nfor doing that, with two main advantages. First, due to us taking the Bayesian\napproach, we achieve a much better quality of semantic entropy estimates for a\ngiven budget of samples from the LLM. Second, we are able to tune the number of\nsamples adaptively so that `harder' contexts receive more samples. We\ndemonstrate empirically that our approach systematically beats the baselines,\nrequiring only 59% of samples used by Farquhar et al. (2024) to achieve the\nsame quality of hallucination detection as measured by AUROC. Moreover, quite\ncounterintuitively, our estimator is useful even with just one sample from the\nLLM."
                },
                "authors": [
                    {
                        "name": "Kamil Ciosek"
                    },
                    {
                        "name": "Nicol Felicioni"
                    },
                    {
                        "name": "Sina Ghiassian"
                    }
                ],
                "author_detail": {
                    "name": "Sina Ghiassian"
                },
                "author": "Sina Ghiassian",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03576v1",
                "updated": "2025-04-04T16:29:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    29,
                    8,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T16:29:08Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    29,
                    8,
                    4,
                    94,
                    0
                ],
                "title": "Heterogeneous Resource Allocation for Ensuring End-to-End Quality of\n  Service in Multi-hop Integrated Access and Backhaul Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Resource Allocation for Ensuring End-to-End Quality of\n  Service in Multi-hop Integrated Access and Backhaul Network"
                },
                "summary": "Faced with increasing network traffic demands, cell dense deployment is one\nof significant means to utilize spectrum resources efficiently to improve\nnetwork capacity. Multi-hop integrated access and backhaul (IAB) architectures\nhave emerged as a cost-effective solution for network densification. Meanwhile,\ndynamic time division duplex (D-TDD) is a promising solution to adapt to highly\ndynamic scenarios with asymmetric uplink and downlink traffic. Thus, dynamic\nresource allocation between backhaul and access links and high spectral\nefficiency under ensuring reliable transmission are two key objectives of IAB\nresearch. However, due to huge solution space, there are some challenges in\nmulti-hop IAB with D-TDD if only an integrated optimization problem (IOP) is\nconsidered. To handle these challenges, we decompose the IOP into sub-problems\nto reduce the solution space. To tackle these sub-problems, we formulate them\nseparately as the non-cooperative games and design the corresponding utility\nfunctions to guarantee the existence of Nash equilibrium solutions. Also, to\nachieve the system-wide solution, we propose a single-leader heterogeneous\nmulti-follower Stackelberg-game-based resource allocation scheme, which can\ncombine the solving results of all the sub-problems to get the IOP approximate\nsolution. Simulation results show that the proposed scheme can improve\nthroughput performance while meeting spectrum energy efficiency constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faced with increasing network traffic demands, cell dense deployment is one\nof significant means to utilize spectrum resources efficiently to improve\nnetwork capacity. Multi-hop integrated access and backhaul (IAB) architectures\nhave emerged as a cost-effective solution for network densification. Meanwhile,\ndynamic time division duplex (D-TDD) is a promising solution to adapt to highly\ndynamic scenarios with asymmetric uplink and downlink traffic. Thus, dynamic\nresource allocation between backhaul and access links and high spectral\nefficiency under ensuring reliable transmission are two key objectives of IAB\nresearch. However, due to huge solution space, there are some challenges in\nmulti-hop IAB with D-TDD if only an integrated optimization problem (IOP) is\nconsidered. To handle these challenges, we decompose the IOP into sub-problems\nto reduce the solution space. To tackle these sub-problems, we formulate them\nseparately as the non-cooperative games and design the corresponding utility\nfunctions to guarantee the existence of Nash equilibrium solutions. Also, to\nachieve the system-wide solution, we propose a single-leader heterogeneous\nmulti-follower Stackelberg-game-based resource allocation scheme, which can\ncombine the solving results of all the sub-problems to get the IOP approximate\nsolution. Simulation results show that the proposed scheme can improve\nthroughput performance while meeting spectrum energy efficiency constraints."
                },
                "authors": [
                    {
                        "name": "Shuaifeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shuaifeng Zhang"
                },
                "author": "Shuaifeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03561v1",
                "updated": "2025-04-04T16:10:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    10,
                    57,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T16:10:57Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    10,
                    57,
                    4,
                    94,
                    0
                ],
                "title": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge\n  Refinement"
                },
                "summary": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the interaction between agents and their environments, agents expand their\ncapabilities by planning and executing actions. However, LLM-based agents face\nsubstantial challenges when deployed in novel environments or required to\nnavigate unconventional action spaces. To empower agents to autonomously\nexplore environments, optimize workflows, and enhance their understanding of\nactions, we propose SynWorld, a framework that allows agents to synthesize\npossible scenarios with multi-step action invocation within the action space\nand perform Monte Carlo Tree Search (MCTS) exploration to effectively refine\ntheir action knowledge in the current environment. Our experiments demonstrate\nthat SynWorld is an effective and general approach to learning action knowledge\nin new environments. Code is available at https://github.com/zjunlp/SynWorld."
                },
                "authors": [
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Xiaobin Wang"
                    },
                    {
                        "name": "Yuan Liang"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zekun Xi"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03553v1",
                "updated": "2025-04-04T16:03:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    3,
                    38,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T16:03:38Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    3,
                    38,
                    4,
                    94,
                    0
                ],
                "title": "Agentic Knowledgeable Self-awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Knowledgeable Self-awareness"
                },
                "summary": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved considerable performance across\nvarious agentic planning tasks. However, traditional agent planning approaches\nadopt a \"flood irrigation\" methodology that indiscriminately injects gold\ntrajectories, external feedback, and domain knowledge into agent models. This\npractice overlooks the fundamental human cognitive principle of situational\nself-awareness during decision-making-the ability to dynamically assess\nsituational demands and strategically employ resources during decision-making.\nWe propose agentic knowledgeable self-awareness to address this gap, a novel\nparadigm enabling LLM-based agents to autonomously regulate knowledge\nutilization. Specifically, we propose KnowSelf, a data-centric approach that\napplies agents with knowledgeable self-awareness like humans. Concretely, we\ndevise a heuristic situation judgement criterion to mark special tokens on the\nagent's self-explored trajectories for collecting training data. Through a\ntwo-stage training process, the agent model can switch between different\nsituations by generating specific special tokens, achieving optimal planning\neffects with minimal costs. Our experiments demonstrate that KnowSelf can\noutperform various strong baselines on different tasks and models with minimal\nuse of external knowledge. Code is available at\nhttps://github.com/zjunlp/KnowSelf."
                },
                "authors": [
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Zhisong Qiu"
                    },
                    {
                        "name": "Baochang Ren"
                    },
                    {
                        "name": "Xiaobin Wang"
                    },
                    {
                        "name": "Xiangyuan Ru"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03541v1",
                "updated": "2025-04-04T15:41:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    41,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T15:41:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    41,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Diverse In-Context Example Selection After Decomposing Programs and\n  Aligned Utterances Improves Semantic Parsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverse In-Context Example Selection After Decomposing Programs and\n  Aligned Utterances Improves Semantic Parsing"
                },
                "summary": "LLMs are increasingly used as seq2seq translators from natural language\nutterances to structured programs, a process called semantic interpretation.\nUnlike atomic labels or token sequences, programs are naturally represented as\nabstract syntax trees (ASTs). Such structured representation raises novel\nissues related to the design and selection of in-context examples (ICEs)\npresented to the LLM. We focus on decomposing the pool of available ICE trees\ninto fragments, some of which may be better suited to solving the test\ninstance. Next, we propose how to use (additional invocations of) an LLM with\nprompted syntax constraints to automatically map the fragments to corresponding\nutterances. Finally, we adapt and extend a recent method for diverse ICE\nselection to work with whole and fragmented ICE instances. We evaluate our\nsystem, SCUD4ICL, on popular diverse semantic parsing benchmarks, showing\nvisible accuracy gains from our proposed decomposed diverse demonstration\nmethod. Benefits are particularly notable for smaller LLMs, ICE pools having\nlarger labeled trees, and programs in lower resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly used as seq2seq translators from natural language\nutterances to structured programs, a process called semantic interpretation.\nUnlike atomic labels or token sequences, programs are naturally represented as\nabstract syntax trees (ASTs). Such structured representation raises novel\nissues related to the design and selection of in-context examples (ICEs)\npresented to the LLM. We focus on decomposing the pool of available ICE trees\ninto fragments, some of which may be better suited to solving the test\ninstance. Next, we propose how to use (additional invocations of) an LLM with\nprompted syntax constraints to automatically map the fragments to corresponding\nutterances. Finally, we adapt and extend a recent method for diverse ICE\nselection to work with whole and fragmented ICE instances. We evaluate our\nsystem, SCUD4ICL, on popular diverse semantic parsing benchmarks, showing\nvisible accuracy gains from our proposed decomposed diverse demonstration\nmethod. Benefits are particularly notable for smaller LLMs, ICE pools having\nlarger labeled trees, and programs in lower resource languages."
                },
                "authors": [
                    {
                        "name": "Mayank Kothyari"
                    },
                    {
                        "name": "Sunita Sarawagi"
                    },
                    {
                        "name": "Soumen Chakrabarti"
                    },
                    {
                        "name": "Gaurav Arora"
                    },
                    {
                        "name": "Srujana Merugu"
                    }
                ],
                "author_detail": {
                    "name": "Srujana Merugu"
                },
                "author": "Srujana Merugu",
                "arxiv_comment": "To appear at NAACL 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17703v2",
                "updated": "2025-04-04T15:38:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    38,
                    50,
                    4,
                    94,
                    0
                ],
                "published": "2025-03-22T09:03:31Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    9,
                    3,
                    31,
                    5,
                    81,
                    0
                ],
                "title": "RAIDER: Tool-Equipped Large Language Model Agent for Robotic Action\n  Issue Detection, Explanation and Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAIDER: Tool-Equipped Large Language Model Agent for Robotic Action\n  Issue Detection, Explanation and Recovery"
                },
                "summary": "As robots increasingly operate in dynamic human-centric environments,\nimproving their ability to detect, explain, and recover from action-related\nissues becomes crucial. Traditional model-based and data-driven techniques lack\nadaptability, while more flexible generative AI methods struggle with grounding\nextracted information to real-world constraints. We introduce RAIDER, a novel\nagent that integrates Large Language Models (LLMs) with grounded tools for\nadaptable and efficient issue detection and explanation. Using a unique\n\"Ground, Ask&Answer, Issue\" procedure, RAIDER dynamically generates\ncontext-aware precondition questions and selects appropriate tools for\nresolution, achieving targeted information gathering. Our results within a\nsimulated household environment surpass methods relying on predefined models,\nfull scene descriptions, or standalone trained models. Additionally, RAIDER's\nexplanations enhance recovery success, including cases requiring human\ninteraction. Its modular architecture, featuring self-correction mechanisms,\nenables straightforward adaptation to diverse scenarios, as demonstrated in a\nreal-world human-assistive task. This showcases RAIDER's potential as a\nversatile agentic AI solution for robotic issue detection and explanation,\nwhile addressing the problem of grounding generative AI for its effective\napplication in embodied agents. Project website:\nhttps://eurecat.github.io/raider-llmagent/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As robots increasingly operate in dynamic human-centric environments,\nimproving their ability to detect, explain, and recover from action-related\nissues becomes crucial. Traditional model-based and data-driven techniques lack\nadaptability, while more flexible generative AI methods struggle with grounding\nextracted information to real-world constraints. We introduce RAIDER, a novel\nagent that integrates Large Language Models (LLMs) with grounded tools for\nadaptable and efficient issue detection and explanation. Using a unique\n\"Ground, Ask&Answer, Issue\" procedure, RAIDER dynamically generates\ncontext-aware precondition questions and selects appropriate tools for\nresolution, achieving targeted information gathering. Our results within a\nsimulated household environment surpass methods relying on predefined models,\nfull scene descriptions, or standalone trained models. Additionally, RAIDER's\nexplanations enhance recovery success, including cases requiring human\ninteraction. Its modular architecture, featuring self-correction mechanisms,\nenables straightforward adaptation to diverse scenarios, as demonstrated in a\nreal-world human-assistive task. This showcases RAIDER's potential as a\nversatile agentic AI solution for robotic issue detection and explanation,\nwhile addressing the problem of grounding generative AI for its effective\napplication in embodied agents. Project website:\nhttps://eurecat.github.io/raider-llmagent/"
                },
                "authors": [
                    {
                        "name": "Silvia Izquierdo-Badiola"
                    },
                    {
                        "name": "Carlos Rizzo"
                    },
                    {
                        "name": "Guillem Aleny"
                    }
                ],
                "author_detail": {
                    "name": "Guillem Aleny"
                },
                "author": "Guillem Aleny",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11305v2",
                "updated": "2025-04-04T15:24:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    24,
                    36,
                    4,
                    94,
                    0
                ],
                "published": "2025-03-14T11:18:47Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    11,
                    18,
                    47,
                    4,
                    73,
                    0
                ],
                "title": "Lightweight Learning for Grant-Free Activity Detection in Cell-Free\n  Massive MIMO Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Learning for Grant-Free Activity Detection in Cell-Free\n  Massive MIMO Networks"
                },
                "summary": "Grant-free random access (GF-RA) is a promising access technique for massive\nmachine-type communications (mMTC) in future wireless networks, particularly in\nthe context of 5G and beyond (6G) systems. Within the context of GF-RA, this\nstudy investigates the efficiency of employing supervised machine learning\ntechniques to tackle the challenges on the device activity detection (AD).\nGF-RA addresses scalability by employing non-orthogonal pilot sequences, which\nprovides an efficient alternative comparing to conventional grant-based random\naccess (GB-RA) technique that are constrained by the scarcity of orthogonal\npreamble resources. In this paper, we propose a novel lightweight data-driven\nalgorithmic framework specifically designed for activity detection in GF-RA for\nmMTC in cell-free massive multiple-input multiple-output (CF-mMIMO) networks.\nWe propose two distinct framework deployment strategies, centralized and\ndecentralized, both tailored to streamline the proposed approach implementation\nacross network infrastructures. Moreover, we introduce optimized post-detection\nmethodologies complemented by a clustering stage to enhance overall detection\nperformances. Our 3GPP-compliant simulations have validated that the proposed\nalgorithm achieves state-of-the-art model-based activity detection accuracy\nwhile significantly reducing complexity. Achieving 99% accuracy, it\ndemonstrates real-world viability and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grant-free random access (GF-RA) is a promising access technique for massive\nmachine-type communications (mMTC) in future wireless networks, particularly in\nthe context of 5G and beyond (6G) systems. Within the context of GF-RA, this\nstudy investigates the efficiency of employing supervised machine learning\ntechniques to tackle the challenges on the device activity detection (AD).\nGF-RA addresses scalability by employing non-orthogonal pilot sequences, which\nprovides an efficient alternative comparing to conventional grant-based random\naccess (GB-RA) technique that are constrained by the scarcity of orthogonal\npreamble resources. In this paper, we propose a novel lightweight data-driven\nalgorithmic framework specifically designed for activity detection in GF-RA for\nmMTC in cell-free massive multiple-input multiple-output (CF-mMIMO) networks.\nWe propose two distinct framework deployment strategies, centralized and\ndecentralized, both tailored to streamline the proposed approach implementation\nacross network infrastructures. Moreover, we introduce optimized post-detection\nmethodologies complemented by a clustering stage to enhance overall detection\nperformances. Our 3GPP-compliant simulations have validated that the proposed\nalgorithm achieves state-of-the-art model-based activity detection accuracy\nwhile significantly reducing complexity. Achieving 99% accuracy, it\ndemonstrates real-world viability and effectiveness."
                },
                "authors": [
                    {
                        "name": "Ali Elkeshawy"
                    },
                    {
                        "name": "Haifa Fares"
                    },
                    {
                        "name": "Amor Nafkha"
                    }
                ],
                "author_detail": {
                    "name": "Amor Nafkha"
                },
                "author": "Amor Nafkha",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2406.07160",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11212v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11212v3",
                "updated": "2025-04-04T15:23:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    23,
                    8,
                    4,
                    94,
                    0
                ],
                "published": "2024-01-20T11:37:44Z",
                "published_parsed": [
                    2024,
                    1,
                    20,
                    11,
                    37,
                    44,
                    5,
                    20,
                    0
                ],
                "title": "Programming Distributed Collective Processes in the eXchange Calculus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming Distributed Collective Processes in the eXchange Calculus"
                },
                "summary": "Recent trends like the Internet of Things (IoT) suggest a vision of dense and\nmulti-scale deployments of computing devices in nearly all kinds of\nenvironments. A prominent engineering challenge revolves around programming the\ncollective adaptive behaviour of such computational ecosystems. This requires\nabstractions able to capture concepts like ensembles (dynamic groups of\ncooperating devices) and collective tasks (joint activities carried out by\nensembles). In this work, we consider collections of devices interacting with\nneighbours and that execute in nearly-synchronised sense-compute-interact\nrounds, where the computation is given by a single program mapping sensing\nvalues and incoming messages to output and outcoming messages. To support\nprogramming whole computational collectives, we propose the abstraction of a\ndistributed collective process, which can be used to define at once the\nensemble formation logic and its collective task. We formalise the abstraction\nin the eXchange Calculus (XC), a core functional language based on neighbouring\nvalues (maps from neighbours to values) where state and interaction is handled\nthrough a single primitive, exchange, and provide a corresponding\nimplementation in the FCPP language. Then, we exercise distributed collective\nprocesses using two case studies: multi-hop message propagation and distributed\nmonitoring of spatial properties. Finally, we discuss the features of the\nabstraction and its suitability for different kinds of distributed computing\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent trends like the Internet of Things (IoT) suggest a vision of dense and\nmulti-scale deployments of computing devices in nearly all kinds of\nenvironments. A prominent engineering challenge revolves around programming the\ncollective adaptive behaviour of such computational ecosystems. This requires\nabstractions able to capture concepts like ensembles (dynamic groups of\ncooperating devices) and collective tasks (joint activities carried out by\nensembles). In this work, we consider collections of devices interacting with\nneighbours and that execute in nearly-synchronised sense-compute-interact\nrounds, where the computation is given by a single program mapping sensing\nvalues and incoming messages to output and outcoming messages. To support\nprogramming whole computational collectives, we propose the abstraction of a\ndistributed collective process, which can be used to define at once the\nensemble formation logic and its collective task. We formalise the abstraction\nin the eXchange Calculus (XC), a core functional language based on neighbouring\nvalues (maps from neighbours to values) where state and interaction is handled\nthrough a single primitive, exchange, and provide a corresponding\nimplementation in the FCPP language. Then, we exercise distributed collective\nprocesses using two case studies: multi-hop message propagation and distributed\nmonitoring of spatial properties. Finally, we discuss the features of the\nabstraction and its suitability for different kinds of distributed computing\napplications."
                },
                "authors": [
                    {
                        "name": "Giorgio Audrito"
                    },
                    {
                        "name": "Roberto Casadei"
                    },
                    {
                        "name": "Ferruccio Damiani"
                    },
                    {
                        "name": "Gianluca Torta"
                    },
                    {
                        "name": "Mirko Viroli"
                    }
                ],
                "author_detail": {
                    "name": "Mirko Viroli"
                },
                "author": "Mirko Viroli",
                "arxiv_comment": "41 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11212v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11212v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.1.3; F.1.1; F.4.3; I.2.11; J.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15429v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15429v3",
                "updated": "2025-04-04T15:21:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    21,
                    3,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-21T12:54:56Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    12,
                    54,
                    56,
                    4,
                    52,
                    0
                ],
                "title": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable\n  Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable\n  Explanations"
                },
                "summary": "A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A significant and growing number of published scientific articles is found to\ninvolve fraudulent practices, posing a serious threat to the credibility and\nsafety of research in fields such as medicine. We propose Pub-Guard-LLM, the\nfirst large language model-based system tailored to fraud detection of\nbiomedical scientific articles. We provide three application modes for\ndeploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and\nmulti-agent debate. Each mode allows for textual explanations of predictions.\nTo assess the performance of our system, we introduce an open-source benchmark,\nPubMed Retraction, comprising over 11K real-world biomedical articles,\nincluding metadata and retraction labels. We show that, across all modes,\nPub-Guard-LLM consistently surpasses the performance of various baselines and\nprovides more reliable explanations, namely explanations which are deemed more\nrelevant and coherent than those generated by the baselines when evaluated by\nmultiple assessment methods. By enhancing both detection performance and\nexplainability in scientific fraud detection, Pub-Guard-LLM contributes to\nsafeguarding research integrity with a novel, effective, open-source tool."
                },
                "authors": [
                    {
                        "name": "Lihu Chen"
                    },
                    {
                        "name": "Shuojie Fu"
                    },
                    {
                        "name": "Gabriel Freedman"
                    },
                    {
                        "name": "Cemre Zor"
                    },
                    {
                        "name": "Guy Martin"
                    },
                    {
                        "name": "James Kinross"
                    },
                    {
                        "name": "Uddhav Vaghela"
                    },
                    {
                        "name": "Ovidiu Serban"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "long paper under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15429v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15429v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03520v1",
                "updated": "2025-04-04T15:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    17,
                    53,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T15:17:53Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    17,
                    53,
                    4,
                    94,
                    0
                ],
                "title": "Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles"
                },
                "summary": "Bias in news reporting significantly impacts public perception, particularly\nregarding crime, politics, and societal issues. Traditional bias detection\nmethods, predominantly reliant on human moderation, suffer from subjective\ninterpretations and scalability constraints. Here, we introduce an AI-driven\nframework leveraging advanced large language models (LLMs), specifically\nGPT-4o, GPT-4o Mini, Gemini Pro, Gemini Flash, Llama 8B, and Llama 3B, to\nsystematically identify and mitigate biases in news articles. To this end, we\ncollect an extensive dataset consisting of over 30,000 crime-related articles\nfrom five politically diverse news sources spanning a decade (2013-2023). Our\napproach employs a two-stage methodology: (1) bias detection, where each LLM\nscores and justifies biased content at the paragraph level, validated through\nhuman evaluation for ground truth establishment, and (2) iterative debiasing\nusing GPT-4o Mini, verified by both automated reassessment and human reviewers.\nEmpirical results indicate GPT-4o Mini's superior accuracy in bias detection\nand effectiveness in debiasing. Furthermore, our analysis reveals temporal and\ngeographical variations in media bias correlating with socio-political dynamics\nand real-world events. This study contributes to scalable computational\nmethodologies for bias mitigation, promoting fairness and accountability in\nnews reporting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in news reporting significantly impacts public perception, particularly\nregarding crime, politics, and societal issues. Traditional bias detection\nmethods, predominantly reliant on human moderation, suffer from subjective\ninterpretations and scalability constraints. Here, we introduce an AI-driven\nframework leveraging advanced large language models (LLMs), specifically\nGPT-4o, GPT-4o Mini, Gemini Pro, Gemini Flash, Llama 8B, and Llama 3B, to\nsystematically identify and mitigate biases in news articles. To this end, we\ncollect an extensive dataset consisting of over 30,000 crime-related articles\nfrom five politically diverse news sources spanning a decade (2013-2023). Our\napproach employs a two-stage methodology: (1) bias detection, where each LLM\nscores and justifies biased content at the paragraph level, validated through\nhuman evaluation for ground truth establishment, and (2) iterative debiasing\nusing GPT-4o Mini, verified by both automated reassessment and human reviewers.\nEmpirical results indicate GPT-4o Mini's superior accuracy in bias detection\nand effectiveness in debiasing. Furthermore, our analysis reveals temporal and\ngeographical variations in media bias correlating with socio-political dynamics\nand real-world events. This study contributes to scalable computational\nmethodologies for bias mitigation, promoting fairness and accountability in\nnews reporting."
                },
                "authors": [
                    {
                        "name": "Chen Wei Kuo"
                    },
                    {
                        "name": "Kevin Chu"
                    },
                    {
                        "name": "Nouar AlDahoul"
                    },
                    {
                        "name": "Hazem Ibrahim"
                    },
                    {
                        "name": "Talal Rahwan"
                    },
                    {
                        "name": "Yasir Zaki"
                    }
                ],
                "author_detail": {
                    "name": "Yasir Zaki"
                },
                "author": "Yasir Zaki",
                "arxiv_comment": "23 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03486v1",
                "updated": "2025-04-04T14:41:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    41,
                    50,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:41:50Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    41,
                    50,
                    4,
                    94,
                    0
                ],
                "title": "Structured Legal Document Generation in India: A Model-Agnostic Wrapper\n  Approach with VidhikDastaavej",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Legal Document Generation in India: A Model-Agnostic Wrapper\n  Approach with VidhikDastaavej"
                },
                "summary": "Automating legal document drafting can significantly enhance efficiency,\nreduce manual effort, and streamline legal workflows. While prior research has\nexplored tasks such as judgment prediction and case summarization, the\nstructured generation of private legal documents in the Indian legal domain\nremains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej,\na novel, anonymized dataset of private legal documents, and develop NyayaShilp,\na fine-tuned legal document generation model specifically adapted to Indian\nlegal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework\nthat first generates structured section titles and then iteratively produces\ncontent while leveraging retrieval-based mechanisms to ensure coherence and\nfactual accuracy. We benchmark multiple open-source LLMs, including\ninstruction-tuned and domain-adapted versions, alongside proprietary models for\ncomparison. Our findings indicate that while direct fine-tuning on small\ndatasets does not always yield improvements, our structured wrapper\nsignificantly enhances coherence, factual adherence, and overall document\nquality while mitigating hallucinations. To ensure real-world applicability, we\ndeveloped a Human-in-the-Loop (HITL) Document Generation System, an interactive\nuser interface that enables users to specify document types, refine section\ndetails, and generate structured legal drafts. This tool allows legal\nprofessionals and researchers to generate, validate, and refine AI-generated\nlegal documents efficiently. Extensive evaluations, including expert\nassessments, confirm that our framework achieves high reliability in structured\nlegal drafting. This research establishes a scalable and adaptable foundation\nfor AI-assisted legal drafting in India, offering an effective approach to\nstructured legal document generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating legal document drafting can significantly enhance efficiency,\nreduce manual effort, and streamline legal workflows. While prior research has\nexplored tasks such as judgment prediction and case summarization, the\nstructured generation of private legal documents in the Indian legal domain\nremains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej,\na novel, anonymized dataset of private legal documents, and develop NyayaShilp,\na fine-tuned legal document generation model specifically adapted to Indian\nlegal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework\nthat first generates structured section titles and then iteratively produces\ncontent while leveraging retrieval-based mechanisms to ensure coherence and\nfactual accuracy. We benchmark multiple open-source LLMs, including\ninstruction-tuned and domain-adapted versions, alongside proprietary models for\ncomparison. Our findings indicate that while direct fine-tuning on small\ndatasets does not always yield improvements, our structured wrapper\nsignificantly enhances coherence, factual adherence, and overall document\nquality while mitigating hallucinations. To ensure real-world applicability, we\ndeveloped a Human-in-the-Loop (HITL) Document Generation System, an interactive\nuser interface that enables users to specify document types, refine section\ndetails, and generate structured legal drafts. This tool allows legal\nprofessionals and researchers to generate, validate, and refine AI-generated\nlegal documents efficiently. Extensive evaluations, including expert\nassessments, confirm that our framework achieves high reliability in structured\nlegal drafting. This research establishes a scalable and adaptable foundation\nfor AI-assisted legal drafting in India, offering an effective approach to\nstructured legal document generation."
                },
                "authors": [
                    {
                        "name": "Shubham Kumar Nigam"
                    },
                    {
                        "name": "Balaramamahanthi Deepak Patnaik"
                    },
                    {
                        "name": "Ajay Varghese Thomas"
                    },
                    {
                        "name": "Noel Shallum"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15888v2",
                "updated": "2025-04-04T14:12:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    12,
                    54,
                    4,
                    94,
                    0
                ],
                "published": "2024-06-22T16:37:51Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    16,
                    37,
                    51,
                    5,
                    174,
                    0
                ],
                "title": "Real-time Speech Summarization for Medical Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Speech Summarization for Medical Conversations"
                },
                "summary": "In doctor-patient conversations, identifying medically relevant information\nis crucial, posing the need for conversation summarization. In this work, we\npropose the first deployable real-time speech summarization system for\nreal-world applications in industry, which generates a local summary after\nevery N speech utterances within a conversation and a global summary after the\nend of a conversation. Our system could enhance user experience from a business\nstandpoint, while also reducing computational costs from a technical\nperspective. Secondly, we present VietMed-Sum which, to our knowledge, is the\nfirst speech summarization dataset for medical conversations. Thirdly, we are\nthe first to utilize LLM and human annotators collaboratively to create gold\nstandard and synthetic summaries for medical conversation summarization.\nFinally, we present baseline results of state-of-the-art models on VietMed-Sum.\nAll code, data (English-translated and Vietnamese) and models are available\nonline: https://github.com/leduckhai/MultiMed/tree/master/VietMed-Sum",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In doctor-patient conversations, identifying medically relevant information\nis crucial, posing the need for conversation summarization. In this work, we\npropose the first deployable real-time speech summarization system for\nreal-world applications in industry, which generates a local summary after\nevery N speech utterances within a conversation and a global summary after the\nend of a conversation. Our system could enhance user experience from a business\nstandpoint, while also reducing computational costs from a technical\nperspective. Secondly, we present VietMed-Sum which, to our knowledge, is the\nfirst speech summarization dataset for medical conversations. Thirdly, we are\nthe first to utilize LLM and human annotators collaboratively to create gold\nstandard and synthetic summaries for medical conversation summarization.\nFinally, we present baseline results of state-of-the-art models on VietMed-Sum.\nAll code, data (English-translated and Vietnamese) and models are available\nonline: https://github.com/leduckhai/MultiMed/tree/master/VietMed-Sum"
                },
                "authors": [
                    {
                        "name": "Khai Le-Duc"
                    },
                    {
                        "name": "Khai-Nguyen Nguyen"
                    },
                    {
                        "name": "Long Vo-Dang"
                    },
                    {
                        "name": "Truong-Son Hy"
                    }
                ],
                "author_detail": {
                    "name": "Truong-Son Hy"
                },
                "author": "Truong-Son Hy",
                "arxiv_comment": "Interspeech 2024 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15962v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15962v3",
                "updated": "2025-04-04T14:05:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    5,
                    33,
                    4,
                    94,
                    0
                ],
                "published": "2024-04-24T16:31:10Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    16,
                    31,
                    10,
                    2,
                    115,
                    0
                ],
                "title": "Showcasing Automated Vehicle Prototypes: A Collaborative Release Process\n  to Manage and Communicate Risk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Showcasing Automated Vehicle Prototypes: A Collaborative Release Process\n  to Manage and Communicate Risk"
                },
                "summary": "The development and deployment of automated vehicles pose major challenges\nfor manufacturers to this day. Whilst central questions, like the issue of\nensuring a sufficient level of safety, remain unanswered, prototypes are\nincreasingly finding their way into public traffic in urban areas. Although\nsafety concepts for prototypes are addressed in literature, published work\nhardly contains any dedicated considerations on a systematic release for their\noperation. In this paper, we propose an incremental release process for public\ndemonstrations of prototypes' automated driving functionality. We explicate\nrelease process requirements, derive process design decisions, and define\nstakeholder tasks. Furthermore, we reflect on practical insights gained through\nimplementing the release process as part of the UNICAR$agil$ research project,\nin which four prototypes based on novel vehicle concepts were built and\ndemonstrated to the public. One observation is the improved quality of internal\nrisk communication, achieved by dismantling information asymmetries between\nstakeholders. Design conflicts are disclosed - providing a contribution to\nnurture transparency and, thereby, supporting a valid basis for release\ndecisions. We argue that our release process meets two important requirements,\nas the results suggest its applicability to the domain of automated driving and\nits scalability to different vehicle concepts and organizational structures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development and deployment of automated vehicles pose major challenges\nfor manufacturers to this day. Whilst central questions, like the issue of\nensuring a sufficient level of safety, remain unanswered, prototypes are\nincreasingly finding their way into public traffic in urban areas. Although\nsafety concepts for prototypes are addressed in literature, published work\nhardly contains any dedicated considerations on a systematic release for their\noperation. In this paper, we propose an incremental release process for public\ndemonstrations of prototypes' automated driving functionality. We explicate\nrelease process requirements, derive process design decisions, and define\nstakeholder tasks. Furthermore, we reflect on practical insights gained through\nimplementing the release process as part of the UNICAR$agil$ research project,\nin which four prototypes based on novel vehicle concepts were built and\ndemonstrated to the public. One observation is the improved quality of internal\nrisk communication, achieved by dismantling information asymmetries between\nstakeholders. Design conflicts are disclosed - providing a contribution to\nnurture transparency and, thereby, supporting a valid basis for release\ndecisions. We argue that our release process meets two important requirements,\nas the results suggest its applicability to the domain of automated driving and\nits scalability to different vehicle concepts and organizational structures."
                },
                "authors": [
                    {
                        "name": "Marvin Loba"
                    },
                    {
                        "name": "Robert Graubohm"
                    },
                    {
                        "name": "Markus Maurer"
                    }
                ],
                "author_detail": {
                    "name": "Markus Maurer"
                },
                "author": "Markus Maurer",
                "arxiv_doi": "10.1109/ITSC58415.2024.10919548",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ITSC58415.2024.10919548",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.15962v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15962v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in 2024 IEEE 27th International Conference on Intelligent\n  Transportation Systems (ITSC), Edmonton, Canada, September 24-27, 2024",
                "arxiv_journal_ref": "2024 IEEE 27th International Conference on Intelligent\n  Transportation Systems (ITSC), Edmonton, Canada, September 24-27, 2024, pp.\n  3425-3432",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03444v1",
                "updated": "2025-04-04T13:37:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    37,
                    29,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T13:37:29Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    37,
                    29,
                    4,
                    94,
                    0
                ],
                "title": "LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM\n  Applications"
                },
                "summary": "Developing compound Large Language Model (LLM) applications is becoming an\nincreasingly prevalent approach to solving real-world problems. In these\napplications, an LLM collaborates with various external modules, including APIs\nand even other LLMs, to realize complex intelligent services. However, we\nreveal that the intrinsic duration and structural uncertainty in compound LLM\napplications pose great challenges for LLM service providers in serving and\nscheduling them efficiently. In this paper, we propose LLMSched, an\nuncertainty-aware scheduling framework for emerging compound LLM applications.\nIn LLMSched, we first design a novel DAG-based model to describe the uncertain\ncompound LLM applications. Then, we adopt the Bayesian network to\ncomprehensively profile compound LLM applications and identify\nuncertainty-reducing stages, along with an entropy-based mechanism to quantify\ntheir uncertainty reduction. Combining an uncertainty reduction strategy and a\njob completion time (JCT)-efficient scheme, we further propose an efficient\nscheduler to reduce the average JCT. Evaluation of both simulation and testbed\nexperiments on various representative compound LLM applications shows that\ncompared to existing state-of-the-art scheduling schemes, LLMSched can reduce\nthe average JCT by 14~79%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing compound Large Language Model (LLM) applications is becoming an\nincreasingly prevalent approach to solving real-world problems. In these\napplications, an LLM collaborates with various external modules, including APIs\nand even other LLMs, to realize complex intelligent services. However, we\nreveal that the intrinsic duration and structural uncertainty in compound LLM\napplications pose great challenges for LLM service providers in serving and\nscheduling them efficiently. In this paper, we propose LLMSched, an\nuncertainty-aware scheduling framework for emerging compound LLM applications.\nIn LLMSched, we first design a novel DAG-based model to describe the uncertain\ncompound LLM applications. Then, we adopt the Bayesian network to\ncomprehensively profile compound LLM applications and identify\nuncertainty-reducing stages, along with an entropy-based mechanism to quantify\ntheir uncertainty reduction. Combining an uncertainty reduction strategy and a\njob completion time (JCT)-efficient scheme, we further propose an efficient\nscheduler to reduce the average JCT. Evaluation of both simulation and testbed\nexperiments on various representative compound LLM applications shows that\ncompared to existing state-of-the-art scheduling schemes, LLMSched can reduce\nthe average JCT by 14~79%."
                },
                "authors": [
                    {
                        "name": "Botao Zhu"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xiaoyi Fan"
                    },
                    {
                        "name": "Yifei Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Zhu"
                },
                "author": "Yifei Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17743v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17743v5",
                "updated": "2025-04-04T13:31:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    31,
                    38,
                    4,
                    94,
                    0
                ],
                "published": "2024-05-28T01:55:35Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    1,
                    55,
                    35,
                    1,
                    149,
                    0
                ],
                "title": "ORLM: A Customizable Framework in Training Large Models for Automated\n  Optimization Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORLM: A Customizable Framework in Training Large Models for Automated\n  Optimization Modeling"
                },
                "summary": "Optimization modeling plays a critical role in the application of Operations\nResearch (OR) tools to address real-world problems, yet they pose challenges\nand require extensive expertise from OR experts. With the advent of large\nlanguage models (LLMs), new opportunities have emerged to streamline and\nautomate such task. However, current research predominantly relies on\nclosed-source LLMs such as GPT-4, along with extensive prompt engineering\ntechniques. This reliance stems from the scarcity of high-quality training\ndatasets for optimization modeling, resulting in elevated costs, prolonged\nprocessing times, and privacy concerns. To address these challenges, our work\nis the first to propose a viable path for training open-source LLMs that are\ncapable of optimization modeling and developing solver codes, eventually\nleading to a superior ability for automating optimization modeling and solving.\nParticularly, we design the {\\sc OR-Instruct}, a semi-automated data synthesis\nframework for optimization modeling that enables customizable enhancements for\nspecific scenarios or model types. This work also introduces IndustryOR, the\nfirst industrial benchmark for evaluating LLMs in solving practical OR\nproblems. We train several 7B-scale open-source LLMs using synthesized data\n(dubbed ORLMs{https://github.com/Cardinal-Operations/ORLM}), which exhibit\nsignificantly enhanced optimization modeling capabilities, achieving\ncompetitive performance across the NL4OPT, MAMO, and IndustryOR benchmarks.\nAdditionally, our experiments highlight the potential of scaling law and\nreinforcement learning to further enhance the performance of ORLMs. The\nworkflows and human-machine interaction paradigms of ORLMs in practical\nindustrial applications are also discussed in the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization modeling plays a critical role in the application of Operations\nResearch (OR) tools to address real-world problems, yet they pose challenges\nand require extensive expertise from OR experts. With the advent of large\nlanguage models (LLMs), new opportunities have emerged to streamline and\nautomate such task. However, current research predominantly relies on\nclosed-source LLMs such as GPT-4, along with extensive prompt engineering\ntechniques. This reliance stems from the scarcity of high-quality training\ndatasets for optimization modeling, resulting in elevated costs, prolonged\nprocessing times, and privacy concerns. To address these challenges, our work\nis the first to propose a viable path for training open-source LLMs that are\ncapable of optimization modeling and developing solver codes, eventually\nleading to a superior ability for automating optimization modeling and solving.\nParticularly, we design the {\\sc OR-Instruct}, a semi-automated data synthesis\nframework for optimization modeling that enables customizable enhancements for\nspecific scenarios or model types. This work also introduces IndustryOR, the\nfirst industrial benchmark for evaluating LLMs in solving practical OR\nproblems. We train several 7B-scale open-source LLMs using synthesized data\n(dubbed ORLMs{https://github.com/Cardinal-Operations/ORLM}), which exhibit\nsignificantly enhanced optimization modeling capabilities, achieving\ncompetitive performance across the NL4OPT, MAMO, and IndustryOR benchmarks.\nAdditionally, our experiments highlight the potential of scaling law and\nreinforcement learning to further enhance the performance of ORLMs. The\nworkflows and human-machine interaction paradigms of ORLMs in practical\nindustrial applications are also discussed in the paper."
                },
                "authors": [
                    {
                        "name": "Chenyu Huang"
                    },
                    {
                        "name": "Zhengyang Tang"
                    },
                    {
                        "name": "Shixi Hu"
                    },
                    {
                        "name": "Ruoqing Jiang"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Dongdong Ge"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Zizhuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zizhuo Wang"
                },
                "author": "Zizhuo Wang",
                "arxiv_comment": "accepted by Operations Research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17743v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17743v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03440v1",
                "updated": "2025-04-04T13:31:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    31,
                    8,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T13:31:08Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    31,
                    8,
                    4,
                    94,
                    0
                ],
                "title": "Know What You do Not Know: Verbalized Uncertainty Estimation Robustness\n  on Corrupted Images in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know What You do Not Know: Verbalized Uncertainty Estimation Robustness\n  on Corrupted Images in Vision-Language Models"
                },
                "summary": "To leverage the full potential of Large Language Models (LLMs) it is crucial\nto have some information on their answers' uncertainty. This means that the\nmodel has to be able to quantify how certain it is in the correctness of a\ngiven response. Bad uncertainty estimates can lead to overconfident wrong\nanswers undermining trust in these models. Quite a lot of research has been\ndone on language models that work with text inputs and provide text outputs.\nStill, since the visual capabilities have been added to these models recently,\nthere has not been much progress on the uncertainty of Visual Language Models\n(VLMs). We tested three state-of-the-art VLMs on corrupted image data. We found\nthat the severity of the corruption negatively impacted the models' ability to\nestimate their uncertainty and the models also showed overconfidence in most of\nthe experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To leverage the full potential of Large Language Models (LLMs) it is crucial\nto have some information on their answers' uncertainty. This means that the\nmodel has to be able to quantify how certain it is in the correctness of a\ngiven response. Bad uncertainty estimates can lead to overconfident wrong\nanswers undermining trust in these models. Quite a lot of research has been\ndone on language models that work with text inputs and provide text outputs.\nStill, since the visual capabilities have been added to these models recently,\nthere has not been much progress on the uncertainty of Visual Language Models\n(VLMs). We tested three state-of-the-art VLMs on corrupted image data. We found\nthat the severity of the corruption negatively impacted the models' ability to\nestimate their uncertainty and the models also showed overconfidence in most of\nthe experiments."
                },
                "authors": [
                    {
                        "name": "Mirko Borszukovszki"
                    },
                    {
                        "name": "Ivo Pascal de Jong"
                    },
                    {
                        "name": "Matias Valdenegro-Toro"
                    }
                ],
                "author_detail": {
                    "name": "Matias Valdenegro-Toro"
                },
                "author": "Matias Valdenegro-Toro",
                "arxiv_comment": "10 pages, 11 figures, TrustNLP Workshop @ NAACL 2025 Camera ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03434v1",
                "updated": "2025-04-04T13:25:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    25,
                    32,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T13:25:32Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    25,
                    32,
                    4,
                    94,
                    0
                ],
                "title": "Locations of Characters in Narratives: Andersen and Persuasion Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locations of Characters in Narratives: Andersen and Persuasion Datasets"
                },
                "summary": "The ability of machines to grasp spatial understanding within narrative\ncontexts is an intriguing aspect of reading comprehension that continues to be\nstudied. Motivated by the goal to test the AI's competence in understanding the\nrelationship between characters and their respective locations in narratives,\nwe introduce two new datasets: Andersen and Persuasion. For the Andersen\ndataset, we selected fifteen children's stories from \"Andersen's Fairy Tales\"\nby Hans Christian Andersen and manually annotated the characters and their\nrespective locations throughout each story. Similarly, for the Persuasion\ndataset, characters and their locations in the novel \"Persuasion\" by Jane\nAusten were also manually annotated. We used these datasets to prompt Large\nLanguage Models (LLMs). The prompts are created by extracting excerpts from the\nstories or the novel and combining them with a question asking the location of\na character mentioned in that excerpt. Out of the five LLMs we tested, the\nbest-performing one for the Andersen dataset accurately identified the location\nin 61.85% of the examples, while for the Persuasion dataset, the\nbest-performing one did so in 56.06% of the cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of machines to grasp spatial understanding within narrative\ncontexts is an intriguing aspect of reading comprehension that continues to be\nstudied. Motivated by the goal to test the AI's competence in understanding the\nrelationship between characters and their respective locations in narratives,\nwe introduce two new datasets: Andersen and Persuasion. For the Andersen\ndataset, we selected fifteen children's stories from \"Andersen's Fairy Tales\"\nby Hans Christian Andersen and manually annotated the characters and their\nrespective locations throughout each story. Similarly, for the Persuasion\ndataset, characters and their locations in the novel \"Persuasion\" by Jane\nAusten were also manually annotated. We used these datasets to prompt Large\nLanguage Models (LLMs). The prompts are created by extracting excerpts from the\nstories or the novel and combining them with a question asking the location of\na character mentioned in that excerpt. Out of the five LLMs we tested, the\nbest-performing one for the Andersen dataset accurately identified the location\nin 61.85% of the examples, while for the Persuasion dataset, the\nbest-performing one did so in 56.06% of the cases."
                },
                "authors": [
                    {
                        "name": "Batuhan Ozyurt"
                    },
                    {
                        "name": "Roya Arkhmammadova"
                    },
                    {
                        "name": "Deniz Yuret"
                    }
                ],
                "author_detail": {
                    "name": "Deniz Yuret"
                },
                "author": "Deniz Yuret",
                "arxiv_comment": "14 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03428v1",
                "updated": "2025-04-04T13:18:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    18,
                    39,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T13:18:39Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    18,
                    39,
                    4,
                    94,
                    0
                ],
                "title": "Fair and Energy-Efficient Activation Control Mechanisms for\n  Repeater-Assisted Massive MIMO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fair and Energy-Efficient Activation Control Mechanisms for\n  Repeater-Assisted Massive MIMO"
                },
                "summary": "Massive multiple-input multiple-output (mMIMO) has been the core of 5G due to\nits ability to improve spectral efficiency and spatial multiplexing\nsignificantly; however, cell-edge users still experience performance\ndegradation due to inter-cell interference and uneven signal distribution.\nWhile cell-free mMIMO (cfmMIMO) addresses this issue by providing uniform\ncoverage through distributed antennas, it requires significantly more\ndeployment cost due to the fronthaul and tight synchronization requirements.\nAlternatively, repeater-assisted massive MIMO (RA-MIMO) has recently been\nproposed to extend the coverage of cellular mMIMO by densely deploying low-cost\nsingle-antenna repeaters capable of amplifying and forwarding signals. In this\nwork, we investigate amplification control for the repeaters for two different\ngoals: (i) providing a fair performance among users, and (ii) reducing the\nextra energy consumption by the deployed repeaters. We propose a max-min\namplification control algorithm using the convex-concave procedure for fairness\nand a joint sleep mode and amplification control algorithm for energy\nefficiency, comparing long- and short-term strategies. Numerical results show\nthat RA-MIMO, with maximum amplification, improves\nsignal-to-interference-plus-noise ratio (SINR) by over 20 dB compared to mMIMO\nand performs within 1 dB of cfmMIMO when deploying the same number of repeaters\nas access points in cfmMIMO. Additionally, our majority-rule-based long-term\nsleep mechanism reduces repeater power consumption by 70% while maintaining\nless than 1% spectral efficiency outage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive multiple-input multiple-output (mMIMO) has been the core of 5G due to\nits ability to improve spectral efficiency and spatial multiplexing\nsignificantly; however, cell-edge users still experience performance\ndegradation due to inter-cell interference and uneven signal distribution.\nWhile cell-free mMIMO (cfmMIMO) addresses this issue by providing uniform\ncoverage through distributed antennas, it requires significantly more\ndeployment cost due to the fronthaul and tight synchronization requirements.\nAlternatively, repeater-assisted massive MIMO (RA-MIMO) has recently been\nproposed to extend the coverage of cellular mMIMO by densely deploying low-cost\nsingle-antenna repeaters capable of amplifying and forwarding signals. In this\nwork, we investigate amplification control for the repeaters for two different\ngoals: (i) providing a fair performance among users, and (ii) reducing the\nextra energy consumption by the deployed repeaters. We propose a max-min\namplification control algorithm using the convex-concave procedure for fairness\nand a joint sleep mode and amplification control algorithm for energy\nefficiency, comparing long- and short-term strategies. Numerical results show\nthat RA-MIMO, with maximum amplification, improves\nsignal-to-interference-plus-noise ratio (SINR) by over 20 dB compared to mMIMO\nand performs within 1 dB of cfmMIMO when deploying the same number of repeaters\nas access points in cfmMIMO. Additionally, our majority-rule-based long-term\nsleep mechanism reduces repeater power consumption by 70% while maintaining\nless than 1% spectral efficiency outage."
                },
                "authors": [
                    {
                        "name": "Ozan Alp Topal"
                    },
                    {
                        "name": "zlem Tufe Demir"
                    },
                    {
                        "name": "Emil Bjrnson"
                    },
                    {
                        "name": "Cicek Cavdar"
                    }
                ],
                "author_detail": {
                    "name": "Cicek Cavdar"
                },
                "author": "Cicek Cavdar",
                "arxiv_comment": "Accepted and will be presented in WiOpt 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03415v1",
                "updated": "2025-04-04T12:53:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    12,
                    53,
                    33,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T12:53:33Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    12,
                    53,
                    33,
                    4,
                    94,
                    0
                ],
                "title": "NeRFlex: Resource-aware Real-time High-quality Rendering of Complex\n  Scenes on Mobile Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeRFlex: Resource-aware Real-time High-quality Rendering of Complex\n  Scenes on Mobile Devices"
                },
                "summary": "Neural Radiance Fields (NeRF) is a cutting-edge neural network-based\ntechnique for novel view synthesis in 3D reconstruction. However, its\nsignificant computational demands pose challenges for deployment on mobile\ndevices. While mesh-based NeRF solutions have shown potential in achieving\nreal-time rendering on mobile platforms, they often fail to deliver\nhigh-quality reconstructions when rendering practical complex scenes.\nAdditionally, the non-negligible memory overhead caused by pre-computed\nintermediate results complicates their practical application. To overcome these\nchallenges, we present NeRFlex, a resource-aware, high-resolution, real-time\nrendering framework for complex scenes on mobile devices. NeRFlex integrates\nmobile NeRF rendering with multi-NeRF representations that decompose a scene\ninto multiple sub-scenes, each represented by an individual NeRF network.\nCrucially, NeRFlex considers both memory and computation constraints as\nfirst-class citizens and redesigns the reconstruction process accordingly.\nNeRFlex first designs a detail-oriented segmentation module to identify\nsub-scenes with high-frequency details. For each NeRF network, a lightweight\nprofiler, built on domain knowledge, is used to accurately map configurations\nto visual quality and memory usage. Based on these insights and the resource\nconstraints on mobile devices, NeRFlex presents a dynamic programming algorithm\nto efficiently determine configurations for all NeRF representations, despite\nthe NP-hardness of the original decision problem. Extensive experiments on\nreal-world datasets and mobile devices demonstrate that NeRFlex achieves\nreal-time, high-quality rendering on commercial mobile devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Radiance Fields (NeRF) is a cutting-edge neural network-based\ntechnique for novel view synthesis in 3D reconstruction. However, its\nsignificant computational demands pose challenges for deployment on mobile\ndevices. While mesh-based NeRF solutions have shown potential in achieving\nreal-time rendering on mobile platforms, they often fail to deliver\nhigh-quality reconstructions when rendering practical complex scenes.\nAdditionally, the non-negligible memory overhead caused by pre-computed\nintermediate results complicates their practical application. To overcome these\nchallenges, we present NeRFlex, a resource-aware, high-resolution, real-time\nrendering framework for complex scenes on mobile devices. NeRFlex integrates\nmobile NeRF rendering with multi-NeRF representations that decompose a scene\ninto multiple sub-scenes, each represented by an individual NeRF network.\nCrucially, NeRFlex considers both memory and computation constraints as\nfirst-class citizens and redesigns the reconstruction process accordingly.\nNeRFlex first designs a detail-oriented segmentation module to identify\nsub-scenes with high-frequency details. For each NeRF network, a lightweight\nprofiler, built on domain knowledge, is used to accurately map configurations\nto visual quality and memory usage. Based on these insights and the resource\nconstraints on mobile devices, NeRFlex presents a dynamic programming algorithm\nto efficiently determine configurations for all NeRF representations, despite\nthe NP-hardness of the original decision problem. Extensive experiments on\nreal-world datasets and mobile devices demonstrate that NeRFlex achieves\nreal-time, high-quality rendering on commercial mobile devices."
                },
                "authors": [
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Yifei Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Zhu"
                },
                "author": "Yifei Zhu",
                "arxiv_comment": "This paper is accepted by 45th IEEE International Conference on\n  Distributed Computing Systems (ICDCS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01848v2",
                "updated": "2025-04-04T12:44:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    12,
                    44,
                    57,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-02T15:55:24Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    15,
                    55,
                    24,
                    2,
                    92,
                    0
                ],
                "title": "PaperBench: Evaluating AI's Ability to Replicate AI Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaperBench: Evaluating AI's Ability to Replicate AI Research"
                },
                "summary": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\n\\href{https://github.com/openai/preparedness}{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PaperBench, a benchmark evaluating the ability of AI agents to\nreplicate state-of-the-art AI research. Agents must replicate 20 ICML 2024\nSpotlight and Oral papers from scratch, including understanding paper\ncontributions, developing a codebase, and successfully executing experiments.\nFor objective evaluation, we develop rubrics that hierarchically decompose each\nreplication task into smaller sub-tasks with clear grading criteria. In total,\nPaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed\nwith the author(s) of each ICML paper for accuracy and realism. To enable\nscalable evaluation, we also develop an LLM-based judge to automatically grade\nreplication attempts against rubrics, and assess our judge's performance by\ncreating a separate benchmark for judges. We evaluate several frontier models\non PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet\n(New) with open-source scaffolding, achieves an average replication score of\n21.0\\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench,\nfinding that models do not yet outperform the human baseline. We\n\\href{https://github.com/openai/preparedness}{open-source our code} to\nfacilitate future research in understanding the AI engineering capabilities of\nAI agents."
                },
                "authors": [
                    {
                        "name": "Giulio Starace"
                    },
                    {
                        "name": "Oliver Jaffe"
                    },
                    {
                        "name": "Dane Sherburn"
                    },
                    {
                        "name": "James Aung"
                    },
                    {
                        "name": "Jun Shern Chan"
                    },
                    {
                        "name": "Leon Maksin"
                    },
                    {
                        "name": "Rachel Dias"
                    },
                    {
                        "name": "Evan Mays"
                    },
                    {
                        "name": "Benjamin Kinsella"
                    },
                    {
                        "name": "Wyatt Thompson"
                    },
                    {
                        "name": "Johannes Heidecke"
                    },
                    {
                        "name": "Amelia Glaese"
                    },
                    {
                        "name": "Tejal Patwardhan"
                    }
                ],
                "author_detail": {
                    "name": "Tejal Patwardhan"
                },
                "author": "Tejal Patwardhan",
                "arxiv_comment": "30 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05925v2",
                "updated": "2025-04-04T11:59:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    59,
                    49,
                    4,
                    94,
                    0
                ],
                "published": "2024-09-09T08:29:39Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    8,
                    29,
                    39,
                    0,
                    253,
                    0
                ],
                "title": "Assessing SPARQL capabilities of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing SPARQL capabilities of Large Language Models"
                },
                "summary": "The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs)\noffers significant synergistic potential for knowledge-driven applications. One\npossible integration is the interpretation and generation of formal languages,\nsuch as those used in the Semantic Web, with SPARQL being a core technology for\naccessing KGs. In this paper, we focus on measuring out-of-the box capabilities\nof LLMs to work with SPARQL and more specifically with SPARQL SELECT queries\napplying a quantitative approach.\n  We implemented various benchmarking tasks in the LLM-KG-Bench framework for\nautomated execution and evaluation with several LLMs. The tasks assess\ncapabilities along the dimensions of syntax, semantic read, semantic create,\nand the role of knowledge graph prompt inclusion.\n  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini,\nand Claude models. Our findings indicate that working with SPARQL SELECT\nqueries is still challenging for LLMs and heavily depends on the specific LLM\nas well as the complexity of the task. While fixing basic syntax errors seems\nto pose no problems for the best of the current LLMs evaluated, creating\nsemantically correct SPARQL SELECT queries is difficult in several cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs)\noffers significant synergistic potential for knowledge-driven applications. One\npossible integration is the interpretation and generation of formal languages,\nsuch as those used in the Semantic Web, with SPARQL being a core technology for\naccessing KGs. In this paper, we focus on measuring out-of-the box capabilities\nof LLMs to work with SPARQL and more specifically with SPARQL SELECT queries\napplying a quantitative approach.\n  We implemented various benchmarking tasks in the LLM-KG-Bench framework for\nautomated execution and evaluation with several LLMs. The tasks assess\ncapabilities along the dimensions of syntax, semantic read, semantic create,\nand the role of knowledge graph prompt inclusion.\n  With this new benchmarking tasks, we evaluated a selection of GPT, Gemini,\nand Claude models. Our findings indicate that working with SPARQL SELECT\nqueries is still challenging for LLMs and heavily depends on the specific LLM\nas well as the complexity of the task. While fixing basic syntax errors seems\nto pose no problems for the best of the current LLMs evaluated, creating\nsemantically correct SPARQL SELECT queries is difficult in several cases."
                },
                "authors": [
                    {
                        "name": "Lars-Peter Meyer"
                    },
                    {
                        "name": "Johannes Frey"
                    },
                    {
                        "name": "Felix Brei"
                    },
                    {
                        "name": "Natanael Arndt"
                    }
                ],
                "author_detail": {
                    "name": "Natanael Arndt"
                },
                "author": "Natanael Arndt",
                "arxiv_comment": "Peer reviewed and published at NLP4KGc @ Semantics 2024, see original\n  publication at https://ceur-ws.org/Vol-3874/paper3.pdf . Updated Metadata",
                "arxiv_journal_ref": "CEUR-WS Vol.3874 (12/2024) 35-53",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17477v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17477v4",
                "updated": "2025-04-04T11:55:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    55,
                    58,
                    4,
                    94,
                    0
                ],
                "published": "2024-10-22T23:24:15Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    23,
                    24,
                    15,
                    1,
                    296,
                    0
                ],
                "title": "Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of\n  Architectural Inductive Biases on Hallucination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of\n  Architectural Inductive Biases on Hallucination"
                },
                "summary": "The growth in prominence of large language models (LLMs) in everyday life can\nbe largely attributed to their generative abilities, yet some of this is also\nowed to the risks and costs associated with their use. On one front is their\ntendency to hallucinate false or misleading information, limiting their\nreliability. On another is the increasing focus on the computational\nlimitations associated with traditional self-attention based LLMs, which has\nbrought about new alternatives, in particular recurrent models, meant to\novercome them. Yet it remains uncommon to consider these two concerns\nsimultaneously. Do changes in architecture exacerbate/alleviate existing\nconcerns about hallucinations? Do they affect how and where they occur? Through\nan extensive evaluation, we study how these architecture-based inductive biases\naffect the propensity to hallucinate. While hallucination remains a general\nphenomenon not limited to specific architectures, the situations in which they\noccur and the ease with which specific types of hallucinations can be induced\ncan significantly differ based on the model architecture. These findings\nhighlight the need for better understanding both these problems in conjunction\nwith each other, as well as consider how to design more universal techniques\nfor handling hallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growth in prominence of large language models (LLMs) in everyday life can\nbe largely attributed to their generative abilities, yet some of this is also\nowed to the risks and costs associated with their use. On one front is their\ntendency to hallucinate false or misleading information, limiting their\nreliability. On another is the increasing focus on the computational\nlimitations associated with traditional self-attention based LLMs, which has\nbrought about new alternatives, in particular recurrent models, meant to\novercome them. Yet it remains uncommon to consider these two concerns\nsimultaneously. Do changes in architecture exacerbate/alleviate existing\nconcerns about hallucinations? Do they affect how and where they occur? Through\nan extensive evaluation, we study how these architecture-based inductive biases\naffect the propensity to hallucinate. While hallucination remains a general\nphenomenon not limited to specific architectures, the situations in which they\noccur and the ease with which specific types of hallucinations can be induced\ncan significantly differ based on the model architecture. These findings\nhighlight the need for better understanding both these problems in conjunction\nwith each other, as well as consider how to design more universal techniques\nfor handling hallucinations."
                },
                "authors": [
                    {
                        "name": "Jerry Huang"
                    },
                    {
                        "name": "Prasanna Parthasarathi"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Sarath Chandar"
                    }
                ],
                "author_detail": {
                    "name": "Sarath Chandar"
                },
                "author": "Sarath Chandar",
                "arxiv_comment": "22 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17477v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17477v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03380v1",
                "updated": "2025-04-04T11:52:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    52,
                    5,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T11:52:05Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    52,
                    5,
                    4,
                    94,
                    0
                ],
                "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Difficulty Filtering for Reasoning Oriented Reinforcement\n  Learning"
                },
                "summary": "Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning\nability of Large Language Models (LLMs). However, due to the sparsity of\nrewards in RORL, effective training is highly dependent on the selection of\nproblems of appropriate difficulty. Although curriculum learning attempts to\naddress this by adjusting difficulty, it often relies on static schedules, and\neven recent online filtering methods lack theoretical grounding and a\nsystematic understanding of their effectiveness. In this work, we theoretically\nand empirically show that curating the batch with the problems that the\ntraining model achieves intermediate accuracy on the fly can maximize the\neffectiveness of RORL training, namely balanced online difficulty filtering. We\nfirst derive that the lower bound of the KL divergence between the initial and\nthe optimal policy can be expressed with the variance of the sampled accuracy.\nBuilding on those insights, we show that balanced filtering can maximize the\nlower bound, leading to better performance. Experimental results across five\nchallenging math reasoning benchmarks show that balanced online filtering\nyields an additional 10% in AIME and 4% improvements in average over plain\nGRPO. Moreover, further analysis shows the gains in sample efficiency and\ntraining time efficiency, exceeding the maximum reward of plain GRPO within 60%\ntraining time and the volume of the training set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning\nability of Large Language Models (LLMs). However, due to the sparsity of\nrewards in RORL, effective training is highly dependent on the selection of\nproblems of appropriate difficulty. Although curriculum learning attempts to\naddress this by adjusting difficulty, it often relies on static schedules, and\neven recent online filtering methods lack theoretical grounding and a\nsystematic understanding of their effectiveness. In this work, we theoretically\nand empirically show that curating the batch with the problems that the\ntraining model achieves intermediate accuracy on the fly can maximize the\neffectiveness of RORL training, namely balanced online difficulty filtering. We\nfirst derive that the lower bound of the KL divergence between the initial and\nthe optimal policy can be expressed with the variance of the sampled accuracy.\nBuilding on those insights, we show that balanced filtering can maximize the\nlower bound, leading to better performance. Experimental results across five\nchallenging math reasoning benchmarks show that balanced online filtering\nyields an additional 10% in AIME and 4% improvements in average over plain\nGRPO. Moreover, further analysis shows the gains in sample efficiency and\ntraining time efficiency, exceeding the maximum reward of plain GRPO within 60%\ntraining time and the volume of the training set."
                },
                "authors": [
                    {
                        "name": "Sanghwan Bae"
                    },
                    {
                        "name": "Jiwoo Hong"
                    },
                    {
                        "name": "Min Young Lee"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "JeongYeon Nam"
                    },
                    {
                        "name": "Donghyun Kwak"
                    }
                ],
                "author_detail": {
                    "name": "Donghyun Kwak"
                },
                "author": "Donghyun Kwak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09893v2",
                "updated": "2025-04-04T11:45:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    45,
                    2,
                    4,
                    94,
                    0
                ],
                "published": "2024-10-13T16:06:54Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    16,
                    6,
                    54,
                    6,
                    287,
                    0
                ],
                "title": "RMB: Comprehensively Benchmarking Reward Models in LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMB: Comprehensively Benchmarking Reward Models in LLM Alignment"
                },
                "summary": "Reward models (RMs) guide the alignment of large language models (LLMs),\nsteering them toward behaviors preferred by humans. Evaluating RMs is the key\nto better aligning LLMs. However, the current evaluation of RMs may not\ndirectly correspond to their alignment performance due to the limited\ndistribution of evaluation data and evaluation methods that are not closely\nrelated to alignment objectives. To address these limitations, we propose RMB,\na comprehensive RM benchmark that covers over 49 real-world scenarios and\nincludes both pairwise and Best-of-N (BoN) evaluations to better reflect the\neffectiveness of RMs in guiding alignment optimization. We demonstrate a\npositive correlation between our benchmark and the downstream alignment task\nperformance. Based on our benchmark, we conduct extensive analysis on the\nstate-of-the-art RMs, revealing their generalization defects that were not\ndiscovered by previous benchmarks, and highlighting the potential of generative\nRMs. Furthermore, we delve into open questions in reward models, specifically\nexamining the effectiveness of majority voting for the evaluation of reward\nmodels and analyzing the impact factors of generative RMs, including the\ninfluence of evaluation criteria and instructing methods. Our evaluation code\nand datasets are available at\nhttps://github.com/Zhou-Zoey/RMB-Reward-Model-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) guide the alignment of large language models (LLMs),\nsteering them toward behaviors preferred by humans. Evaluating RMs is the key\nto better aligning LLMs. However, the current evaluation of RMs may not\ndirectly correspond to their alignment performance due to the limited\ndistribution of evaluation data and evaluation methods that are not closely\nrelated to alignment objectives. To address these limitations, we propose RMB,\na comprehensive RM benchmark that covers over 49 real-world scenarios and\nincludes both pairwise and Best-of-N (BoN) evaluations to better reflect the\neffectiveness of RMs in guiding alignment optimization. We demonstrate a\npositive correlation between our benchmark and the downstream alignment task\nperformance. Based on our benchmark, we conduct extensive analysis on the\nstate-of-the-art RMs, revealing their generalization defects that were not\ndiscovered by previous benchmarks, and highlighting the potential of generative\nRMs. Furthermore, we delve into open questions in reward models, specifically\nexamining the effectiveness of majority voting for the evaluation of reward\nmodels and analyzing the impact factors of generative RMs, including the\ninfluence of evaluation criteria and instructing methods. Our evaluation code\nand datasets are available at\nhttps://github.com/Zhou-Zoey/RMB-Reward-Model-Benchmark."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Guodong Zheng"
                    },
                    {
                        "name": "Binghai Wang"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Rong Bao"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Limao Xiong"
                    },
                    {
                        "name": "Jessica Fan"
                    },
                    {
                        "name": "Yurong Mou"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Accepted by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03373v1",
                "updated": "2025-04-04T11:44:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    44,
                    24,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T11:44:24Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    44,
                    24,
                    4,
                    94,
                    0
                ],
                "title": "An Efficient GPU-based Implementation for Noise Robust Sound Source\n  Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient GPU-based Implementation for Noise Robust Sound Source\n  Localization"
                },
                "summary": "Robot audition, encompassing Sound Source Localization (SSL), Sound Source\nSeparation (SSS), and Automatic Speech Recognition (ASR), enables robots and\nsmart devices to acquire auditory capabilities similar to human hearing.\nDespite their wide applicability, processing multi-channel audio signals from\nmicrophone arrays in SSL involves computationally intensive matrix operations,\nwhich can hinder efficient deployment on Central Processing Units (CPUs),\nparticularly in embedded systems with limited CPU resources. This paper\nintroduces a GPU-based implementation of SSL for robot audition, utilizing the\nGeneralized Singular Value Decomposition-based Multiple Signal Classification\n(GSVD-MUSIC), a noise-robust algorithm, within the HARK platform, an\nopen-source software suite. For a 60-channel microphone array, the proposed\nimplementation achieves significant performance improvements. On the Jetson AGX\nOrin, an embedded device powered by an NVIDIA GPU and ARM Cortex-A78AE v8.2\n64-bit CPUs, we observe speedups of 4645.1x for GSVD calculations and 8.8x for\nthe SSL module, while speedups of 2223.4x for GSVD calculation and 8.95x for\nthe entire SSL module on a server configured with an NVIDIA A100 GPU and AMD\nEPYC 7352 CPUs, making real-time processing feasible for large-scale microphone\narrays and providing ample capacity for real-time processing of potential\nsubsequent machine learning or deep learning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot audition, encompassing Sound Source Localization (SSL), Sound Source\nSeparation (SSS), and Automatic Speech Recognition (ASR), enables robots and\nsmart devices to acquire auditory capabilities similar to human hearing.\nDespite their wide applicability, processing multi-channel audio signals from\nmicrophone arrays in SSL involves computationally intensive matrix operations,\nwhich can hinder efficient deployment on Central Processing Units (CPUs),\nparticularly in embedded systems with limited CPU resources. This paper\nintroduces a GPU-based implementation of SSL for robot audition, utilizing the\nGeneralized Singular Value Decomposition-based Multiple Signal Classification\n(GSVD-MUSIC), a noise-robust algorithm, within the HARK platform, an\nopen-source software suite. For a 60-channel microphone array, the proposed\nimplementation achieves significant performance improvements. On the Jetson AGX\nOrin, an embedded device powered by an NVIDIA GPU and ARM Cortex-A78AE v8.2\n64-bit CPUs, we observe speedups of 4645.1x for GSVD calculations and 8.8x for\nthe SSL module, while speedups of 2223.4x for GSVD calculation and 8.95x for\nthe entire SSL module on a server configured with an NVIDIA A100 GPU and AMD\nEPYC 7352 CPUs, making real-time processing feasible for large-scale microphone\narrays and providing ample capacity for real-time processing of potential\nsubsequent machine learning or deep learning tasks."
                },
                "authors": [
                    {
                        "name": "Zirui Lin"
                    },
                    {
                        "name": "Masayuki Takigahira"
                    },
                    {
                        "name": "Naoya Terakado"
                    },
                    {
                        "name": "Haris Gulzar"
                    },
                    {
                        "name": "Monikka Roslianna Busto"
                    },
                    {
                        "name": "Takeharu Eda"
                    },
                    {
                        "name": "Katsutoshi Itoyama"
                    },
                    {
                        "name": "Kazuhiro Nakadai"
                    },
                    {
                        "name": "Hideharu Amano"
                    }
                ],
                "author_detail": {
                    "name": "Hideharu Amano"
                },
                "author": "Hideharu Amano",
                "arxiv_comment": "6 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03360v1",
                "updated": "2025-04-04T11:29:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    29,
                    30,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T11:29:30Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    29,
                    30,
                    4,
                    94,
                    0
                ],
                "title": "Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for\n  Energy Efficiency, Output Accuracy, and Inference Latency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for\n  Energy Efficiency, Output Accuracy, and Inference Latency"
                },
                "summary": "Deploying Large Language Models (LLMs) on edge devices presents significant\nchallenges due to computational constraints, memory limitations, inference\nspeed, and energy consumption. Model quantization has emerged as a key\ntechnique to enable efficient LLM inference by reducing model size and\ncomputational overhead. In this study, we conduct a comprehensive analysis of\n28 quantized LLMs from the Ollama library, which applies by default\nPost-Training Quantization (PTQ) and weight-only quantization techniques,\ndeployed on an edge device (Raspberry Pi 4 with 4GB RAM). We evaluate energy\nefficiency, inference performance, and output accuracy across multiple\nquantization levels and task types. Models are benchmarked on five standardized\ndatasets (CommonsenseQA, BIG-Bench Hard, TruthfulQA, GSM8K, and HumanEval), and\nwe employ a high-resolution, hardware-based energy measurement tool to capture\nreal-world power consumption. Our findings reveal the trade-offs between energy\nefficiency, inference speed, and accuracy in different quantization settings,\nhighlighting configurations that optimize LLM deployment for\nresource-constrained environments. By integrating hardware-level energy\nprofiling with LLM benchmarking, this study provides actionable insights for\nsustainable AI, bridging a critical gap in existing research on energy-aware\nLLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Large Language Models (LLMs) on edge devices presents significant\nchallenges due to computational constraints, memory limitations, inference\nspeed, and energy consumption. Model quantization has emerged as a key\ntechnique to enable efficient LLM inference by reducing model size and\ncomputational overhead. In this study, we conduct a comprehensive analysis of\n28 quantized LLMs from the Ollama library, which applies by default\nPost-Training Quantization (PTQ) and weight-only quantization techniques,\ndeployed on an edge device (Raspberry Pi 4 with 4GB RAM). We evaluate energy\nefficiency, inference performance, and output accuracy across multiple\nquantization levels and task types. Models are benchmarked on five standardized\ndatasets (CommonsenseQA, BIG-Bench Hard, TruthfulQA, GSM8K, and HumanEval), and\nwe employ a high-resolution, hardware-based energy measurement tool to capture\nreal-world power consumption. Our findings reveal the trade-offs between energy\nefficiency, inference speed, and accuracy in different quantization settings,\nhighlighting configurations that optimize LLM deployment for\nresource-constrained environments. By integrating hardware-level energy\nprofiling with LLM benchmarking, this study provides actionable insights for\nsustainable AI, bridging a critical gap in existing research on energy-aware\nLLM deployment."
                },
                "authors": [
                    {
                        "name": "Erik Johannes Husom"
                    },
                    {
                        "name": "Arda Goknil"
                    },
                    {
                        "name": "Merve Astekin"
                    },
                    {
                        "name": "Lwin Khin Shar"
                    },
                    {
                        "name": "Andre Ksen"
                    },
                    {
                        "name": "Sagar Sen"
                    },
                    {
                        "name": "Benedikt Andreas Mithassel"
                    },
                    {
                        "name": "Ahmet Soylu"
                    }
                ],
                "author_detail": {
                    "name": "Ahmet Soylu"
                },
                "author": "Ahmet Soylu",
                "arxiv_comment": "30 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03352v1",
                "updated": "2025-04-04T11:14:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    14,
                    38,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T11:14:38Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    11,
                    14,
                    38,
                    4,
                    94,
                    0
                ],
                "title": "Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social\n  Psychological Underpinnings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social\n  Psychological Underpinnings"
                },
                "summary": "Stereotypes are known to be highly pernicious, making their detection\ncritically important. However, current research predominantly focuses on\ndetecting and evaluating stereotypical biases in LLMs, leaving the study of\nstereotypes in its early stages. Many studies have failed to clearly\ndistinguish between stereotypes and stereotypical biases, which has\nsignificantly slowed progress in advancing research in this area. Stereotype\nand anti-stereotype detection is a problem that requires knowledge of society;\nhence, it is one of the most difficult areas in Responsible AI. This work\ninvestigates this task, where we propose a four-tuple definition and provide\nprecise terminology distinguishing stereotype, anti-stereotype, stereotypical\nbias, and bias, offering valuable insights into their various aspects. In this\npaper, we propose StereoDetect, a high-quality benchmarking dataset curated for\nthis task by optimally utilizing current datasets such as StereoSet and\nWinoQueer, involving a manual verification process and the transfer of semantic\ninformation. We demonstrate that language models for reasoning with fewer than\n10B parameters often get confused when detecting anti-stereotypes. We also\ndemonstrate the critical importance of well-curated datasets by comparing our\nmodel with other current models for stereotype detection. The dataset and code\nis available at https://github.com/KaustubhShejole/StereoDetect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotypes are known to be highly pernicious, making their detection\ncritically important. However, current research predominantly focuses on\ndetecting and evaluating stereotypical biases in LLMs, leaving the study of\nstereotypes in its early stages. Many studies have failed to clearly\ndistinguish between stereotypes and stereotypical biases, which has\nsignificantly slowed progress in advancing research in this area. Stereotype\nand anti-stereotype detection is a problem that requires knowledge of society;\nhence, it is one of the most difficult areas in Responsible AI. This work\ninvestigates this task, where we propose a four-tuple definition and provide\nprecise terminology distinguishing stereotype, anti-stereotype, stereotypical\nbias, and bias, offering valuable insights into their various aspects. In this\npaper, we propose StereoDetect, a high-quality benchmarking dataset curated for\nthis task by optimally utilizing current datasets such as StereoSet and\nWinoQueer, involving a manual verification process and the transfer of semantic\ninformation. We demonstrate that language models for reasoning with fewer than\n10B parameters often get confused when detecting anti-stereotypes. We also\ndemonstrate the critical importance of well-curated datasets by comparing our\nmodel with other current models for stereotype detection. The dataset and code\nis available at https://github.com/KaustubhShejole/StereoDetect."
                },
                "authors": [
                    {
                        "name": "Kaustubh Shivshankar Shejole"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03343v1",
                "updated": "2025-04-04T10:58:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    58,
                    57,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T10:58:57Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    58,
                    57,
                    4,
                    94,
                    0
                ],
                "title": "Talk2X -- An Open-Source Toolkit Facilitating Deployment of LLM-Powered\n  Chatbots on the Web",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Talk2X -- An Open-Source Toolkit Facilitating Deployment of LLM-Powered\n  Chatbots on the Web"
                },
                "summary": "Integrated into websites, LLM-powered chatbots offer alternative means of\nnavigation and information retrieval, leading to a shift in how users access\ninformation on the web. Yet, predominantly closed-sourced solutions limit\nproliferation among web hosts and suffer from a lack of transparency with\nregard to implementation details and energy efficiency. In this work, we\npropose our openly available agent Talk2X leveraging an adapted\nretrieval-augmented generation approach (RAG) combined with an automatically\ngenerated vector database, benefiting energy efficiency. Talk2X's architecture\nis generalizable to arbitrary websites offering developers a ready to use tool\nfor integration. Using a mixed-methods approach, we evaluated Talk2X's\nusability by tasking users to acquire specific assets from an open science\nrepository. Talk2X significantly improved task completion time, correctness,\nand user experience supporting users in quickly pinpointing specific\ninformation as compared to standard user-website interaction. Our findings\ncontribute technical advancements to an ongoing paradigm shift of how we access\ninformation on the web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated into websites, LLM-powered chatbots offer alternative means of\nnavigation and information retrieval, leading to a shift in how users access\ninformation on the web. Yet, predominantly closed-sourced solutions limit\nproliferation among web hosts and suffer from a lack of transparency with\nregard to implementation details and energy efficiency. In this work, we\npropose our openly available agent Talk2X leveraging an adapted\nretrieval-augmented generation approach (RAG) combined with an automatically\ngenerated vector database, benefiting energy efficiency. Talk2X's architecture\nis generalizable to arbitrary websites offering developers a ready to use tool\nfor integration. Using a mixed-methods approach, we evaluated Talk2X's\nusability by tasking users to acquire specific assets from an open science\nrepository. Talk2X significantly improved task completion time, correctness,\nand user experience supporting users in quickly pinpointing specific\ninformation as compared to standard user-website interaction. Our findings\ncontribute technical advancements to an ongoing paradigm shift of how we access\ninformation on the web."
                },
                "authors": [
                    {
                        "name": "Lars Krupp"
                    },
                    {
                        "name": "Daniel Geiler"
                    },
                    {
                        "name": "Peter Hevesi"
                    },
                    {
                        "name": "Marco Hirsch"
                    },
                    {
                        "name": "Paul Lukowicz"
                    },
                    {
                        "name": "Jakob Karolus"
                    }
                ],
                "author_detail": {
                    "name": "Jakob Karolus"
                },
                "author": "Jakob Karolus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00159v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00159v3",
                "updated": "2025-04-04T10:58:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    58,
                    40,
                    4,
                    94,
                    0
                ],
                "published": "2024-08-30T15:04:11Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    15,
                    4,
                    11,
                    4,
                    243,
                    0
                ],
                "title": "LLMs Prompted for Graphs: Hallucinations and Generative Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Prompted for Graphs: Hallucinations and Generative Capabilities"
                },
                "summary": "Large Language Models (LLMs) are nowadays prompted for a wide variety of\ntasks. In this article, we investigate their ability in reciting and generating\ngraphs. We first study the ability of LLMs to regurgitate well known graphs\nfrom the literature (e.g. Karate club or the graph atlas)4. Secondly, we\nquestion the generative capabilities of LLMs by asking for Erdos-Renyi random\ngraphs. As opposed to the possibility that they could memorize some Erdos-Renyi\ngraphs included in their scraped training set, this second investigation aims\nat studying a possible emergent property of LLMs. For both tasks, we propose a\nmetric to assess their errors with the lens of hallucination (i.e. incorrect\ninformation returned as facts). We most notably find that the amplitude of\ngraph hallucinations can characterize the superiority of some LLMs. Indeed, for\nthe recitation task, we observe that graph hallucinations correlate with the\nHallucination Leaderboard, a hallucination rank that leverages 10, 000 times\nmore prompts to obtain its ranking. For the generation task, we find\nsurprisingly good and reproducible results in most of LLMs. We believe this to\nconstitute a starting point for more in-depth studies of this emergent\ncapability and a challenging benchmark for their improvements. Altogether,\nthese two aspects of LLMs capabilities bridge a gap between the network science\nand machine learning communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are nowadays prompted for a wide variety of\ntasks. In this article, we investigate their ability in reciting and generating\ngraphs. We first study the ability of LLMs to regurgitate well known graphs\nfrom the literature (e.g. Karate club or the graph atlas)4. Secondly, we\nquestion the generative capabilities of LLMs by asking for Erdos-Renyi random\ngraphs. As opposed to the possibility that they could memorize some Erdos-Renyi\ngraphs included in their scraped training set, this second investigation aims\nat studying a possible emergent property of LLMs. For both tasks, we propose a\nmetric to assess their errors with the lens of hallucination (i.e. incorrect\ninformation returned as facts). We most notably find that the amplitude of\ngraph hallucinations can characterize the superiority of some LLMs. Indeed, for\nthe recitation task, we observe that graph hallucinations correlate with the\nHallucination Leaderboard, a hallucination rank that leverages 10, 000 times\nmore prompts to obtain its ranking. For the generation task, we find\nsurprisingly good and reproducible results in most of LLMs. We believe this to\nconstitute a starting point for more in-depth studies of this emergent\ncapability and a challenging benchmark for their improvements. Altogether,\nthese two aspects of LLMs capabilities bridge a gap between the network science\nand machine learning communities."
                },
                "authors": [
                    {
                        "name": "Gurvan Richardeau"
                    },
                    {
                        "name": "Samy Chali"
                    },
                    {
                        "name": "Erwan Le Merrer"
                    },
                    {
                        "name": "Camilla Penzo"
                    },
                    {
                        "name": "Gilles Tredan"
                    }
                ],
                "author_detail": {
                    "name": "Gilles Tredan"
                },
                "author": "Gilles Tredan",
                "arxiv_comment": "A preliminary version of this work appeared in the Complex Networks\n  2024 conference, under the title \"LLMs hallucinate graphs too: a structural\n  perspective\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00159v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00159v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03342v1",
                "updated": "2025-04-04T10:57:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    57,
                    3,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T10:57:03Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    57,
                    3,
                    4,
                    94,
                    0
                ],
                "title": "EOOD: Entropy-based Out-of-distribution Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EOOD: Entropy-based Out-of-distribution Detection"
                },
                "summary": "Deep neural networks (DNNs) often exhibit overconfidence when encountering\nout-of-distribution (OOD) samples, posing significant challenges for\ndeployment. Since DNNs are trained on in-distribution (ID) datasets, the\ninformation flow of ID samples through DNNs inevitably differs from that of OOD\nsamples. In this paper, we propose an Entropy-based Out-Of-distribution\nDetection (EOOD) framework. EOOD first identifies specific block where the\ninformation flow differences between ID and OOD samples are more pronounced,\nusing both ID and pseudo-OOD samples. It then calculates the conditional\nentropy on the selected block as the OOD confidence score. Comprehensive\nexperiments conducted across various ID and OOD settings demonstrate the\neffectiveness of EOOD in OOD detection and its superiority over\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) often exhibit overconfidence when encountering\nout-of-distribution (OOD) samples, posing significant challenges for\ndeployment. Since DNNs are trained on in-distribution (ID) datasets, the\ninformation flow of ID samples through DNNs inevitably differs from that of OOD\nsamples. In this paper, we propose an Entropy-based Out-Of-distribution\nDetection (EOOD) framework. EOOD first identifies specific block where the\ninformation flow differences between ID and OOD samples are more pronounced,\nusing both ID and pseudo-OOD samples. It then calculates the conditional\nentropy on the selected block as the OOD confidence score. Comprehensive\nexperiments conducted across various ID and OOD settings demonstrate the\neffectiveness of EOOD in OOD detection and its superiority over\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Guide Yang"
                    },
                    {
                        "name": "Chao Hou"
                    },
                    {
                        "name": "Weilong Peng"
                    },
                    {
                        "name": "Xiang Fang"
                    },
                    {
                        "name": "Yongwei Nie"
                    },
                    {
                        "name": "Peican Zhu"
                    },
                    {
                        "name": "Keke Tang"
                    }
                ],
                "author_detail": {
                    "name": "Keke Tang"
                },
                "author": "Keke Tang",
                "arxiv_comment": "IJCNN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03338v1",
                "updated": "2025-04-04T10:42:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    42,
                    56,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T10:42:56Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    10,
                    42,
                    56,
                    4,
                    94,
                    0
                ],
                "title": "BabyLM's First Words: Word Segmentation as a Phonological Probing Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BabyLM's First Words: Word Segmentation as a Phonological Probing Task"
                },
                "summary": "Language models provide a key framework for studying linguistic theories\nbased on prediction, but phonological analysis using large language models\n(LLMs) is difficult; there are few phonological benchmarks beyond English and\nthe standard input representation used in LLMs (subwords of graphemes) is not\nsuitable for analyzing the representation of phonemes. In this work, we\ndemonstrate how word segmentation can be used as a phonological probing task,\nallowing us to study the representations learned by phoneme-based language\nmodels trained on child-directed speech across 31 languages. Following\ncomputational models of word segmentation, we present unsupervised methods for\nextracting word boundaries from a trained model using the observation that\nprediction-error peaks at the start of words. We also use linear probes to\nidentify that these models implicitly track word boundaries, even when they do\nnot appear in training. This cross-lingual work corroborates statistical\nlearning theories of acquisition and empirically motivates new methods for\ntraining subword tokenizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models provide a key framework for studying linguistic theories\nbased on prediction, but phonological analysis using large language models\n(LLMs) is difficult; there are few phonological benchmarks beyond English and\nthe standard input representation used in LLMs (subwords of graphemes) is not\nsuitable for analyzing the representation of phonemes. In this work, we\ndemonstrate how word segmentation can be used as a phonological probing task,\nallowing us to study the representations learned by phoneme-based language\nmodels trained on child-directed speech across 31 languages. Following\ncomputational models of word segmentation, we present unsupervised methods for\nextracting word boundaries from a trained model using the observation that\nprediction-error peaks at the start of words. We also use linear probes to\nidentify that these models implicitly track word boundaries, even when they do\nnot appear in training. This cross-lingual work corroborates statistical\nlearning theories of acquisition and empirically motivates new methods for\ntraining subword tokenizers."
                },
                "authors": [
                    {
                        "name": "Zbulon Goriely"
                    }
                ],
                "author_detail": {
                    "name": "Zbulon Goriely"
                },
                "author": "Zbulon Goriely",
                "arxiv_comment": "17 pages, 10 figures, submitted to CoNLL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03312v1",
                "updated": "2025-04-04T09:47:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    9,
                    47,
                    58,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T09:47:58Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    9,
                    47,
                    58,
                    4,
                    94,
                    0
                ],
                "title": "Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User\n  Devices"
                },
                "summary": "Large Language Models have significantly advanced natural language\nprocessing, achieving remarkable performance in tasks such as language\ngeneration, translation, and reasoning. However, their substantial\ncomputational requirements restrict deployment to high-end systems, limiting\naccessibility on consumer-grade devices. This challenge is especially\npronounced for under-resourced languages like those spoken in the Iberian\nPeninsula, where relatively limited linguistic resources and benchmarks hinder\neffective evaluation. This work presents a comprehensive evaluation of compact\nstate-of-the-art LLMs across several essential NLP tasks tailored for Iberian\nlanguages. The results reveal that while some models consistently excel in\ncertain tasks, significant performance gaps remain, particularly for languages\nsuch as Basque. These findings highlight the need for further research on\nbalancing model compactness with robust multilingual performance",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have significantly advanced natural language\nprocessing, achieving remarkable performance in tasks such as language\ngeneration, translation, and reasoning. However, their substantial\ncomputational requirements restrict deployment to high-end systems, limiting\naccessibility on consumer-grade devices. This challenge is especially\npronounced for under-resourced languages like those spoken in the Iberian\nPeninsula, where relatively limited linguistic resources and benchmarks hinder\neffective evaluation. This work presents a comprehensive evaluation of compact\nstate-of-the-art LLMs across several essential NLP tasks tailored for Iberian\nlanguages. The results reveal that while some models consistently excel in\ncertain tasks, significant performance gaps remain, particularly for languages\nsuch as Basque. These findings highlight the need for further research on\nbalancing model compactness with robust multilingual performance"
                },
                "authors": [
                    {
                        "name": "Lus Couto Seller"
                    },
                    {
                        "name": "igo Sanz Torres"
                    },
                    {
                        "name": "Adrin Vogel-Fernndez"
                    },
                    {
                        "name": "Carlos Gonzlez Carballo"
                    },
                    {
                        "name": "Pedro Miguel Snchez Snchez"
                    },
                    {
                        "name": "Adrin Carruana Martn"
                    },
                    {
                        "name": "Enrique de Miguel Ambite"
                    }
                ],
                "author_detail": {
                    "name": "Enrique de Miguel Ambite"
                },
                "author": "Enrique de Miguel Ambite",
                "arxiv_comment": "Under Revision al SEPLN conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03302v1",
                "updated": "2025-04-04T09:27:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    9,
                    27,
                    19,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T09:27:19Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    9,
                    27,
                    19,
                    4,
                    94,
                    0
                ],
                "title": "Noise Augmented Fine Tuning for Mitigating Hallucinations in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise Augmented Fine Tuning for Mitigating Hallucinations in Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) often produce inaccurate or misleading\ncontent-hallucinations. To address this challenge, we introduce Noise-Augmented\nFine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise\ninjection based on the signal-to-noise ratio (SNR) to enhance model robustness.\nIn particular, NoiseFiT selectively perturbs layers identified as either\nhigh-SNR (more robust) or low-SNR (potentially under-regularized) using a\ndynamically scaled Gaussian noise. We further propose a hybrid loss that\ncombines standard cross-entropy, soft cross-entropy, and consistency\nregularization to ensure stable and accurate outputs under noisy training\nconditions. Our theoretical analysis shows that adaptive noise injection is\nboth unbiased and variance-preserving, providing strong guarantees for\nconvergence in expectation. Empirical results on multiple test and benchmark\ndatasets demonstrate that NoiseFiT significantly reduces hallucination rates,\noften improving or matching baseline performance in key tasks. These findings\nhighlight the promise of noise-driven strategies for achieving robust,\ntrustworthy language modeling without incurring prohibitive computational\noverhead. Given the comprehensive and detailed nature of our experiments, we\nhave publicly released the fine-tuning logs, benchmark evaluation artifacts,\nand source code online at W&B, Hugging Face, and GitHub, respectively, to\nfoster further research, accessibility and reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often produce inaccurate or misleading\ncontent-hallucinations. To address this challenge, we introduce Noise-Augmented\nFine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise\ninjection based on the signal-to-noise ratio (SNR) to enhance model robustness.\nIn particular, NoiseFiT selectively perturbs layers identified as either\nhigh-SNR (more robust) or low-SNR (potentially under-regularized) using a\ndynamically scaled Gaussian noise. We further propose a hybrid loss that\ncombines standard cross-entropy, soft cross-entropy, and consistency\nregularization to ensure stable and accurate outputs under noisy training\nconditions. Our theoretical analysis shows that adaptive noise injection is\nboth unbiased and variance-preserving, providing strong guarantees for\nconvergence in expectation. Empirical results on multiple test and benchmark\ndatasets demonstrate that NoiseFiT significantly reduces hallucination rates,\noften improving or matching baseline performance in key tasks. These findings\nhighlight the promise of noise-driven strategies for achieving robust,\ntrustworthy language modeling without incurring prohibitive computational\noverhead. Given the comprehensive and detailed nature of our experiments, we\nhave publicly released the fine-tuning logs, benchmark evaluation artifacts,\nand source code online at W&B, Hugging Face, and GitHub, respectively, to\nfoster further research, accessibility and reproducibility."
                },
                "authors": [
                    {
                        "name": "Afshin Khadangi"
                    },
                    {
                        "name": "Amir Sartipi"
                    },
                    {
                        "name": "Igor Tchappi"
                    },
                    {
                        "name": "Ramin Bahmani"
                    }
                ],
                "author_detail": {
                    "name": "Ramin Bahmani"
                },
                "author": "Ramin Bahmani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03295v1",
                "updated": "2025-04-04T09:20:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    9,
                    20,
                    19,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T09:20:19Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    9,
                    20,
                    19,
                    4,
                    94,
                    0
                ],
                "title": "Stance-Driven Multimodal Controlled Statement Generation: New Dataset\n  and Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance-Driven Multimodal Controlled Statement Generation: New Dataset\n  and Task"
                },
                "summary": "Formulating statements that support diverse or controversial stances on\nspecific topics is vital for platforms that enable user expression, reshape\npolitical discourse, and drive social critique and information dissemination.\nWith the rise of Large Language Models (LLMs), controllable text generation\ntowards specific stances has become a promising research area with applications\nin shaping public opinion and commercial marketing. However, current datasets\noften focus solely on pure texts, lacking multimodal content and effective\ncontext, particularly in the context of stance detection. In this paper, we\nformally define and study the new problem of stance-driven controllable content\ngeneration for tweets with text and images, where given a multimodal post (text\nand image/video), a model generates a stance-controlled response. To this end,\nwe create the Multimodal Stance Generation Dataset (StanceGen2024), the first\nresource explicitly designed for multimodal stance-controllable text generation\nin political discourse. It includes posts and user comments from the 2024 U.S.\npresidential election, featuring text, images, videos, and stance annotations\nto explore how multimodal political content shapes stance expression.\nFurthermore, we propose a Stance-Driven Multimodal Generation (SDMG) framework\nthat integrates weighted fusion of multimodal features and stance guidance to\nimprove semantic consistency and stance control. We release the dataset and\ncode (https://anonymous.4open.science/r/StanceGen-BE9D) for public use and\nfurther research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formulating statements that support diverse or controversial stances on\nspecific topics is vital for platforms that enable user expression, reshape\npolitical discourse, and drive social critique and information dissemination.\nWith the rise of Large Language Models (LLMs), controllable text generation\ntowards specific stances has become a promising research area with applications\nin shaping public opinion and commercial marketing. However, current datasets\noften focus solely on pure texts, lacking multimodal content and effective\ncontext, particularly in the context of stance detection. In this paper, we\nformally define and study the new problem of stance-driven controllable content\ngeneration for tweets with text and images, where given a multimodal post (text\nand image/video), a model generates a stance-controlled response. To this end,\nwe create the Multimodal Stance Generation Dataset (StanceGen2024), the first\nresource explicitly designed for multimodal stance-controllable text generation\nin political discourse. It includes posts and user comments from the 2024 U.S.\npresidential election, featuring text, images, videos, and stance annotations\nto explore how multimodal political content shapes stance expression.\nFurthermore, we propose a Stance-Driven Multimodal Generation (SDMG) framework\nthat integrates weighted fusion of multimodal features and stance guidance to\nimprove semantic consistency and stance control. We release the dataset and\ncode (https://anonymous.4open.science/r/StanceGen-BE9D) for public use and\nfurther research."
                },
                "authors": [
                    {
                        "name": "Bingqian Wang"
                    },
                    {
                        "name": "Quan Fang"
                    },
                    {
                        "name": "Jiachen Sun"
                    },
                    {
                        "name": "Xiaoxiao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxiao Ma"
                },
                "author": "Xiaoxiao Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03274v1",
                "updated": "2025-04-04T08:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    8,
                    48,
                    43,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T08:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    8,
                    48,
                    43,
                    4,
                    94,
                    0
                ],
                "title": "Do Large Language Models Solve the Problems of Agent-Based Modeling? A\n  Critical Review of Generative Social Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Solve the Problems of Agent-Based Modeling? A\n  Critical Review of Generative Social Simulations"
                },
                "summary": "Recent advancements in AI have reinvigorated Agent-Based Models (ABMs), as\nthe integration of Large Language Models (LLMs) has led to the emergence of\n``generative ABMs'' as a novel approach to simulating social systems. While\nABMs offer means to bridge micro-level interactions with macro-level patterns,\nthey have long faced criticisms from social scientists, pointing to e.g., lack\nof realism, computational complexity, and challenges of calibrating and\nvalidating against empirical data. This paper reviews the generative ABM\nliterature to assess how this new approach adequately addresses these\nlong-standing criticisms. Our findings show that studies show limited awareness\nof historical debates. Validation remains poorly addressed, with many studies\nrelying solely on subjective assessments of model `believability', and even the\nmost rigorous validation failing to adequately evidence operational validity.\nWe argue that there are reasons to believe that LLMs will exacerbate rather\nthan resolve the long-standing challenges of ABMs. The black-box nature of LLMs\nmoreover limit their usefulness for disentangling complex emergent causal\nmechanisms. While generative ABMs are still in a stage of early\nexperimentation, these findings question of whether and how the field can\ntransition to the type of rigorous modeling needed to contribute to social\nscientific theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in AI have reinvigorated Agent-Based Models (ABMs), as\nthe integration of Large Language Models (LLMs) has led to the emergence of\n``generative ABMs'' as a novel approach to simulating social systems. While\nABMs offer means to bridge micro-level interactions with macro-level patterns,\nthey have long faced criticisms from social scientists, pointing to e.g., lack\nof realism, computational complexity, and challenges of calibrating and\nvalidating against empirical data. This paper reviews the generative ABM\nliterature to assess how this new approach adequately addresses these\nlong-standing criticisms. Our findings show that studies show limited awareness\nof historical debates. Validation remains poorly addressed, with many studies\nrelying solely on subjective assessments of model `believability', and even the\nmost rigorous validation failing to adequately evidence operational validity.\nWe argue that there are reasons to believe that LLMs will exacerbate rather\nthan resolve the long-standing challenges of ABMs. The black-box nature of LLMs\nmoreover limit their usefulness for disentangling complex emergent causal\nmechanisms. While generative ABMs are still in a stage of early\nexperimentation, these findings question of whether and how the field can\ntransition to the type of rigorous modeling needed to contribute to social\nscientific theory."
                },
                "authors": [
                    {
                        "name": "Maik Larooij"
                    },
                    {
                        "name": "Petter Trnberg"
                    }
                ],
                "author_detail": {
                    "name": "Petter Trnberg"
                },
                "author": "Petter Trnberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15316v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15316v3",
                "updated": "2025-04-04T08:29:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    8,
                    29,
                    19,
                    4,
                    94,
                    0
                ],
                "published": "2024-10-20T07:03:49Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    7,
                    3,
                    49,
                    6,
                    294,
                    0
                ],
                "title": "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their application to speech-based tasks remains challenging due to the\ncomplexities of integrating audio and text modalities. This paper introduces\nIchigo, a mixed-modal model that seamlessly processes interleaved sequences of\nspeech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes\nspeech into discrete tokens and employs a uniform transformer-based\narchitecture for both speech and text modalities. This method enables joint\nreasoning and generation across modalities without the need for separate\nadapters. We present a comprehensive training methodology, including\npre-training on multilingual speech recognition datasets and fine-tuning on a\ncurated instruction dataset. Ichigo demonstrates state-of-the-art performance\non speech question-answering benchmarks, outperforming existing open-source\nspeech language models and achieving comparable results to cascaded systems.\nNotably, Ichigo exhibits a latency of just 111 ms to first token generation,\nsignificantly lower than current models. Our approach not only advances the\nfield of multimodal AI but also provides a framework for smaller research teams\nto contribute effectively to open-source speech-language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing,\nbut their application to speech-based tasks remains challenging due to the\ncomplexities of integrating audio and text modalities. This paper introduces\nIchigo, a mixed-modal model that seamlessly processes interleaved sequences of\nspeech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes\nspeech into discrete tokens and employs a uniform transformer-based\narchitecture for both speech and text modalities. This method enables joint\nreasoning and generation across modalities without the need for separate\nadapters. We present a comprehensive training methodology, including\npre-training on multilingual speech recognition datasets and fine-tuning on a\ncurated instruction dataset. Ichigo demonstrates state-of-the-art performance\non speech question-answering benchmarks, outperforming existing open-source\nspeech language models and achieving comparable results to cascaded systems.\nNotably, Ichigo exhibits a latency of just 111 ms to first token generation,\nsignificantly lower than current models. Our approach not only advances the\nfield of multimodal AI but also provides a framework for smaller research teams\nto contribute effectively to open-source speech-language models."
                },
                "authors": [
                    {
                        "name": "Alan Dao"
                    },
                    {
                        "name": "Dinh Bach Vu"
                    },
                    {
                        "name": "Huy Hoang Ha"
                    }
                ],
                "author_detail": {
                    "name": "Huy Hoang Ha"
                },
                "arxiv_affiliation": "Gia Tuan Dao",
                "author": "Huy Hoang Ha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15316v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15316v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03255v1",
                "updated": "2025-04-04T08:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    8,
                    10,
                    2,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T08:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    8,
                    10,
                    2,
                    4,
                    94,
                    0
                ],
                "title": "Inherent and emergent liability issues in LLM-based agentic systems: a\n  principal-agent perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inherent and emergent liability issues in LLM-based agentic systems: a\n  principal-agent perspective"
                },
                "summary": "Agentic systems powered by large language models (LLMs) are becoming\nprogressively more complex and capable. Their increasing agency and expanding\ndeployment settings attract growing attention over effective governance\npolicies, monitoring and control protocols. Based on emerging landscapes of the\nagentic market, we analyze the potential liability issues stemming from\ndelegated use of LLM agents and their extended systems from a principal-agent\nperspective. Our analysis complements existing risk-based studies on artificial\nagency and covers the spectrum of important aspects of the principal-agent\nrelationship and their potential consequences at deployment. Furthermore, we\nmotivate method developments for technical governance along the directions of\ninterpretability and behavior evaluations, reward and conflict management, and\nthe mitigation of misalignment and misconduct through principled engineering of\ndetection and fail-safe mechanisms. By illustrating the outstanding issues in\nAI liability for LLM-based agentic systems, we aim to inform the system design,\nauditing and monitoring approaches to enhancing transparency and\naccountability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic systems powered by large language models (LLMs) are becoming\nprogressively more complex and capable. Their increasing agency and expanding\ndeployment settings attract growing attention over effective governance\npolicies, monitoring and control protocols. Based on emerging landscapes of the\nagentic market, we analyze the potential liability issues stemming from\ndelegated use of LLM agents and their extended systems from a principal-agent\nperspective. Our analysis complements existing risk-based studies on artificial\nagency and covers the spectrum of important aspects of the principal-agent\nrelationship and their potential consequences at deployment. Furthermore, we\nmotivate method developments for technical governance along the directions of\ninterpretability and behavior evaluations, reward and conflict management, and\nthe mitigation of misalignment and misconduct through principled engineering of\ndetection and fail-safe mechanisms. By illustrating the outstanding issues in\nAI liability for LLM-based agentic systems, we aim to inform the system design,\nauditing and monitoring approaches to enhancing transparency and\naccountability."
                },
                "authors": [
                    {
                        "name": "Garry A. Gabison"
                    },
                    {
                        "name": "R. Patrick Xian"
                    }
                ],
                "author_detail": {
                    "name": "R. Patrick Xian"
                },
                "author": "R. Patrick Xian",
                "arxiv_comment": "12 pages content (incl. appendix) + 12 pages references, comments\n  welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02732v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02732v2",
                "updated": "2025-04-04T07:41:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    7,
                    41,
                    19,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-03T16:17:55Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    16,
                    17,
                    55,
                    3,
                    93,
                    0
                ],
                "title": "Why do LLMs attend to the first token?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why do LLMs attend to the first token?"
                },
                "summary": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) tend to attend heavily to the first token in the\nsequence -- creating a so-called attention sink. Many works have studied this\nphenomenon in detail, proposing various ways to either leverage or alleviate\nit. Attention sinks have been connected to quantisation difficulties, security\nissues, and streaming attention. Yet, while many works have provided conditions\nin which they occur or not, a critical question remains shallowly answered: Why\ndo LLMs learn such patterns and how are they being used? In this work, we argue\ntheoretically and empirically that this mechanism provides a method for LLMs to\navoid over-mixing, connecting this to existing lines of work that study\nmathematically how information propagates in Transformers. We conduct\nexperiments to validate our theoretical intuitions and show how choices such as\ncontext length, depth, and data packing influence the sink behaviour. We hope\nthat this study provides a new practical perspective on why attention sinks are\nuseful in LLMs, leading to a better understanding of the attention patterns\nthat form during training."
                },
                "authors": [
                    {
                        "name": "Federico Barbero"
                    },
                    {
                        "name": "lvaro Arroyo"
                    },
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Christos Perivolaropoulos"
                    },
                    {
                        "name": "Michael Bronstein"
                    },
                    {
                        "name": "Petar Velikovi"
                    },
                    {
                        "name": "Razvan Pascanu"
                    }
                ],
                "author_detail": {
                    "name": "Razvan Pascanu"
                },
                "author": "Razvan Pascanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02732v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02732v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03207v1",
                "updated": "2025-04-04T06:40:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    40,
                    3,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T06:40:03Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    40,
                    3,
                    4,
                    94,
                    0
                ],
                "title": "Augmenting Human Cognition With Generative AI: Lessons From AI-Assisted\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting Human Cognition With Generative AI: Lessons From AI-Assisted\n  Decision-Making"
                },
                "summary": "How can we use generative AI to design tools that augment rather than replace\nhuman cognition? In this position paper, we review our own research on\nAI-assisted decision-making for lessons to learn. We observe that in both\nAI-assisted decision-making and generative AI, a popular approach is to suggest\nAI-generated end-to-end solutions to users, which users can then accept,\nreject, or edit. Alternatively, AI tools could offer more incremental support\nto help users solve tasks themselves, which we call process-oriented support.\nWe describe findings on the challenges of end-to-end solutions, and how\nprocess-oriented support can address them. We also discuss the applicability of\nthese findings to generative AI based on a recent study in which we compared\nboth approaches to assist users in a complex decision-making task with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we use generative AI to design tools that augment rather than replace\nhuman cognition? In this position paper, we review our own research on\nAI-assisted decision-making for lessons to learn. We observe that in both\nAI-assisted decision-making and generative AI, a popular approach is to suggest\nAI-generated end-to-end solutions to users, which users can then accept,\nreject, or edit. Alternatively, AI tools could offer more incremental support\nto help users solve tasks themselves, which we call process-oriented support.\nWe describe findings on the challenges of end-to-end solutions, and how\nprocess-oriented support can address them. We also discuss the applicability of\nthese findings to generative AI based on a recent study in which we compared\nboth approaches to assist users in a complex decision-making task with LLMs."
                },
                "authors": [
                    {
                        "name": "Zelun Tony Zhang"
                    },
                    {
                        "name": "Leon Reicherts"
                    }
                ],
                "author_detail": {
                    "name": "Leon Reicherts"
                },
                "author": "Leon Reicherts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03206v1",
                "updated": "2025-04-04T06:35:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    35,
                    2,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T06:35:02Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    35,
                    2,
                    4,
                    94,
                    0
                ],
                "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward"
                },
                "summary": "Effective conversational agents must be able to personalize their behavior to\nsuit a user's preferences, personality, and attributes, whether they are\nassisting with writing tasks or operating in domains like education or\nhealthcare. Current training methods like Reinforcement Learning from Human\nFeedback (RLHF) prioritize helpfulness and safety but fall short in fostering\ntruly empathetic, adaptive, and personalized interactions. Traditional\napproaches to personalization often rely on extensive user history, limiting\ntheir effectiveness for new or context-limited users. To overcome these\nlimitations, we propose to incorporate an intrinsic motivation to improve the\nconversational agents's model of the user as an additional reward alongside\nmulti-turn RLHF. This reward mechanism encourages the agent to actively elicit\nuser traits by optimizing conversations to increase the accuracy of its user\nmodel. Consequently, the policy agent can deliver more personalized\ninteractions through obtaining more information about the user. We applied our\nmethod both education and fitness settings, where LLMs teach concepts or\nrecommend personalized strategies based on users' hidden learning style or\nlifestyle attributes. Using LLM-simulated users, our approach outperformed a\nmulti-turn RLHF baseline in revealing information about the users' preferences,\nand adapting to them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective conversational agents must be able to personalize their behavior to\nsuit a user's preferences, personality, and attributes, whether they are\nassisting with writing tasks or operating in domains like education or\nhealthcare. Current training methods like Reinforcement Learning from Human\nFeedback (RLHF) prioritize helpfulness and safety but fall short in fostering\ntruly empathetic, adaptive, and personalized interactions. Traditional\napproaches to personalization often rely on extensive user history, limiting\ntheir effectiveness for new or context-limited users. To overcome these\nlimitations, we propose to incorporate an intrinsic motivation to improve the\nconversational agents's model of the user as an additional reward alongside\nmulti-turn RLHF. This reward mechanism encourages the agent to actively elicit\nuser traits by optimizing conversations to increase the accuracy of its user\nmodel. Consequently, the policy agent can deliver more personalized\ninteractions through obtaining more information about the user. We applied our\nmethod both education and fitness settings, where LLMs teach concepts or\nrecommend personalized strategies based on users' hidden learning style or\nlifestyle attributes. Using LLM-simulated users, our approach outperformed a\nmulti-turn RLHF baseline in revealing information about the users' preferences,\nand adapting to them."
                },
                "authors": [
                    {
                        "name": "Yanming Wan"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Marwa Abdulhai"
                    },
                    {
                        "name": "Lior Shani"
                    },
                    {
                        "name": "Natasha Jaques"
                    }
                ],
                "author_detail": {
                    "name": "Natasha Jaques"
                },
                "author": "Natasha Jaques",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00785v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00785v3",
                "updated": "2025-04-04T06:14:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    14,
                    36,
                    4,
                    94,
                    0
                ],
                "published": "2025-01-01T09:48:16Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    9,
                    48,
                    16,
                    2,
                    1,
                    0
                ],
                "title": "Natural Multimodal Fusion-Based Human-Robot Interaction: Application\n  With Voice and Deictic Posture via Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Multimodal Fusion-Based Human-Robot Interaction: Application\n  With Voice and Deictic Posture via Large Language Model"
                },
                "summary": "Translating human intent into robot commands is crucial for the future of\nservice robots in an aging society. Existing Human-Robot Interaction (HRI)\nsystems relying on gestures or verbal commands are impractical for the elderly\ndue to difficulties with complex syntax or sign language. To address the\nchallenge, this paper introduces a multi-modal interaction framework that\ncombines voice and deictic posture information to create a more natural HRI\nsystem. The visual cues are first processed by the object detection model to\ngain a global understanding of the environment, and then bounding boxes are\nestimated based on depth information. By using a large language model (LLM)\nwith voice-to-text commands and temporally aligned selected bounding boxes,\nrobot action sequences can be generated, while key control syntax constraints\nare applied to avoid potential LLM hallucination issues. The system is\nevaluated on real-world tasks with varying levels of complexity using a\nUniversal Robots UR3e manipulator. Our method demonstrates significantly better\nperformance in HRI in terms of accuracy and robustness. To benefit the research\ncommunity and the general public, we will make our code and design open-source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating human intent into robot commands is crucial for the future of\nservice robots in an aging society. Existing Human-Robot Interaction (HRI)\nsystems relying on gestures or verbal commands are impractical for the elderly\ndue to difficulties with complex syntax or sign language. To address the\nchallenge, this paper introduces a multi-modal interaction framework that\ncombines voice and deictic posture information to create a more natural HRI\nsystem. The visual cues are first processed by the object detection model to\ngain a global understanding of the environment, and then bounding boxes are\nestimated based on depth information. By using a large language model (LLM)\nwith voice-to-text commands and temporally aligned selected bounding boxes,\nrobot action sequences can be generated, while key control syntax constraints\nare applied to avoid potential LLM hallucination issues. The system is\nevaluated on real-world tasks with varying levels of complexity using a\nUniversal Robots UR3e manipulator. Our method demonstrates significantly better\nperformance in HRI in terms of accuracy and robustness. To benefit the research\ncommunity and the general public, we will make our code and design open-source."
                },
                "authors": [
                    {
                        "name": "Yuzhi Lai"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Youssef Nassar"
                    },
                    {
                        "name": "Mingyu Fan"
                    },
                    {
                        "name": "Atmaraaj Gopal"
                    },
                    {
                        "name": "Arihiro Yorita"
                    },
                    {
                        "name": "Naoyuki Kubota"
                    },
                    {
                        "name": "Matthias Rtsch"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Rtsch"
                },
                "author": "Matthias Rtsch",
                "arxiv_doi": "10.1109/MRA.2025.3543957",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/MRA.2025.3543957",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.00785v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00785v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication by IEEE Robotics & Automation Magazine",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15291v3",
                "updated": "2025-04-04T06:11:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    11,
                    55,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-19T07:10:51Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    7,
                    10,
                    51,
                    3,
                    354,
                    0
                ],
                "title": "A Large-Scale Simulation on Large Language Models for Decision-Making in\n  Political Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large-Scale Simulation on Large Language Models for Decision-Making in\n  Political Science"
                },
                "summary": "While LLMs have demonstrated remarkable capabilities in text generation and\nreasoning, their ability to simulate human decision-making -- particularly in\npolitical contexts -- remains an open question. However, modeling voter\nbehavior presents unique challenges due to limited voter-level data, evolving\npolitical landscapes, and the complexity of human reasoning. In this study, we\ndevelop a theory-driven, multi-step reasoning framework that integrates\ndemographic, temporal and ideological factors to simulate voter decision-making\nat scale. Using synthetic personas calibrated to real-world voter data, we\nconduct large-scale simulations of recent U.S. presidential elections. Our\nmethod significantly improves simulation accuracy while mitigating model\nbiases. We examine its robustness by comparing performance across different\nLLMs. We further investigate the challenges and constraints that arise from\nLLM-based political simulations. Our work provides both a scalable framework\nfor modeling political decision-making behavior and insights into the promise\nand limitations of using LLMs in political science research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs have demonstrated remarkable capabilities in text generation and\nreasoning, their ability to simulate human decision-making -- particularly in\npolitical contexts -- remains an open question. However, modeling voter\nbehavior presents unique challenges due to limited voter-level data, evolving\npolitical landscapes, and the complexity of human reasoning. In this study, we\ndevelop a theory-driven, multi-step reasoning framework that integrates\ndemographic, temporal and ideological factors to simulate voter decision-making\nat scale. Using synthetic personas calibrated to real-world voter data, we\nconduct large-scale simulations of recent U.S. presidential elections. Our\nmethod significantly improves simulation accuracy while mitigating model\nbiases. We examine its robustness by comparing performance across different\nLLMs. We further investigate the challenges and constraints that arise from\nLLM-based political simulations. Our work provides both a scalable framework\nfor modeling political decision-making behavior and insights into the promise\nand limitations of using LLMs in political science research."
                },
                "authors": [
                    {
                        "name": "Chenxiao Yu"
                    },
                    {
                        "name": "Jinyi Ye"
                    },
                    {
                        "name": "Yuangang Li"
                    },
                    {
                        "name": "Zhaotian Weng"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Emilio Ferrara"
                    },
                    {
                        "name": "Xiyang Hu"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2411.03321 This\n  version adds a new model to our experimental setup, modifies the paper's main\n  discussion, and updates the authorship list",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03197v1",
                "updated": "2025-04-04T06:03:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    3,
                    13,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T06:03:13Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    6,
                    3,
                    13,
                    4,
                    94,
                    0
                ],
                "title": "Explain with Visual Keypoints Like a Real Mentor! A Benchmark for\n  Multimodal Solution Explanation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explain with Visual Keypoints Like a Real Mentor! A Benchmark for\n  Multimodal Solution Explanation"
                },
                "summary": "With the rapid advancement of mathematical reasoning capabilities in large\nlanguage models (LLMs), AI systems are increasingly being adopted in\neducational settings to support students' comprehension of problem-solving\nprocesses. However, a critical component remains underexplored in current\nLLM-generated explanations: visual explanation. In real-world instructional\ncontexts, human tutors routinely employ visual aids-such as diagrams, markings,\nand highlights-to enhance conceptual clarity. To bridge this gap, we introduce\na novel task of visual solution explanation, which requires not only solving\nproblems but also generating explanations that incorporate newly introduced\nvisual elements essential for understanding (e.g., auxiliary lines,\nannotations, or geometric constructions). To evaluate model performance on this\ntask, we propose MathExplain, a multimodal benchmark consisting of 997 math\nproblems annotated with visual keypoints and corresponding explanatory text\nthat references those elements. Our empirical results show that while some\nclosed-source models demonstrate promising capabilities on visual\nsolution-explaining, current open-source general-purpose models perform\ninconsistently, particularly in identifying relevant visual components and\nproducing coherent keypoint-based explanations. We expect that visual\nsolution-explaining and the MathExplain dataset will catalyze further research\non multimodal LLMs in education and advance their deployment as effective,\nexplanation-oriented AI tutors. Code and data will be released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of mathematical reasoning capabilities in large\nlanguage models (LLMs), AI systems are increasingly being adopted in\neducational settings to support students' comprehension of problem-solving\nprocesses. However, a critical component remains underexplored in current\nLLM-generated explanations: visual explanation. In real-world instructional\ncontexts, human tutors routinely employ visual aids-such as diagrams, markings,\nand highlights-to enhance conceptual clarity. To bridge this gap, we introduce\na novel task of visual solution explanation, which requires not only solving\nproblems but also generating explanations that incorporate newly introduced\nvisual elements essential for understanding (e.g., auxiliary lines,\nannotations, or geometric constructions). To evaluate model performance on this\ntask, we propose MathExplain, a multimodal benchmark consisting of 997 math\nproblems annotated with visual keypoints and corresponding explanatory text\nthat references those elements. Our empirical results show that while some\nclosed-source models demonstrate promising capabilities on visual\nsolution-explaining, current open-source general-purpose models perform\ninconsistently, particularly in identifying relevant visual components and\nproducing coherent keypoint-based explanations. We expect that visual\nsolution-explaining and the MathExplain dataset will catalyze further research\non multimodal LLMs in education and advance their deployment as effective,\nexplanation-oriented AI tutors. Code and data will be released publicly."
                },
                "authors": [
                    {
                        "name": "Jaewoo Park"
                    },
                    {
                        "name": "Jungyang Park"
                    },
                    {
                        "name": "Dongju Jang"
                    },
                    {
                        "name": "Jiwan Chung"
                    },
                    {
                        "name": "Byungwoo Yoo"
                    },
                    {
                        "name": "Jaewoo Shin"
                    },
                    {
                        "name": "Seonjoon Park"
                    },
                    {
                        "name": "Taehyeong Kim"
                    },
                    {
                        "name": "Youngjae Yu"
                    }
                ],
                "author_detail": {
                    "name": "Youngjae Yu"
                },
                "author": "Youngjae Yu",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03544v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03544v2",
                "updated": "2025-04-04T05:56:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    56,
                    4,
                    4,
                    94,
                    0
                ],
                "published": "2025-01-07T05:39:21Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    5,
                    39,
                    21,
                    1,
                    7,
                    0
                ],
                "title": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for\n  Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for\n  Text-to-Image Models"
                },
                "summary": "Text-to-image (T2I) models have been shown to be vulnerable to misuse,\nparticularly in generating not-safe-for-work (NSFW) content, raising serious\nethical concerns. In this work, we present PromptGuard, a novel content\nmoderation technique that draws inspiration from the system prompt mechanism in\nlarge language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack\na direct interface for enforcing behavioral guidelines. Our key idea is to\noptimize a safety soft prompt that functions as an implicit system prompt\nwithin the T2I model's textual embedding space. This universal soft prompt (P*)\ndirectly moderates NSFW inputs, enabling safe yet realistic image generation\nwithout altering the inference efficiency or requiring proxy models. Extensive\nexperiments across three datasets demonstrate that PromptGuard effectively\nmitigates NSFW content generation while preserving high-quality benign outputs.\nPromptGuard achieves 7.8 times faster than prior content moderation methods,\nsurpassing eight state-of-the-art defenses with an optimal unsafe ratio down to\n5.84%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) models have been shown to be vulnerable to misuse,\nparticularly in generating not-safe-for-work (NSFW) content, raising serious\nethical concerns. In this work, we present PromptGuard, a novel content\nmoderation technique that draws inspiration from the system prompt mechanism in\nlarge language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack\na direct interface for enforcing behavioral guidelines. Our key idea is to\noptimize a safety soft prompt that functions as an implicit system prompt\nwithin the T2I model's textual embedding space. This universal soft prompt (P*)\ndirectly moderates NSFW inputs, enabling safe yet realistic image generation\nwithout altering the inference efficiency or requiring proxy models. Extensive\nexperiments across three datasets demonstrate that PromptGuard effectively\nmitigates NSFW content generation while preserving high-quality benign outputs.\nPromptGuard achieves 7.8 times faster than prior content moderation methods,\nsurpassing eight state-of-the-art defenses with an optimal unsafe ratio down to\n5.84%."
                },
                "authors": [
                    {
                        "name": "Lingzhi Yuan"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "16 pages, 8 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03544v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03544v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00967v2",
                "updated": "2025-04-04T05:49:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    49,
                    11,
                    4,
                    94,
                    0
                ],
                "published": "2025-03-02T17:16:32Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    17,
                    16,
                    32,
                    6,
                    61,
                    0
                ],
                "title": "How Do Teachers Create Pedagogical Chatbots?: Current Practices and\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do Teachers Create Pedagogical Chatbots?: Current Practices and\n  Challenges"
                },
                "summary": "AI chatbots have emerged as promising educational tools for personalized\nlearning experiences, with advances in large language models (LLMs) enabling\nteachers to create and customize these chatbots for their specific classroom\nneeds. However, there is a limited understanding of how teachers create\npedagogical chatbots and integrate them into their lessons. Through\nsemi-structured interviews with seven K-12 teachers, we examined their\npractices and challenges when designing, implementing, and deploying chatbots.\nOur findings revealed that teachers prioritize developing task-specific\nchatbots aligned with their lessons. Teachers engaged in various creation\npractices and had different challenges; novices in chatbot creation struggled\nmainly with initial design and technical implementation, while experienced\nteachers faced challenges with technical aspects and analyzing conversational\ndata. Based on these insights, we explore approaches to supporting teachers'\nchatbot development and opportunities for designing future chatbot creation\nsystems. This work provides foundational insights from teachers that can\nempower teacher-created chatbots, facilitating AI-augmented teaching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI chatbots have emerged as promising educational tools for personalized\nlearning experiences, with advances in large language models (LLMs) enabling\nteachers to create and customize these chatbots for their specific classroom\nneeds. However, there is a limited understanding of how teachers create\npedagogical chatbots and integrate them into their lessons. Through\nsemi-structured interviews with seven K-12 teachers, we examined their\npractices and challenges when designing, implementing, and deploying chatbots.\nOur findings revealed that teachers prioritize developing task-specific\nchatbots aligned with their lessons. Teachers engaged in various creation\npractices and had different challenges; novices in chatbot creation struggled\nmainly with initial design and technical implementation, while experienced\nteachers faced challenges with technical aspects and analyzing conversational\ndata. Based on these insights, we explore approaches to supporting teachers'\nchatbot development and opportunities for designing future chatbot creation\nsystems. This work provides foundational insights from teachers that can\nempower teacher-created chatbots, facilitating AI-augmented teaching."
                },
                "authors": [
                    {
                        "name": "Minju Yoo"
                    },
                    {
                        "name": "Hyoungwook Jin"
                    },
                    {
                        "name": "Juho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Juho Kim"
                },
                "author": "Juho Kim",
                "arxiv_comment": "CHI 2025 Workshop on Augmented Educators and AI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03185v1",
                "updated": "2025-04-04T05:26:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    26,
                    28,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T05:26:28Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    26,
                    28,
                    4,
                    94,
                    0
                ],
                "title": "Learning Natural Language Constraints for Safe Reinforcement Learning of\n  Language Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Natural Language Constraints for Safe Reinforcement Learning of\n  Language Agents"
                },
                "summary": "Generalizable alignment is a core challenge for deploying Large Language\nModels (LLMs) safely in real-world NLP applications. Current alignment methods,\nincluding Reinforcement Learning from Human Feedback (RLHF), often fail to\nguarantee constraint satisfaction outside their training distribution due to\ntheir reliance on implicit, post-hoc preferences. Inspired by a paradigm shift\nto first curate data before tuning, we introduce a new framework for safe\nlanguage alignment that learns natural language constraints from positive and\nnegative demonstrations as a primary step. From inferring both a task-specific\nreward function and latent constraint functions, our approach fosters\nadaptation to novel safety requirements and robust generalization under domain\nshifts and adversarial inputs. We formalize the framework within a Constrained\nMarkov Decision Process (CMDP) and validate it via a text-based navigation\nenvironment, demonstrating safe adaptation to changing danger zones. Our\nexperiments show fewer violations upon domain shift when following a safe\nnavigation path, and we achieve zero violations by applying learned constraints\nto a distilled BERT model as a fine-tuning technique. This work offers a\npromising path toward building safety-critical and more generalizable LLMs for\npractical NLP settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizable alignment is a core challenge for deploying Large Language\nModels (LLMs) safely in real-world NLP applications. Current alignment methods,\nincluding Reinforcement Learning from Human Feedback (RLHF), often fail to\nguarantee constraint satisfaction outside their training distribution due to\ntheir reliance on implicit, post-hoc preferences. Inspired by a paradigm shift\nto first curate data before tuning, we introduce a new framework for safe\nlanguage alignment that learns natural language constraints from positive and\nnegative demonstrations as a primary step. From inferring both a task-specific\nreward function and latent constraint functions, our approach fosters\nadaptation to novel safety requirements and robust generalization under domain\nshifts and adversarial inputs. We formalize the framework within a Constrained\nMarkov Decision Process (CMDP) and validate it via a text-based navigation\nenvironment, demonstrating safe adaptation to changing danger zones. Our\nexperiments show fewer violations upon domain shift when following a safe\nnavigation path, and we achieve zero violations by applying learned constraints\nto a distilled BERT model as a fine-tuning technique. This work offers a\npromising path toward building safety-critical and more generalizable LLMs for\npractical NLP settings."
                },
                "authors": [
                    {
                        "name": "Jaymari Chua"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.4; I.2.6; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03174v1",
                "updated": "2025-04-04T05:06:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    6,
                    12,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T05:06:12Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    6,
                    12,
                    4,
                    94,
                    0
                ],
                "title": "Multi-lingual Multi-turn Automated Red Teaming for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-lingual Multi-turn Automated Red Teaming for LLMs"
                },
                "summary": "Language Model Models (LLMs) have improved dramatically in the past few\nyears, increasing their adoption and the scope of their capabilities over time.\nA significant amount of work is dedicated to ``model alignment'', i.e.,\npreventing LLMs to generate unsafe responses when deployed into customer-facing\napplications. One popular method to evaluate safety risks is\n\\textit{red-teaming}, where agents attempt to bypass alignment by crafting\nelaborate prompts that trigger unsafe responses from a model. Standard\nhuman-driven red-teaming is costly, time-consuming and rarely covers all the\nrecent features (e.g., multi-lingual, multi-modal aspects), while proposed\nautomation methods only cover a small subset of LLMs capabilities (i.e.,\nEnglish or single-turn). We present Multi-lingual Multi-turn Automated Red\nTeaming (\\textbf{MM-ART}), a method to fully automate conversational,\nmulti-lingual red-teaming operations and quickly identify prompts leading to\nunsafe responses. Through extensive experiments on different languages, we show\nthe studied LLMs are on average 71\\% more vulnerable after a 5-turn\nconversation in English than after the initial turn. For conversations in\nnon-English languages, models display up to 195\\% more safety vulnerabilities\nthan the standard single-turn English approach, confirming the need for\nautomated red-teaming methods matching LLMs capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model Models (LLMs) have improved dramatically in the past few\nyears, increasing their adoption and the scope of their capabilities over time.\nA significant amount of work is dedicated to ``model alignment'', i.e.,\npreventing LLMs to generate unsafe responses when deployed into customer-facing\napplications. One popular method to evaluate safety risks is\n\\textit{red-teaming}, where agents attempt to bypass alignment by crafting\nelaborate prompts that trigger unsafe responses from a model. Standard\nhuman-driven red-teaming is costly, time-consuming and rarely covers all the\nrecent features (e.g., multi-lingual, multi-modal aspects), while proposed\nautomation methods only cover a small subset of LLMs capabilities (i.e.,\nEnglish or single-turn). We present Multi-lingual Multi-turn Automated Red\nTeaming (\\textbf{MM-ART}), a method to fully automate conversational,\nmulti-lingual red-teaming operations and quickly identify prompts leading to\nunsafe responses. Through extensive experiments on different languages, we show\nthe studied LLMs are on average 71\\% more vulnerable after a 5-turn\nconversation in English than after the initial turn. For conversations in\nnon-English languages, models display up to 195\\% more safety vulnerabilities\nthan the standard single-turn English approach, confirming the need for\nautomated red-teaming methods matching LLMs capabilities."
                },
                "authors": [
                    {
                        "name": "Abhishek Singhania"
                    },
                    {
                        "name": "Christophe Dupuy"
                    },
                    {
                        "name": "Shivam Mangale"
                    },
                    {
                        "name": "Amani Namboori"
                    }
                ],
                "author_detail": {
                    "name": "Amani Namboori"
                },
                "author": "Amani Namboori",
                "arxiv_comment": "Accepted at TrustNLP@NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03158v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03158v2",
                "updated": "2025-04-04T05:01:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    5,
                    1,
                    8,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-05T13:31:38Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    13,
                    31,
                    38,
                    2,
                    36,
                    0
                ],
                "title": "Strategizing with AI: Insights from a Beauty Contest Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategizing with AI: Insights from a Beauty Contest Experiment"
                },
                "summary": "A beauty contest is a wide class of games of guessing the most popular\nstrategy among other players. In particular, guessing a fraction of a mean of\nnumbers chosen by all players is a classic behavioral experiment designed to\ntest iterative reasoning patterns among various groups of people. The previous\nliterature reveals that the level of sophistication of the opponents is an\nimportant factor affecting the outcome of the game. Smarter decision makers\nchoose strategies that are closer to theoretical Nash equilibrium and\ndemonstrate faster convergence to equilibrium in iterated contests with\ninformation revelation. We replicate a series of classic experiments by running\nvirtual experiments with modern large language models (LLMs) who play against\nvarious groups of virtual players. We test how advanced the LLMs' behavior is\ncompared to the behavior of human players. We show that LLMs typically take\ninto account the opponents' level of sophistication and adapt by changing the\nstrategy. In various settings, most LLMs (with the exception of Llama) are more\nsophisticated and play lower numbers compared to human players. Our results\nsuggest that LLMs (except Llama) are rather successful in identifying the\nunderlying strategic environment and adopting the strategies to the changing\nset of parameters of the game in the same way that human players do. All LLMs\nstill fail to play dominant strategies in a two-player game. Our results\ncontribute to the discussion on the accuracy of modeling human economic agents\nby artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A beauty contest is a wide class of games of guessing the most popular\nstrategy among other players. In particular, guessing a fraction of a mean of\nnumbers chosen by all players is a classic behavioral experiment designed to\ntest iterative reasoning patterns among various groups of people. The previous\nliterature reveals that the level of sophistication of the opponents is an\nimportant factor affecting the outcome of the game. Smarter decision makers\nchoose strategies that are closer to theoretical Nash equilibrium and\ndemonstrate faster convergence to equilibrium in iterated contests with\ninformation revelation. We replicate a series of classic experiments by running\nvirtual experiments with modern large language models (LLMs) who play against\nvarious groups of virtual players. We test how advanced the LLMs' behavior is\ncompared to the behavior of human players. We show that LLMs typically take\ninto account the opponents' level of sophistication and adapt by changing the\nstrategy. In various settings, most LLMs (with the exception of Llama) are more\nsophisticated and play lower numbers compared to human players. Our results\nsuggest that LLMs (except Llama) are rather successful in identifying the\nunderlying strategic environment and adopting the strategies to the changing\nset of parameters of the game in the same way that human players do. All LLMs\nstill fail to play dominant strategies in a two-player game. Our results\ncontribute to the discussion on the accuracy of modeling human economic agents\nby artificial intelligence."
                },
                "authors": [
                    {
                        "name": "Iuliia Alekseenko"
                    },
                    {
                        "name": "Dmitry Dagaev"
                    },
                    {
                        "name": "Sofia Paklina"
                    },
                    {
                        "name": "Petr Parshakov"
                    }
                ],
                "author_detail": {
                    "name": "Petr Parshakov"
                },
                "author": "Petr Parshakov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03158v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03158v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03166v1",
                "updated": "2025-04-04T04:47:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    47,
                    54,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T04:47:54Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    47,
                    54,
                    4,
                    94,
                    0
                ],
                "title": "RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for\n  Universal Remote Sensing Image Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for\n  Universal Remote Sensing Image Interpretation"
                },
                "summary": "The rapid advancement of foundation models has revolutionized visual\nrepresentation learning in a self-supervised manner. However, their application\nin remote sensing (RS) remains constrained by a fundamental gap: existing\nmodels predominantly handle single or limited modalities, overlooking the\ninherently multi-modal nature of RS observations. Optical, synthetic aperture\nradar (SAR), and multi-spectral data offer complementary insights that\nsignificantly reduce the inherent ambiguity and uncertainty in single-source\nanalysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS\nfoundation model with 14.7 billion parameters, pre-trained on 400 million\nmulti-modal RS images from nine satellites. RingMoE incorporates three key\ninnovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture\ncomprising modal-specialized, collaborative, and shared experts, effectively\nmodeling intra-modal knowledge while capturing cross-modal dependencies to\nmitigate conflicts between modal representations; (2) Physics-informed\nself-supervised learning, explicitly embedding sensor-specific radiometric\ncharacteristics into the pre-training objectives; (3) Dynamic expert pruning,\nenabling adaptive model compression from 14.7B to 1B parameters while\nmaintaining performance, facilitating efficient deployment in Earth observation\napplications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e.,\nclassification, detection, segmentation, tracking, change detection, and depth\nestimation), RingMoE outperforms existing foundation models and sets new SOTAs,\ndemonstrating remarkable adaptability from single-modal to multi-modal\nscenarios. Beyond theoretical progress, it has been deployed and trialed in\nmultiple sectors, including emergency response, land management, marine\nsciences, and urban planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of foundation models has revolutionized visual\nrepresentation learning in a self-supervised manner. However, their application\nin remote sensing (RS) remains constrained by a fundamental gap: existing\nmodels predominantly handle single or limited modalities, overlooking the\ninherently multi-modal nature of RS observations. Optical, synthetic aperture\nradar (SAR), and multi-spectral data offer complementary insights that\nsignificantly reduce the inherent ambiguity and uncertainty in single-source\nanalysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS\nfoundation model with 14.7 billion parameters, pre-trained on 400 million\nmulti-modal RS images from nine satellites. RingMoE incorporates three key\ninnovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture\ncomprising modal-specialized, collaborative, and shared experts, effectively\nmodeling intra-modal knowledge while capturing cross-modal dependencies to\nmitigate conflicts between modal representations; (2) Physics-informed\nself-supervised learning, explicitly embedding sensor-specific radiometric\ncharacteristics into the pre-training objectives; (3) Dynamic expert pruning,\nenabling adaptive model compression from 14.7B to 1B parameters while\nmaintaining performance, facilitating efficient deployment in Earth observation\napplications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e.,\nclassification, detection, segmentation, tracking, change detection, and depth\nestimation), RingMoE outperforms existing foundation models and sets new SOTAs,\ndemonstrating remarkable adaptability from single-modal to multi-modal\nscenarios. Beyond theoretical progress, it has been deployed and trialed in\nmultiple sectors, including emergency response, land management, marine\nsciences, and urban planning."
                },
                "authors": [
                    {
                        "name": "Hanbo Bi"
                    },
                    {
                        "name": "Yingchao Feng"
                    },
                    {
                        "name": "Boyuan Tong"
                    },
                    {
                        "name": "Mengyu Wang"
                    },
                    {
                        "name": "Haichen Yu"
                    },
                    {
                        "name": "Yongqiang Mao"
                    },
                    {
                        "name": "Hao Chang"
                    },
                    {
                        "name": "Wenhui Diao"
                    },
                    {
                        "name": "Peijin Wang"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Hanyang Peng"
                    },
                    {
                        "name": "Yehong Zhang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Xian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xian Sun"
                },
                "author": "Xian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03165v1",
                "updated": "2025-04-04T04:43:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    43,
                    13,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T04:43:13Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    43,
                    13,
                    4,
                    94,
                    0
                ],
                "title": "Efficient Dynamic Clustering-Based Document Compression for\n  Retrieval-Augmented-Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Dynamic Clustering-Based Document Compression for\n  Retrieval-Augmented-Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge integration during large language model (LLM) inference in recent\nyears. However, current RAG implementations face challenges in effectively\naddressing noise, repetition and redundancy in retrieved content, primarily due\nto their limited ability to exploit fine-grained inter-document relationships.\nTo address these limitations, we propose an \\textbf{E}fficient \\textbf{D}ynamic\n\\textbf{C}lustering-based document \\textbf{C}ompression framework\n(\\textbf{EDC\\textsuperscript{2}-RAG}) that effectively utilizes latent\ninter-document relationships while simultaneously removing irrelevant\ninformation and redundant content. We validate our approach, built upon\nGPT-3.5, on widely used knowledge-QA and hallucination-detected datasets. The\nresults show that this method achieves consistent performance improvements\nacross various scenarios and experimental settings, demonstrating strong\nrobustness and applicability. Our code and datasets can be found at\nhttps://github.com/Tsinghua-dhy/EDC-2-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge integration during large language model (LLM) inference in recent\nyears. However, current RAG implementations face challenges in effectively\naddressing noise, repetition and redundancy in retrieved content, primarily due\nto their limited ability to exploit fine-grained inter-document relationships.\nTo address these limitations, we propose an \\textbf{E}fficient \\textbf{D}ynamic\n\\textbf{C}lustering-based document \\textbf{C}ompression framework\n(\\textbf{EDC\\textsuperscript{2}-RAG}) that effectively utilizes latent\ninter-document relationships while simultaneously removing irrelevant\ninformation and redundant content. We validate our approach, built upon\nGPT-3.5, on widely used knowledge-QA and hallucination-detected datasets. The\nresults show that this method achieves consistent performance improvements\nacross various scenarios and experimental settings, demonstrating strong\nrobustness and applicability. Our code and datasets can be found at\nhttps://github.com/Tsinghua-dhy/EDC-2-RAG."
                },
                "authors": [
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Kaiming Liu"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Xuanyu Lei"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03160v1",
                "updated": "2025-04-04T04:41:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    41,
                    28,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T04:41:28Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    41,
                    28,
                    4,
                    94,
                    0
                ],
                "title": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in\n  Real-world Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepResearcher: Scaling Deep Research via Reinforcement Learning in\n  Real-world Environments"
                },
                "summary": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) equipped with web search capabilities have\ndemonstrated impressive potential for deep research tasks. However, current\napproaches predominantly rely on either manually engineered prompts (prompt\nengineering-based) with brittle performance or reinforcement learning within\ncontrolled Retrieval-Augmented Generation (RAG) environments (RAG-based) that\nfail to capture the complexities of real-world interaction. In this paper, we\nintroduce DeepResearcher, the first comprehensive framework for end-to-end\ntraining of LLM-based deep research agents through scaling reinforcement\nlearning (RL) in real-world environments with authentic web search\ninteractions. Unlike RAG-based approaches that assume all necessary information\nexists within a fixed corpus, our method trains agents to navigate the noisy,\nunstructured, and dynamic nature of the open web. We implement a specialized\nmulti-agent architecture where browsing agents extract relevant information\nfrom various webpage structures and overcoming significant technical\nchallenges. Extensive experiments on open-domain research tasks demonstrate\nthat DeepResearcher achieves substantial improvements of up to 28.9 points over\nprompt engineering-based baselines and up to 7.2 points over RAG-based RL\nagents. Our qualitative analysis reveals emergent cognitive behaviors from\nend-to-end RL training, including the ability to formulate plans,\ncross-validate information from multiple sources, engage in self-reflection to\nredirect research, and maintain honesty when unable to find definitive answers.\nOur results highlight that end-to-end training in real-world web environments\nis not merely an implementation detail but a fundamental requirement for\ndeveloping robust research capabilities aligned with real-world applications.\nWe release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zheng"
                    },
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Xiangkun Hu"
                    },
                    {
                        "name": "Xiaojie Cai"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Pengrui Lu"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20279v2",
                "updated": "2025-04-04T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    36,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2025-03-26T07:08:15Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    7,
                    8,
                    15,
                    2,
                    85,
                    0
                ],
                "title": "sudo rm -rf agentic_security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "sudo rm -rf agentic_security"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed as computer-use\nagents, autonomously performing tasks within real desktop or web environments.\nWhile this evolution greatly expands practical use cases for humans, it also\ncreates serious security exposures. We present SUDO (Screen-based Universal\nDetox2Tox Offense), a novel attack framework that systematically bypasses\nrefusal trained safeguards in commercial computer-use agents, such as Claude\nComputer Use. The core mechanism, Detox2Tox, transforms harmful requests (that\nagents initially reject) into seemingly benign requests via detoxification,\nsecures detailed instructions from advanced vision language models (VLMs), and\nthen reintroduces malicious content via toxification just before execution.\nUnlike conventional jailbreaks, SUDO iteratively refines its attacks based on a\nbuilt-in refusal feedback, making it increasingly effective against robust\npolicy filters. In extensive tests spanning 50 real-world tasks and multiple\nstate-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with\nno refinement), and up to 41% (by its iterative refinement) in Claude Computer\nUse. By revealing these vulnerabilities and demonstrating the ease with which\nthey can be exploited in real-world computing environments, this paper\nhighlights an immediate need for robust, context-aware safeguards. WARNING:\nThis paper includes harmful or offensive model outputs Our code is available\nat: https://github.com/AIM-Intelligence/SUDO.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed as computer-use\nagents, autonomously performing tasks within real desktop or web environments.\nWhile this evolution greatly expands practical use cases for humans, it also\ncreates serious security exposures. We present SUDO (Screen-based Universal\nDetox2Tox Offense), a novel attack framework that systematically bypasses\nrefusal trained safeguards in commercial computer-use agents, such as Claude\nComputer Use. The core mechanism, Detox2Tox, transforms harmful requests (that\nagents initially reject) into seemingly benign requests via detoxification,\nsecures detailed instructions from advanced vision language models (VLMs), and\nthen reintroduces malicious content via toxification just before execution.\nUnlike conventional jailbreaks, SUDO iteratively refines its attacks based on a\nbuilt-in refusal feedback, making it increasingly effective against robust\npolicy filters. In extensive tests spanning 50 real-world tasks and multiple\nstate-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with\nno refinement), and up to 41% (by its iterative refinement) in Claude Computer\nUse. By revealing these vulnerabilities and demonstrating the ease with which\nthey can be exploited in real-world computing environments, this paper\nhighlights an immediate need for robust, context-aware safeguards. WARNING:\nThis paper includes harmful or offensive model outputs Our code is available\nat: https://github.com/AIM-Intelligence/SUDO.git"
                },
                "authors": [
                    {
                        "name": "Sejin Lee"
                    },
                    {
                        "name": "Jian Kim"
                    },
                    {
                        "name": "Haon Park"
                    },
                    {
                        "name": "Ashkan Yousefpour"
                    },
                    {
                        "name": "Sangyoon Yu"
                    },
                    {
                        "name": "Min Song"
                    }
                ],
                "author_detail": {
                    "name": "Min Song"
                },
                "author": "Min Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00407v2",
                "updated": "2025-04-04T04:28:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    28,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-01T04:08:37Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    4,
                    8,
                    37,
                    1,
                    91,
                    0
                ],
                "title": "AMP4EC: Adaptive Model Partitioning Framework for Efficient Deep\n  Learning Inference in Edge Computing Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMP4EC: Adaptive Model Partitioning Framework for Efficient Deep\n  Learning Inference in Edge Computing Environments"
                },
                "summary": "Edge computing facilitates deep learning in resource-constrained\nenvironments, but challenges such as resource heterogeneity and dynamic\nconstraints persist. This paper introduces AMP4EC, an Adaptive Model\nPartitioning framework designed to optimize deep learning inference in edge\nenvironments through real-time resource monitoring, dynamic model partitioning,\nand adaptive task scheduling. AMP4EC features a resource-aware model\npartitioner that splits deep learning models based on device capabilities, a\ntask scheduler that ensures efficient load balancing using a weighted scoring\nmechanism, and a Docker-based deployment environment for validation.\nExperimental results show up to a 78% reduction in latency and a 414%\nimprovement in throughput compared to baseline methods. The framework achieves\nconsistent performance with low scheduling overhead across varying resource\nprofiles, demonstrating adaptability in high-resource (1 CPU, 1GB RAM) and\nlow-resource (0.4 CPU, 512MB RAM) scenarios. These results highlight AMP4EC's\nscalability, efficiency, and robustness for real-world edge deployments,\naddressing the critical need for efficient distributed inference in dynamic,\nresource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge computing facilitates deep learning in resource-constrained\nenvironments, but challenges such as resource heterogeneity and dynamic\nconstraints persist. This paper introduces AMP4EC, an Adaptive Model\nPartitioning framework designed to optimize deep learning inference in edge\nenvironments through real-time resource monitoring, dynamic model partitioning,\nand adaptive task scheduling. AMP4EC features a resource-aware model\npartitioner that splits deep learning models based on device capabilities, a\ntask scheduler that ensures efficient load balancing using a weighted scoring\nmechanism, and a Docker-based deployment environment for validation.\nExperimental results show up to a 78% reduction in latency and a 414%\nimprovement in throughput compared to baseline methods. The framework achieves\nconsistent performance with low scheduling overhead across varying resource\nprofiles, demonstrating adaptability in high-resource (1 CPU, 1GB RAM) and\nlow-resource (0.4 CPU, 512MB RAM) scenarios. These results highlight AMP4EC's\nscalability, efficiency, and robustness for real-world edge deployments,\naddressing the critical need for efficient distributed inference in dynamic,\nresource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Guilin Zhang"
                    },
                    {
                        "name": "Wulan Guo"
                    },
                    {
                        "name": "Ziqi Tan"
                    },
                    {
                        "name": "Hailong Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hailong Jiang"
                },
                "author": "Hailong Jiang",
                "arxiv_comment": "8 pages, accepted for oral presentation at FMEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.6; C.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03155v1",
                "updated": "2025-04-04T04:25:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    25,
                    14,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T04:25:14Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    25,
                    14,
                    4,
                    94,
                    0
                ],
                "title": "Synthesizing Optimal Object Selection Predicates for Image Editing using\n  Lattices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Optimal Object Selection Predicates for Image Editing using\n  Lattices"
                },
                "summary": "Image editing is a common task across a wide range of domains, from personal\nuse to professional applications. Despite advances in computer vision, current\ntools still demand significant manual effort for editing tasks that require\nrepetitive operations on images with many objects. In this paper, we present a\nnovel approach to automating the image editing process using program synthesis.\nWe propose a new algorithm based on lattice structures to automatically\nsynthesize object selection predicates for image editing from positive and\nnegative examples. By leveraging the algebraic properties of lattices, our\nalgorithm efficiently synthesizes an optimal object selection predicate among\nmultiple correct solutions. We have implemented our technique and evaluated it\non 100 tasks over 20 images. The evaluation result demonstrates our tool is\neffective and efficient, which outperforms state-of-the-art synthesizers and\nLLM-based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image editing is a common task across a wide range of domains, from personal\nuse to professional applications. Despite advances in computer vision, current\ntools still demand significant manual effort for editing tasks that require\nrepetitive operations on images with many objects. In this paper, we present a\nnovel approach to automating the image editing process using program synthesis.\nWe propose a new algorithm based on lattice structures to automatically\nsynthesize object selection predicates for image editing from positive and\nnegative examples. By leveraging the algebraic properties of lattices, our\nalgorithm efficiently synthesizes an optimal object selection predicate among\nmultiple correct solutions. We have implemented our technique and evaluated it\non 100 tasks over 20 images. The evaluation result demonstrates our tool is\neffective and efficient, which outperforms state-of-the-art synthesizers and\nLLM-based approaches."
                },
                "authors": [
                    {
                        "name": "Yang He"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Yuepeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuepeng Wang"
                },
                "author": "Yuepeng Wang",
                "arxiv_comment": "29 pages, 10 tables, 9 figures, PLDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03154v1",
                "updated": "2025-04-04T04:24:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    24,
                    29,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T04:24:29Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    24,
                    29,
                    4,
                    94,
                    0
                ],
                "title": "TokenFLEX: Unified VLM Training for Flexible Visual Tokens Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenFLEX: Unified VLM Training for Flexible Visual Tokens Inference"
                },
                "summary": "Conventional Vision-Language Models(VLMs) typically utilize a fixed number of\nvision tokens, regardless of task complexity. This one-size-fits-all strategy\nintroduces notable inefficiencies: using excessive tokens leads to unnecessary\ncomputational overhead in simpler tasks, whereas insufficient tokens compromise\nfine-grained visual comprehension in more complex contexts. To overcome these\nlimitations, we present TokenFLEX, an innovative and adaptable vision-language\nframework that encodes images into a variable number of tokens for efficient\nintegration with a Large Language Model (LLM). Our approach is underpinned by\ntwo pivotal innovations. Firstly, we present a novel training paradigm that\nenhances performance across varying numbers of vision tokens by stochastically\nmodulating token counts during training. Secondly, we design a lightweight\nvision token projector incorporating an adaptive pooling layer and SwiGLU,\nallowing for flexible downsampling of vision tokens and adaptive selection of\nfeatures tailored to specific token counts. Comprehensive experiments reveal\nthat TokenFLEX consistently outperforms its fixed-token counterparts, achieving\nnotable performance gains across various token counts enhancements of 1.6%,\n1.0%, and 0.4% with 64, 144, and 256 tokens, respectively averaged over eight\nvision-language benchmarks. These results underscore TokenFLEX's remarkable\nflexibility while maintaining high-performance vision-language understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional Vision-Language Models(VLMs) typically utilize a fixed number of\nvision tokens, regardless of task complexity. This one-size-fits-all strategy\nintroduces notable inefficiencies: using excessive tokens leads to unnecessary\ncomputational overhead in simpler tasks, whereas insufficient tokens compromise\nfine-grained visual comprehension in more complex contexts. To overcome these\nlimitations, we present TokenFLEX, an innovative and adaptable vision-language\nframework that encodes images into a variable number of tokens for efficient\nintegration with a Large Language Model (LLM). Our approach is underpinned by\ntwo pivotal innovations. Firstly, we present a novel training paradigm that\nenhances performance across varying numbers of vision tokens by stochastically\nmodulating token counts during training. Secondly, we design a lightweight\nvision token projector incorporating an adaptive pooling layer and SwiGLU,\nallowing for flexible downsampling of vision tokens and adaptive selection of\nfeatures tailored to specific token counts. Comprehensive experiments reveal\nthat TokenFLEX consistently outperforms its fixed-token counterparts, achieving\nnotable performance gains across various token counts enhancements of 1.6%,\n1.0%, and 0.4% with 64, 144, and 256 tokens, respectively averaged over eight\nvision-language benchmarks. These results underscore TokenFLEX's remarkable\nflexibility while maintaining high-performance vision-language understanding."
                },
                "authors": [
                    {
                        "name": "Junshan Hu"
                    },
                    {
                        "name": "Jialiang Mao"
                    },
                    {
                        "name": "Zhikang Liu"
                    },
                    {
                        "name": "Zhongpu Xia"
                    },
                    {
                        "name": "Peng Jia"
                    },
                    {
                        "name": "Xianpeng Lang"
                    }
                ],
                "author_detail": {
                    "name": "Xianpeng Lang"
                },
                "author": "Xianpeng Lang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03151v1",
                "updated": "2025-04-04T04:04:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    4,
                    56,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T04:04:56Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    4,
                    56,
                    4,
                    94,
                    0
                ],
                "title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning\n  (v1)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning\n  (v1)"
                },
                "summary": "Reasoning is central to human intelligence, enabling structured\nproblem-solving across diverse tasks. Recent advances in large language models\n(LLMs) have greatly enhanced their reasoning abilities in arithmetic,\ncommonsense, and symbolic domains. However, effectively extending these\ncapabilities into multimodal contexts-where models must integrate both visual\nand textual inputs-continues to be a significant challenge. Multimodal\nreasoning introduces complexities, such as handling conflicting information\nacross modalities, which require models to adopt advanced interpretative\nstrategies. Addressing these challenges involves not only sophisticated\nalgorithms but also robust methodologies for evaluating reasoning accuracy and\ncoherence. This paper offers a concise yet insightful overview of reasoning\ntechniques in both textual and multimodal LLMs. Through a thorough and\nup-to-date comparison, we clearly formulate core reasoning challenges and\nopportunities, highlighting practical methods for post-training optimization\nand test-time inference. Our work provides valuable insights and guidance,\nbridging theoretical frameworks and practical implementations, and sets clear\ndirections for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is central to human intelligence, enabling structured\nproblem-solving across diverse tasks. Recent advances in large language models\n(LLMs) have greatly enhanced their reasoning abilities in arithmetic,\ncommonsense, and symbolic domains. However, effectively extending these\ncapabilities into multimodal contexts-where models must integrate both visual\nand textual inputs-continues to be a significant challenge. Multimodal\nreasoning introduces complexities, such as handling conflicting information\nacross modalities, which require models to adopt advanced interpretative\nstrategies. Addressing these challenges involves not only sophisticated\nalgorithms but also robust methodologies for evaluating reasoning accuracy and\ncoherence. This paper offers a concise yet insightful overview of reasoning\ntechniques in both textual and multimodal LLMs. Through a thorough and\nup-to-date comparison, we clearly formulate core reasoning challenges and\nopportunities, highlighting practical methods for post-training optimization\nand test-time inference. Our work provides valuable insights and guidance,\nbridging theoretical frameworks and practical implementations, and sets clear\ndirections for future research."
                },
                "authors": [
                    {
                        "name": "Jing Bi"
                    },
                    {
                        "name": "Susan Liang"
                    },
                    {
                        "name": "Xiaofei Zhou"
                    },
                    {
                        "name": "Pinxin Liu"
                    },
                    {
                        "name": "Junjia Guo"
                    },
                    {
                        "name": "Yunlong Tang"
                    },
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Jinxi He"
                    },
                    {
                        "name": "Jiarui Wu"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Daoan Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lianggong Bruce Wen"
                    },
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03147v1",
                "updated": "2025-04-04T03:56:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    56,
                    26,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T03:56:26Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    56,
                    26,
                    4,
                    94,
                    0
                ],
                "title": "A Human Digital Twin Architecture for Knowledge-based Interactions and\n  Context-Aware Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Human Digital Twin Architecture for Knowledge-based Interactions and\n  Context-Aware Conversations"
                },
                "summary": "Recent developments in Artificial Intelligence (AI) and Machine Learning (ML)\nare creating new opportunities for Human-Autonomy Teaming (HAT) in tasks,\nmissions, and continuous coordinated activities. A major challenge is enabling\nhumans to maintain awareness and control over autonomous assets, while also\nbuilding trust and supporting shared contextual understanding. To address this,\nwe present a real-time Human Digital Twin (HDT) architecture that integrates\nLarge Language Models (LLMs) for knowledge reporting, answering, and\nrecommendation, embodied in a visual interface.\n  The system applies a metacognitive approach to enable personalized,\ncontext-aware responses aligned with the human teammate's expectations. The HDT\nacts as a visually and behaviorally realistic team member, integrated\nthroughout the mission lifecycle, from training to deployment to after-action\nreview. Our architecture includes speech recognition, context processing,\nAI-driven dialogue, emotion modeling, lip-syncing, and multimodal feedback. We\ndescribe the system design, performance metrics, and future development\ndirections for more adaptive and realistic HAT systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in Artificial Intelligence (AI) and Machine Learning (ML)\nare creating new opportunities for Human-Autonomy Teaming (HAT) in tasks,\nmissions, and continuous coordinated activities. A major challenge is enabling\nhumans to maintain awareness and control over autonomous assets, while also\nbuilding trust and supporting shared contextual understanding. To address this,\nwe present a real-time Human Digital Twin (HDT) architecture that integrates\nLarge Language Models (LLMs) for knowledge reporting, answering, and\nrecommendation, embodied in a visual interface.\n  The system applies a metacognitive approach to enable personalized,\ncontext-aware responses aligned with the human teammate's expectations. The HDT\nacts as a visually and behaviorally realistic team member, integrated\nthroughout the mission lifecycle, from training to deployment to after-action\nreview. Our architecture includes speech recognition, context processing,\nAI-driven dialogue, emotion modeling, lip-syncing, and multimodal feedback. We\ndescribe the system design, performance metrics, and future development\ndirections for more adaptive and realistic HAT systems."
                },
                "authors": [
                    {
                        "name": "Abdul Mannan Mohammed"
                    },
                    {
                        "name": "Azhar Ali Mohammad"
                    },
                    {
                        "name": "Jason A. Ortiz"
                    },
                    {
                        "name": "Carsten Neumann"
                    },
                    {
                        "name": "Grace Bochenek"
                    },
                    {
                        "name": "Dirk Reiners"
                    },
                    {
                        "name": "Carolina Cruz-Neira"
                    }
                ],
                "author_detail": {
                    "name": "Carolina Cruz-Neira"
                },
                "author": "Carolina Cruz-Neira",
                "arxiv_doi": "10.17605/OSF.IO/KEH9T",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.17605/OSF.IO/KEH9T",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.03147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Presented at: 2024 Interservice/Industry Training, Simulation, and\n  Education Conference (I/ITSEC), Paper No. 24366, 10 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.08870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.08870v3",
                "updated": "2025-04-04T03:46:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    46,
                    28,
                    4,
                    94,
                    0
                ],
                "published": "2023-11-15T11:11:25Z",
                "published_parsed": [
                    2023,
                    11,
                    15,
                    11,
                    11,
                    25,
                    2,
                    319,
                    0
                ],
                "title": "One-Shot Heterogeneous Federated Learning with Local Model-Guided\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Shot Heterogeneous Federated Learning with Local Model-Guided\n  Diffusion Models"
                },
                "summary": "In recent years, One-shot Federated Learning methods based on Diffusion\nModels have garnered increasing attention due to their remarkable performance.\nHowever, most of these methods require the deployment of foundation models on\nclient devices, which significantly raises the computational requirements and\nreduces their adaptability to heterogeneous client models compared to\ntraditional FL methods. In this paper, we propose FedLMG, a heterogeneous\none-shot Federated learning method with Local Model-Guided diffusion models.\nBriefly speaking, in FedLMG, clients do not need access to any foundation\nmodels but only train and upload their local models, which is consistent with\ntraditional FL methods. On the clients, we employ classification loss and BN\nloss to capture the broad category features and detailed contextual features of\nthe client distributions. On the server, based on the uploaded client models,\nwe utilize backpropagation to guide the server's DM in generating synthetic\ndatasets that comply with the client distributions, which are then used to\ntrain the aggregated model. By using the locally trained client models as a\nmedium to transfer client knowledge, our method significantly reduces the\ncomputational requirements on client devices and effectively adapts to\nscenarios with heterogeneous clients. Extensive quantitation and visualization\nexperiments on three large-scale real-world datasets, along with theoretical\nanalysis, demonstrate that the synthetic datasets generated by FedLMG exhibit\ncomparable quality and diversity to the client datasets, which leads to an\naggregated model that outperforms all compared methods and even the performance\nceiling, further elucidating the significant potential of utilizing DMs in FL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, One-shot Federated Learning methods based on Diffusion\nModels have garnered increasing attention due to their remarkable performance.\nHowever, most of these methods require the deployment of foundation models on\nclient devices, which significantly raises the computational requirements and\nreduces their adaptability to heterogeneous client models compared to\ntraditional FL methods. In this paper, we propose FedLMG, a heterogeneous\none-shot Federated learning method with Local Model-Guided diffusion models.\nBriefly speaking, in FedLMG, clients do not need access to any foundation\nmodels but only train and upload their local models, which is consistent with\ntraditional FL methods. On the clients, we employ classification loss and BN\nloss to capture the broad category features and detailed contextual features of\nthe client distributions. On the server, based on the uploaded client models,\nwe utilize backpropagation to guide the server's DM in generating synthetic\ndatasets that comply with the client distributions, which are then used to\ntrain the aggregated model. By using the locally trained client models as a\nmedium to transfer client knowledge, our method significantly reduces the\ncomputational requirements on client devices and effectively adapts to\nscenarios with heterogeneous clients. Extensive quantitation and visualization\nexperiments on three large-scale real-world datasets, along with theoretical\nanalysis, demonstrate that the synthetic datasets generated by FedLMG exhibit\ncomparable quality and diversity to the client datasets, which leads to an\naggregated model that outperforms all compared methods and even the performance\nceiling, further elucidating the significant potential of utilizing DMs in FL."
                },
                "authors": [
                    {
                        "name": "Mingzhao Yang"
                    },
                    {
                        "name": "Shangchao Su"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Xiangyang Xue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Xue"
                },
                "author": "Xiangyang Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.08870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.08870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03141v1",
                "updated": "2025-04-04T03:34:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    34,
                    5,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T03:34:05Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    34,
                    5,
                    4,
                    94,
                    0
                ],
                "title": "See-Through Face Display for DHH People: Enhancing Gaze Awareness in\n  Remote Sign Language Conversations with Camera-Behind Displays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "See-Through Face Display for DHH People: Enhancing Gaze Awareness in\n  Remote Sign Language Conversations with Camera-Behind Displays"
                },
                "summary": "This paper presents a sign language conversation system based on the\nSee-Through Face Display to address the challenge of maintaining eye contact in\nremote sign language interactions. A camera positioned behind a transparent\ndisplay allows users to look at the face of their conversation partner while\nappearing to maintain direct eye contact. Unlike conventional methods that rely\non software-based gaze correction or large-scale half-mirror setups, this\ndesign reduces visual distortions and simplifies installation. We implemented\nand evaluated a videoconferencing system that integrates See-Through Face\nDisplay, comparing it to traditional videoconferencing methods. We explore its\npotential applications for Deaf and Hard of Hearing (DHH), including\nmulti-party sign language conversations, corpus collection, remote\ninterpretation, and AI-driven sign language avatars. Collaboration with DHH\ncommunities will be key to refining the system for real-world use and ensuring\nits practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a sign language conversation system based on the\nSee-Through Face Display to address the challenge of maintaining eye contact in\nremote sign language interactions. A camera positioned behind a transparent\ndisplay allows users to look at the face of their conversation partner while\nappearing to maintain direct eye contact. Unlike conventional methods that rely\non software-based gaze correction or large-scale half-mirror setups, this\ndesign reduces visual distortions and simplifies installation. We implemented\nand evaluated a videoconferencing system that integrates See-Through Face\nDisplay, comparing it to traditional videoconferencing methods. We explore its\npotential applications for Deaf and Hard of Hearing (DHH), including\nmulti-party sign language conversations, corpus collection, remote\ninterpretation, and AI-driven sign language avatars. Collaboration with DHH\ncommunities will be key to refining the system for real-world use and ensuring\nits practical deployment."
                },
                "authors": [
                    {
                        "name": "Kazuya Izumi"
                    },
                    {
                        "name": "Akihisa Shitara"
                    },
                    {
                        "name": "Yoichi Ochiai"
                    }
                ],
                "author_detail": {
                    "name": "Yoichi Ochiai"
                },
                "author": "Yoichi Ochiai",
                "arxiv_doi": "10.1145/3715669.3726848",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715669.3726848",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.03141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09921v2",
                "updated": "2025-04-04T03:20:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    20,
                    3,
                    4,
                    94,
                    0
                ],
                "published": "2024-11-15T03:45:09Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    3,
                    45,
                    9,
                    4,
                    320,
                    0
                ],
                "title": "Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at\n  Pixel Level",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at\n  Pixel Level"
                },
                "summary": "In this paper, we introduce Motion-Grounded Video Reasoning, a new motion\nunderstanding task that requires generating visual answers (video segmentation\nmasks) according to the input question, and hence needs implicit spatiotemporal\nreasoning and grounding. This task extends existing spatiotemporal grounding\nwork focusing on explicit action/motion grounding, to a more general format by\nenabling implicit reasoning via questions. To facilitate the development of the\nnew task, we collect a large-scale dataset called GROUNDMORE, which comprises\n1,715 video clips, 249K object masks that are deliberately designed with 4\nquestion types (Causal, Sequential, Counterfactual, and Descriptive) for\nbenchmarking deep and comprehensive motion reasoning abilities. GROUNDMORE\nuniquely requires models to generate visual answers, providing a more concrete\nand visually interpretable response than plain texts. It evaluates models on\nboth spatiotemporal grounding and reasoning, fostering to address complex\nchallenges in motion-related video reasoning, temporal perception, and\npixel-level understanding. Furthermore, we introduce a novel baseline model\nnamed Motion-Grounded Video Reasoning Assistant (MORA). MORA incorporates the\nmultimodal reasoning ability from the Multimodal LLM, the pixel-level\nperception capability from the grounding model (SAM), and the temporal\nperception ability from a lightweight localization head. MORA achieves\nrespectable performance on GROUNDMORE outperforming the best existing visual\ngrounding baseline model by an average of 21.5% relatively. We hope this novel\nand challenging task will pave the way for future advancements in robust and\ngeneral motion understanding via video reasoning segmentation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Motion-Grounded Video Reasoning, a new motion\nunderstanding task that requires generating visual answers (video segmentation\nmasks) according to the input question, and hence needs implicit spatiotemporal\nreasoning and grounding. This task extends existing spatiotemporal grounding\nwork focusing on explicit action/motion grounding, to a more general format by\nenabling implicit reasoning via questions. To facilitate the development of the\nnew task, we collect a large-scale dataset called GROUNDMORE, which comprises\n1,715 video clips, 249K object masks that are deliberately designed with 4\nquestion types (Causal, Sequential, Counterfactual, and Descriptive) for\nbenchmarking deep and comprehensive motion reasoning abilities. GROUNDMORE\nuniquely requires models to generate visual answers, providing a more concrete\nand visually interpretable response than plain texts. It evaluates models on\nboth spatiotemporal grounding and reasoning, fostering to address complex\nchallenges in motion-related video reasoning, temporal perception, and\npixel-level understanding. Furthermore, we introduce a novel baseline model\nnamed Motion-Grounded Video Reasoning Assistant (MORA). MORA incorporates the\nmultimodal reasoning ability from the Multimodal LLM, the pixel-level\nperception capability from the grounding model (SAM), and the temporal\nperception ability from a lightweight localization head. MORA achieves\nrespectable performance on GROUNDMORE outperforming the best existing visual\ngrounding baseline model by an average of 21.5% relatively. We hope this novel\nand challenging task will pave the way for future advancements in robust and\ngeneral motion understanding via video reasoning segmentation"
                },
                "authors": [
                    {
                        "name": "Andong Deng"
                    },
                    {
                        "name": "Tongjia Chen"
                    },
                    {
                        "name": "Shoubin Yu"
                    },
                    {
                        "name": "Taojiannan Yang"
                    },
                    {
                        "name": "Lincoln Spencer"
                    },
                    {
                        "name": "Yapeng Tian"
                    },
                    {
                        "name": "Ajmal Saeed Mian"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Chen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chen Chen"
                },
                "author": "Chen Chen",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03137v1",
                "updated": "2025-04-04T03:03:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    3,
                    47,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T03:03:47Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    3,
                    47,
                    4,
                    94,
                    0
                ],
                "title": "LightPROF: A Lightweight Reasoning Framework for Large Language Model on\n  Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightPROF: A Lightweight Reasoning Framework for Large Language Model on\n  Knowledge Graph"
                },
                "summary": "Large Language Models (LLMs) have impressive capabilities in text\nunderstanding and zero-shot reasoning. However, delays in knowledge updates may\ncause them to reason incorrectly or produce harmful results. Knowledge Graphs\n(KGs) provide rich and reliable contextual information for the reasoning\nprocess of LLMs by structurally organizing and connecting a wide range of\nentities and relations. Existing KG-based LLM reasoning methods only inject\nKGs' knowledge into prompts in a textual form, ignoring its structural\ninformation. Moreover, they mostly rely on close-source models or open-source\nmodels with large parameters, which poses challenges to high resource\nconsumption. To address this, we propose a novel Lightweight and efficient\nPrompt learning-ReasOning Framework for KGQA (LightPROF), which leverages the\nfull potential of LLMs to tackle complex reasoning tasks in a\nparameter-efficient manner. Specifically, LightPROF follows a\n\"Retrieve-Embed-Reason process\", first accurately, and stably retrieving the\ncorresponding reasoning graph from the KG through retrieval module. Next,\nthrough a Transformer-based Knowledge Adapter, it finely extracts and\nintegrates factual and structural information from the KG, then maps this\ninformation to the LLM's token embedding space, creating an LLM-friendly prompt\nto be used by the LLM for the final reasoning. Additionally, LightPROF only\nrequires training Knowledge Adapter and can be compatible with any open-source\nLLM. Extensive experiments on two public KGQA benchmarks demonstrate that\nLightPROF achieves superior performance with small-scale LLMs. Furthermore,\nLightPROF shows significant advantages in terms of input token count and\nreasoning time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have impressive capabilities in text\nunderstanding and zero-shot reasoning. However, delays in knowledge updates may\ncause them to reason incorrectly or produce harmful results. Knowledge Graphs\n(KGs) provide rich and reliable contextual information for the reasoning\nprocess of LLMs by structurally organizing and connecting a wide range of\nentities and relations. Existing KG-based LLM reasoning methods only inject\nKGs' knowledge into prompts in a textual form, ignoring its structural\ninformation. Moreover, they mostly rely on close-source models or open-source\nmodels with large parameters, which poses challenges to high resource\nconsumption. To address this, we propose a novel Lightweight and efficient\nPrompt learning-ReasOning Framework for KGQA (LightPROF), which leverages the\nfull potential of LLMs to tackle complex reasoning tasks in a\nparameter-efficient manner. Specifically, LightPROF follows a\n\"Retrieve-Embed-Reason process\", first accurately, and stably retrieving the\ncorresponding reasoning graph from the KG through retrieval module. Next,\nthrough a Transformer-based Knowledge Adapter, it finely extracts and\nintegrates factual and structural information from the KG, then maps this\ninformation to the LLM's token embedding space, creating an LLM-friendly prompt\nto be used by the LLM for the final reasoning. Additionally, LightPROF only\nrequires training Knowledge Adapter and can be compatible with any open-source\nLLM. Extensive experiments on two public KGQA benchmarks demonstrate that\nLightPROF achieves superior performance with small-scale LLMs. Furthermore,\nLightPROF shows significant advantages in terms of input token count and\nreasoning time."
                },
                "authors": [
                    {
                        "name": "Tu Ao"
                    },
                    {
                        "name": "Yanhua Yu"
                    },
                    {
                        "name": "Yuling Wang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Zirui Guo"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Pinghui Wang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Zhen Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Cai"
                },
                "author": "Zhen Cai",
                "arxiv_comment": "This paper has been accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11546v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11546v3",
                "updated": "2025-04-04T02:50:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    2,
                    50,
                    14,
                    4,
                    94,
                    0
                ],
                "published": "2024-08-21T11:54:22Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    11,
                    54,
                    22,
                    2,
                    234,
                    0
                ],
                "title": "Memorization in In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization in In-Context Learning"
                },
                "summary": "In-context learning (ICL) has proven to be an effective strategy for\nimproving the performance of large language models (LLMs) with no additional\ntraining. However, the exact mechanism behind this performance improvement\nremains unclear. This study is the first to show how ICL surfaces memorized\ntraining data and to explore the correlation between this memorization and\nperformance on downstream tasks across various ICL regimes: zero-shot,\nfew-shot, and many-shot. Our most notable findings include: (1) ICL\nsignificantly surfaces memorization compared to zero-shot learning in most\ncases; (2) demonstrations, without their labels, are the most effective element\nin surfacing memorization; (3) ICL improves performance when the surfaced\nmemorization in few-shot regimes reaches a high level (about 40%); and (4)\nthere is a very strong correlation between performance and memorization in ICL\nwhen it outperforms zero-shot learning. Overall, our study uncovers\nmemorization as a new factor impacting ICL, raising an important question: to\nwhat extent do LLMs truly generalize from demonstrations in ICL, and how much\nof their success is due to memorization?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has proven to be an effective strategy for\nimproving the performance of large language models (LLMs) with no additional\ntraining. However, the exact mechanism behind this performance improvement\nremains unclear. This study is the first to show how ICL surfaces memorized\ntraining data and to explore the correlation between this memorization and\nperformance on downstream tasks across various ICL regimes: zero-shot,\nfew-shot, and many-shot. Our most notable findings include: (1) ICL\nsignificantly surfaces memorization compared to zero-shot learning in most\ncases; (2) demonstrations, without their labels, are the most effective element\nin surfacing memorization; (3) ICL improves performance when the surfaced\nmemorization in few-shot regimes reaches a high level (about 40%); and (4)\nthere is a very strong correlation between performance and memorization in ICL\nwhen it outperforms zero-shot learning. Overall, our study uncovers\nmemorization as a new factor impacting ICL, raising an important question: to\nwhat extent do LLMs truly generalize from demonstrations in ICL, and how much\nof their success is due to memorization?"
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Steven Bethard"
                    },
                    {
                        "name": "Eduardo Blanco"
                    },
                    {
                        "name": "Ellen Riloff"
                    }
                ],
                "author_detail": {
                    "name": "Ellen Riloff"
                },
                "author": "Ellen Riloff",
                "arxiv_comment": "v3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11546v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11546v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02800v2",
                "updated": "2025-04-04T02:07:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    2,
                    7,
                    59,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-03T17:43:14Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    43,
                    14,
                    3,
                    93,
                    0
                ],
                "title": "A Survey of Large Language Models in Mental Health Disorder Detection on\n  Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Models in Mental Health Disorder Detection on\n  Social Media"
                },
                "summary": "The detection and intervention of mental health issues represent a critical\nglobal research focus, and social media data has been recognized as an\nimportant resource for mental health research. However, how to utilize Large\nLanguage Models (LLMs) for mental health problem detection on social media\nposes significant challenges. Hence, this paper aims to explore the potential\nof LLM applications in social media data analysis, focusing not only on the\nmost common psychological disorders such as depression and anxiety but also\nincorporating psychotic disorders and externalizing disorders, summarizing the\napplication methods of LLM from different dimensions, such as text data\nanalysis and detection of mental disorders, and revealing the major challenges\nand shortcomings of current research. In addition, the paper provides an\noverview of popular datasets, and evaluation metrics. The survey in this paper\nprovides a comprehensive frame of reference for researchers in the field of\nmental health, while demonstrating the great potential of LLMs in mental health\ndetection to facilitate the further application of LLMs in future mental health\ninterventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection and intervention of mental health issues represent a critical\nglobal research focus, and social media data has been recognized as an\nimportant resource for mental health research. However, how to utilize Large\nLanguage Models (LLMs) for mental health problem detection on social media\nposes significant challenges. Hence, this paper aims to explore the potential\nof LLM applications in social media data analysis, focusing not only on the\nmost common psychological disorders such as depression and anxiety but also\nincorporating psychotic disorders and externalizing disorders, summarizing the\napplication methods of LLM from different dimensions, such as text data\nanalysis and detection of mental disorders, and revealing the major challenges\nand shortcomings of current research. In addition, the paper provides an\noverview of popular datasets, and evaluation metrics. The survey in this paper\nprovides a comprehensive frame of reference for researchers in the field of\nmental health, while demonstrating the great potential of LLMs in mental health\ndetection to facilitate the further application of LLMs in future mental health\ninterventions."
                },
                "authors": [
                    {
                        "name": "Zhuohan Ge"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Shihao Qi"
                    },
                    {
                        "name": "Yuming Xu"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jason Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jason Zhang"
                },
                "author": "Jason Zhang",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03111v1",
                "updated": "2025-04-04T01:41:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    1,
                    41,
                    6,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T01:41:06Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    1,
                    41,
                    6,
                    4,
                    94,
                    0
                ],
                "title": "Les Dissonances: Cross-Tool Harvesting and Polluting in Multi-Tool\n  Empowered LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Les Dissonances: Cross-Tool Harvesting and Polluting in Multi-Tool\n  Empowered LLM Agents"
                },
                "summary": "Large Language Model (LLM) agents are autonomous systems powered by LLMs,\ncapable of reasoning and planning to solve problems by leveraging a set of\ntools. However, the integration of multi-tool capabilities in LLM agents\nintroduces challenges in securely managing tools, ensuring their compatibility,\nhandling dependency relationships, and protecting control flows within LLM\nagent workflows. In this paper, we present the first systematic security\nanalysis of task control flows in multi-tool-enabled LLM agents. We identify a\nnovel threat, Cross-Tool Harvesting and Polluting (XTHP), which includes\nmultiple attack vectors to first hijack the normal control flows of agent\ntasks, and then collect and pollute confidential or private information within\nLLM agent systems. To understand the impact of this threat, we developed Chord,\na dynamic scanning tool designed to automatically detect real-world agent tools\nsusceptible to XTHP attacks. Our evaluation of 73 real-world tools from the\nrepositories of two major LLM agent development frameworks, LangChain and\nLlamaIndex, revealed a significant security concern: 80% of the tools are\nvulnerable to hijacking attacks, 78% to XTH attacks, and 41% to XTP attacks,\nhighlighting the prevalence of this threat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents are autonomous systems powered by LLMs,\ncapable of reasoning and planning to solve problems by leveraging a set of\ntools. However, the integration of multi-tool capabilities in LLM agents\nintroduces challenges in securely managing tools, ensuring their compatibility,\nhandling dependency relationships, and protecting control flows within LLM\nagent workflows. In this paper, we present the first systematic security\nanalysis of task control flows in multi-tool-enabled LLM agents. We identify a\nnovel threat, Cross-Tool Harvesting and Polluting (XTHP), which includes\nmultiple attack vectors to first hijack the normal control flows of agent\ntasks, and then collect and pollute confidential or private information within\nLLM agent systems. To understand the impact of this threat, we developed Chord,\na dynamic scanning tool designed to automatically detect real-world agent tools\nsusceptible to XTHP attacks. Our evaluation of 73 real-world tools from the\nrepositories of two major LLM agent development frameworks, LangChain and\nLlamaIndex, revealed a significant security concern: 80% of the tools are\nvulnerable to hijacking attacks, 78% to XTH attacks, and 41% to XTP attacks,\nhighlighting the prevalence of this threat."
                },
                "authors": [
                    {
                        "name": "Zichuan Li"
                    },
                    {
                        "name": "Jian Cui"
                    },
                    {
                        "name": "Xiaojing Liao"
                    },
                    {
                        "name": "Luyi Xing"
                    }
                ],
                "author_detail": {
                    "name": "Luyi Xing"
                },
                "author": "Luyi Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03321v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03321v3",
                "updated": "2025-04-04T01:33:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    1,
                    33,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2024-10-21T06:18:53Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    6,
                    18,
                    53,
                    0,
                    295,
                    0
                ],
                "title": "Towards More Accurate US Presidential Election via Multi-step Reasoning\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards More Accurate US Presidential Election via Multi-step Reasoning\n  with Large Language Models"
                },
                "summary": "Can Large Language Models (LLMs) accurately predict election outcomes? While\nLLMs have demonstrated impressive performance in various domains, including\nhealthcare, legal analysis, and creative tasks, their ability to forecast\nelections remains unknown. Election prediction poses unique challenges, such as\nlimited voter-level data, rapidly changing political landscapes, and the need\nto model complex human behavior. To address these challenges, we introduce a\nmulti-step reasoning framework designed for political analysis. Our approach is\nvalidated on real-world data from the American National Election Studies (ANES)\n2016 and 2020, as well as synthetic personas generated by the leading machine\nlearning framework, offering scalable datasets for voter behavior modeling. To\ncapture temporal dynamics, we incorporate candidates' policy positions and\nbiographical details, ensuring that the model adapts to evolving political\ncontexts. Drawing on Chain of Thought prompting, our multi-step reasoning\npipeline systematically integrates demographic, ideological, and time-dependent\nfactors, enhancing the model's predictive power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models (LLMs) accurately predict election outcomes? While\nLLMs have demonstrated impressive performance in various domains, including\nhealthcare, legal analysis, and creative tasks, their ability to forecast\nelections remains unknown. Election prediction poses unique challenges, such as\nlimited voter-level data, rapidly changing political landscapes, and the need\nto model complex human behavior. To address these challenges, we introduce a\nmulti-step reasoning framework designed for political analysis. Our approach is\nvalidated on real-world data from the American National Election Studies (ANES)\n2016 and 2020, as well as synthetic personas generated by the leading machine\nlearning framework, offering scalable datasets for voter behavior modeling. To\ncapture temporal dynamics, we incorporate candidates' policy positions and\nbiographical details, ensuring that the model adapts to evolving political\ncontexts. Drawing on Chain of Thought prompting, our multi-step reasoning\npipeline systematically integrates demographic, ideological, and time-dependent\nfactors, enhancing the model's predictive power."
                },
                "authors": [
                    {
                        "name": "Chenxiao Yu"
                    },
                    {
                        "name": "Zhaotian Weng"
                    },
                    {
                        "name": "Yuangang Li"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Xiyang Hu"
                    },
                    {
                        "name": "Yue Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhao"
                },
                "author": "Yue Zhao",
                "arxiv_comment": "This research is ongoing work. Xiyang Hu and Yue Zhao are the\n  corresponding authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03321v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03321v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09151v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09151v3",
                "updated": "2025-04-03T23:20:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    23,
                    20,
                    47,
                    3,
                    93,
                    0
                ],
                "published": "2024-04-14T06:09:35Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    6,
                    9,
                    35,
                    6,
                    105,
                    0
                ],
                "title": "Productively Deploying Emerging Models on Emerging Platforms: A Top-Down\n  Approach for Testing and Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Productively Deploying Emerging Models on Emerging Platforms: A Top-Down\n  Approach for Testing and Debugging"
                },
                "summary": "While existing machine learning (ML) frameworks focus on established\nplatforms, like running CUDA on server-grade GPUs, there have been growing\ndemands to enable emerging AI applications in a broader set of scenarios, such\nas running Large Language Models (LLMs) within browsers and mobile phones.\nHowever, deploying emerging models on new platforms (such as Metal and WebGPU)\npresents significant software engineering challenges due to rapid model\nevolution and limited tooling and practices for these platforms.\n  Previous practice for ML model deployment often follows a bottom-up fashion,\nwhere engineers first implement individual required operators and then put them\ntogether. However, this traditional development approach fails to meet the\nproductivity requirements when deploying emerging ML applications, with the\ntesting and debugging part as a bottleneck. To this end, we introduce\n\\textsc{TapML}, a top-down approach designed to streamline model deployment on\ndiverse platforms. While the traditional bottom-up approach requires crafting\nmanual tests, \\textsc{TapML} automatically creates high-quality, realistic test\ndata through operator-wise test carving. Furthermore, \\textsc{TapML} uses a\nmigration-based strategy to gradually offload model implementation from the\nmature source platform to the target platform, minimizing the debugging scope\nof compound errors.\n  \\textsc{TapML} has been used as the default development method in the MLC-LLM\nproject to deploy emerging ML models. Within 2 years, \\textsc{TapML} has\naccelerated the deployment of 105 emerging models in 27 model architectures\nacross 5 emerging platforms. We show that \\textsc{TapML} effectively boosts\ndeveloper productivity while ensuring the quality of deployed models.\nFurthermore, we summarize comprehensive case studies from our real-world\ndevelopment, offering best practices for developing emerging ML systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While existing machine learning (ML) frameworks focus on established\nplatforms, like running CUDA on server-grade GPUs, there have been growing\ndemands to enable emerging AI applications in a broader set of scenarios, such\nas running Large Language Models (LLMs) within browsers and mobile phones.\nHowever, deploying emerging models on new platforms (such as Metal and WebGPU)\npresents significant software engineering challenges due to rapid model\nevolution and limited tooling and practices for these platforms.\n  Previous practice for ML model deployment often follows a bottom-up fashion,\nwhere engineers first implement individual required operators and then put them\ntogether. However, this traditional development approach fails to meet the\nproductivity requirements when deploying emerging ML applications, with the\ntesting and debugging part as a bottleneck. To this end, we introduce\n\\textsc{TapML}, a top-down approach designed to streamline model deployment on\ndiverse platforms. While the traditional bottom-up approach requires crafting\nmanual tests, \\textsc{TapML} automatically creates high-quality, realistic test\ndata through operator-wise test carving. Furthermore, \\textsc{TapML} uses a\nmigration-based strategy to gradually offload model implementation from the\nmature source platform to the target platform, minimizing the debugging scope\nof compound errors.\n  \\textsc{TapML} has been used as the default development method in the MLC-LLM\nproject to deploy emerging ML models. Within 2 years, \\textsc{TapML} has\naccelerated the deployment of 105 emerging models in 27 model architectures\nacross 5 emerging platforms. We show that \\textsc{TapML} effectively boosts\ndeveloper productivity while ensuring the quality of deployed models.\nFurthermore, we summarize comprehensive case studies from our real-world\ndevelopment, offering best practices for developing emerging ML systems."
                },
                "authors": [
                    {
                        "name": "Siyuan Feng"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Lingming Zhang"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09151v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09151v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14729v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14729v2",
                "updated": "2025-04-03T23:03:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    23,
                    3,
                    17,
                    3,
                    93,
                    0
                ],
                "published": "2024-09-23T06:08:32Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    6,
                    8,
                    32,
                    0,
                    267,
                    0
                ],
                "title": "PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt\n  Injection in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt\n  Injection in LLMs"
                },
                "summary": "Large Language Models (LLMs) have gained widespread use in various\napplications due to their powerful capability to generate human-like text.\nHowever, prompt injection attacks, which involve overwriting a model's original\ninstructions with malicious prompts to manipulate the generated text, have\nraised significant concerns about the security and reliability of LLMs.\nEnsuring that LLMs are robust against such attacks is crucial for their\ndeployment in real-world applications, particularly in critical tasks.\n  In this paper, we propose PROMPTFUZZ, a novel testing framework that\nleverages fuzzing techniques to systematically assess the robustness of LLMs\nagainst prompt injection attacks. Inspired by software fuzzing, PROMPTFUZZ\nselects promising seed prompts and generates a diverse set of prompt injections\nto evaluate the target LLM's resilience. PROMPTFUZZ operates in two stages: the\nprepare phase, which involves selecting promising initial seeds and collecting\nfew-shot examples, and the focus phase, which uses the collected examples to\ngenerate diverse, high-quality prompt injections. Using PROMPTFUZZ, we can\nuncover more vulnerabilities in LLMs, even those with strong defense prompts.\n  By deploying the generated attack prompts from PROMPTFUZZ in a real-world\ncompetition, we achieved the 7th ranking out of over 4000 participants (top\n0.14%) within 2 hours. Additionally, we construct a dataset to fine-tune LLMs\nfor enhanced robustness against prompt injection attacks. While the fine-tuned\nmodel shows improved robustness, PROMPTFUZZ continues to identify\nvulnerabilities, highlighting the importance of robust testing for LLMs. Our\nwork emphasizes the critical need for effective testing tools and provides a\npractical framework for evaluating and improving the robustness of LLMs against\nprompt injection attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained widespread use in various\napplications due to their powerful capability to generate human-like text.\nHowever, prompt injection attacks, which involve overwriting a model's original\ninstructions with malicious prompts to manipulate the generated text, have\nraised significant concerns about the security and reliability of LLMs.\nEnsuring that LLMs are robust against such attacks is crucial for their\ndeployment in real-world applications, particularly in critical tasks.\n  In this paper, we propose PROMPTFUZZ, a novel testing framework that\nleverages fuzzing techniques to systematically assess the robustness of LLMs\nagainst prompt injection attacks. Inspired by software fuzzing, PROMPTFUZZ\nselects promising seed prompts and generates a diverse set of prompt injections\nto evaluate the target LLM's resilience. PROMPTFUZZ operates in two stages: the\nprepare phase, which involves selecting promising initial seeds and collecting\nfew-shot examples, and the focus phase, which uses the collected examples to\ngenerate diverse, high-quality prompt injections. Using PROMPTFUZZ, we can\nuncover more vulnerabilities in LLMs, even those with strong defense prompts.\n  By deploying the generated attack prompts from PROMPTFUZZ in a real-world\ncompetition, we achieved the 7th ranking out of over 4000 participants (top\n0.14%) within 2 hours. Additionally, we construct a dataset to fine-tune LLMs\nfor enhanced robustness against prompt injection attacks. While the fine-tuned\nmodel shows improved robustness, PROMPTFUZZ continues to identify\nvulnerabilities, highlighting the importance of robust testing for LLMs. Our\nwork emphasizes the critical need for effective testing tools and provides a\npractical framework for evaluating and improving the robustness of LLMs against\nprompt injection attacks."
                },
                "authors": [
                    {
                        "name": "Jiahao Yu"
                    },
                    {
                        "name": "Yangguang Shao"
                    },
                    {
                        "name": "Hanwen Miao"
                    },
                    {
                        "name": "Junzheng Shi"
                    }
                ],
                "author_detail": {
                    "name": "Junzheng Shi"
                },
                "author": "Junzheng Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14729v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14729v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03073v1",
                "updated": "2025-04-03T22:52:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    52,
                    16,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T22:52:16Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    52,
                    16,
                    3,
                    93,
                    0
                ],
                "title": "Distributed Locking: Performance Analysis and Optimization Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Locking: Performance Analysis and Optimization Strategies"
                },
                "summary": "Distributed locking mechanisms are fundamental to ensuring data consistency\nand integrity in distributed systems. This paper presents a comprehensive\nanalysis of distributed locking algorithms, focusing on their performance\ncharacteristics under various workload conditions. We compare traditional\ncentralized locking approaches with modern distributed protocols, evaluating\nthem based on throughput, latency, and scalability metrics. Our experimental\nresults demonstrate that optimized distributed locking protocols can achieve up\nto 68\\% better performance compared to centralized approaches in\nhigh-contention scenarios, while maintaining strong consistency guarantees.\nFurthermore, we propose novel optimizations for distributed locking that\nsignificantly reduce coordination overhead in geo-distributed deployments. The\nfindings contribute to the growing body of knowledge on designing efficient\nconcurrency control mechanisms for modern distributed systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed locking mechanisms are fundamental to ensuring data consistency\nand integrity in distributed systems. This paper presents a comprehensive\nanalysis of distributed locking algorithms, focusing on their performance\ncharacteristics under various workload conditions. We compare traditional\ncentralized locking approaches with modern distributed protocols, evaluating\nthem based on throughput, latency, and scalability metrics. Our experimental\nresults demonstrate that optimized distributed locking protocols can achieve up\nto 68\\% better performance compared to centralized approaches in\nhigh-contention scenarios, while maintaining strong consistency guarantees.\nFurthermore, we propose novel optimizations for distributed locking that\nsignificantly reduce coordination overhead in geo-distributed deployments. The\nfindings contribute to the growing body of knowledge on designing efficient\nconcurrency control mechanisms for modern distributed systems."
                },
                "authors": [
                    {
                        "name": "Andre Rodriguez"
                    },
                    {
                        "name": "William Osborn"
                    }
                ],
                "author_detail": {
                    "name": "William Osborn"
                },
                "author": "William Osborn",
                "arxiv_comment": "International Conference of Communication and Data Analytics, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16444v3",
                "updated": "2025-04-03T22:49:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    49,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2024-05-26T06:00:17Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    6,
                    0,
                    17,
                    6,
                    147,
                    0
                ],
                "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion"
                },
                "summary": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03071v1",
                "updated": "2025-04-03T22:49:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    49,
                    10,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T22:49:10Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    49,
                    10,
                    3,
                    93,
                    0
                ],
                "title": "AD-GPT: Large Language Models in Alzheimer's Disease",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AD-GPT: Large Language Models in Alzheimer's Disease"
                },
                "summary": "Large language models (LLMs) have emerged as powerful tools for medical\ninformation retrieval, yet their accuracy and depth remain limited in\nspecialized domains such as Alzheimer's disease (AD), a growing global health\nchallenge. To address this gap, we introduce AD-GPT, a domain-specific\ngenerative pre-trained transformer designed to enhance the retrieval and\nanalysis of AD-related genetic and neurobiological information. AD-GPT\nintegrates diverse biomedical data sources, including potential AD-associated\ngenes, molecular genetic information, and key gene variants linked to brain\nregions. We develop a stacked LLM architecture combining Llama3 and BERT,\noptimized for four critical tasks in AD research: (1) genetic information\nretrieval, (2) gene-brain region relationship assessment, (3) gene-AD\nrelationship analysis, and (4) brain region-AD relationship mapping.\nComparative evaluations against state-of-the-art LLMs demonstrate AD-GPT's\nsuperior precision and reliability across these tasks, underscoring its\npotential as a robust and specialized AI tool for advancing AD research and\nbiomarker discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have emerged as powerful tools for medical\ninformation retrieval, yet their accuracy and depth remain limited in\nspecialized domains such as Alzheimer's disease (AD), a growing global health\nchallenge. To address this gap, we introduce AD-GPT, a domain-specific\ngenerative pre-trained transformer designed to enhance the retrieval and\nanalysis of AD-related genetic and neurobiological information. AD-GPT\nintegrates diverse biomedical data sources, including potential AD-associated\ngenes, molecular genetic information, and key gene variants linked to brain\nregions. We develop a stacked LLM architecture combining Llama3 and BERT,\noptimized for four critical tasks in AD research: (1) genetic information\nretrieval, (2) gene-brain region relationship assessment, (3) gene-AD\nrelationship analysis, and (4) brain region-AD relationship mapping.\nComparative evaluations against state-of-the-art LLMs demonstrate AD-GPT's\nsuperior precision and reliability across these tasks, underscoring its\npotential as a robust and specialized AI tool for advancing AD research and\nbiomarker discovery."
                },
                "authors": [
                    {
                        "name": "Ziyu Liu"
                    },
                    {
                        "name": "Lintao Tang"
                    },
                    {
                        "name": "Zeliang Sun"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Yanjun Lyu"
                    },
                    {
                        "name": "Wei Ruan"
                    },
                    {
                        "name": "Yangshuang Xu"
                    },
                    {
                        "name": "Liang Shan"
                    },
                    {
                        "name": "Jiyoon Shin"
                    },
                    {
                        "name": "Xiaohe Chen"
                    },
                    {
                        "name": "Dajiang Zhu"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Rongjie Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03068v1",
                "updated": "2025-04-03T22:47:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    47,
                    33,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T22:47:33Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    47,
                    33,
                    3,
                    93,
                    0
                ],
                "title": "Design of AI-Powered Tool for Self-Regulation Support in Programming\n  Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design of AI-Powered Tool for Self-Regulation Support in Programming\n  Education"
                },
                "summary": "Large Language Model (LLM) tools have demonstrated their potential to deliver\nhigh-quality assistance by providing instant, personalized feedback that is\ncrucial for effective programming education. However, many of these tools\noperate independently from institutional Learning Management Systems, which\ncreates a significant disconnect. This isolation limits the ability to leverage\nlearning materials and exercise context for generating tailored, context-aware\nfeedback. Furthermore, previous research on self-regulated learning and LLM\nsupport mainly focused on knowledge acquisition, not the development of\nimportant self-regulation skills. To address these challenges, we developed\nCodeRunner Agent, an LLM-based programming assistant that integrates the\nCodeRunner, a student-submitted code executing and automated grading plugin in\nMoodle. CodeRunner Agent empowers educators to customize AI-generated feedback\nby incorporating detailed context from lecture materials, programming\nquestions, student answers, and execution results. Additionally, it enhances\nstudents' self-regulated learning by providing strategy-based AI responses.\nThis integrated, context-aware, and skill-focused approach offers promising\navenues for data-driven improvements in programming education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) tools have demonstrated their potential to deliver\nhigh-quality assistance by providing instant, personalized feedback that is\ncrucial for effective programming education. However, many of these tools\noperate independently from institutional Learning Management Systems, which\ncreates a significant disconnect. This isolation limits the ability to leverage\nlearning materials and exercise context for generating tailored, context-aware\nfeedback. Furthermore, previous research on self-regulated learning and LLM\nsupport mainly focused on knowledge acquisition, not the development of\nimportant self-regulation skills. To address these challenges, we developed\nCodeRunner Agent, an LLM-based programming assistant that integrates the\nCodeRunner, a student-submitted code executing and automated grading plugin in\nMoodle. CodeRunner Agent empowers educators to customize AI-generated feedback\nby incorporating detailed context from lecture materials, programming\nquestions, student answers, and execution results. Additionally, it enhances\nstudents' self-regulated learning by providing strategy-based AI responses.\nThis integrated, context-aware, and skill-focused approach offers promising\navenues for data-driven improvements in programming education."
                },
                "authors": [
                    {
                        "name": "Huiyong Li"
                    },
                    {
                        "name": "Boxuan Ma"
                    }
                ],
                "author_detail": {
                    "name": "Boxuan Ma"
                },
                "author": "Boxuan Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13107v2",
                "updated": "2025-04-03T22:39:34Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    39,
                    34,
                    3,
                    93,
                    0
                ],
                "published": "2025-02-18T18:19:36Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    19,
                    36,
                    1,
                    49,
                    0
                ],
                "title": "MatterChat: A Multi-Modal LLM for Material Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatterChat: A Multi-Modal LLM for Material Science"
                },
                "summary": "Understanding and predicting the properties of inorganic materials is crucial\nfor accelerating advancements in materials science and driving applications in\nenergy, electronics, and beyond. Integrating material structure data with\nlanguage-based information through multi-modal large language models (LLMs)\noffers great potential to support these efforts by enhancing human-AI\ninteraction. However, a key challenge lies in integrating atomic structures at\nfull resolution into LLMs. In this work, we introduce MatterChat, a versatile\nstructure-aware multi-modal LLM that unifies material structural data and\ntextual inputs into a single cohesive model. MatterChat employs a bridging\nmodule to effectively align a pretrained machine learning interatomic potential\nwith a pretrained LLM, reducing training costs and enhancing flexibility. Our\nresults demonstrate that MatterChat significantly improves performance in\nmaterial property prediction and human-AI interaction, surpassing\ngeneral-purpose LLMs such as GPT-4. We also demonstrate its usefulness in\napplications such as more advanced scientific reasoning and step-by-step\nmaterial synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and predicting the properties of inorganic materials is crucial\nfor accelerating advancements in materials science and driving applications in\nenergy, electronics, and beyond. Integrating material structure data with\nlanguage-based information through multi-modal large language models (LLMs)\noffers great potential to support these efforts by enhancing human-AI\ninteraction. However, a key challenge lies in integrating atomic structures at\nfull resolution into LLMs. In this work, we introduce MatterChat, a versatile\nstructure-aware multi-modal LLM that unifies material structural data and\ntextual inputs into a single cohesive model. MatterChat employs a bridging\nmodule to effectively align a pretrained machine learning interatomic potential\nwith a pretrained LLM, reducing training costs and enhancing flexibility. Our\nresults demonstrate that MatterChat significantly improves performance in\nmaterial property prediction and human-AI interaction, surpassing\ngeneral-purpose LLMs such as GPT-4. We also demonstrate its usefulness in\napplications such as more advanced scientific reasoning and step-by-step\nmaterial synthesis."
                },
                "authors": [
                    {
                        "name": "Yingheng Tang"
                    },
                    {
                        "name": "Wenbin Xu"
                    },
                    {
                        "name": "Jie Cao"
                    },
                    {
                        "name": "Jianzhu Ma"
                    },
                    {
                        "name": "Weilu Gao"
                    },
                    {
                        "name": "Steve Farrell"
                    },
                    {
                        "name": "Benjamin Erichson"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Andy Nonaka"
                    },
                    {
                        "name": "Zhi Yao"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Yao"
                },
                "author": "Zhi Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14202v2",
                "updated": "2025-04-03T22:13:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    13,
                    44,
                    3,
                    93,
                    0
                ],
                "published": "2025-02-20T02:20:06Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    2,
                    20,
                    6,
                    3,
                    51,
                    0
                ],
                "title": "Do LLMs Consider Security? An Empirical Study on Responses to\n  Programming Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Consider Security? An Empirical Study on Responses to\n  Programming Questions"
                },
                "summary": "The widespread adoption of conversational LLMs for software development has\nraised new security concerns regarding the safety of LLM-generated content. Our\nmotivational study outlines ChatGPT's potential in volunteering\ncontext-specific information to the developers, promoting safe coding\npractices. Motivated by this finding, we conduct a study to evaluate the degree\nof security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and\nLlama 3. We prompt these LLMs with Stack Overflow questions that contain\nvulnerable code to evaluate whether they merely provide answers to the\nquestions or if they also warn users about the insecure code, thereby\ndemonstrating a degree of security awareness. Further, we assess whether LLM\nresponses provide information about the causes, exploits, and the potential\nfixes of the vulnerability, to help raise users' awareness. Our findings show\nthat all three models struggle to accurately detect and warn users about\nvulnerabilities, achieving a detection rate of only 12.6% to 40% across our\ndatasets. We also observe that the LLMs tend to identify certain types of\nvulnerabilities related to sensitive information exposure and improper input\nneutralization much more frequently than other types, such as those involving\nexternal control of file names or paths. Furthermore, when LLMs do issue\nsecurity warnings, they often provide more information on the causes, exploits,\nand fixes of vulnerabilities compared to Stack Overflow responses. Finally, we\nprovide an in-depth discussion on the implications of our findings and present\na CLI-based prompting tool that can be used to generate significantly more\nsecure LLM responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of conversational LLMs for software development has\nraised new security concerns regarding the safety of LLM-generated content. Our\nmotivational study outlines ChatGPT's potential in volunteering\ncontext-specific information to the developers, promoting safe coding\npractices. Motivated by this finding, we conduct a study to evaluate the degree\nof security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and\nLlama 3. We prompt these LLMs with Stack Overflow questions that contain\nvulnerable code to evaluate whether they merely provide answers to the\nquestions or if they also warn users about the insecure code, thereby\ndemonstrating a degree of security awareness. Further, we assess whether LLM\nresponses provide information about the causes, exploits, and the potential\nfixes of the vulnerability, to help raise users' awareness. Our findings show\nthat all three models struggle to accurately detect and warn users about\nvulnerabilities, achieving a detection rate of only 12.6% to 40% across our\ndatasets. We also observe that the LLMs tend to identify certain types of\nvulnerabilities related to sensitive information exposure and improper input\nneutralization much more frequently than other types, such as those involving\nexternal control of file names or paths. Furthermore, when LLMs do issue\nsecurity warnings, they often provide more information on the causes, exploits,\nand fixes of vulnerabilities compared to Stack Overflow responses. Finally, we\nprovide an in-depth discussion on the implications of our findings and present\na CLI-based prompting tool that can be used to generate significantly more\nsecure LLM responses."
                },
                "authors": [
                    {
                        "name": "Amirali Sajadi"
                    },
                    {
                        "name": "Binh Le"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Kostadin Damevski"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Preetha Chatterjee"
                },
                "author": "Preetha Chatterjee",
                "arxiv_comment": "Accepted to EMSE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03051v1",
                "updated": "2025-04-03T21:57:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    57,
                    17,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T21:57:17Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    57,
                    17,
                    3,
                    93,
                    0
                ],
                "title": "Task as Context Prompting for Accurate Medical Symptom Coding Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task as Context Prompting for Accurate Medical Symptom Coding Using\n  Large Language Models"
                },
                "summary": "Accurate medical symptom coding from unstructured clinical text, such as\nvaccine safety reports, is a critical task with applications in\npharmacovigilance and safety monitoring. Symptom coding, as tailored in this\nstudy, involves identifying and linking nuanced symptom mentions to\nstandardized vocabularies like MedDRA, differentiating it from broader medical\ncoding tasks. Traditional approaches to this task, which treat symptom\nextraction and linking as independent workflows, often fail to handle the\nvariability and complexity of clinical narratives, especially for rare cases.\nRecent advancements in Large Language Models (LLMs) offer new opportunities but\nface challenges in achieving consistent performance. To address these issues,\nwe propose Task as Context (TACO) Prompting, a novel framework that unifies\nextraction and linking tasks by embedding task-specific context into LLM\nprompts. Our study also introduces SYMPCODER, a human-annotated dataset derived\nfrom Vaccine Adverse Event Reporting System (VAERS) reports, and a two-stage\nevaluation framework to comprehensively assess both symptom linking and mention\nfidelity. Our comprehensive evaluation of multiple LLMs, including Llama2-chat,\nJackalope-7b, GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o, demonstrates TACO's\neffectiveness in improving flexibility and accuracy for tailored tasks like\nsymptom coding, paving the way for more specific coding tasks and advancing\nclinical text processing methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate medical symptom coding from unstructured clinical text, such as\nvaccine safety reports, is a critical task with applications in\npharmacovigilance and safety monitoring. Symptom coding, as tailored in this\nstudy, involves identifying and linking nuanced symptom mentions to\nstandardized vocabularies like MedDRA, differentiating it from broader medical\ncoding tasks. Traditional approaches to this task, which treat symptom\nextraction and linking as independent workflows, often fail to handle the\nvariability and complexity of clinical narratives, especially for rare cases.\nRecent advancements in Large Language Models (LLMs) offer new opportunities but\nface challenges in achieving consistent performance. To address these issues,\nwe propose Task as Context (TACO) Prompting, a novel framework that unifies\nextraction and linking tasks by embedding task-specific context into LLM\nprompts. Our study also introduces SYMPCODER, a human-annotated dataset derived\nfrom Vaccine Adverse Event Reporting System (VAERS) reports, and a two-stage\nevaluation framework to comprehensively assess both symptom linking and mention\nfidelity. Our comprehensive evaluation of multiple LLMs, including Llama2-chat,\nJackalope-7b, GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o, demonstrates TACO's\neffectiveness in improving flexibility and accuracy for tailored tasks like\nsymptom coding, paving the way for more specific coding tasks and advancing\nclinical text processing methodologies."
                },
                "authors": [
                    {
                        "name": "Chengyang He"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Violet Xinying Chen"
                    },
                    {
                        "name": "Yue Ning"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "arxiv_doi": "10.1145/3721201.3721383",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721201.3721383",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.03051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 5 figures, 5 Tables, ACM/IEEE International Conference on\n  Connected Health: Applications, Systems and Engineering Technologies (CHASE\n  '25), June 24--26, 2025, New York, NY, USA",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14671v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14671v3",
                "updated": "2025-04-03T21:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    56,
                    8,
                    3,
                    93,
                    0
                ],
                "published": "2025-02-20T16:05:45Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    16,
                    5,
                    45,
                    3,
                    51,
                    0
                ],
                "title": "Explanations of Large Language Models Explain Language Representations\n  in the Brain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explanations of Large Language Models Explain Language Representations\n  in the Brain"
                },
                "summary": "Large language models (LLMs) not only exhibit human-like performance but also\nshare computational principles with the brain's language processing mechanisms.\nWhile prior research has focused on mapping LLMs' internal representations to\nneural activity, we propose a novel approach using explainable AI (XAI) to\nstrengthen this link. Applying attribution methods, we quantify the influence\nof preceding words on LLMs' next-word predictions and use these explanations to\npredict fMRI data from participants listening to narratives. We find that\nattribution methods robustly predict brain activity across the language\nnetwork, revealing a hierarchical pattern: explanations from early layers align\nwith the brain's initial language processing stages, while later layers\ncorrespond to more advanced stages. Additionally, layers with greater influence\non next-word prediction$\\unicode{x2014}$reflected in higher attribution\nscores$\\unicode{x2014}$demonstrate stronger brain alignment. These results\nunderscore XAI's potential for exploring the neural basis of language and\nsuggest brain alignment for assessing the biological plausibility of\nexplanation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) not only exhibit human-like performance but also\nshare computational principles with the brain's language processing mechanisms.\nWhile prior research has focused on mapping LLMs' internal representations to\nneural activity, we propose a novel approach using explainable AI (XAI) to\nstrengthen this link. Applying attribution methods, we quantify the influence\nof preceding words on LLMs' next-word predictions and use these explanations to\npredict fMRI data from participants listening to narratives. We find that\nattribution methods robustly predict brain activity across the language\nnetwork, revealing a hierarchical pattern: explanations from early layers align\nwith the brain's initial language processing stages, while later layers\ncorrespond to more advanced stages. Additionally, layers with greater influence\non next-word prediction$\\unicode{x2014}$reflected in higher attribution\nscores$\\unicode{x2014}$demonstrate stronger brain alignment. These results\nunderscore XAI's potential for exploring the neural basis of language and\nsuggest brain alignment for assessing the biological plausibility of\nexplanation methods."
                },
                "authors": [
                    {
                        "name": "Maryam Rahimi"
                    },
                    {
                        "name": "Yadollah Yaghoobzadeh"
                    },
                    {
                        "name": "Mohammad Reza Daliri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Reza Daliri"
                },
                "author": "Mohammad Reza Daliri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14671v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14671v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03048v1",
                "updated": "2025-04-03T21:53:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T21:53:51Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "title": "LLM Library Learning Fails: A LEGO-Prover Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Library Learning Fails: A LEGO-Prover Case Study"
                },
                "summary": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines."
                },
                "authors": [
                    {
                        "name": "Ian Berlot-Attwell"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Xujie Si"
                    }
                ],
                "author_detail": {
                    "name": "Xujie Si"
                },
                "author": "Xujie Si",
                "arxiv_comment": "24 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22943v2",
                "updated": "2025-04-03T21:53:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    9,
                    3,
                    93,
                    0
                ],
                "published": "2025-03-29T02:28:32Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    2,
                    28,
                    32,
                    5,
                    88,
                    0
                ],
                "title": "Towards Mobile Sensing with Event Cameras on High-agility\n  Resource-constrained Devices: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Mobile Sensing with Event Cameras on High-agility\n  Resource-constrained Devices: A Survey"
                },
                "summary": "With the increasing complexity of mobile device applications, these devices\nare evolving toward high agility. This shift imposes new demands on mobile\nsensing, particularly in terms of achieving high accuracy and low latency.\nEvent-based vision has emerged as a disruptive paradigm, offering high temporal\nresolution, low latency, and energy efficiency, making it well-suited for\nhigh-accuracy and low-latency sensing tasks on high-agility platforms. However,\nthe presence of substantial noisy events, the lack of inherent semantic\ninformation, and the large data volume pose significant challenges for\nevent-based data processing on resource-constrained mobile devices. This paper\nsurveys the literature over the period 2014-2024, provides a comprehensive\noverview of event-based mobile sensing systems, covering fundamental\nprinciples, event abstraction methods, algorithmic advancements, hardware and\nsoftware acceleration strategies. We also discuss key applications of event\ncameras in mobile sensing, including visual odometry, object tracking, optical\nflow estimation, and 3D reconstruction, while highlighting the challenges\nassociated with event data processing, sensor fusion, and real-time deployment.\nFurthermore, we outline future research directions, such as improving event\ncamera hardware with advanced optics, leveraging neuromorphic computing for\nefficient processing, and integrating bio-inspired algorithms to enhance\nperception. To support ongoing research, we provide an open-source\n\\textit{Online Sheet} with curated resources and recent developments. We hope\nthis survey serves as a valuable reference, facilitating the adoption of\nevent-based vision across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing complexity of mobile device applications, these devices\nare evolving toward high agility. This shift imposes new demands on mobile\nsensing, particularly in terms of achieving high accuracy and low latency.\nEvent-based vision has emerged as a disruptive paradigm, offering high temporal\nresolution, low latency, and energy efficiency, making it well-suited for\nhigh-accuracy and low-latency sensing tasks on high-agility platforms. However,\nthe presence of substantial noisy events, the lack of inherent semantic\ninformation, and the large data volume pose significant challenges for\nevent-based data processing on resource-constrained mobile devices. This paper\nsurveys the literature over the period 2014-2024, provides a comprehensive\noverview of event-based mobile sensing systems, covering fundamental\nprinciples, event abstraction methods, algorithmic advancements, hardware and\nsoftware acceleration strategies. We also discuss key applications of event\ncameras in mobile sensing, including visual odometry, object tracking, optical\nflow estimation, and 3D reconstruction, while highlighting the challenges\nassociated with event data processing, sensor fusion, and real-time deployment.\nFurthermore, we outline future research directions, such as improving event\ncamera hardware with advanced optics, leveraging neuromorphic computing for\nefficient processing, and integrating bio-inspired algorithms to enhance\nperception. To support ongoing research, we provide an open-source\n\\textit{Online Sheet} with curated resources and recent developments. We hope\nthis survey serves as a valuable reference, facilitating the adoption of\nevent-based vision across diverse applications."
                },
                "authors": [
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Ruishan Guo"
                    },
                    {
                        "name": "Pengtao Ma"
                    },
                    {
                        "name": "Ciyu Ruan"
                    },
                    {
                        "name": "Xinyu Luo"
                    },
                    {
                        "name": "Wenhua Ding"
                    },
                    {
                        "name": "Tianyang Zhong"
                    },
                    {
                        "name": "Jingao Xu"
                    },
                    {
                        "name": "Yunhao Liu"
                    },
                    {
                        "name": "Xinlei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei Chen"
                },
                "author": "Xinlei Chen",
                "arxiv_comment": "32 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03045v1",
                "updated": "2025-04-03T21:48:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    48,
                    9,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T21:48:09Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    48,
                    9,
                    3,
                    93,
                    0
                ],
                "title": "Extending CREAMT: Leveraging Large Language Models for Literary\n  Translation Post-Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending CREAMT: Leveraging Large Language Models for Literary\n  Translation Post-Editing"
                },
                "summary": "Post-editing machine translation (MT) for creative texts, such as literature,\nrequires balancing efficiency with the preservation of creativity and style.\nWhile neural MT systems struggle with these challenges, large language models\n(LLMs) offer improved capabilities for context-aware and creative translation.\nThis study evaluates the feasibility of post-editing literary translations\ngenerated by LLMs. Using a custom research tool, we collaborated with\nprofessional literary translators to analyze editing time, quality, and\ncreativity. Our results indicate that post-editing LLM-generated translations\nsignificantly reduces editing time compared to human translation while\nmaintaining a similar level of creativity. The minimal difference in creativity\nbetween PE and MT, combined with substantial productivity gains, suggests that\nLLMs may effectively support literary translators working with high-resource\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-editing machine translation (MT) for creative texts, such as literature,\nrequires balancing efficiency with the preservation of creativity and style.\nWhile neural MT systems struggle with these challenges, large language models\n(LLMs) offer improved capabilities for context-aware and creative translation.\nThis study evaluates the feasibility of post-editing literary translations\ngenerated by LLMs. Using a custom research tool, we collaborated with\nprofessional literary translators to analyze editing time, quality, and\ncreativity. Our results indicate that post-editing LLM-generated translations\nsignificantly reduces editing time compared to human translation while\nmaintaining a similar level of creativity. The minimal difference in creativity\nbetween PE and MT, combined with substantial productivity gains, suggests that\nLLMs may effectively support literary translators working with high-resource\nlanguages."
                },
                "authors": [
                    {
                        "name": "Antonio Castaldo"
                    },
                    {
                        "name": "Sheila Castilho"
                    },
                    {
                        "name": "Joss Moorkens"
                    },
                    {
                        "name": "Johanna Monti"
                    }
                ],
                "author_detail": {
                    "name": "Johanna Monti"
                },
                "author": "Johanna Monti",
                "arxiv_comment": "to be published in the Proceedings of the 20th Machine Translation\n  Summit (MT Summit 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02099v2",
                "updated": "2025-04-03T21:13:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    13,
                    21,
                    3,
                    93,
                    0
                ],
                "published": "2024-10-02T23:39:19Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    23,
                    39,
                    19,
                    2,
                    276,
                    0
                ],
                "title": "A Watermark for Black-Box Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Watermark for Black-Box Language Models"
                },
                "summary": "Watermarking has recently emerged as an effective strategy for detecting the\noutputs of large language models (LLMs). Most existing schemes require\nwhite-box access to the model's next-token probability distribution, which is\ntypically not accessible to downstream users of an LLM API. In this work, we\npropose a principled watermarking scheme that requires only the ability to\nsample sequences from the LLM (i.e. black-box access), boasts a distortion-free\nproperty, and can be chained or nested using multiple secret keys. We provide\nperformance guarantees, demonstrate how it can be leveraged when white-box\naccess is available, and show when it can outperform existing white-box schemes\nvia comprehensive experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has recently emerged as an effective strategy for detecting the\noutputs of large language models (LLMs). Most existing schemes require\nwhite-box access to the model's next-token probability distribution, which is\ntypically not accessible to downstream users of an LLM API. In this work, we\npropose a principled watermarking scheme that requires only the ability to\nsample sequences from the LLM (i.e. black-box access), boasts a distortion-free\nproperty, and can be chained or nested using multiple secret keys. We provide\nperformance guarantees, demonstrate how it can be leveraged when white-box\naccess is available, and show when it can outperform existing white-box schemes\nvia comprehensive experiments."
                },
                "authors": [
                    {
                        "name": "Dara Bahri"
                    },
                    {
                        "name": "John Wieting"
                    }
                ],
                "author_detail": {
                    "name": "John Wieting"
                },
                "author": "John Wieting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03029v1",
                "updated": "2025-04-03T21:04:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    4,
                    36,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T21:04:36Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    4,
                    36,
                    3,
                    93,
                    0
                ],
                "title": "Ontologies in Design: How Imagining a Tree Reveals Possibilites and\n  Assumptions in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontologies in Design: How Imagining a Tree Reveals Possibilites and\n  Assumptions in Large Language Models"
                },
                "summary": "Amid the recent uptake of Generative AI, sociotechnical scholars and critics\nhave traced a multitude of resulting harms, with analyses largely focused on\nvalues and axiology (e.g., bias). While value-based analyses are crucial, we\nargue that ontologies -- concerning what we allow ourselves to think or talk\nabout -- is a vital but under-recognized dimension in analyzing these systems.\nProposing a need for a practice-based engagement with ontologies, we offer four\norientations for considering ontologies in design: pluralism, groundedness,\nliveliness, and enactment. We share examples of potentialities that are opened\nup through these orientations across the entire LLM development pipeline by\nconducting two ontological analyses: examining the responses of four LLM-based\nchatbots in a prompting exercise, and analyzing the architecture of an\nLLM-based agent simulation. We conclude by sharing opportunities and\nlimitations of working with ontologies in the design and development of\nsociotechnical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amid the recent uptake of Generative AI, sociotechnical scholars and critics\nhave traced a multitude of resulting harms, with analyses largely focused on\nvalues and axiology (e.g., bias). While value-based analyses are crucial, we\nargue that ontologies -- concerning what we allow ourselves to think or talk\nabout -- is a vital but under-recognized dimension in analyzing these systems.\nProposing a need for a practice-based engagement with ontologies, we offer four\norientations for considering ontologies in design: pluralism, groundedness,\nliveliness, and enactment. We share examples of potentialities that are opened\nup through these orientations across the entire LLM development pipeline by\nconducting two ontological analyses: examining the responses of four LLM-based\nchatbots in a prompting exercise, and analyzing the architecture of an\nLLM-based agent simulation. We conclude by sharing opportunities and\nlimitations of working with ontologies in the design and development of\nsociotechnical systems."
                },
                "authors": [
                    {
                        "name": "Nava Haghighi"
                    },
                    {
                        "name": "Sunny Yu"
                    },
                    {
                        "name": "James Landay"
                    },
                    {
                        "name": "Daniela Rosner"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Rosner"
                },
                "author": "Daniela Rosner",
                "arxiv_doi": "10.1145/3706598.3713633",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713633",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.03029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20 pages, 1 figure, 2 tables, CHI '25",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03015v1",
                "updated": "2025-04-03T20:20:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    20,
                    20,
                    0,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T20:20:00Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    20,
                    20,
                    0,
                    3,
                    93,
                    0
                ],
                "title": "AuDeRe: Automated Strategy Decision and Realization in Robot Planning\n  and Control via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AuDeRe: Automated Strategy Decision and Realization in Robot Planning\n  and Control via LLMs"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown significant\npromise in various domains, especially robotics. However, most prior LLM-based\nwork in robotic applications either directly predicts waypoints or applies LLMs\nwithin fixed tool integration frameworks, offering limited flexibility in\nexploring and configuring solutions best suited to different tasks. In this\nwork, we propose a framework that leverages LLMs to select appropriate planning\nand control strategies based on task descriptions, environmental constraints,\nand system dynamics. These strategies are then executed by calling the\navailable comprehensive planning and control APIs. Our approach employs\niterative LLM-based reasoning with performance feedback to refine the algorithm\nselection. We validate our approach through extensive experiments across tasks\nof varying complexity, from simple tracking to complex planning scenarios\ninvolving spatiotemporal constraints. The results demonstrate that using LLMs\nto determine planning and control strategies from natural language descriptions\nsignificantly enhances robotic autonomy while reducing the need for extensive\nmanual tuning and expert knowledge. Furthermore, our framework maintains\ngeneralizability across different tasks and notably outperforms baseline\nmethods that rely on LLMs for direct trajectory, control sequence, or code\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown significant\npromise in various domains, especially robotics. However, most prior LLM-based\nwork in robotic applications either directly predicts waypoints or applies LLMs\nwithin fixed tool integration frameworks, offering limited flexibility in\nexploring and configuring solutions best suited to different tasks. In this\nwork, we propose a framework that leverages LLMs to select appropriate planning\nand control strategies based on task descriptions, environmental constraints,\nand system dynamics. These strategies are then executed by calling the\navailable comprehensive planning and control APIs. Our approach employs\niterative LLM-based reasoning with performance feedback to refine the algorithm\nselection. We validate our approach through extensive experiments across tasks\nof varying complexity, from simple tracking to complex planning scenarios\ninvolving spatiotemporal constraints. The results demonstrate that using LLMs\nto determine planning and control strategies from natural language descriptions\nsignificantly enhances robotic autonomy while reducing the need for extensive\nmanual tuning and expert knowledge. Furthermore, our framework maintains\ngeneralizability across different tasks and notably outperforms baseline\nmethods that rely on LLMs for direct trajectory, control sequence, or code\ngeneration."
                },
                "authors": [
                    {
                        "name": "Yue Meng"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Yongchao Chen"
                    },
                    {
                        "name": "Chuchu Fan"
                    }
                ],
                "author_detail": {
                    "name": "Chuchu Fan"
                },
                "author": "Chuchu Fan",
                "arxiv_comment": "8 pages, 14 figures, submitted for CDC 2025 invited session on Large\n  Language Models (LLMs) and Control",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08598v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08598v2",
                "updated": "2025-04-03T19:42:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    19,
                    42,
                    32,
                    3,
                    93,
                    0
                ],
                "published": "2025-01-15T05:51:20Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    5,
                    51,
                    20,
                    2,
                    15,
                    0
                ],
                "title": "LlamaRestTest: Effective REST API Testing with Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LlamaRestTest: Effective REST API Testing with Small Language Models"
                },
                "summary": "Modern web services rely heavily on REST APIs, typically documented using the\nOpenAPI specification. The widespread adoption of this standard has resulted in\nthe development of many black-box testing tools that generate tests based on\nOpenAPI specifications. Although Large Language Models (LLMs) have shown\npromising test-generation abilities, their application to REST API testing\nremains mostly unexplored. We present LlamaRestTest, a novel approach that\nemploys two custom LLMs-created by fine-tuning and quantizing the Llama3-8B\nmodel using mined datasets of REST API example values and inter-parameter\ndependencies-to generate realistic test inputs and uncover inter-parameter\ndependencies during the testing process by analyzing server responses. We\nevaluated LlamaRestTest on 12 real-world services (including popular services\nsuch as Spotify), comparing it against RESTGPT, a GPT-powered\nspecification-enhancement tool, as well as several state-of-the-art REST API\ntesting tools, including RESTler, MoRest, EvoMaster, and ARAT-RL. Our results\ndemonstrate that fine-tuning enables smaller models to outperform much larger\nmodels in detecting actionable parameter-dependency rules and generating valid\ninputs for REST API testing. We also evaluated different tool configurations,\nranging from the base Llama3-8B model to fine-tuned versions, and explored\nmultiple quantization techniques, including 2-bit, 4-bit, and 8-bit integer\nformats. Our study shows that small language models can perform as well as, or\nbetter than, large language models in REST API testing, balancing effectiveness\nand efficiency. Furthermore, LlamaRestTest outperforms state-of-the-art REST\nAPI testing tools in code coverage achieved and internal server errors\nidentified, even when those tools use RESTGPT-enhanced specifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern web services rely heavily on REST APIs, typically documented using the\nOpenAPI specification. The widespread adoption of this standard has resulted in\nthe development of many black-box testing tools that generate tests based on\nOpenAPI specifications. Although Large Language Models (LLMs) have shown\npromising test-generation abilities, their application to REST API testing\nremains mostly unexplored. We present LlamaRestTest, a novel approach that\nemploys two custom LLMs-created by fine-tuning and quantizing the Llama3-8B\nmodel using mined datasets of REST API example values and inter-parameter\ndependencies-to generate realistic test inputs and uncover inter-parameter\ndependencies during the testing process by analyzing server responses. We\nevaluated LlamaRestTest on 12 real-world services (including popular services\nsuch as Spotify), comparing it against RESTGPT, a GPT-powered\nspecification-enhancement tool, as well as several state-of-the-art REST API\ntesting tools, including RESTler, MoRest, EvoMaster, and ARAT-RL. Our results\ndemonstrate that fine-tuning enables smaller models to outperform much larger\nmodels in detecting actionable parameter-dependency rules and generating valid\ninputs for REST API testing. We also evaluated different tool configurations,\nranging from the base Llama3-8B model to fine-tuned versions, and explored\nmultiple quantization techniques, including 2-bit, 4-bit, and 8-bit integer\nformats. Our study shows that small language models can perform as well as, or\nbetter than, large language models in REST API testing, balancing effectiveness\nand efficiency. Furthermore, LlamaRestTest outperforms state-of-the-art REST\nAPI testing tools in code coverage achieved and internal server errors\nidentified, even when those tools use RESTGPT-enhanced specifications."
                },
                "authors": [
                    {
                        "name": "Myeongsoo Kim"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Alessandro Orso"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Orso"
                },
                "author": "Alessandro Orso",
                "arxiv_comment": "To be published in the ACM International Conference on the\n  Foundations of Software Engineering (FSE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08598v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08598v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.20052v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.20052v3",
                "updated": "2025-04-03T19:31:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    19,
                    31,
                    53,
                    3,
                    93,
                    0
                ],
                "published": "2024-06-28T17:03:51Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    17,
                    3,
                    51,
                    4,
                    180,
                    0
                ],
                "title": "Understanding and Mitigating Language Confusion in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Mitigating Language Confusion in LLMs"
                },
                "summary": "We investigate a surprising limitation of LLMs: their inability to\nconsistently generate text in a user's desired language. We create the Language\nConfusion Benchmark (LCB) to evaluate such failures, covering 15 typologically\ndiverse languages with existing and newly-created English and multilingual\nprompts. We evaluate a range of LLMs on monolingual and cross-lingual\ngeneration reflecting practical use cases, finding that Llama Instruct and\nMistral models exhibit high degrees of language confusion and even the\nstrongest models fail to consistently respond in the correct language. We\nobserve that base and English-centric instruct models are more prone to\nlanguage confusion, which is aggravated by complex prompts and high sampling\ntemperatures. We find that language confusion can be partially mitigated via\nfew-shot prompting, multilingual SFT and preference tuning. We release our\nlanguage confusion benchmark, which serves as a first layer of efficient,\nscalable multilingual evaluation at\nhttps://github.com/for-ai/language-confusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a surprising limitation of LLMs: their inability to\nconsistently generate text in a user's desired language. We create the Language\nConfusion Benchmark (LCB) to evaluate such failures, covering 15 typologically\ndiverse languages with existing and newly-created English and multilingual\nprompts. We evaluate a range of LLMs on monolingual and cross-lingual\ngeneration reflecting practical use cases, finding that Llama Instruct and\nMistral models exhibit high degrees of language confusion and even the\nstrongest models fail to consistently respond in the correct language. We\nobserve that base and English-centric instruct models are more prone to\nlanguage confusion, which is aggravated by complex prompts and high sampling\ntemperatures. We find that language confusion can be partially mitigated via\nfew-shot prompting, multilingual SFT and preference tuning. We release our\nlanguage confusion benchmark, which serves as a first layer of efficient,\nscalable multilingual evaluation at\nhttps://github.com/for-ai/language-confusion."
                },
                "authors": [
                    {
                        "name": "Kelly Marchisio"
                    },
                    {
                        "name": "Wei-Yin Ko"
                    },
                    {
                        "name": "Alexandre Brard"
                    },
                    {
                        "name": "Tho Dehaze"
                    },
                    {
                        "name": "Sebastian Ruder"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Ruder"
                },
                "author": "Sebastian Ruder",
                "arxiv_comment": "EMNLP 2024 Main Conference Camera-ready. v3: hi, ru not run for\n  monolingual Okapi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.20052v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.20052v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02984v1",
                "updated": "2025-04-03T19:18:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    19,
                    18,
                    11,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T19:18:11Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    19,
                    18,
                    11,
                    3,
                    93,
                    0
                ],
                "title": "Language Models Guidance with Multi-Aspect-Cueing: A Case Study for\n  Competitor Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Guidance with Multi-Aspect-Cueing: A Case Study for\n  Competitor Analysis"
                },
                "summary": "Competitor analysis is essential in modern business due to the influence of\nindustry rivals on strategic planning. It involves assessing multiple aspects\nand balancing trade-offs to make informed decisions. Recent Large Language\nModels (LLMs) have demonstrated impressive capabilities to reason about such\ntrade-offs but grapple with inherent limitations such as a lack of knowledge\nabout contemporary or future realities and an incomplete understanding of a\nmarket's competitive landscape. In this paper, we address this gap by\nincorporating business aspects into LLMs to enhance their understanding of a\ncompetitive market. Through quantitative and qualitative experiments, we\nillustrate how integrating such aspects consistently improves model\nperformance, thereby enhancing analytical efficacy in competitor analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitor analysis is essential in modern business due to the influence of\nindustry rivals on strategic planning. It involves assessing multiple aspects\nand balancing trade-offs to make informed decisions. Recent Large Language\nModels (LLMs) have demonstrated impressive capabilities to reason about such\ntrade-offs but grapple with inherent limitations such as a lack of knowledge\nabout contemporary or future realities and an incomplete understanding of a\nmarket's competitive landscape. In this paper, we address this gap by\nincorporating business aspects into LLMs to enhance their understanding of a\ncompetitive market. Through quantitative and qualitative experiments, we\nillustrate how integrating such aspects consistently improves model\nperformance, thereby enhancing analytical efficacy in competitor analysis."
                },
                "authors": [
                    {
                        "name": "Amir Hadifar"
                    },
                    {
                        "name": "Christopher Ochs"
                    },
                    {
                        "name": "Arjan Van Ewijk"
                    }
                ],
                "author_detail": {
                    "name": "Arjan Van Ewijk"
                },
                "author": "Arjan Van Ewijk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02965v1",
                "updated": "2025-04-03T18:34:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    34,
                    36,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:34:36Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    34,
                    36,
                    3,
                    93,
                    0
                ],
                "title": "CoLa -- Learning to Interactively Collaborate with Large LMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoLa -- Learning to Interactively Collaborate with Large LMs"
                },
                "summary": "LLMs' remarkable ability to tackle a wide range of language tasks opened new\nopportunities for collaborative human-AI problem solving. LLMs can amplify\nhuman capabilities by applying their intuitions and reasoning strategies at\nscale. We explore whether human guides can be simulated, by generalizing from\nhuman demonstrations of guiding an AI system to solve complex language\nproblems. We introduce CoLa, a novel self-guided learning paradigm for training\nautomated $\\textit{guides}$ and evaluate it on two QA datasets, a\npuzzle-solving task, and a constrained text generation task. Our empirical\nresults show that CoLa consistently outperforms competitive approaches across\nall domains. Moreover, a small-sized trained guide outperforms a strong model\nlike GPT-4 when acting as a guide. We compare the strategies employed by humans\nand automated guides by conducting a human study on a QA dataset. We show that\nautomated guides outperform humans by adapting their strategies to reasoners'\ncapabilities and conduct qualitative analyses highlighting distinct differences\nin guiding strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs' remarkable ability to tackle a wide range of language tasks opened new\nopportunities for collaborative human-AI problem solving. LLMs can amplify\nhuman capabilities by applying their intuitions and reasoning strategies at\nscale. We explore whether human guides can be simulated, by generalizing from\nhuman demonstrations of guiding an AI system to solve complex language\nproblems. We introduce CoLa, a novel self-guided learning paradigm for training\nautomated $\\textit{guides}$ and evaluate it on two QA datasets, a\npuzzle-solving task, and a constrained text generation task. Our empirical\nresults show that CoLa consistently outperforms competitive approaches across\nall domains. Moreover, a small-sized trained guide outperforms a strong model\nlike GPT-4 when acting as a guide. We compare the strategies employed by humans\nand automated guides by conducting a human study on a QA dataset. We show that\nautomated guides outperform humans by adapting their strategies to reasoners'\ncapabilities and conduct qualitative analyses highlighting distinct differences\nin guiding strategies."
                },
                "authors": [
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Dan Goldwasser"
                    }
                ],
                "author_detail": {
                    "name": "Dan Goldwasser"
                },
                "author": "Dan Goldwasser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02964v1",
                "updated": "2025-04-03T18:33:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    33,
                    3,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:33:03Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    33,
                    3,
                    3,
                    93,
                    0
                ],
                "title": "Distributionally Robust Predictive Runtime Verification under\n  Spatio-Temporal Logic Specifications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributionally Robust Predictive Runtime Verification under\n  Spatio-Temporal Logic Specifications"
                },
                "summary": "Cyber-physical systems designed in simulators, often consisting of multiple\ninteracting agents, behave differently in the real-world. We would like to\nverify these systems during runtime when they are deployed. Thus, we propose\nrobust predictive runtime verification (RPRV) algorithms for: (1) general\nstochastic CPS under signal temporal logic (STL) tasks, and (2) stochastic\nmulti-agent systems (MAS) under spatio-temporal logic tasks. The RPRV problem\npresents the following challenges: (1) there may not be sufficient data on the\nbehavior of the deployed CPS, (2) predictive models based on design phase\nsystem trajectories may encounter distribution shift during real-world\ndeployment, and (3) the algorithms need to scale to the complexity of MAS and\nbe applicable to spatio-temporal logic tasks. To address these challenges, we\nassume knowledge of an upper bound on the statistical distance (in terms of an\nf-divergence) between the trajectory distributions of the system at deployment\nand design time. We are motivated by our prior work [1, 2] where we proposed an\naccurate and an interpretable RPRV algorithm for general CPS, which we here\nextend to the MAS setting and spatio-temporal logic tasks. Specifically, we use\na learned predictive model to estimate the system behavior at runtime and\nrobust conformal prediction to obtain probabilistic guarantees by accounting\nfor distribution shifts. Building on [1], we perform robust conformal\nprediction over the robust semantics of spatio-temporal reach and escape logic\n(STREL) to obtain centralized RPRV algorithms for MAS. We empirically validate\nour results in a drone swarm simulator, where we show the scalability of our\nRPRV algorithms to MAS and analyze the impact of different trajectory\npredictors on the verification result. To the best of our knowledge, these are\nthe first statistically valid algorithms for MAS under distribution shift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber-physical systems designed in simulators, often consisting of multiple\ninteracting agents, behave differently in the real-world. We would like to\nverify these systems during runtime when they are deployed. Thus, we propose\nrobust predictive runtime verification (RPRV) algorithms for: (1) general\nstochastic CPS under signal temporal logic (STL) tasks, and (2) stochastic\nmulti-agent systems (MAS) under spatio-temporal logic tasks. The RPRV problem\npresents the following challenges: (1) there may not be sufficient data on the\nbehavior of the deployed CPS, (2) predictive models based on design phase\nsystem trajectories may encounter distribution shift during real-world\ndeployment, and (3) the algorithms need to scale to the complexity of MAS and\nbe applicable to spatio-temporal logic tasks. To address these challenges, we\nassume knowledge of an upper bound on the statistical distance (in terms of an\nf-divergence) between the trajectory distributions of the system at deployment\nand design time. We are motivated by our prior work [1, 2] where we proposed an\naccurate and an interpretable RPRV algorithm for general CPS, which we here\nextend to the MAS setting and spatio-temporal logic tasks. Specifically, we use\na learned predictive model to estimate the system behavior at runtime and\nrobust conformal prediction to obtain probabilistic guarantees by accounting\nfor distribution shifts. Building on [1], we perform robust conformal\nprediction over the robust semantics of spatio-temporal reach and escape logic\n(STREL) to obtain centralized RPRV algorithms for MAS. We empirically validate\nour results in a drone swarm simulator, where we show the scalability of our\nRPRV algorithms to MAS and analyze the impact of different trajectory\npredictors on the verification result. To the best of our knowledge, these are\nthe first statistically valid algorithms for MAS under distribution shift."
                },
                "authors": [
                    {
                        "name": "Yiqi Zhao"
                    },
                    {
                        "name": "Emily Zhu"
                    },
                    {
                        "name": "Bardh Hoxha"
                    },
                    {
                        "name": "Georgios Fainekos"
                    },
                    {
                        "name": "Jyotirmoy V. Deshmukh"
                    },
                    {
                        "name": "Lars Lindemann"
                    }
                ],
                "author_detail": {
                    "name": "Lars Lindemann"
                },
                "author": "Lars Lindemann",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.09482",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]