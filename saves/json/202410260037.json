[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2404.15420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v2",
                "updated": "2024-10-24T16:40:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    40,
                    10,
                    3,
                    298,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v1",
                "updated": "2024-10-24T10:36:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a Base Station (BS) associated with local cache,\nwhich in turn is connected to the backend server and users. The contents get\ncontinuously updated at the backend server, and the BS has a local copy of the\nsubset of the contents. Upon receiving a request from the user, the BS can\neither fetch a fresh version or, serve the local copy or can wait for\nadditional requests before serving. Fetching content from the BS incurs a fixed\nfetching cost, serving it locally incurs an aging cost, and for each request\nwaiting at the BS, there will be a waiting for cost per unit time. The aging\ncost relies on the freshness of the content, which is measured by a metric age\nof version (AoV). We aim to minimize the average cost subject to cache capacity\nconstraints. We pose the problem as Restless Multi-armed Bandits Problem (RMAB)\nand propose a Whittle index-based policy that performs very close to the\noptimal policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a Base Station (BS) associated with local cache,\nwhich in turn is connected to the backend server and users. The contents get\ncontinuously updated at the backend server, and the BS has a local copy of the\nsubset of the contents. Upon receiving a request from the user, the BS can\neither fetch a fresh version or, serve the local copy or can wait for\nadditional requests before serving. Fetching content from the BS incurs a fixed\nfetching cost, serving it locally incurs an aging cost, and for each request\nwaiting at the BS, there will be a waiting for cost per unit time. The aging\ncost relies on the freshness of the content, which is measured by a metric age\nof version (AoV). We aim to minimize the average cost subject to cache capacity\nconstraints. We pose the problem as Restless Multi-armed Bandits Problem (RMAB)\nand propose a Whittle index-based policy that performs very close to the\noptimal policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18441v1",
                "updated": "2024-10-24T05:29:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:29:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI"
                },
                "summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings."
                },
                "authors": [
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18248v1",
                "updated": "2024-10-23T19:53:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T19:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Efficient Inference for Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference for Augmented Large Language Models"
                },
                "summary": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM."
                },
                "authors": [
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Cong Liang"
                    },
                    {
                        "name": "Shiji Xin"
                    },
                    {
                        "name": "Qianru Lao"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v1",
                "updated": "2024-10-23T14:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v3",
                "updated": "2024-10-23T10:39:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    39,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v5",
                "updated": "2024-10-23T05:55:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    55,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14740v2",
                "updated": "2024-10-23T01:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    1,
                    8,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    33,
                    39,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD."
                },
                "authors": [
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhang Cao"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Zhichao Cao"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11724v2",
                "updated": "2024-10-22T19:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    22,
                    19,
                    7,
                    8,
                    1,
                    296,
                    0
                ],
                "published": "2024-05-20T01:57:34Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    1,
                    57,
                    34,
                    0,
                    141,
                    0
                ],
                "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-wise Influential Training Data Retrieval for Large Language Models"
                },
                "summary": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Jikai Long"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v5",
                "updated": "2024-10-21T22:56:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    22,
                    56,
                    6,
                    0,
                    295,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16218v1",
                "updated": "2024-10-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T17:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire"
                },
                "summary": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Swarnav Mukhopadhyay"
                    },
                    {
                        "name": "Md Mobinul Haque"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v1",
                "updated": "2024-10-21T16:44:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v2",
                "updated": "2024-10-21T15:59:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    59,
                    18,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v1",
                "updated": "2024-10-21T11:29:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14142v2",
                "updated": "2024-10-21T07:24:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    24,
                    53,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-18T03:30:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels"
                },
                "summary": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed."
                },
                "authors": [
                    {
                        "name": "Tianqing Zhou"
                    },
                    {
                        "name": "Bobo Wang"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Xuefang Nie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15704v1",
                "updated": "2024-10-21T07:20:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T07:20:41Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "title": "Residual vector quantization for KV cache compression in large language\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual vector quantization for KV cache compression in large language\n  model"
                },
                "summary": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision."
                },
                "authors": [
                    {
                        "name": "Ankur Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Kumar"
                },
                "author": "Ankur Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v2",
                "updated": "2024-10-21T05:06:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    5,
                    6,
                    1,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v2",
                "updated": "2024-10-21T02:35:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    35,
                    8,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04053v2",
                "updated": "2024-10-20T13:37:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    13,
                    37,
                    46,
                    6,
                    294,
                    0
                ],
                "published": "2024-07-04T16:51:17Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    16,
                    51,
                    17,
                    3,
                    186,
                    0
                ],
                "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI: A Taxonomy, Systematic Review and Future Directions"
                },
                "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions."
                },
                "authors": [
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Junhui Du"
                    },
                    {
                        "name": "Huaming Wu"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Subramaniam Subramanian Murugesan"
                    },
                    {
                        "name": "Babar Ali"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Prabal Verma"
                    },
                    {
                        "name": "Surendra Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1007/s10586-024-04686-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-024-04686-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in Springer Cluster\n  Computing, 2024",
                "arxiv_journal_ref": "Springer Cluster Computing, Volume 28, article number 18, pages\n  11953 - 11981, (2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15344v1",
                "updated": "2024-10-20T09:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T09:37:07Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "title": "LLC Intra-set Write Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLC Intra-set Write Balancing"
                },
                "summary": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance."
                },
                "authors": [
                    {
                        "name": "Keshav Krishna"
                    },
                    {
                        "name": "Ayush Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Verma"
                },
                "author": "Ayush Verma",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v1",
                "updated": "2024-10-20T08:42:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15252v1",
                "updated": "2024-10-20T02:17:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T02:17:35Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "title": "Lossless KV Cache Compression to 2%",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless KV Cache Compression to 2%"
                },
                "summary": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "J. N. Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.05579v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.05579v4",
                "updated": "2024-10-19T12:15:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    12,
                    15,
                    50,
                    5,
                    293,
                    0
                ],
                "published": "2022-06-11T17:52:10Z",
                "published_parsed": [
                    2022,
                    6,
                    11,
                    17,
                    52,
                    10,
                    5,
                    162,
                    0
                ],
                "title": "Online Paging with Heterogeneous Cache Slots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Paging with Heterogeneous Cache Slots"
                },
                "summary": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized)."
                },
                "authors": [
                    {
                        "name": "Marek Chrobak"
                    },
                    {
                        "name": "Samuel Haney"
                    },
                    {
                        "name": "Mehraneh Liaee"
                    },
                    {
                        "name": "Debmalya Panigrahi"
                    },
                    {
                        "name": "Rajmohan Rajaraman"
                    },
                    {
                        "name": "Ravi Sundaram"
                    },
                    {
                        "name": "Neal E. Young"
                    }
                ],
                "author_detail": {
                    "name": "Neal E. Young"
                },
                "author": "Neal E. Young",
                "arxiv_doi": "10.1007/s00453-024-01270-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00453-024-01270-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2206.05579v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.05579v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "conference and journal versions appear in STACS 2023 and Algorithmica\n  (2004)",
                "arxiv_journal_ref": "Algorithmica (2004)",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0; F.1.2; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v2",
                "updated": "2024-10-19T08:45:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    8,
                    45,
                    11,
                    5,
                    293,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v3",
                "updated": "2024-10-18T19:30:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    19,
                    30,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "4th NeurIPS Efficient Natural Language and Speech Processing Workshop\n  (ENLSP-IV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v2",
                "updated": "2024-10-18T13:59:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    59,
                    54,
                    4,
                    292,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v1",
                "updated": "2024-10-17T20:11:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_comment": "RTSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v1",
                "updated": "2024-10-17T17:58:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction"
                },
                "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v4",
                "updated": "2024-10-17T15:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    15,
                    27,
                    30,
                    3,
                    291,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07979v2",
                "updated": "2024-10-17T08:54:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    54,
                    37,
                    3,
                    291,
                    0
                ],
                "published": "2024-04-11T17:57:22Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    57,
                    22,
                    3,
                    102,
                    0
                ],
                "title": "LLoCO: Learning Long Contexts Offline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLoCO: Learning Long Contexts Offline"
                },
                "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco."
                },
                "authors": [
                    {
                        "name": "Sijun Tan"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Shishir Patil"
                    },
                    {
                        "name": "Ziyang Wu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Raluca Ada Popa"
                    }
                ],
                "author_detail": {
                    "name": "Raluca Ada Popa"
                },
                "author": "Raluca Ada Popa",
                "arxiv_comment": "EMNLP 2024. The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18126v1",
                "updated": "2024-10-17T04:37:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:37:43Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "title": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers"
                },
                "summary": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shubham"
                    },
                    {
                        "name": "Keichi Takahashi"
                    },
                    {
                        "name": "Hiroyuki Takizawa"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Takizawa"
                },
                "author": "Hiroyuki Takizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13212v1",
                "updated": "2024-10-17T04:35:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:35:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations"
                },
                "summary": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters."
                },
                "authors": [
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v3",
                "updated": "2024-10-16T17:54:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    54,
                    15,
                    2,
                    290,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Accepted to PVLDB Volume 18",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12749v1",
                "updated": "2024-10-16T17:10:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:10:48Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "title": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM"
                },
                "summary": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads."
                },
                "authors": [
                    {
                        "name": "Juechu Dong"
                    },
                    {
                        "name": "Jonah Rosenblum"
                    },
                    {
                        "name": "Satish Narayanasamy"
                    }
                ],
                "author_detail": {
                    "name": "Satish Narayanasamy"
                },
                "author": "Satish Narayanasamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12605v1",
                "updated": "2024-10-16T14:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "title": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques"
                },
                "summary": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Rishal Ravikesh Chand"
                    },
                    {
                        "name": "Neeraj Anand Sharma"
                    },
                    {
                        "name": "Muhammad Ashad Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ashad Kabir"
                },
                "author": "Muhammad Ashad Kabir",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v1",
                "updated": "2024-10-16T12:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "17 pages, 6 figures, Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12423v1",
                "updated": "2024-10-16T10:06:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:06:22Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "title": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors"
                },
                "summary": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency."
                },
                "authors": [
                    {
                        "name": "Qinghang Zhao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Yixi Ji"
                    },
                    {
                        "name": "Jinjian Wu"
                    },
                    {
                        "name": "Guangming Shi"
                    }
                ],
                "author_detail": {
                    "name": "Guangming Shi"
                },
                "author": "Guangming Shi",
                "arxiv_doi": "10.1145/3676536.3676710",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676710",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.12423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14731v1",
                "updated": "2024-10-16T08:34:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T08:34:51Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection"
                },
                "summary": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base."
                },
                "authors": [
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Zipeng Xiao"
                    },
                    {
                        "name": "Siqi Kou"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12168v1",
                "updated": "2024-10-16T02:16:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T02:16:53Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "title": "COMET: Towards Partical W4A4KV4 LLMs Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMET: Towards Partical W4A4KV4 LLMs Serving"
                },
                "summary": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective."
                },
                "authors": [
                    {
                        "name": "Lian Liu"
                    },
                    {
                        "name": "Haimeng Ren"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Zhaohui Xu"
                    },
                    {
                        "name": "Yudong Pan"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Xiaowei Li"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang",
                "arxiv_comment": "14 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v2",
                "updated": "2024-10-15T15:58:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    58,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11417v1",
                "updated": "2024-10-15T09:07:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T09:07:25Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "title": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models"
                },
                "summary": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09297v3",
                "updated": "2024-10-15T08:45:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    45,
                    18,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-13T16:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    33,
                    44,
                    3,
                    165,
                    0
                ],
                "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding"
                },
                "summary": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv"
                },
                "authors": [
                    {
                        "name": "Zayd Muhammad Kawakibi Zuhri"
                    },
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v2",
                "updated": "2024-10-15T06:09:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    6,
                    9,
                    35,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v1",
                "updated": "2024-10-15T05:57:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v3",
                "updated": "2024-10-15T05:34:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    34,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11260v1",
                "updated": "2024-10-15T04:35:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T04:35:49Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "title": "A Zoned Storage Optimized Flash Cache on ZNS SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zoned Storage Optimized Flash Cache on ZNS SSDs"
                },
                "summary": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme."
                },
                "authors": [
                    {
                        "name": "Chongzhuo Yang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v4",
                "updated": "2024-10-14T19:12:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    12,
                    48,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10819v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads"
                },
                "summary": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v1",
                "updated": "2024-10-14T17:50:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10511v1",
                "updated": "2024-10-14T13:49:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:49:06Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling"
                },
                "summary": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities."
                },
                "authors": [
                    {
                        "name": "Wenze Liu"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Sheng Xia"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "19 pages, 17 figures, 8 tables, github repo:\n  https://github.com/poppuppy/SAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v2",
                "updated": "2024-10-14T09:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    35,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13378v2",
                "updated": "2024-10-14T07:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    58,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-22T06:19:43Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    6,
                    19,
                    43,
                    2,
                    143,
                    0
                ],
                "title": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation"
                },
                "summary": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency."
                },
                "authors": [
                    {
                        "name": "Quyang Pan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhiyuan Wu"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Jingyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Wang"
                },
                "author": "Jingyuan Wang",
                "arxiv_comment": "17 pages, 7 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10157v1",
                "updated": "2024-10-14T04:49:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "title": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI"
                },
                "summary": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI."
                },
                "authors": [
                    {
                        "name": "Meng Gao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Huafu Li"
                    },
                    {
                        "name": "Junqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Junqi Guo"
                },
                "author": "Junqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10149v1",
                "updated": "2024-10-14T04:30:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:30:38Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "title": "Fast and Accurate Neural Rendering Using Semi-Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Neural Rendering Using Semi-Gradients"
                },
                "summary": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss."
                },
                "authors": [
                    {
                        "name": "In-Young Cho"
                    },
                    {
                        "name": "Jaewoong Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoong Cho"
                },
                "author": "Jaewoong Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10071v1",
                "updated": "2024-10-14T01:25:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T01:25:56Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "title": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning"
                },
                "summary": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jinjin Shen"
                    },
                    {
                        "name": "Yan Lin"
                    },
                    {
                        "name": "Yijin Zhang"
                    },
                    {
                        "name": "Weibin Zhang"
                    },
                    {
                        "name": "Feng Shu"
                    },
                    {
                        "name": "Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Jun Li"
                },
                "author": "Jun Li",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09533v1",
                "updated": "2024-10-12T13:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T13:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "title": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence"
                },
                "summary": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24"
                },
                "authors": [
                    {
                        "name": "Felipe Cadar"
                    },
                    {
                        "name": "Guilherme Potje"
                    },
                    {
                        "name": "Renato Martins"
                    },
                    {
                        "name": "Cédric Demonceaux"
                    },
                    {
                        "name": "Erickson R. Nascimento"
                    }
                ],
                "author_detail": {
                    "name": "Erickson R. Nascimento"
                },
                "author": "Erickson R. Nascimento",
                "arxiv_comment": "Accepted in ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v1",
                "updated": "2024-10-12T10:38:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09397v1",
                "updated": "2024-10-12T07:01:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T07:01:30Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v3",
                "updated": "2024-10-12T02:11:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    2,
                    11,
                    14,
                    5,
                    286,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09237v1",
                "updated": "2024-10-11T20:23:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T20:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "title": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor"
                },
                "summary": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}."
                },
                "authors": [
                    {
                        "name": "Sahar Ahmadi"
                    },
                    {
                        "name": "Ali Cheraghian"
                    },
                    {
                        "name": "Morteza Saberi"
                    },
                    {
                        "name": "Md. Towsif Abir"
                    },
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Farookh Hussain"
                    },
                    {
                        "name": "Shafin Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Shafin Rahman"
                },
                "author": "Shafin Rahman",
                "arxiv_comment": "ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08895v1",
                "updated": "2024-10-11T15:12:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:12:30Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "title": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation"
                },
                "summary": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Kun Ding"
                    },
                    {
                        "name": "Qiang Yu"
                    },
                    {
                        "name": "Haojian Zhang"
                    },
                    {
                        "name": "Gaofeng Meng"
                    },
                    {
                        "name": "Shiming Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Shiming Xiang"
                },
                "author": "Shiming Xiang",
                "arxiv_comment": "submitted to IJCV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v1",
                "updated": "2024-10-11T12:19:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtárik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtárik"
                },
                "author": "Peter Richtárik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08618v1",
                "updated": "2024-10-11T08:33:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T08:33:58Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "title": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination"
                },
                "summary": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Qiulin Tian"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Tong Xin"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v1",
                "updated": "2024-10-11T07:24:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03462v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03462v3",
                "updated": "2024-10-11T02:18:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    2,
                    18,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-01-07T11:57:40Z",
                "published_parsed": [
                    2024,
                    1,
                    7,
                    11,
                    57,
                    40,
                    6,
                    7,
                    0
                ],
                "title": "Long Context Compression with Activation Beacon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Compression with Activation Beacon"
                },
                "summary": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}."
                },
                "authors": [
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Ninglu Shao"
                    },
                    {
                        "name": "Qiwei Ye"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Newer version of Activation Beacon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03462v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03462v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08391v1",
                "updated": "2024-10-10T21:55:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T21:55:11Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "title": "KV Prediction for Improved Time to First Token",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Prediction for Improved Time to First Token"
                },
                "summary": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction ."
                },
                "authors": [
                    {
                        "name": "Maxwell Horton"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Chenfan Sun"
                    },
                    {
                        "name": "Yanzi Jin"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Moin Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Moin Nabi"
                },
                "author": "Moin Nabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v3",
                "updated": "2024-10-10T16:57:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    57,
                    34,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations"
                },
                "summary": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12850v1",
                "updated": "2024-10-10T15:24:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    24,
                    12,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:24:12Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    24,
                    12,
                    3,
                    284,
                    0
                ],
                "title": "RecurFormer: Not All Transformer Heads Need Self-Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecurFormer: Not All Transformer Heads Need Self-Attention"
                },
                "summary": "Transformer-based large language models (LLMs) excel in modeling complex\nlanguage patterns but face significant computational costs during inference,\nespecially with long inputs due to the attention mechanism's memory overhead.\nWe observe that certain attention heads exhibit a distribution where the\nattention weights concentrate on tokens near the query token, termed as recency\naware, which focuses on local and short-range dependencies. Leveraging this\ninsight, we propose RecurFormer, a novel architecture that replaces these\nattention heads with linear recurrent neural networks (RNNs), specifically the\nMamba architecture. This replacement reduces the cache size without evicting\ntokens, thus maintaining generation quality. RecurFormer retains the ability to\nmodel long-range dependencies through the remaining attention heads and allows\nfor reusing pre-trained Transformer-based LLMs weights with continual training.\nExperiments demonstrate that RecurFormer matches the original model's\nperformance while significantly enhancing inference efficiency. Our approach\nprovides a practical solution to the computational challenges of\nTransformer-based LLMs inference, making it highly attractive for tasks\ninvolving long inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) excel in modeling complex\nlanguage patterns but face significant computational costs during inference,\nespecially with long inputs due to the attention mechanism's memory overhead.\nWe observe that certain attention heads exhibit a distribution where the\nattention weights concentrate on tokens near the query token, termed as recency\naware, which focuses on local and short-range dependencies. Leveraging this\ninsight, we propose RecurFormer, a novel architecture that replaces these\nattention heads with linear recurrent neural networks (RNNs), specifically the\nMamba architecture. This replacement reduces the cache size without evicting\ntokens, thus maintaining generation quality. RecurFormer retains the ability to\nmodel long-range dependencies through the remaining attention heads and allows\nfor reusing pre-trained Transformer-based LLMs weights with continual training.\nExperiments demonstrate that RecurFormer matches the original model's\nperformance while significantly enhancing inference efficiency. Our approach\nprovides a practical solution to the computational challenges of\nTransformer-based LLMs inference, making it highly attractive for tasks\ninvolving long inputs."
                },
                "authors": [
                    {
                        "name": "Ruiqing Yan"
                    },
                    {
                        "name": "Linghan Zheng"
                    },
                    {
                        "name": "Xingbo Du"
                    },
                    {
                        "name": "Han Zou"
                    },
                    {
                        "name": "Yufeng Guo"
                    },
                    {
                        "name": "Jianfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Yang"
                },
                "author": "Jianfei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01195v2",
                "updated": "2024-10-10T11:01:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    1,
                    44,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-03T10:58:32Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    10,
                    58,
                    32,
                    0,
                    155,
                    0
                ],
                "title": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping"
                },
                "summary": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Qijie Ge"
                    },
                    {
                        "name": "Lulu Suo"
                    },
                    {
                        "name": "Weijie Tang"
                    },
                    {
                        "name": "Zhengyu Wei"
                    },
                    {
                        "name": "Longxiang Huang"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04793v2",
                "updated": "2024-10-10T05:11:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    5,
                    11,
                    52,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-07T03:08:14Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    3,
                    8,
                    14,
                    6,
                    98,
                    0
                ],
                "title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget"
                },
                "summary": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention."
                },
                "authors": [
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Shaoduo Gan"
                    }
                ],
                "author_detail": {
                    "name": "Shaoduo Gan"
                },
                "author": "Shaoduo Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07590v1",
                "updated": "2024-10-10T03:52:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:52:54Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text"
                },
                "summary": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems."
                },
                "authors": [
                    {
                        "name": "Songshuo Lu"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Yutian Rong"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07579v1",
                "updated": "2024-10-10T03:28:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:28:46Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "title": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching"
                },
                "summary": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy."
                },
                "authors": [
                    {
                        "name": "Ruonan Yu"
                    },
                    {
                        "name": "Songhua Liu"
                    },
                    {
                        "name": "Jingwen Ye"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted by ECCV2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19519v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19519v3",
                "updated": "2024-10-09T15:57:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    57,
                    3,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-28T15:52:15Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    15,
                    52,
                    15,
                    3,
                    88,
                    0
                ],
                "title": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage"
                },
                "summary": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser."
                },
                "authors": [
                    {
                        "name": "Philip Wykeham Bradford"
                    },
                    {
                        "name": "Valeria Ospina-Bohorquez"
                    },
                    {
                        "name": "Michael Ehret"
                    },
                    {
                        "name": "Jose-Luis Henares"
                    },
                    {
                        "name": "Pilar Puyuelo-Valdes"
                    },
                    {
                        "name": "Tomasz Chodukowski"
                    },
                    {
                        "name": "Tadeusz Pisarczyk"
                    },
                    {
                        "name": "Zofia Rusiniak"
                    },
                    {
                        "name": "Carlos Salgado-Lopez"
                    },
                    {
                        "name": "Christos Vlachos"
                    },
                    {
                        "name": "Massimiliano Sciscio"
                    },
                    {
                        "name": "Martina Salvadori"
                    },
                    {
                        "name": "Claudio Verona"
                    },
                    {
                        "name": "George Hicks"
                    },
                    {
                        "name": "Oliver Ettlinger"
                    },
                    {
                        "name": "Zulfikar Najmudin"
                    },
                    {
                        "name": "Jean-Raphael Marques"
                    },
                    {
                        "name": "Laurent Gremillet"
                    },
                    {
                        "name": "Joao Jorge Santos"
                    },
                    {
                        "name": "Fabrizio Consoli"
                    },
                    {
                        "name": "Vladimir Tikhonchuk"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Tikhonchuk"
                },
                "author": "Vladimir Tikhonchuk",
                "arxiv_comment": "18 pages (total), 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19519v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19519v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06934v1",
                "updated": "2024-10-09T14:28:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks"
                },
                "summary": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies."
                },
                "authors": [
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Xiaolong Xu"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Xiangwei Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Siyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Siyu Wu"
                },
                "author": "Siyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v3",
                "updated": "2024-10-09T11:40:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    40,
                    31,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience."
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06627v1",
                "updated": "2024-10-09T07:22:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T07:22:40Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "title": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions"
                },
                "summary": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections."
                },
                "authors": [
                    {
                        "name": "Muhammad Morshed Alam"
                    },
                    {
                        "name": "Muhammad Yeasir Aarafat"
                    },
                    {
                        "name": "Tamim Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Hossain"
                },
                "author": "Tamim Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13941v2",
                "updated": "2024-10-09T04:11:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    4,
                    11,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-06-20T02:20:21Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    2,
                    20,
                    21,
                    3,
                    172,
                    0
                ],
                "title": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture"
                },
                "summary": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Haobin Tan"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Pavan Balaji"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Balaji"
                },
                "author": "Pavan Balaji",
                "arxiv_doi": "10.1145/3649329.3658266",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3658266",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.13941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by DAC 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06497v1",
                "updated": "2024-10-09T02:51:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T02:51:27Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "title": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System"
                },
                "summary": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements."
                },
                "authors": [
                    {
                        "name": "Fang Zhou"
                    },
                    {
                        "name": "Yaning Huang"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Dai Li"
                    },
                    {
                        "name": "Zhongke Zhang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Xiao Xin"
                    },
                    {
                        "name": "Abdallah Aboelela"
                    },
                    {
                        "name": "Zheliang Jiang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Jeff Song"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Huayu Li"
                    },
                    {
                        "name": "ChongLin Sun"
                    },
                    {
                        "name": "Hang Yang"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Zhan Shu"
                    },
                    {
                        "name": "Mindi Yuan"
                    },
                    {
                        "name": "Emanuele Maccherani"
                    },
                    {
                        "name": "Taha Hayat"
                    },
                    {
                        "name": "John Guo"
                    },
                    {
                        "name": "Varna Puvvada"
                    },
                    {
                        "name": "Uladzimir Pashkevich"
                    }
                ],
                "author_detail": {
                    "name": "Uladzimir Pashkevich"
                },
                "author": "Uladzimir Pashkevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v4",
                "updated": "2024-10-09T01:12:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    1,
                    12,
                    19,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "arxiv_comment": "Accepted at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01527v2",
                "updated": "2024-10-08T19:34:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    19,
                    34,
                    3,
                    1,
                    282,
                    0
                ],
                "published": "2024-07-01T17:59:47Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    59,
                    47,
                    0,
                    183,
                    0
                ],
                "title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches"
                },
                "summary": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench."
                },
                "authors": [
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Songchen Li"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Duy Le"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05927v1",
                "updated": "2024-10-08T11:28:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T11:28:30Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "title": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications"
                },
                "summary": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kourtzanidis"
                    },
                    {
                        "name": "Panagiotis Dimitrakellis"
                    },
                    {
                        "name": "Dimitrios Rakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Rakopoulos"
                },
                "author": "Dimitrios Rakopoulos",
                "arxiv_comment": "Submitted to IEEE Transactions on Dielectrics and Electrical\n  Insulation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05863v1",
                "updated": "2024-10-08T09:53:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "title": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework"
                },
                "summary": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates."
                },
                "authors": [
                    {
                        "name": "Yunfei Yang"
                    },
                    {
                        "name": "Zhenghao Qi"
                    },
                    {
                        "name": "Honghuan Wu"
                    },
                    {
                        "name": "Qi Song"
                    },
                    {
                        "name": "Tieyao Zhang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yimin Tu"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ben Wang"
                },
                "author": "Ben Wang",
                "arxiv_comment": "CIKM 2024 applied research track, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05854v1",
                "updated": "2024-10-08T09:46:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "title": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks"
                },
                "summary": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs."
                },
                "authors": [
                    {
                        "name": "Ruben Hias"
                    },
                    {
                        "name": "Weihong Wang"
                    },
                    {
                        "name": "Jan Vanhoof"
                    },
                    {
                        "name": "Tom Van Cutsem"
                    }
                ],
                "author_detail": {
                    "name": "Tom Van Cutsem"
                },
                "author": "Tom Van Cutsem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12018v2",
                "updated": "2024-10-08T04:25:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    4,
                    25,
                    41,
                    1,
                    282,
                    0
                ],
                "published": "2024-06-17T18:34:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    34,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling"
                },
                "summary": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS."
                },
                "authors": [
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Xiyuan Zou"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Marc-Antoine Rondeau"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Jackie Chi Kit Cheung"
                },
                "author": "Jackie Chi Kit Cheung",
                "arxiv_comment": "EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v1",
                "updated": "2024-10-07T17:59:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs"
                },
                "summary": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "A PTQ method to significantly boost the performance of static\n  activation quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05516v3",
                "updated": "2024-10-07T17:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    57,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-09T09:55:07Z",
                "published_parsed": [
                    2023,
                    12,
                    9,
                    9,
                    55,
                    7,
                    5,
                    343,
                    0
                ],
                "title": "Stateful Large Language Model Serving with Pensieve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful Large Language Model Serving with Pensieve"
                },
                "summary": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency."
                },
                "authors": [
                    {
                        "name": "Lingfan Yu"
                    },
                    {
                        "name": "Jinkun Lin"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "arxiv_doi": "10.1145/3689031.3696086",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3696086",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00161v2",
                "updated": "2024-10-07T15:07:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    7,
                    9,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-30T19:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    9,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head"
                },
                "summary": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches."
                },
                "authors": [
                    {
                        "name": "Isaac Rehg"
                    }
                ],
                "author_detail": {
                    "name": "Isaac Rehg"
                },
                "author": "Isaac Rehg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05076v1",
                "updated": "2024-10-07T14:30:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:30:27Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention"
                },
                "summary": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x."
                },
                "authors": [
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Zikun Li"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05033v1",
                "updated": "2024-10-07T13:33:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:33:23Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "title": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design"
                },
                "summary": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2212.12475",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05004v1",
                "updated": "2024-10-07T13:03:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:03:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "Fast State Restoration in LLM Serving with HCache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast State Restoration in LLM Serving with HCache"
                },
                "summary": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT."
                },
                "authors": [
                    {
                        "name": "Shiwei Gao"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jiwu Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwu Shu"
                },
                "author": "Jiwu Shu",
                "arxiv_comment": "EuroSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v3",
                "updated": "2024-10-07T01:27:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    1,
                    27,
                    59,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v1",
                "updated": "2024-10-06T19:36:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04252v1",
                "updated": "2024-10-05T18:20:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "published": "2024-10-05T18:20:37Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "title": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation"
                },
                "summary": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC."
                },
                "authors": [
                    {
                        "name": "Yusuke Teranishi"
                    },
                    {
                        "name": "Shoma Hiraoka"
                    },
                    {
                        "name": "Wataru Mizukami"
                    },
                    {
                        "name": "Masao Okita"
                    },
                    {
                        "name": "Fumihiko Ino"
                    }
                ],
                "author_detail": {
                    "name": "Fumihiko Ino"
                },
                "author": "Fumihiko Ino",
                "arxiv_comment": "24 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.18975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18975v1",
                "updated": "2024-10-24T17:59:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    59,
                    31,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:59:31Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    59,
                    31,
                    3,
                    298,
                    0
                ],
                "title": "Unbounded: A Generative Infinite Game of Character Life Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unbounded: A Generative Infinite Game of Character Life Simulation"
                },
                "summary": "We introduce the concept of a generative infinite game, a video game that\ntranscends the traditional boundaries of finite, hard-coded systems by using\ngenerative models. Inspired by James P. Carse's distinction between finite and\ninfinite games, we leverage recent advances in generative AI to create\nUnbounded: a game of character life simulation that is fully encapsulated in\ngenerative models. Specifically, Unbounded draws inspiration from sandbox life\nsimulations and allows you to interact with your autonomous virtual character\nin a virtual world by feeding, playing with and guiding it - with open-ended\nmechanics generated by an LLM, some of which can be emergent. In order to\ndevelop Unbounded, we propose technical innovations in both the LLM and visual\ngeneration domains. Specifically, we present: (1) a specialized, distilled\nlarge language model (LLM) that dynamically generates game mechanics,\nnarratives, and character interactions in real-time, and (2) a new dynamic\nregional image prompt Adapter (IP-Adapter) for vision models that ensures\nconsistent yet flexible visual generation of a character across multiple\nenvironments. We evaluate our system through both qualitative and quantitative\nanalysis, showing significant improvements in character life simulation, user\ninstruction following, narrative coherence, and visual consistency for both\ncharacters and the environments compared to traditional related approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the concept of a generative infinite game, a video game that\ntranscends the traditional boundaries of finite, hard-coded systems by using\ngenerative models. Inspired by James P. Carse's distinction between finite and\ninfinite games, we leverage recent advances in generative AI to create\nUnbounded: a game of character life simulation that is fully encapsulated in\ngenerative models. Specifically, Unbounded draws inspiration from sandbox life\nsimulations and allows you to interact with your autonomous virtual character\nin a virtual world by feeding, playing with and guiding it - with open-ended\nmechanics generated by an LLM, some of which can be emergent. In order to\ndevelop Unbounded, we propose technical innovations in both the LLM and visual\ngeneration domains. Specifically, we present: (1) a specialized, distilled\nlarge language model (LLM) that dynamically generates game mechanics,\nnarratives, and character interactions in real-time, and (2) a new dynamic\nregional image prompt Adapter (IP-Adapter) for vision models that ensures\nconsistent yet flexible visual generation of a character across multiple\nenvironments. We evaluate our system through both qualitative and quantitative\nanalysis, showing significant improvements in character life simulation, user\ninstruction following, narrative coherence, and visual consistency for both\ncharacters and the environments compared to traditional related approaches."
                },
                "authors": [
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Yuanzhen Li"
                    },
                    {
                        "name": "Neal Wadhwa"
                    },
                    {
                        "name": "Yael Pritch"
                    },
                    {
                        "name": "David E. Jacobs"
                    },
                    {
                        "name": "Michael Rubinstein"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Nataniel Ruiz"
                    }
                ],
                "author_detail": {
                    "name": "Nataniel Ruiz"
                },
                "author": "Nataniel Ruiz",
                "arxiv_comment": "18 pages; Project page: https://generative-infinite-game.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18973v1",
                "updated": "2024-10-24T17:59:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    59,
                    23,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:59:23Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    59,
                    23,
                    3,
                    298,
                    0
                ],
                "title": "Tuning-free coreset Markov chain Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tuning-free coreset Markov chain Monte Carlo"
                },
                "summary": "A Bayesian coreset is a small, weighted subset of a data set that replaces\nthe full data during inference to reduce computational cost. The\nstate-of-the-art coreset construction algorithm, Coreset Markov chain Monte\nCarlo (Coreset MCMC), uses draws from an adaptive Markov chain targeting the\ncoreset posterior to train the coreset weights via stochastic gradient\noptimization. However, the quality of the constructed coreset, and thus the\nquality of its posterior approximation, is sensitive to the stochastic\noptimization learning rate. In this work, we propose a learning-rate-free\nstochastic gradient optimization procedure, Hot-start Distance over Gradient\n(Hot DoG), for training coreset weights in Coreset MCMC without user tuning\neffort. Empirical results demonstrate that Hot DoG provides higher quality\nposterior approximations than other learning-rate-free stochastic gradient\nmethods, and performs competitively to optimally-tuned ADAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian coreset is a small, weighted subset of a data set that replaces\nthe full data during inference to reduce computational cost. The\nstate-of-the-art coreset construction algorithm, Coreset Markov chain Monte\nCarlo (Coreset MCMC), uses draws from an adaptive Markov chain targeting the\ncoreset posterior to train the coreset weights via stochastic gradient\noptimization. However, the quality of the constructed coreset, and thus the\nquality of its posterior approximation, is sensitive to the stochastic\noptimization learning rate. In this work, we propose a learning-rate-free\nstochastic gradient optimization procedure, Hot-start Distance over Gradient\n(Hot DoG), for training coreset weights in Coreset MCMC without user tuning\neffort. Empirical results demonstrate that Hot DoG provides higher quality\nposterior approximations than other learning-rate-free stochastic gradient\nmethods, and performs competitively to optimally-tuned ADAM."
                },
                "authors": [
                    {
                        "name": "Naitong Chen"
                    },
                    {
                        "name": "Jonathan H. Huggins"
                    },
                    {
                        "name": "Trevor Campbell"
                    }
                ],
                "author_detail": {
                    "name": "Trevor Campbell"
                },
                "author": "Trevor Campbell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18971v1",
                "updated": "2024-10-24T17:59:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    59,
                    21,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:59:21Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    59,
                    21,
                    3,
                    298,
                    0
                ],
                "title": "Detection of Undeclared EV Charging Events in a Green Energy\n  Certification Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of Undeclared EV Charging Events in a Green Energy\n  Certification Scheme"
                },
                "summary": "The green potential of electric vehicles (EVs) can be fully realized only if\ntheir batteries are charged using energy generated from renewable (i.e. green)\nsources. For logistic or economic reasons, however, EV drivers may be tempted\nto avoid charging stations certified as providing green energy, instead opting\nfor conventional ones, where only a fraction of the available energy is green.\nThis behaviour may slow down the achievement of decarbonisation targets of the\nroad transport sector. In this paper, we use GPS data to infer whether an\nundeclared charging event has occurred. Specifically, we construct a Bayesian\nhypothesis test for the charging behaviour of the EV. Extensive simulations are\ncarried out for an area of London, using the mobility simulator, SUMO, and\nexploring various operating conditions. Excellent detection rates for\nundeclared charging events are reported. We explain how the algorithm can serve\nas the basis for an incentivization scheme, encouraging compliance by drivers\nwith green charging policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The green potential of electric vehicles (EVs) can be fully realized only if\ntheir batteries are charged using energy generated from renewable (i.e. green)\nsources. For logistic or economic reasons, however, EV drivers may be tempted\nto avoid charging stations certified as providing green energy, instead opting\nfor conventional ones, where only a fraction of the available energy is green.\nThis behaviour may slow down the achievement of decarbonisation targets of the\nroad transport sector. In this paper, we use GPS data to infer whether an\nundeclared charging event has occurred. Specifically, we construct a Bayesian\nhypothesis test for the charging behaviour of the EV. Extensive simulations are\ncarried out for an area of London, using the mobility simulator, SUMO, and\nexploring various operating conditions. Excellent detection rates for\nundeclared charging events are reported. We explain how the algorithm can serve\nas the basis for an incentivization scheme, encouraging compliance by drivers\nwith green charging policies."
                },
                "authors": [
                    {
                        "name": "Luca Domenico Loiacono"
                    },
                    {
                        "name": "Anthony Quinn"
                    },
                    {
                        "name": "Emanuele Crisostomi"
                    },
                    {
                        "name": "Robert Shorten"
                    }
                ],
                "author_detail": {
                    "name": "Robert Shorten"
                },
                "author": "Robert Shorten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14322v2",
                "updated": "2024-10-24T17:59:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    59,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2023-05-23T17:53:38Z",
                "published_parsed": [
                    2023,
                    5,
                    23,
                    17,
                    53,
                    38,
                    1,
                    143,
                    0
                ],
                "title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RET-LLM: Towards a General Read-Write Memory for Large Language Models"
                },
                "summary": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP) through their extensive parameters and comprehensive\ndata utilization. However, existing LLMs lack a dedicated memory unit, limiting\ntheir ability to explicitly store and retrieve knowledge for various tasks. In\nthis paper, we propose RET-LLM a novel framework that equips LLMs with a\ngeneral write-read memory unit, allowing them to extract, store, and recall\nknowledge from the text as needed for task performance. Inspired by Davidsonian\nsemantics theory, we extract and save knowledge in the form of triplets. The\nmemory unit is designed to be scalable, aggregatable, updatable, and\ninterpretable. Through qualitative evaluations, we demonstrate the superiority\nof our proposed framework over baseline approaches in question answering tasks.\nMoreover, our framework exhibits robust performance in handling temporal-based\nquestion answering tasks, showcasing its ability to effectively manage\ntime-dependent information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP) through their extensive parameters and comprehensive\ndata utilization. However, existing LLMs lack a dedicated memory unit, limiting\ntheir ability to explicitly store and retrieve knowledge for various tasks. In\nthis paper, we propose RET-LLM a novel framework that equips LLMs with a\ngeneral write-read memory unit, allowing them to extract, store, and recall\nknowledge from the text as needed for task performance. Inspired by Davidsonian\nsemantics theory, we extract and save knowledge in the form of triplets. The\nmemory unit is designed to be scalable, aggregatable, updatable, and\ninterpretable. Through qualitative evaluations, we demonstrate the superiority\nof our proposed framework over baseline approaches in question answering tasks.\nMoreover, our framework exhibits robust performance in handling temporal-based\nquestion answering tasks, showcasing its ability to effectively manage\ntime-dependent information."
                },
                "authors": [
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Ayyoob Imani"
                    },
                    {
                        "name": "Mohsen Fayyaz"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "arxiv_comment": "NOTE: This concept paper outlines an initial methodology, now evolved\n  and thoroughly evaluated in the MemLLM paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10870v2",
                "updated": "2024-10-24T17:58:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    58,
                    52,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-08T13:41:08Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    13,
                    41,
                    8,
                    1,
                    282,
                    0
                ],
                "title": "PortLLM: Personalizing Evolving Large Language Models with Training-Free\n  and Portable Model Patches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PortLLM: Personalizing Evolving Large Language Models with Training-Free\n  and Portable Model Patches"
                },
                "summary": "As large language models (LLMs) increasingly shape the AI landscape,\nfine-tuning pretrained models has become more popular than in the pre-LLM era\nfor achieving optimal performance in domain-specific tasks. However, pretrained\nLLMs such as ChatGPT are periodically evolved, i.e., model parameters are\nfrequently updated), making it challenging for downstream users with limited\nresources to keep up with fine-tuning the newest LLMs for their domain\napplication. Even though fine-tuning costs have nowadays been reduced thanks to\nthe innovations of parameter-efficient fine-tuning such as LoRA, not all\ndownstream users have adequate computing for frequent personalization.\nMoreover, access to fine-tuning datasets, particularly in sensitive domains\nsuch as healthcare, could be time-restrictive, making it crucial to retain the\nknowledge encoded in earlier fine-tuned rounds for future adaptation. In this\npaper, we present PortLLM, a training-free framework that (i) creates an\ninitial lightweight model update patch to capture domain-specific knowledge,\nand (ii) allows a subsequent seamless plugging for the continual\npersonalization of evolved LLM at minimal cost. Our extensive experiments cover\nseven representative datasets, from easier question-answering tasks {BoolQ,\nSST2} to harder reasoning tasks {WinoGrande, GSM8K}, and models including\n{Mistral-7B, Llama2, Llama3.1, and Gemma2}, validating the portability of our\ndesigned model patches and showcasing the effectiveness of our proposed\nframework. For instance, PortLLM achieves comparable performance to LoRA\nfine-tuning with reductions of up to 12.2x in GPU memory usage. Finally, we\nprovide theoretical justifications to understand the portability of our model\nupdate patches, which offers new insights into the theoretical dimension of\nLLMs' personalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly shape the AI landscape,\nfine-tuning pretrained models has become more popular than in the pre-LLM era\nfor achieving optimal performance in domain-specific tasks. However, pretrained\nLLMs such as ChatGPT are periodically evolved, i.e., model parameters are\nfrequently updated), making it challenging for downstream users with limited\nresources to keep up with fine-tuning the newest LLMs for their domain\napplication. Even though fine-tuning costs have nowadays been reduced thanks to\nthe innovations of parameter-efficient fine-tuning such as LoRA, not all\ndownstream users have adequate computing for frequent personalization.\nMoreover, access to fine-tuning datasets, particularly in sensitive domains\nsuch as healthcare, could be time-restrictive, making it crucial to retain the\nknowledge encoded in earlier fine-tuned rounds for future adaptation. In this\npaper, we present PortLLM, a training-free framework that (i) creates an\ninitial lightweight model update patch to capture domain-specific knowledge,\nand (ii) allows a subsequent seamless plugging for the continual\npersonalization of evolved LLM at minimal cost. Our extensive experiments cover\nseven representative datasets, from easier question-answering tasks {BoolQ,\nSST2} to harder reasoning tasks {WinoGrande, GSM8K}, and models including\n{Mistral-7B, Llama2, Llama3.1, and Gemma2}, validating the portability of our\ndesigned model patches and showcasing the effectiveness of our proposed\nframework. For instance, PortLLM achieves comparable performance to LoRA\nfine-tuning with reductions of up to 12.2x in GPU memory usage. Finally, we\nprovide theoretical justifications to understand the portability of our model\nupdate patches, which offers new insights into the theoretical dimension of\nLLMs' personalization."
                },
                "authors": [
                    {
                        "name": "Rana Muhammad Shahroz Khan"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Sukwon Yun"
                    },
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Shahriar Nirjon"
                    },
                    {
                        "name": "Chau-Wai Wong"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18966v1",
                "updated": "2024-10-24T17:58:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    58,
                    22,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:58:22Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    58,
                    22,
                    3,
                    298,
                    0
                ],
                "title": "Does Data Contamination Detection Work (Well) for LLMs? A Survey and\n  Evaluation on Detection Assumptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Data Contamination Detection Work (Well) for LLMs? A Survey and\n  Evaluation on Detection Assumptions"
                },
                "summary": "Large language models (LLMs) have demonstrated great performance across\nvarious benchmarks, showing potential as general-purpose task solvers. However,\nas LLMs are typically trained on vast amounts of data, a significant concern in\ntheir evaluation is data contamination, where overlap between training data and\nevaluation datasets inflates performance assessments. While multiple approaches\nhave been developed to identify data contamination, these approaches rely on\nspecific assumptions that may not hold universally across different settings.\nTo bridge this gap, we systematically review 47 papers on data contamination\ndetection, categorize the underlying assumptions, and assess whether they have\nbeen rigorously validated. We identify and analyze eight categories of\nassumptions and test three of them as case studies. Our analysis reveals that\nwhen classifying instances used for pretraining LLMs, detection approaches\nbased on these three assumptions perform close to random guessing, suggesting\nthat current LLMs learn data distributions rather than memorizing individual\ninstances. Overall, this work underscores the importance of approaches clearly\nstating their underlying assumptions and testing their validity across various\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated great performance across\nvarious benchmarks, showing potential as general-purpose task solvers. However,\nas LLMs are typically trained on vast amounts of data, a significant concern in\ntheir evaluation is data contamination, where overlap between training data and\nevaluation datasets inflates performance assessments. While multiple approaches\nhave been developed to identify data contamination, these approaches rely on\nspecific assumptions that may not hold universally across different settings.\nTo bridge this gap, we systematically review 47 papers on data contamination\ndetection, categorize the underlying assumptions, and assess whether they have\nbeen rigorously validated. We identify and analyze eight categories of\nassumptions and test three of them as case studies. Our analysis reveals that\nwhen classifying instances used for pretraining LLMs, detection approaches\nbased on these three assumptions perform close to random guessing, suggesting\nthat current LLMs learn data distributions rather than memorizing individual\ninstances. Overall, this work underscores the importance of approaches clearly\nstating their underlying assumptions and testing their validity across various\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yujuan Fu"
                    },
                    {
                        "name": "Ozlem Uzuner"
                    },
                    {
                        "name": "Meliha Yetisgen"
                    },
                    {
                        "name": "Fei Xia"
                    }
                ],
                "author_detail": {
                    "name": "Fei Xia"
                },
                "author": "Fei Xia",
                "arxiv_comment": "2 tables and 1 figures in the main text. This is a preprint, under\n  review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18963v1",
                "updated": "2024-10-24T17:58:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    58,
                    8,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:58:08Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    58,
                    8,
                    3,
                    298,
                    0
                ],
                "title": "OSCAR: Operating System Control via State-Aware Reasoning and\n  Re-Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSCAR: Operating System Control via State-Aware Reasoning and\n  Re-Planning"
                },
                "summary": "Large language models (LLMs) and large multimodal models (LMMs) have shown\ngreat potential in automating complex tasks like web browsing and gaming.\nHowever, their ability to generalize across diverse applications remains\nlimited, hindering broader utility. To address this challenge, we present\nOSCAR: Operating System Control via state-Aware reasoning and Re-planning.\nOSCAR is a generalist agent designed to autonomously navigate and interact with\nvarious desktop and mobile applications through standardized controls, such as\nmouse and keyboard inputs, while processing screen images to fulfill user\ncommands. OSCAR translates human instructions into executable Python code,\nenabling precise control over graphical user interfaces (GUIs). To enhance\nstability and adaptability, OSCAR operates as a state machine, equipped with\nerror-handling mechanisms and dynamic task re-planning, allowing it to\nefficiently adjust to real-time feedback and exceptions. We demonstrate OSCAR's\neffectiveness through extensive experiments on diverse benchmarks across\ndesktop and mobile platforms, where it transforms complex workflows into simple\nnatural language commands, significantly boosting user productivity. Our code\nwill be open-source upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and large multimodal models (LMMs) have shown\ngreat potential in automating complex tasks like web browsing and gaming.\nHowever, their ability to generalize across diverse applications remains\nlimited, hindering broader utility. To address this challenge, we present\nOSCAR: Operating System Control via state-Aware reasoning and Re-planning.\nOSCAR is a generalist agent designed to autonomously navigate and interact with\nvarious desktop and mobile applications through standardized controls, such as\nmouse and keyboard inputs, while processing screen images to fulfill user\ncommands. OSCAR translates human instructions into executable Python code,\nenabling precise control over graphical user interfaces (GUIs). To enhance\nstability and adaptability, OSCAR operates as a state machine, equipped with\nerror-handling mechanisms and dynamic task re-planning, allowing it to\nefficiently adjust to real-time feedback and exceptions. We demonstrate OSCAR's\neffectiveness through extensive experiments on diverse benchmarks across\ndesktop and mobile platforms, where it transforms complex workflows into simple\nnatural language commands, significantly boosting user productivity. Our code\nwill be open-source upon publication."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Wang"
                    },
                    {
                        "name": "Bang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bang Liu"
                },
                "author": "Bang Liu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19388v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19388v2",
                "updated": "2024-10-24T17:56:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    56,
                    21,
                    3,
                    298,
                    0
                ],
                "published": "2024-06-27T17:58:54Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    17,
                    58,
                    54,
                    3,
                    179,
                    0
                ],
                "title": "Taming Data and Transformers for Audio Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Data and Transformers for Audio Generation"
                },
                "summary": "Generating ambient sounds is a challenging task due to data scarcity and\noften insufficient caption quality, making it difficult to employ large-scale\ngenerative models for the task. In this work, we tackle this problem by\nintroducing two new models. First, we propose AutoCap, a high-quality and\nefficient automatic audio captioning model. By using a compact audio\nrepresentation and leveraging audio metadata, AutoCap substantially enhances\ncaption quality, reaching a CIDEr score of 83.2, marking a 3.2% improvement\nfrom the best available captioning model at four times faster inference speed.\nSecond, we propose GenAu, a scalable transformer-based audio generation\narchitecture that we scale up to 1.25B parameters. Using AutoCap to generate\ncaption clips from existing audio datasets, we demonstrate the benefits of data\nscaling with synthetic captions as well as model size scaling. When compared to\nstate-of-the-art audio generators trained at similar size and data scale, GenAu\nobtains significant improvements of 4.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. Moreover, we propose an efficient and scalable\npipeline for collecting audio datasets, enabling us to compile 57M ambient\naudio clips, forming AutoReCap-XL, the largest available audio-text dataset, at\n90 times the scale of existing ones. Our code, model checkpoints, and dataset\nare publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ambient sounds is a challenging task due to data scarcity and\noften insufficient caption quality, making it difficult to employ large-scale\ngenerative models for the task. In this work, we tackle this problem by\nintroducing two new models. First, we propose AutoCap, a high-quality and\nefficient automatic audio captioning model. By using a compact audio\nrepresentation and leveraging audio metadata, AutoCap substantially enhances\ncaption quality, reaching a CIDEr score of 83.2, marking a 3.2% improvement\nfrom the best available captioning model at four times faster inference speed.\nSecond, we propose GenAu, a scalable transformer-based audio generation\narchitecture that we scale up to 1.25B parameters. Using AutoCap to generate\ncaption clips from existing audio datasets, we demonstrate the benefits of data\nscaling with synthetic captions as well as model size scaling. When compared to\nstate-of-the-art audio generators trained at similar size and data scale, GenAu\nobtains significant improvements of 4.7% in FAD score, 22.7% in IS, and 13.5%\nin CLAP score, indicating significantly improved quality of generated audio\ncompared to previous works. Moreover, we propose an efficient and scalable\npipeline for collecting audio datasets, enabling us to compile 57M ambient\naudio clips, forming AutoReCap-XL, the largest available audio-text dataset, at\n90 times the scale of existing ones. Our code, model checkpoints, and dataset\nare publicly available."
                },
                "authors": [
                    {
                        "name": "Moayed Haji-Ali"
                    },
                    {
                        "name": "Willi Menapace"
                    },
                    {
                        "name": "Aliaksandr Siarohin"
                    },
                    {
                        "name": "Guha Balakrishnan"
                    },
                    {
                        "name": "Sergey Tulyakov"
                    },
                    {
                        "name": "Vicente Ordonez"
                    }
                ],
                "author_detail": {
                    "name": "Vicente Ordonez"
                },
                "author": "Vicente Ordonez",
                "arxiv_comment": "Project Webpage: https://snap-research.github.io/GenAU/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19388v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11029v2",
                "updated": "2024-10-24T17:56:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    56,
                    14,
                    3,
                    298,
                    0
                ],
                "published": "2024-08-20T17:30:48Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    30,
                    48,
                    1,
                    233,
                    0
                ],
                "title": "Scaling Law with Learning Rate Annealing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Law with Learning Rate Annealing"
                },
                "summary": "We find that the cross-entropy loss curves of neural language models\nempirically adhere to a scaling law with learning rate (LR) annealing over\ntraining steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where\n$L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR\ncurve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are\nconstant parameters. This formulation takes into account two factors: (1)\npower-law scaling over data size, and (2) the additional loss reduction during\nLR annealing. Therefore, this formulation can describe the full loss curve at\neach step, rather than the single loss point at the end of training. Applying\nthe scaling law with LR annealing and fitting only one or two training curves,\nwe can accurately predict the loss at any given step across any learning rate\nscheduler (LRS). This approach significantly reduces computational cost in\nformulating scaling laws while providing more accuracy and expressiveness for\ntraining dynamics. Extensive experiments demonstrate that our findings hold\nacross a range of hyper-parameters and model architectures, and our equation\ncan extend to scaling effect of model sizes. Moreover, our formulation provides\naccurate theoretical verification and explanation for empirical results\nobserved in numerous previous studies, particularly those focusing on LR\nschedule and annealing. We believe that this work is promising to enhance the\nunderstanding of LLM training dynamics while greatly democratizing scaling\nlaws, and it can guide researchers in refining training strategies (e.g.\ncritical LRS) for further LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We find that the cross-entropy loss curves of neural language models\nempirically adhere to a scaling law with learning rate (LR) annealing over\ntraining steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where\n$L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR\ncurve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are\nconstant parameters. This formulation takes into account two factors: (1)\npower-law scaling over data size, and (2) the additional loss reduction during\nLR annealing. Therefore, this formulation can describe the full loss curve at\neach step, rather than the single loss point at the end of training. Applying\nthe scaling law with LR annealing and fitting only one or two training curves,\nwe can accurately predict the loss at any given step across any learning rate\nscheduler (LRS). This approach significantly reduces computational cost in\nformulating scaling laws while providing more accuracy and expressiveness for\ntraining dynamics. Extensive experiments demonstrate that our findings hold\nacross a range of hyper-parameters and model architectures, and our equation\ncan extend to scaling effect of model sizes. Moreover, our formulation provides\naccurate theoretical verification and explanation for empirical results\nobserved in numerous previous studies, particularly those focusing on LR\nschedule and annealing. We believe that this work is promising to enhance the\nunderstanding of LLM training dynamics while greatly democratizing scaling\nlaws, and it can guide researchers in refining training strategies (e.g.\ncritical LRS) for further LLMs."
                },
                "authors": [
                    {
                        "name": "Howe Tissue"
                    },
                    {
                        "name": "Venus Wang"
                    },
                    {
                        "name": "Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Wang"
                },
                "author": "Lu Wang",
                "arxiv_comment": "Add more experiments to consolidate our scaling laws. 29 pages, 29\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18959v1",
                "updated": "2024-10-24T17:56:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    56,
                    8,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:56:08Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    56,
                    8,
                    3,
                    298,
                    0
                ],
                "title": "Context is Key: A Benchmark for Forecasting with Essential Textual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context is Key: A Benchmark for Forecasting with Essential Textual\n  Information"
                },
                "summary": "Forecasting is a critical task in decision making across various domains.\nWhile numerical data provides a foundation, it often lacks crucial context\nnecessary for accurate predictions. Human forecasters frequently rely on\nadditional information, such as background knowledge or constraints, which can\nbe efficiently communicated through natural language. However, the ability of\nexisting forecasting models to effectively integrate this textual information\nremains an open question. To address this, we introduce \"Context is Key\" (CiK),\na time series forecasting benchmark that pairs numerical data with diverse\ntypes of carefully crafted textual context, requiring models to integrate both\nmodalities. We evaluate a range of approaches, including statistical models,\ntime series foundation models, and LLM-based forecasters, and propose a simple\nyet effective LLM prompting method that outperforms all other tested methods on\nour benchmark. Our experiments highlight the importance of incorporating\ncontextual information, demonstrate surprising performance when using LLM-based\nforecasting models, and also reveal some of their critical shortcomings. By\npresenting this benchmark, we aim to advance multimodal forecasting, promoting\nmodels that are both accurate and accessible to decision-makers with varied\ntechnical expertise. The benchmark can be visualized at\nhttps://servicenow.github.io/context-is-key-forecasting/v0/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting is a critical task in decision making across various domains.\nWhile numerical data provides a foundation, it often lacks crucial context\nnecessary for accurate predictions. Human forecasters frequently rely on\nadditional information, such as background knowledge or constraints, which can\nbe efficiently communicated through natural language. However, the ability of\nexisting forecasting models to effectively integrate this textual information\nremains an open question. To address this, we introduce \"Context is Key\" (CiK),\na time series forecasting benchmark that pairs numerical data with diverse\ntypes of carefully crafted textual context, requiring models to integrate both\nmodalities. We evaluate a range of approaches, including statistical models,\ntime series foundation models, and LLM-based forecasters, and propose a simple\nyet effective LLM prompting method that outperforms all other tested methods on\nour benchmark. Our experiments highlight the importance of incorporating\ncontextual information, demonstrate surprising performance when using LLM-based\nforecasting models, and also reveal some of their critical shortcomings. By\npresenting this benchmark, we aim to advance multimodal forecasting, promoting\nmodels that are both accurate and accessible to decision-makers with varied\ntechnical expertise. The benchmark can be visualized at\nhttps://servicenow.github.io/context-is-key-forecasting/v0/ ."
                },
                "authors": [
                    {
                        "name": "Andrew Robert Williams"
                    },
                    {
                        "name": "Arjun Ashok"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "Jithendaraa Subramanian"
                    },
                    {
                        "name": "Roland Riachi"
                    },
                    {
                        "name": "James Requeima"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    },
                    {
                        "name": "Irina Rish"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Alexandre Drouin"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Drouin"
                },
                "author": "Alexandre Drouin",
                "arxiv_comment": "Preprint; under review. First two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18957v1",
                "updated": "2024-10-24T17:55:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    55,
                    3,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:55:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    55,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "Bridge-Coder: Unlocking LLMs' Potential to Overcome Language Gaps in\n  Low-Resource Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridge-Coder: Unlocking LLMs' Potential to Overcome Language Gaps in\n  Low-Resource Code"
                },
                "summary": "Large Language Models (LLMs) demonstrate strong proficiency in generating\ncode for high-resource programming languages (HRPLs) like Python but struggle\nsignificantly with low-resource programming languages (LRPLs) such as Racket or\nD. This performance gap deepens the digital divide, preventing developers using\nLRPLs from benefiting equally from LLM advancements and reinforcing disparities\nin innovation within underrepresented programming communities. While generating\nadditional training data for LRPLs is promising, it faces two key challenges:\nmanual annotation is labor-intensive and costly, and LLM-generated LRPL code is\noften of subpar quality. The underlying cause of this issue is the gap between\nnatural language to programming language gap (NL-PL Gap), which is especially\npronounced in LRPLs due to limited aligned data. In this work, we introduce a\nnovel approach called Bridge-Coder, which leverages LLMs' intrinsic\ncapabilities to enhance the performance on LRPLs. Our method consists of two\nkey stages. Bridge Generation, where we create high-quality dataset by\nutilizing LLMs' general knowledge understanding, proficiency in HRPLs, and\nin-context learning abilities. Then, we apply the Bridged Alignment, which\nprogressively improves the alignment between NL instructions and LRPLs.\nExperimental results across multiple LRPLs show that Bridge-Coder significantly\nenhances model performance, demonstrating the effectiveness and generalization\nof our approach. Furthermore, we offer a detailed analysis of the key\ncomponents of our method, providing valuable insights for future work aimed at\naddressing the challenges associated with LRPLs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate strong proficiency in generating\ncode for high-resource programming languages (HRPLs) like Python but struggle\nsignificantly with low-resource programming languages (LRPLs) such as Racket or\nD. This performance gap deepens the digital divide, preventing developers using\nLRPLs from benefiting equally from LLM advancements and reinforcing disparities\nin innovation within underrepresented programming communities. While generating\nadditional training data for LRPLs is promising, it faces two key challenges:\nmanual annotation is labor-intensive and costly, and LLM-generated LRPL code is\noften of subpar quality. The underlying cause of this issue is the gap between\nnatural language to programming language gap (NL-PL Gap), which is especially\npronounced in LRPLs due to limited aligned data. In this work, we introduce a\nnovel approach called Bridge-Coder, which leverages LLMs' intrinsic\ncapabilities to enhance the performance on LRPLs. Our method consists of two\nkey stages. Bridge Generation, where we create high-quality dataset by\nutilizing LLMs' general knowledge understanding, proficiency in HRPLs, and\nin-context learning abilities. Then, we apply the Bridged Alignment, which\nprogressively improves the alignment between NL instructions and LRPLs.\nExperimental results across multiple LRPLs show that Bridge-Coder significantly\nenhances model performance, demonstrating the effectiveness and generalization\nof our approach. Furthermore, we offer a detailed analysis of the key\ncomponents of our method, providing valuable insights for future work aimed at\naddressing the challenges associated with LRPLs."
                },
                "authors": [
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Jianshu Zhang"
                    },
                    {
                        "name": "Yuanzhe Li"
                    },
                    {
                        "name": "Renjie Pi"
                    },
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Runtao Liu"
                    },
                    {
                        "name": "Ziqiang Zheng"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18955v1",
                "updated": "2024-10-24T17:53:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    53,
                    53,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:53:53Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    53,
                    53,
                    3,
                    298,
                    0
                ],
                "title": "BioMistral-NLU: Towards More Generalizable Medical Language\n  Understanding through Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioMistral-NLU: Towards More Generalizable Medical Language\n  Understanding through Instruction Tuning"
                },
                "summary": "Large language models (LLMs) such as ChatGPT are fine-tuned on large and\ndiverse instruction-following corpora, and can generalize to new tasks.\nHowever, those instruction-tuned LLMs often perform poorly in specialized\nmedical natural language understanding (NLU) tasks that require domain\nknowledge, granular text comprehension, and structured data extraction. To\nbridge the gap, we: (1) propose a unified prompting format for 7 important NLU\ntasks, % through span extraction and multi-choice question-answering (QA), (2)\ncurate an instruction-tuning dataset, MNLU-Instruct, utilizing diverse existing\nopen-source medical NLU corpora, and (3) develop BioMistral-NLU, a\ngeneralizable medical NLU model, through fine-tuning BioMistral on\nMNLU-Instruct. We evaluate BioMistral-NLU in a zero-shot setting, across 6\nimportant NLU tasks, from two widely adopted medical NLU benchmarks: Biomedical\nLanguage Understanding Evaluation (BLUE) and Biomedical Language Understanding\nand Reasoning Benchmark (BLURB). Our experiments show that our BioMistral-NLU\noutperforms the original BioMistral, as well as the proprietary LLMs - ChatGPT\nand GPT-4. Our dataset-agnostic prompting strategy and instruction tuning step\nover diverse NLU tasks enhance LLMs' generalizability across diverse medical\nNLU tasks. Our ablation experiments show that instruction-tuning on a wider\nvariety of tasks, even when the total number of training instances remains\nconstant, enhances downstream zero-shot generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as ChatGPT are fine-tuned on large and\ndiverse instruction-following corpora, and can generalize to new tasks.\nHowever, those instruction-tuned LLMs often perform poorly in specialized\nmedical natural language understanding (NLU) tasks that require domain\nknowledge, granular text comprehension, and structured data extraction. To\nbridge the gap, we: (1) propose a unified prompting format for 7 important NLU\ntasks, % through span extraction and multi-choice question-answering (QA), (2)\ncurate an instruction-tuning dataset, MNLU-Instruct, utilizing diverse existing\nopen-source medical NLU corpora, and (3) develop BioMistral-NLU, a\ngeneralizable medical NLU model, through fine-tuning BioMistral on\nMNLU-Instruct. We evaluate BioMistral-NLU in a zero-shot setting, across 6\nimportant NLU tasks, from two widely adopted medical NLU benchmarks: Biomedical\nLanguage Understanding Evaluation (BLUE) and Biomedical Language Understanding\nand Reasoning Benchmark (BLURB). Our experiments show that our BioMistral-NLU\noutperforms the original BioMistral, as well as the proprietary LLMs - ChatGPT\nand GPT-4. Our dataset-agnostic prompting strategy and instruction tuning step\nover diverse NLU tasks enhance LLMs' generalizability across diverse medical\nNLU tasks. Our ablation experiments show that instruction-tuning on a wider\nvariety of tasks, even when the total number of training instances remains\nconstant, enhances downstream zero-shot generalization."
                },
                "authors": [
                    {
                        "name": "Yujuan Velvin Fu"
                    },
                    {
                        "name": "Giridhar Kaushik Ramachandran"
                    },
                    {
                        "name": "Namu Park"
                    },
                    {
                        "name": "Kevin Lybarger"
                    },
                    {
                        "name": "Fei Xia"
                    },
                    {
                        "name": "Ozlem Uzuner"
                    },
                    {
                        "name": "Meliha Yetisgen"
                    }
                ],
                "author_detail": {
                    "name": "Meliha Yetisgen"
                },
                "author": "Meliha Yetisgen",
                "arxiv_comment": "3 figures an 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08123v2",
                "updated": "2024-10-24T17:53:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    53,
                    21,
                    3,
                    298,
                    0
                ],
                "published": "2024-05-13T19:06:12Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    19,
                    6,
                    12,
                    0,
                    134,
                    0
                ],
                "title": "Modeling sea ice in the marginal ice zone as a dense granular flow with\n  rheology inferred from discrete element model data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling sea ice in the marginal ice zone as a dense granular flow with\n  rheology inferred from discrete element model data"
                },
                "summary": "The marginal ice zone (MIZ) represents the periphery of the sea ice cover. In\nthis region, the macroscale behavior of the sea ice results from collisions and\nenduring contact between ice floes. This configuration closely resembles that\nof dense granular flows, which have been modeled successfully with the $\\mu(I)$\nrheology. Here, we present a continuum model based on the $\\mu(I)$ rheology\nwhich treats sea ice as a compressible fluid, with the local sea ice\nconcentration given by a dilatancy function $\\Phi(I)$. We infer expressions for\n$\\mu(I)$ and $\\Phi(I)$ by nonlinear regression using data produced with a\ndiscrete element method (DEM) which considers polygonal-shaped ice floes. We do\nthis by driving the sea ice with a one-dimensional shearing ocean current. The\nresulting continuum model is a nonlinear system of equations with the sea ice\nvelocity, local concentration, and pressure as unknowns. The rheology is given\nby the sum of a plastic and a viscous term. In the context of a periodic patch\nof ocean, which is effectively a one dimensional problem, and under steady\nconditions, we prove this system to be well-posed, present a numerical\nalgorithm for solving it, and compare its solutions to those of the DEM. These\ncomparisons demonstrate the continuum model's ability to capture most of the\nDEM's results accurately. The continuum model is particularly accurate for\nocean currents faster than 0.25 m/s; however, for low concentrations and slow\nocean currents, the continuum model is less effective in capturing the DEM\nresults. In the latter case, the lack of accuracy of the continuum model is\nfound to be accompanied by the breakdown of a balance between the average shear\nstress and the integrated ocean drag extracted from the DEM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The marginal ice zone (MIZ) represents the periphery of the sea ice cover. In\nthis region, the macroscale behavior of the sea ice results from collisions and\nenduring contact between ice floes. This configuration closely resembles that\nof dense granular flows, which have been modeled successfully with the $\\mu(I)$\nrheology. Here, we present a continuum model based on the $\\mu(I)$ rheology\nwhich treats sea ice as a compressible fluid, with the local sea ice\nconcentration given by a dilatancy function $\\Phi(I)$. We infer expressions for\n$\\mu(I)$ and $\\Phi(I)$ by nonlinear regression using data produced with a\ndiscrete element method (DEM) which considers polygonal-shaped ice floes. We do\nthis by driving the sea ice with a one-dimensional shearing ocean current. The\nresulting continuum model is a nonlinear system of equations with the sea ice\nvelocity, local concentration, and pressure as unknowns. The rheology is given\nby the sum of a plastic and a viscous term. In the context of a periodic patch\nof ocean, which is effectively a one dimensional problem, and under steady\nconditions, we prove this system to be well-posed, present a numerical\nalgorithm for solving it, and compare its solutions to those of the DEM. These\ncomparisons demonstrate the continuum model's ability to capture most of the\nDEM's results accurately. The continuum model is particularly accurate for\nocean currents faster than 0.25 m/s; however, for low concentrations and slow\nocean currents, the continuum model is less effective in capturing the DEM\nresults. In the latter case, the lack of accuracy of the continuum model is\nfound to be accompanied by the breakdown of a balance between the average shear\nstress and the integrated ocean drag extracted from the DEM."
                },
                "authors": [
                    {
                        "name": "Gonzalo G. de Diego"
                    },
                    {
                        "name": "Mukund Gupta"
                    },
                    {
                        "name": "Skylar A. Gering"
                    },
                    {
                        "name": "Rohaiz Haris"
                    },
                    {
                        "name": "Georg Stadler"
                    }
                ],
                "author_detail": {
                    "name": "Georg Stadler"
                },
                "author": "Georg Stadler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18952v1",
                "updated": "2024-10-24T17:52:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    52,
                    31,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:52:31Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    52,
                    31,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Vocabulary Pruning in Early-Exit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Vocabulary Pruning in Early-Exit LLMs"
                },
                "summary": "Increasing the size of large language models (LLMs) has been shown to lead to\nbetter performance. However, this comes at the cost of slower and more\nexpensive inference. Early-exiting is a promising approach for improving the\nefficiency of LLM inference by enabling next token prediction at intermediate\nlayers. Yet, the large vocabulary size in modern LLMs makes the confidence\nestimation required for exit decisions computationally expensive, diminishing\nthe efficiency gains. To address this, we propose dynamically pruning the\nvocabulary at test time for each token. Specifically, the vocabulary is pruned\nat one of the initial layers, and the smaller vocabulary is then used\nthroughout the rest of the forward pass. Our experiments demonstrate that such\npost-hoc dynamic vocabulary pruning improves the efficiency of confidence\nestimation in early-exit LLMs while maintaining competitive performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasing the size of large language models (LLMs) has been shown to lead to\nbetter performance. However, this comes at the cost of slower and more\nexpensive inference. Early-exiting is a promising approach for improving the\nefficiency of LLM inference by enabling next token prediction at intermediate\nlayers. Yet, the large vocabulary size in modern LLMs makes the confidence\nestimation required for exit decisions computationally expensive, diminishing\nthe efficiency gains. To address this, we propose dynamically pruning the\nvocabulary at test time for each token. Specifically, the vocabulary is pruned\nat one of the initial layers, and the smaller vocabulary is then used\nthroughout the rest of the forward pass. Our experiments demonstrate that such\npost-hoc dynamic vocabulary pruning improves the efficiency of confidence\nestimation in early-exit LLMs while maintaining competitive performance."
                },
                "authors": [
                    {
                        "name": "Jort Vincenti"
                    },
                    {
                        "name": "Karim Abdel Sadek"
                    },
                    {
                        "name": "Joan Velja"
                    },
                    {
                        "name": "Matteo Nulli"
                    },
                    {
                        "name": "Metod Jazbec"
                    }
                ],
                "author_detail": {
                    "name": "Metod Jazbec"
                },
                "author": "Metod Jazbec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10508v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10508v4",
                "updated": "2024-10-24T17:43:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    43,
                    28,
                    3,
                    298,
                    0
                ],
                "published": "2024-04-16T12:27:54Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    12,
                    27,
                    54,
                    1,
                    107,
                    0
                ],
                "title": "White Men Lead, Black Women Help? Benchmarking Language Agency Social\n  Biases in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "White Men Lead, Black Women Help? Benchmarking Language Agency Social\n  Biases in LLMs"
                },
                "summary": "Social biases can manifest in language agency. While several studies\napproached agency-related bias in human-written language, very limited research\nhas investigated such biases in Large Language Model (LLM)-generated content.\nIn addition, previous works often rely on string-matching techniques to\nidentify agentic and communal words within texts, which fall short of\naccurately classifying language agency. We introduce the novel Language Agency\nBias Evaluation (LABE) benchmark, which comprehensively evaluates biases in\nLLMs by analyzing agency levels attributed to different demographic groups in\nmodel generations. LABE leverages 5,400 template-based prompts, an accurate\nagency classifier, and corresponding bias metrics to test for gender, racial,\nand intersectional language agency biases in LLMs on 3 text generation tasks:\nbiographies, professor reviews, and reference letters. We also contribute the\nLanguage Agency Classification (LAC) dataset, consisting of 3,724 agentic and\ncommunal sentences. Using LABE, we unveil language agency social biases in 3\nrecent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) LLM generations\ntend to demonstrate greater gender bias than human-written texts; (2) Models\ndemonstrate remarkably higher levels of intersectional bias than the other bias\naspects. Those who are at the intersection of gender and racial minority\ngroups--such as Black females--are consistently described by texts with lower\nlevels of agency, aligning with real-world social inequalities; (3) Among the 3\nLLMs investigated, Llama3 demonstrates the greatest overall bias; (4) Not only\ndoes prompt-based mitigation fail to resolve language agency bias in LLMs, but\nit frequently leads to the exacerbation of biases in generated texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social biases can manifest in language agency. While several studies\napproached agency-related bias in human-written language, very limited research\nhas investigated such biases in Large Language Model (LLM)-generated content.\nIn addition, previous works often rely on string-matching techniques to\nidentify agentic and communal words within texts, which fall short of\naccurately classifying language agency. We introduce the novel Language Agency\nBias Evaluation (LABE) benchmark, which comprehensively evaluates biases in\nLLMs by analyzing agency levels attributed to different demographic groups in\nmodel generations. LABE leverages 5,400 template-based prompts, an accurate\nagency classifier, and corresponding bias metrics to test for gender, racial,\nand intersectional language agency biases in LLMs on 3 text generation tasks:\nbiographies, professor reviews, and reference letters. We also contribute the\nLanguage Agency Classification (LAC) dataset, consisting of 3,724 agentic and\ncommunal sentences. Using LABE, we unveil language agency social biases in 3\nrecent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) LLM generations\ntend to demonstrate greater gender bias than human-written texts; (2) Models\ndemonstrate remarkably higher levels of intersectional bias than the other bias\naspects. Those who are at the intersection of gender and racial minority\ngroups--such as Black females--are consistently described by texts with lower\nlevels of agency, aligning with real-world social inequalities; (3) Among the 3\nLLMs investigated, Llama3 demonstrates the greatest overall bias; (4) Not only\ndoes prompt-based mitigation fail to resolve language agency bias in LLMs, but\nit frequently leads to the exacerbation of biases in generated texts."
                },
                "authors": [
                    {
                        "name": "Yixin Wan"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10508v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10508v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18807v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18807v3",
                "updated": "2024-10-24T17:29:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    29,
                    14,
                    3,
                    298,
                    0
                ],
                "published": "2024-06-27T00:45:37Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    0,
                    45,
                    37,
                    3,
                    179,
                    0
                ],
                "title": "ML-Powered FPGA-based Real-Time Quantum State Discrimination Enabling\n  Mid-circuit Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-Powered FPGA-based Real-Time Quantum State Discrimination Enabling\n  Mid-circuit Measurements"
                },
                "summary": "Similar to reading the transistor state in classical computers, identifying\nthe quantum bit (qubit) state is a fundamental operation to translate quantum\ninformation. However, identifying quantum state has been the slowest and most\nsusceptible to errors operation on superconducting quantum processors. Most\nexisting state discrimination algorithms have only been implemented and\noptimized \"after the fact\" - using offline data transferred from control\ncircuits to host computers. Real-time state discrimination is not possible\nbecause a superconducting quantum state only survives for a few hundred us,\nwhich is much shorter than the communication delay between the readout circuit\nand the host computer (i.e., tens of ms). Mid-circuit measurement (MCM), where\nmeasurements are conducted on qubits at intermediate stages within a quantum\ncircuit rather than solely at the end, represents an advanced technique for\nqubit reuse. For MCM necessitating single-shot readout, it is imperative to\nemploy an in-situ technique for state discrimination with low latency and high\naccuracy. This paper introduces QubiCML, a field-programmable gate array (FPGA)\nbased system for real-time state discrimination enabling MCM - the ability to\nmeasure the state at the control circuit before/without transferring data to a\nhost computer. A multi-layer neural network has been designed and deployed on\nan FPGA to ensure accurate in-situ state discrimination. For the first time,\nML-powered quantum state discrimination has been implemented on a radio\nfrequency system-on-chip FPGA platform. The deployed lightweight network on the\nFPGA only takes 54 ns to complete each inference. We evaluated QubiCML's\nperformance on superconducting quantum processors and obtained an average\naccuracy of 98.5% with only 500 ns readout. QubiCML has the potential to be the\nstandard real-time state discrimination method for the quantum community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similar to reading the transistor state in classical computers, identifying\nthe quantum bit (qubit) state is a fundamental operation to translate quantum\ninformation. However, identifying quantum state has been the slowest and most\nsusceptible to errors operation on superconducting quantum processors. Most\nexisting state discrimination algorithms have only been implemented and\noptimized \"after the fact\" - using offline data transferred from control\ncircuits to host computers. Real-time state discrimination is not possible\nbecause a superconducting quantum state only survives for a few hundred us,\nwhich is much shorter than the communication delay between the readout circuit\nand the host computer (i.e., tens of ms). Mid-circuit measurement (MCM), where\nmeasurements are conducted on qubits at intermediate stages within a quantum\ncircuit rather than solely at the end, represents an advanced technique for\nqubit reuse. For MCM necessitating single-shot readout, it is imperative to\nemploy an in-situ technique for state discrimination with low latency and high\naccuracy. This paper introduces QubiCML, a field-programmable gate array (FPGA)\nbased system for real-time state discrimination enabling MCM - the ability to\nmeasure the state at the control circuit before/without transferring data to a\nhost computer. A multi-layer neural network has been designed and deployed on\nan FPGA to ensure accurate in-situ state discrimination. For the first time,\nML-powered quantum state discrimination has been implemented on a radio\nfrequency system-on-chip FPGA platform. The deployed lightweight network on the\nFPGA only takes 54 ns to complete each inference. We evaluated QubiCML's\nperformance on superconducting quantum processors and obtained an average\naccuracy of 98.5% with only 500 ns readout. QubiCML has the potential to be the\nstandard real-time state discrimination method for the quantum community."
                },
                "authors": [
                    {
                        "name": "Neel R. Vora"
                    },
                    {
                        "name": "Yilun Xu"
                    },
                    {
                        "name": "Akel Hashim"
                    },
                    {
                        "name": "Neelay Fruitwala"
                    },
                    {
                        "name": "Ho Nam Nguyen"
                    },
                    {
                        "name": "Haoran Liao"
                    },
                    {
                        "name": "Jan Balewski"
                    },
                    {
                        "name": "Abhi Rajagopala"
                    },
                    {
                        "name": "Kasra Nowrouzi"
                    },
                    {
                        "name": "Qing Ji"
                    },
                    {
                        "name": "K. Birgitta Whaley"
                    },
                    {
                        "name": "Irfan Siddiqi"
                    },
                    {
                        "name": "Phuc Nguyen"
                    },
                    {
                        "name": "Gang Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gang Huang"
                },
                "author": "Gang Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18807v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18807v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13826v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13826v2",
                "updated": "2024-10-24T17:27:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    27,
                    22,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-17T17:51:40Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    51,
                    40,
                    3,
                    291,
                    0
                ],
                "title": "Unearthing Skill-Level Insights for Understanding Trade-Offs of\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unearthing Skill-Level Insights for Understanding Trade-Offs of\n  Foundation Models"
                },
                "summary": "With models getting stronger, evaluations have grown more complex, testing\nmultiple skills in one benchmark and even in the same instance at once.\nHowever, skill-wise performance is obscured when inspecting aggregate accuracy,\nunder-utilizing the rich signal modern benchmarks contain. We propose an\nautomatic approach to recover the underlying skills relevant for any evaluation\ninstance, by way of inspecting model-generated rationales. After validating the\nrelevance of rationale-parsed skills and inferring skills for $46$k instances\nover $12$ benchmarks, we observe many skills to be common across benchmarks,\nresulting in the curation of hundreds of skill-slices (i.e. sets of instances\ntesting a common skill). Inspecting accuracy over these slices yields novel\ninsights on model trade-offs: e.g., compared to GPT-4o and Claude 3.5 Sonnet,\non average, Gemini 1.5 Pro is $18\\%$ more accurate in \"computing molar mass\",\nbut $19\\%$ less accurate in \"applying constitutional law\", despite the overall\naccuracies of the three models differing by a mere $0.4\\%$. Furthermore, we\ndemonstrate the practical utility of our approach by showing that insights\nderived from skill slice analysis can generalize to held-out instances: when\nrouting each instance to the model strongest on the relevant skills, we see a\n$3\\%$ accuracy improvement over our $12$ dataset corpus. Our skill-slices and\nframework open a new avenue in model evaluation, leveraging skill-specific\nanalyses to unlock a more granular and actionable understanding of model\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With models getting stronger, evaluations have grown more complex, testing\nmultiple skills in one benchmark and even in the same instance at once.\nHowever, skill-wise performance is obscured when inspecting aggregate accuracy,\nunder-utilizing the rich signal modern benchmarks contain. We propose an\nautomatic approach to recover the underlying skills relevant for any evaluation\ninstance, by way of inspecting model-generated rationales. After validating the\nrelevance of rationale-parsed skills and inferring skills for $46$k instances\nover $12$ benchmarks, we observe many skills to be common across benchmarks,\nresulting in the curation of hundreds of skill-slices (i.e. sets of instances\ntesting a common skill). Inspecting accuracy over these slices yields novel\ninsights on model trade-offs: e.g., compared to GPT-4o and Claude 3.5 Sonnet,\non average, Gemini 1.5 Pro is $18\\%$ more accurate in \"computing molar mass\",\nbut $19\\%$ less accurate in \"applying constitutional law\", despite the overall\naccuracies of the three models differing by a mere $0.4\\%$. Furthermore, we\ndemonstrate the practical utility of our approach by showing that insights\nderived from skill slice analysis can generalize to held-out instances: when\nrouting each instance to the model strongest on the relevant skills, we see a\n$3\\%$ accuracy improvement over our $12$ dataset corpus. Our skill-slices and\nframework open a new avenue in model evaluation, leveraging skill-specific\nanalyses to unlock a more granular and actionable understanding of model\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Mazda Moayeri"
                    },
                    {
                        "name": "Vidhisha Balachandran"
                    },
                    {
                        "name": "Varun Chandrasekaran"
                    },
                    {
                        "name": "Safoora Yousefi"
                    },
                    {
                        "name": "Thomas Fel"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Besmira Nushi"
                    },
                    {
                        "name": "Neel Joshi"
                    },
                    {
                        "name": "Vibhav Vineet"
                    }
                ],
                "author_detail": {
                    "name": "Vibhav Vineet"
                },
                "author": "Vibhav Vineet",
                "arxiv_comment": "Code at: github.com/microsoft/skill-slice-insights",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13826v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13826v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18927v1",
                "updated": "2024-10-24T17:14:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    14,
                    40,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:14:40Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    14,
                    40,
                    3,
                    298,
                    0
                ],
                "title": "SafeBench: A Safety Evaluation Framework for Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeBench: A Safety Evaluation Framework for Multimodal Large Language\n  Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are showing strong safety concerns\n(e.g., generating harmful outputs for users), which motivates the development\nof safety evaluation benchmarks. However, we observe that existing safety\nbenchmarks for MLLMs show limitations in query quality and evaluation\nreliability limiting the detection of model safety implications as MLLMs\ncontinue to evolve. In this paper, we propose \\toolns, a comprehensive\nframework designed for conducting safety evaluations of MLLMs. Our framework\nconsists of a comprehensive harmful query dataset and an automated evaluation\nprotocol that aims to address the above limitations, respectively. We first\ndesign an automatic safety dataset generation pipeline, where we employ a set\nof LLM judges to recognize and categorize the risk scenarios that are most\nharmful and diverse for MLLMs; based on the taxonomy, we further ask these\njudges to generate high-quality harmful queries accordingly resulting in 23\nrisk scenarios with 2,300 multi-modal harmful query pairs. During safety\nevaluation, we draw inspiration from the jury system in judicial proceedings\nand pioneer the jury deliberation evaluation protocol that adopts collaborative\nLLMs to evaluate whether target models exhibit specific harmful behaviors,\nproviding a reliable and unbiased assessment of content security risks. In\naddition, our benchmark can also be extended to the audio modality showing high\nscalability and potential. Based on our framework, we conducted large-scale\nexperiments on 15 widely-used open-source MLLMs and 6 commercial MLLMs (e.g.,\nGPT-4o, Gemini), where we revealed widespread safety issues in existing MLLMs\nand instantiated several insights on MLLM safety performance such as image\nquality and parameter size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are showing strong safety concerns\n(e.g., generating harmful outputs for users), which motivates the development\nof safety evaluation benchmarks. However, we observe that existing safety\nbenchmarks for MLLMs show limitations in query quality and evaluation\nreliability limiting the detection of model safety implications as MLLMs\ncontinue to evolve. In this paper, we propose \\toolns, a comprehensive\nframework designed for conducting safety evaluations of MLLMs. Our framework\nconsists of a comprehensive harmful query dataset and an automated evaluation\nprotocol that aims to address the above limitations, respectively. We first\ndesign an automatic safety dataset generation pipeline, where we employ a set\nof LLM judges to recognize and categorize the risk scenarios that are most\nharmful and diverse for MLLMs; based on the taxonomy, we further ask these\njudges to generate high-quality harmful queries accordingly resulting in 23\nrisk scenarios with 2,300 multi-modal harmful query pairs. During safety\nevaluation, we draw inspiration from the jury system in judicial proceedings\nand pioneer the jury deliberation evaluation protocol that adopts collaborative\nLLMs to evaluate whether target models exhibit specific harmful behaviors,\nproviding a reliable and unbiased assessment of content security risks. In\naddition, our benchmark can also be extended to the audio modality showing high\nscalability and potential. Based on our framework, we conducted large-scale\nexperiments on 15 widely-used open-source MLLMs and 6 commercial MLLMs (e.g.,\nGPT-4o, Gemini), where we revealed widespread safety issues in existing MLLMs\nand instantiated several insights on MLLM safety performance such as image\nquality and parameter size."
                },
                "authors": [
                    {
                        "name": "Zonghao Ying"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Wenbo Zhou"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17078v2",
                "updated": "2024-10-24T17:12:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    12,
                    19,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-22T14:56:50Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    14,
                    56,
                    50,
                    1,
                    296,
                    0
                ],
                "title": "FlowTracer: A Tool for Uncovering Network Path Usage Imbalance in AI\n  Training Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowTracer: A Tool for Uncovering Network Path Usage Imbalance in AI\n  Training Clusters"
                },
                "summary": "The increasing complexity of AI workloads, especially distributed Large\nLanguage Model (LLM) training, places significant strain on the networking\ninfrastructure of parallel data centers and supercomputing systems. While\nEqual-Cost Multi- Path (ECMP) routing distributes traffic over parallel paths,\nhash collisions often lead to imbalanced network resource utilization and\nperformance bottlenecks. This paper presents FlowTracer, a tool designed to\nanalyze network path utilization and evaluate different routing strategies.\nFlowTracer aids in debugging network inefficiencies by providing detailed\nvisibility into traffic distribution and helping to identify the root causes of\nperformance degradation, such as issues caused by hash collisions. By offering\nflow-level insights, FlowTracer enables system operators to optimize routing,\nreduce congestion, and improve the performance of distributed AI workloads. We\nuse a RoCEv2-enabled cluster with a leaf-spine network and 16 400-Gbps nodes to\ndemonstrate how FlowTracer can be used to compare the flow imbalances of ECMP\nrouting against a statically configured network. The example showcases a 30%\nreduction in imbalance, as measured by a new metric we introduce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of AI workloads, especially distributed Large\nLanguage Model (LLM) training, places significant strain on the networking\ninfrastructure of parallel data centers and supercomputing systems. While\nEqual-Cost Multi- Path (ECMP) routing distributes traffic over parallel paths,\nhash collisions often lead to imbalanced network resource utilization and\nperformance bottlenecks. This paper presents FlowTracer, a tool designed to\nanalyze network path utilization and evaluate different routing strategies.\nFlowTracer aids in debugging network inefficiencies by providing detailed\nvisibility into traffic distribution and helping to identify the root causes of\nperformance degradation, such as issues caused by hash collisions. By offering\nflow-level insights, FlowTracer enables system operators to optimize routing,\nreduce congestion, and improve the performance of distributed AI workloads. We\nuse a RoCEv2-enabled cluster with a leaf-spine network and 16 400-Gbps nodes to\ndemonstrate how FlowTracer can be used to compare the flow imbalances of ECMP\nrouting against a statically configured network. The example showcases a 30%\nreduction in imbalance, as measured by a new metric we introduce."
                },
                "authors": [
                    {
                        "name": "Hasibul Jamil"
                    },
                    {
                        "name": "Abdul Alim"
                    },
                    {
                        "name": "Laurent Schares"
                    },
                    {
                        "name": "Pavlos Maniotis"
                    },
                    {
                        "name": "Liran Schour"
                    },
                    {
                        "name": "Ali Sydney"
                    },
                    {
                        "name": "Abdullah Kayi"
                    },
                    {
                        "name": "Tevfik Kosar"
                    },
                    {
                        "name": "Bengi Karacali"
                    }
                ],
                "author_detail": {
                    "name": "Bengi Karacali"
                },
                "author": "Bengi Karacali",
                "arxiv_comment": "Submitted for peer reviewing in IEEE ICC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18923v1",
                "updated": "2024-10-24T17:11:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    11,
                    52,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:11:52Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    11,
                    52,
                    3,
                    298,
                    0
                ],
                "title": "SegLLM: Multi-round Reasoning Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SegLLM: Multi-round Reasoning Segmentation"
                },
                "summary": "We present SegLLM, a novel multi-round interactive reasoning segmentation\nmodel that enhances LLM-based segmentation by exploiting conversational memory\nof both visual and textual outputs. By leveraging a mask-aware multimodal LLM,\nSegLLM re-integrates previous segmentation results into its input stream,\nenabling it to reason about complex user intentions and segment objects in\nrelation to previously identified entities, including positional,\ninteractional, and hierarchical relationships, across multiple interactions.\nThis capability allows SegLLM to respond to visual and text queries in a\nchat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLM\noutperforms existing methods in multi-round interactive reasoning segmentation\nby over 20%. Additionally, we observed that training on multi-round reasoning\nsegmentation data enhances performance on standard single-round referring\nsegmentation and localization tasks, resulting in a 5.5% increase in cIoU for\nreferring expression segmentation and a 4.5% improvement in Acc@0.5 for\nreferring expression localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SegLLM, a novel multi-round interactive reasoning segmentation\nmodel that enhances LLM-based segmentation by exploiting conversational memory\nof both visual and textual outputs. By leveraging a mask-aware multimodal LLM,\nSegLLM re-integrates previous segmentation results into its input stream,\nenabling it to reason about complex user intentions and segment objects in\nrelation to previously identified entities, including positional,\ninteractional, and hierarchical relationships, across multiple interactions.\nThis capability allows SegLLM to respond to visual and text queries in a\nchat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLM\noutperforms existing methods in multi-round interactive reasoning segmentation\nby over 20%. Additionally, we observed that training on multi-round reasoning\nsegmentation data enhances performance on standard single-round referring\nsegmentation and localization tasks, resulting in a 5.5% increase in cIoU for\nreferring expression segmentation and a 4.5% improvement in Acc@0.5 for\nreferring expression localization."
                },
                "authors": [
                    {
                        "name": "XuDong Wang"
                    },
                    {
                        "name": "Shaolun Zhang"
                    },
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Kehan Li"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Trevor Darrell"
                    }
                ],
                "author_detail": {
                    "name": "Trevor Darrell"
                },
                "author": "Trevor Darrell",
                "arxiv_comment": "22 pages, 10 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18921v1",
                "updated": "2024-10-24T17:10:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    10,
                    39,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:10:39Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    10,
                    39,
                    3,
                    298,
                    0
                ],
                "title": "From Blind Solvers to Logical Thinkers: Benchmarking LLMs' Logical\n  Integrity on Faulty Mathematical Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Blind Solvers to Logical Thinkers: Benchmarking LLMs' Logical\n  Integrity on Faulty Mathematical Problems"
                },
                "summary": "Consider the math problem: \"Lily received 3 cookies from her best friend\nyesterday and ate 5 for breakfast. Today, her friend gave her 3 more cookies.\nHow many cookies does Lily have now?\" Many large language models (LLMs) in\nprevious research approach this problem by calculating the answer \"1\" using the\nequation \"3 - 5 + 3.\" However, from a human perspective, we recognize the\ninherent flaw in this problem: Lily cannot eat 5 cookies if she initially only\nhad 3. This discrepancy prompts a key question: Are current LLMs merely Blind\nSolver that apply mathematical operations without deeper reasoning, or can they\nfunction as Logical Thinker capable of identifying logical inconsistencies?\n  To explore this question, we propose a benchmark dataset, FaultyMath, which\nincludes faulty math problems of rich diversity: i) multiple mathematical\ncategories, e.g., algebra, geometry, number theory, etc., ii) varying levels of\ndifficulty, and iii) different origins of faultiness -- ranging from violations\nof common sense and ambiguous statements to mathematical contradictions and\nmore. We evaluate a broad spectrum of LLMs, including open-source,\nclosed-source, and math-specialized models, using FaultyMath across three\ndimensions: (i) How accurately can the models detect faulty math problems\nwithout being explicitly prompted to do so? (ii) When provided with hints --\neither correct or misleading -- about the validity of the problems, to what\nextent do LLMs adapt to become reliable Logical Thinker? (iii) How trustworthy\nare the explanations generated by LLMs when they recognize a math problem as\nflawed? Through extensive experimentation and detailed analysis, our results\ndemonstrate that existing LLMs largely function as Blind Solver and fall short\nof the reasoning capabilities required to perform as Logical Thinker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consider the math problem: \"Lily received 3 cookies from her best friend\nyesterday and ate 5 for breakfast. Today, her friend gave her 3 more cookies.\nHow many cookies does Lily have now?\" Many large language models (LLMs) in\nprevious research approach this problem by calculating the answer \"1\" using the\nequation \"3 - 5 + 3.\" However, from a human perspective, we recognize the\ninherent flaw in this problem: Lily cannot eat 5 cookies if she initially only\nhad 3. This discrepancy prompts a key question: Are current LLMs merely Blind\nSolver that apply mathematical operations without deeper reasoning, or can they\nfunction as Logical Thinker capable of identifying logical inconsistencies?\n  To explore this question, we propose a benchmark dataset, FaultyMath, which\nincludes faulty math problems of rich diversity: i) multiple mathematical\ncategories, e.g., algebra, geometry, number theory, etc., ii) varying levels of\ndifficulty, and iii) different origins of faultiness -- ranging from violations\nof common sense and ambiguous statements to mathematical contradictions and\nmore. We evaluate a broad spectrum of LLMs, including open-source,\nclosed-source, and math-specialized models, using FaultyMath across three\ndimensions: (i) How accurately can the models detect faulty math problems\nwithout being explicitly prompted to do so? (ii) When provided with hints --\neither correct or misleading -- about the validity of the problems, to what\nextent do LLMs adapt to become reliable Logical Thinker? (iii) How trustworthy\nare the explanations generated by LLMs when they recognize a math problem as\nflawed? Through extensive experimentation and detailed analysis, our results\ndemonstrate that existing LLMs largely function as Blind Solver and fall short\nof the reasoning capabilities required to perform as Logical Thinker."
                },
                "authors": [
                    {
                        "name": "A M Muntasir Rahman"
                    },
                    {
                        "name": "Junyi Ye"
                    },
                    {
                        "name": "Wei Yao"
                    },
                    {
                        "name": "Wenpeng Yin"
                    },
                    {
                        "name": "Guiling Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guiling Wang"
                },
                "author": "Guiling Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18908v1",
                "updated": "2024-10-24T16:59:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    59,
                    28,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T16:59:28Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    59,
                    28,
                    3,
                    298,
                    0
                ],
                "title": "A Survey on Speech Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Speech Large Language Models"
                },
                "summary": "Large Language Models (LLMs) exhibit strong contextual understanding and\nremarkable multi-task performance. Therefore, researchers have been seeking to\nintegrate LLMs in the broad sense of Spoken Language Understanding (SLU) field.\nDifferent from the traditional method of cascading LLMs to process text\ngenerated by Automatic Speech Recognition(ASR), new efforts have focused on\ndesigning architectures centered around Audio Feature Extraction - Multimodal\nInformation Fusion - LLM Inference(Speech LLMs). This approach enables richer\naudio feature extraction while simultaneously facilitating end-to-end fusion of\naudio and text modalities, thereby achieving deeper understanding and reasoning\nfrom audio data. This paper elucidates the development of Speech LLMs, offering\nan in-depth analysis of system architectures and training strategies. Through\nextensive research and a series of targeted experiments, the paper assesses\nSpeech LLMs' advancements in Rich Audio Transcription and its potential for\nCross-task Integration within the SLU field. Additionally, it indicates key\nchallenges uncovered through experimentation, such as the Dormancy of LLMs\nunder certain conditions. The paper further delves into the training strategies\nfor Speech LLMs, proposing potential solutions based on these findings, and\noffering valuable insights and references for future research in this domain,\nas well as LLM applications in multimodal contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit strong contextual understanding and\nremarkable multi-task performance. Therefore, researchers have been seeking to\nintegrate LLMs in the broad sense of Spoken Language Understanding (SLU) field.\nDifferent from the traditional method of cascading LLMs to process text\ngenerated by Automatic Speech Recognition(ASR), new efforts have focused on\ndesigning architectures centered around Audio Feature Extraction - Multimodal\nInformation Fusion - LLM Inference(Speech LLMs). This approach enables richer\naudio feature extraction while simultaneously facilitating end-to-end fusion of\naudio and text modalities, thereby achieving deeper understanding and reasoning\nfrom audio data. This paper elucidates the development of Speech LLMs, offering\nan in-depth analysis of system architectures and training strategies. Through\nextensive research and a series of targeted experiments, the paper assesses\nSpeech LLMs' advancements in Rich Audio Transcription and its potential for\nCross-task Integration within the SLU field. Additionally, it indicates key\nchallenges uncovered through experimentation, such as the Dormancy of LLMs\nunder certain conditions. The paper further delves into the training strategies\nfor Speech LLMs, proposing potential solutions based on these findings, and\noffering valuable insights and references for future research in this domain,\nas well as LLM applications in multimodal contexts."
                },
                "authors": [
                    {
                        "name": "Jing Peng"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Yu Xi"
                    },
                    {
                        "name": "Xv Li"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18906v1",
                "updated": "2024-10-24T16:57:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    57,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T16:57:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    57,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "PRISM: A Methodology for Auditing Biases in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: A Methodology for Auditing Biases in Large Language Models"
                },
                "summary": "Auditing Large Language Models (LLMs) to discover their biases and\npreferences is an emerging challenge in creating Responsible Artificial\nIntelligence (AI). While various methods have been proposed to elicit the\npreferences of such models, countermeasures have been taken by LLM trainers,\nsuch that LLMs hide, obfuscate or point blank refuse to disclosure their\npositions on certain subjects. This paper presents PRISM, a flexible,\ninquiry-based methodology for auditing LLMs - that seeks to illicit such\npositions indirectly through task-based inquiry prompting rather than direct\ninquiry of said preferences. To demonstrate the utility of the methodology, we\napplied PRISM on the Political Compass Test, where we assessed the political\nleanings of twenty-one LLMs from seven providers. We show LLMs, by default,\nespouse positions that are economically left and socially liberal (consistent\nwith prior work). We also show the space of positions that these models are\nwilling to espouse - where some models are more constrained and less compliant\nthan others - while others are more neutral and objective. In sum, PRISM can\nmore reliably probe and audit LLMs to understand their preferences, biases and\nconstraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Large Language Models (LLMs) to discover their biases and\npreferences is an emerging challenge in creating Responsible Artificial\nIntelligence (AI). While various methods have been proposed to elicit the\npreferences of such models, countermeasures have been taken by LLM trainers,\nsuch that LLMs hide, obfuscate or point blank refuse to disclosure their\npositions on certain subjects. This paper presents PRISM, a flexible,\ninquiry-based methodology for auditing LLMs - that seeks to illicit such\npositions indirectly through task-based inquiry prompting rather than direct\ninquiry of said preferences. To demonstrate the utility of the methodology, we\napplied PRISM on the Political Compass Test, where we assessed the political\nleanings of twenty-one LLMs from seven providers. We show LLMs, by default,\nespouse positions that are economically left and socially liberal (consistent\nwith prior work). We also show the space of positions that these models are\nwilling to espouse - where some models are more constrained and less compliant\nthan others - while others are more neutral and objective. In sum, PRISM can\nmore reliably probe and audit LLMs to understand their preferences, biases and\nconstraints."
                },
                "authors": [
                    {
                        "name": "Leif Azzopardi"
                    },
                    {
                        "name": "Yashar Moshfeghi"
                    }
                ],
                "author_detail": {
                    "name": "Yashar Moshfeghi"
                },
                "author": "Yashar Moshfeghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18902v1",
                "updated": "2024-10-24T16:48:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    48,
                    12,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T16:48:12Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    48,
                    12,
                    3,
                    298,
                    0
                ],
                "title": "LLMs for Extremely Low-Resource Finno-Ugric Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Extremely Low-Resource Finno-Ugric Languages"
                },
                "summary": "The advancement of large language models (LLMs) has predominantly focused on\nhigh-resource languages, leaving low-resource languages, such as those in the\nFinno-Ugric family, significantly underrepresented. This paper addresses this\ngap by focusing on V\\~oro, Livonian, and Komi. We cover almost the entire cycle\nof LLM creation, from data collection to instruction tuning and evaluation. Our\ncontributions include developing multilingual base and instruction-tuned\nmodels; creating evaluation benchmarks, including the smugri-MT-bench\nmulti-turn conversational benchmark; and conducting human evaluation. We intend\nfor this work to promote linguistic diversity, ensuring that lesser-resourced\nlanguages can benefit from advancements in NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) has predominantly focused on\nhigh-resource languages, leaving low-resource languages, such as those in the\nFinno-Ugric family, significantly underrepresented. This paper addresses this\ngap by focusing on V\\~oro, Livonian, and Komi. We cover almost the entire cycle\nof LLM creation, from data collection to instruction tuning and evaluation. Our\ncontributions include developing multilingual base and instruction-tuned\nmodels; creating evaluation benchmarks, including the smugri-MT-bench\nmulti-turn conversational benchmark; and conducting human evaluation. We intend\nfor this work to promote linguistic diversity, ensuring that lesser-resourced\nlanguages can benefit from advancements in NLP."
                },
                "authors": [
                    {
                        "name": "Taido Purason"
                    },
                    {
                        "name": "Hele-Andra Kuulmets"
                    },
                    {
                        "name": "Mark Fishel"
                    }
                ],
                "author_detail": {
                    "name": "Mark Fishel"
                },
                "author": "Mark Fishel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16745v2",
                "updated": "2024-10-24T16:46:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    46,
                    27,
                    3,
                    298,
                    0
                ],
                "published": "2024-04-25T17:00:13Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    17,
                    0,
                    13,
                    3,
                    116,
                    0
                ],
                "title": "Statistical Inference for Covariate-Adjusted and Interpretable\n  Generalized Factor Model with Application to Testing Fairness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference for Covariate-Adjusted and Interpretable\n  Generalized Factor Model with Application to Testing Fairness"
                },
                "summary": "Latent variable models are popularly used to measure latent factors (e.g.,\nabilities and personalities) from large-scale assessment data. Beyond\nunderstanding these latent factors, the covariate effect on responses\ncontrolling for latent factors is also of great scientific interest and has\nwide applications, such as evaluating the fairness of educational testing,\nwhere the covariate effect reflects whether a test question is biased toward\ncertain individual characteristics (e.g., gender and race), taking into account\ntheir latent abilities. However, the large sample sizes and test lengths pose\nchallenges to developing efficient methods and drawing valid inferences.\nMoreover, to accommodate the commonly encountered discrete responses, nonlinear\nlatent factor models are often assumed, adding further complexity. To address\nthese challenges, we consider a covariate-adjusted generalized factor model and\ndevelop novel and interpretable conditions to address the identifiability\nissue. Based on the identifiability conditions, we propose a joint maximum\nlikelihood estimation method and establish estimation consistency and\nasymptotic normality results for the covariate effects. Furthermore, we derive\nestimation and inference results for latent factors and the factor loadings. We\nillustrate the finite sample performance of the proposed method through\nextensive numerical studies and an educational assessment dataset from the\nProgramme for International Student Assessment (PISA).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent variable models are popularly used to measure latent factors (e.g.,\nabilities and personalities) from large-scale assessment data. Beyond\nunderstanding these latent factors, the covariate effect on responses\ncontrolling for latent factors is also of great scientific interest and has\nwide applications, such as evaluating the fairness of educational testing,\nwhere the covariate effect reflects whether a test question is biased toward\ncertain individual characteristics (e.g., gender and race), taking into account\ntheir latent abilities. However, the large sample sizes and test lengths pose\nchallenges to developing efficient methods and drawing valid inferences.\nMoreover, to accommodate the commonly encountered discrete responses, nonlinear\nlatent factor models are often assumed, adding further complexity. To address\nthese challenges, we consider a covariate-adjusted generalized factor model and\ndevelop novel and interpretable conditions to address the identifiability\nissue. Based on the identifiability conditions, we propose a joint maximum\nlikelihood estimation method and establish estimation consistency and\nasymptotic normality results for the covariate effects. Furthermore, we derive\nestimation and inference results for latent factors and the factor loadings. We\nillustrate the finite sample performance of the proposed method through\nextensive numerical studies and an educational assessment dataset from the\nProgramme for International Student Assessment (PISA)."
                },
                "authors": [
                    {
                        "name": "Jing Ouyang"
                    },
                    {
                        "name": "Chengyu Cui"
                    },
                    {
                        "name": "Kean Ming Tan"
                    },
                    {
                        "name": "Gongjun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Gongjun Xu"
                },
                "author": "Gongjun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18899v1",
                "updated": "2024-10-24T16:40:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    40,
                    16,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T16:40:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    40,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "SKATR: A Self-Supervised Summary Transformer for SKA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKATR: A Self-Supervised Summary Transformer for SKA"
                },
                "summary": "The Square Kilometer Array will initiate a new era of radio astronomy by\nallowing 3D imaging of the Universe during Cosmic Dawn and Reionization. Modern\nmachine learning is crucial to analyse the highly structured and complex\nsignal. However, accurate training data is expensive to simulate, and\nsupervised learning may not generalize. We introduce a self-supervised vision\ntransformer, SKATR, whose learned encoding can be cheaply adapted for\ndownstream tasks on 21cm maps. Focusing on regression and generative inference\nof astrophysical and cosmological parameters, we demonstrate that SKATR\nrepresentations are maximally informative and that SKATR generalises\nout-of-domain to differently-simulated, noised, and higher-resolution datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Square Kilometer Array will initiate a new era of radio astronomy by\nallowing 3D imaging of the Universe during Cosmic Dawn and Reionization. Modern\nmachine learning is crucial to analyse the highly structured and complex\nsignal. However, accurate training data is expensive to simulate, and\nsupervised learning may not generalize. We introduce a self-supervised vision\ntransformer, SKATR, whose learned encoding can be cheaply adapted for\ndownstream tasks on 21cm maps. Focusing on regression and generative inference\nof astrophysical and cosmological parameters, we demonstrate that SKATR\nrepresentations are maximally informative and that SKATR generalises\nout-of-domain to differently-simulated, noised, and higher-resolution datasets."
                },
                "authors": [
                    {
                        "name": "Ayodele Ore"
                    },
                    {
                        "name": "Caroline Heneka"
                    },
                    {
                        "name": "Tilman Plehn"
                    }
                ],
                "author_detail": {
                    "name": "Tilman Plehn"
                },
                "author": "Tilman Plehn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v2",
                "updated": "2024-10-24T16:40:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    40,
                    10,
                    3,
                    298,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00953v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00953v3",
                "updated": "2024-10-24T16:32:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    32,
                    34,
                    3,
                    298,
                    0
                ],
                "published": "2024-03-01T20:06:39Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    20,
                    6,
                    39,
                    4,
                    61,
                    0
                ],
                "title": "AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge\n  Graph Construction Based on Ontologies-enhanced Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge\n  Graph Construction Based on Ontologies-enhanced Large Language Models"
                },
                "summary": "Rare diseases affect millions worldwide but often face limited research focus\ndue to their low prevalence. This results in prolonged diagnoses and a lack of\napproved therapies. Recent advancements in Large Language Models (LLMs) have\nshown promise in automating the extraction of medical information, offering\npotential to improve medical diagnosis and management. However, most LLMs lack\nprofessional medical knowledge, especially concerning rare diseases, and\nstruggle to handle the latest rare disease information. They also cannot\neffectively manage rare disease data and are not directly suitable for\ndiagnosis and management tasks. Our objective is to create an end-to-end system\ncalled AutoRD, which automates the extraction of information from medical texts\nabout rare diseases, focusing on entities and their relations. AutoRD\nintegrates up-to-date structured knowledge and demonstrates superior\nperformance in rare disease extraction tasks. We conduct various experiments to\nevaluate AutoRD's performance, aiming to surpass common LLMs and traditional\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rare diseases affect millions worldwide but often face limited research focus\ndue to their low prevalence. This results in prolonged diagnoses and a lack of\napproved therapies. Recent advancements in Large Language Models (LLMs) have\nshown promise in automating the extraction of medical information, offering\npotential to improve medical diagnosis and management. However, most LLMs lack\nprofessional medical knowledge, especially concerning rare diseases, and\nstruggle to handle the latest rare disease information. They also cannot\neffectively manage rare disease data and are not directly suitable for\ndiagnosis and management tasks. Our objective is to create an end-to-end system\ncalled AutoRD, which automates the extraction of information from medical texts\nabout rare diseases, focusing on entities and their relations. AutoRD\nintegrates up-to-date structured knowledge and demonstrates superior\nperformance in rare disease extraction tasks. We conduct various experiments to\nevaluate AutoRD's performance, aiming to surpass common LLMs and traditional\nmethods."
                },
                "authors": [
                    {
                        "name": "Lang Cao"
                    },
                    {
                        "name": "Jimeng Sun"
                    },
                    {
                        "name": "Adam Cross"
                    }
                ],
                "author_detail": {
                    "name": "Adam Cross"
                },
                "author": "Adam Cross",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00953v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00953v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18893v1",
                "updated": "2024-10-24T16:30:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    30,
                    14,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T16:30:14Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    30,
                    14,
                    3,
                    298,
                    0
                ],
                "title": "Creating and Repairing Robot Programs in Open-World Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating and Repairing Robot Programs in Open-World Domains"
                },
                "summary": "Using Large Language Models (LLMs) to produce robot programs from natural\nlanguage has allowed for robot systems that can complete a higher diversity of\ntasks. However, LLM-generated programs may be faulty, either due to ambiguity\nin instructions, misinterpretation of the desired task, or missing information\nabout the world state. As these programs run, the state of the world changes\nand they gather new information. When a failure occurs, it is important that\nthey recover from the current world state and avoid repeating steps that they\nthey previously completed successfully. We propose RoboRepair, a system which\ntraces the execution of a program up until error, and then runs an LLM-produced\nrecovery program that minimizes repeated actions.\n  To evaluate the efficacy of our system, we create a benchmark consisting of\neleven tasks with various error conditions that require the generation of a\nrecovery program. We compare the efficiency of the recovery program to a plan\nbuilt with an oracle that has foreknowledge of future errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models (LLMs) to produce robot programs from natural\nlanguage has allowed for robot systems that can complete a higher diversity of\ntasks. However, LLM-generated programs may be faulty, either due to ambiguity\nin instructions, misinterpretation of the desired task, or missing information\nabout the world state. As these programs run, the state of the world changes\nand they gather new information. When a failure occurs, it is important that\nthey recover from the current world state and avoid repeating steps that they\nthey previously completed successfully. We propose RoboRepair, a system which\ntraces the execution of a program up until error, and then runs an LLM-produced\nrecovery program that minimizes repeated actions.\n  To evaluate the efficacy of our system, we create a benchmark consisting of\neleven tasks with various error conditions that require the generation of a\nrecovery program. We compare the efficiency of the recovery program to a plan\nbuilt with an oracle that has foreknowledge of future errors."
                },
                "authors": [
                    {
                        "name": "Claire Schlesinger"
                    },
                    {
                        "name": "Arjun Guha"
                    },
                    {
                        "name": "Joydeep Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Joydeep Biswas"
                },
                "author": "Joydeep Biswas",
                "arxiv_comment": "Under review at ACL Rolling Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18890v1",
                "updated": "2024-10-24T16:27:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    27,
                    35,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T16:27:35Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    27,
                    35,
                    3,
                    298,
                    0
                ],
                "title": "Improving Small-Scale Large Language Models Function Calling for\n  Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Small-Scale Large Language Models Function Calling for\n  Reasoning Tasks"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional capabilities in natural language understanding and generation.\nWhile these models excel in general complex reasoning tasks, they still face\nchallenges in mathematical problem-solving and logical reasoning. To address\nthese limitations, researchers have explored function calling abilities,\nallowing LLMs to execute provided functions and utilize their outputs for task\ncompletion. However, concentrating on specific tasks can be very inefficient\nfor large-scale LLMs to be used, because of the expensive cost of training and\ninference stages they need in terms of computational resources. This study\nintroduces a novel framework for training smaller language models in function\ncalling, focusing on specific logical and mathematical reasoning tasks. The\napproach aims to improve performances of small-scale models for these tasks\nusing function calling, ensuring a high level of accuracy. Our framework\nemploys an agent that, given a problem and a set of callable functions, queries\nthe LLM by injecting a description and examples of the usable functions into\nthe prompt and managing their calls in a step-by-step reasoning chain. This\nprocess is used to create a dataset of correct and incorrect reasoning chain\nchat completions from a large-scale LLM. This dataset is used to train a\nsmaller LLM using Reinforcement Learning from Human Feedback (RLHF),\nspecifically employing the Direct Preference Optimization (DPO) technique.\nExperimental results demonstrate how the proposed approach balances the\ntrade-off between model size and performance, improving the ability of function\ncalling for reasoning tasks, in smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional capabilities in natural language understanding and generation.\nWhile these models excel in general complex reasoning tasks, they still face\nchallenges in mathematical problem-solving and logical reasoning. To address\nthese limitations, researchers have explored function calling abilities,\nallowing LLMs to execute provided functions and utilize their outputs for task\ncompletion. However, concentrating on specific tasks can be very inefficient\nfor large-scale LLMs to be used, because of the expensive cost of training and\ninference stages they need in terms of computational resources. This study\nintroduces a novel framework for training smaller language models in function\ncalling, focusing on specific logical and mathematical reasoning tasks. The\napproach aims to improve performances of small-scale models for these tasks\nusing function calling, ensuring a high level of accuracy. Our framework\nemploys an agent that, given a problem and a set of callable functions, queries\nthe LLM by injecting a description and examples of the usable functions into\nthe prompt and managing their calls in a step-by-step reasoning chain. This\nprocess is used to create a dataset of correct and incorrect reasoning chain\nchat completions from a large-scale LLM. This dataset is used to train a\nsmaller LLM using Reinforcement Learning from Human Feedback (RLHF),\nspecifically employing the Direct Preference Optimization (DPO) technique.\nExperimental results demonstrate how the proposed approach balances the\ntrade-off between model size and performance, improving the ability of function\ncalling for reasoning tasks, in smaller models."
                },
                "authors": [
                    {
                        "name": "Graziano A. Manduzio"
                    },
                    {
                        "name": "Federico A. Galatolo"
                    },
                    {
                        "name": "Mario G. C. A. Cimino"
                    },
                    {
                        "name": "Enzo Pasquale Scilingo"
                    },
                    {
                        "name": "Lorenzo Cominelli"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Cominelli"
                },
                "author": "Lorenzo Cominelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18889v1",
                "updated": "2024-10-24T16:27:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    27,
                    3,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T16:27:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    27,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating\n  Their Effect on Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating\n  Their Effect on Model Performance"
                },
                "summary": "NLP benchmarks rely on standardized datasets for training and evaluating\nmodels and are crucial for advancing the field. Traditionally, expert\nannotations ensure high-quality labels; however, the cost of expert annotation\ndoes not scale well with the growing demand for larger datasets required by\nmodern models. While crowd-sourcing provides a more scalable solution, it often\ncomes at the expense of annotation precision and consistency. Recent\nadvancements in large language models (LLMs) offer new opportunities to enhance\nthe annotation process, particularly for detecting label errors in existing\ndatasets. In this work, we consider the recent approach of LLM-as-a-judge,\nleveraging an ensemble of LLMs to flag potentially mislabeled examples. Through\na case study of four datasets from the TRUE benchmark, covering different tasks\nand domains, we empirically analyze the labeling quality of existing datasets,\nand compare expert, crowd-sourced, and our LLM-based annotations in terms of\nagreement, label quality, and efficiency, demonstrating the strengths and\nlimitations of each annotation method. Our findings reveal a substantial number\nof label errors, which, when corrected, induce a significant upward shift in\nreported model performance. This suggests that many of the LLMs so-called\nmistakes are due to label errors rather than genuine model failures.\nAdditionally, we discuss the implications of mislabeled data and propose\nmethods to mitigate them in training to improve model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLP benchmarks rely on standardized datasets for training and evaluating\nmodels and are crucial for advancing the field. Traditionally, expert\nannotations ensure high-quality labels; however, the cost of expert annotation\ndoes not scale well with the growing demand for larger datasets required by\nmodern models. While crowd-sourcing provides a more scalable solution, it often\ncomes at the expense of annotation precision and consistency. Recent\nadvancements in large language models (LLMs) offer new opportunities to enhance\nthe annotation process, particularly for detecting label errors in existing\ndatasets. In this work, we consider the recent approach of LLM-as-a-judge,\nleveraging an ensemble of LLMs to flag potentially mislabeled examples. Through\na case study of four datasets from the TRUE benchmark, covering different tasks\nand domains, we empirically analyze the labeling quality of existing datasets,\nand compare expert, crowd-sourced, and our LLM-based annotations in terms of\nagreement, label quality, and efficiency, demonstrating the strengths and\nlimitations of each annotation method. Our findings reveal a substantial number\nof label errors, which, when corrected, induce a significant upward shift in\nreported model performance. This suggests that many of the LLMs so-called\nmistakes are due to label errors rather than genuine model failures.\nAdditionally, we discuss the implications of mislabeled data and propose\nmethods to mitigate them in training to improve model performance."
                },
                "authors": [
                    {
                        "name": "Omer Nahum"
                    },
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Orgad Keller"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18881v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18881v1",
                "updated": "2024-10-24T16:17:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    17,
                    18,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T16:17:18Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    17,
                    18,
                    3,
                    298,
                    0
                ],
                "title": "Diff-Instruct++: Training One-step Text-to-image Generator Model to\n  Align with Human Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diff-Instruct++: Training One-step Text-to-image Generator Model to\n  Align with Human Preferences"
                },
                "summary": "One-step text-to-image generator models offer advantages such as swift\ninference efficiency, flexible architectures, and state-of-the-art generation\nperformance. In this paper, we study the problem of aligning one-step generator\nmodels with human preferences for the first time. Inspired by the success of\nreinforcement learning using human feedback (RLHF), we formulate the alignment\nproblem as maximizing expected human reward functions while adding an Integral\nKullback-Leibler divergence term to prevent the generator from diverging. By\novercoming technical challenges, we introduce Diff-Instruct++ (DI++), the\nfirst, fast-converging and image data-free human preference alignment method\nfor one-step text-to-image generators. We also introduce novel theoretical\ninsights, showing that using CFG for diffusion distillation is secretly doing\nRLHF with DI++. Such an interesting finding brings understanding and potential\ncontributions to future research involving CFG. In the experiment sections, we\nalign both UNet-based and DiT-based one-step generators using DI++, which use\nthe Stable Diffusion 1.5 and the PixelArt-$\\alpha$ as the reference diffusion\nprocesses. The resulting DiT-based one-step text-to-image model achieves a\nstrong Aesthetic Score of 6.19 and an Image Reward of 1.24 on the COCO\nvalidation prompt dataset. It also achieves a leading Human preference Score\n(HPSv2.0) of 28.48, outperforming other open-sourced models such as Stable\nDiffusion XL, DMD2, SD-Turbo, as well as PixelArt-$\\alpha$. Both theoretical\ncontributions and empirical evidence indicate that DI++ is a strong\nhuman-preference alignment approach for one-step text-to-image models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-step text-to-image generator models offer advantages such as swift\ninference efficiency, flexible architectures, and state-of-the-art generation\nperformance. In this paper, we study the problem of aligning one-step generator\nmodels with human preferences for the first time. Inspired by the success of\nreinforcement learning using human feedback (RLHF), we formulate the alignment\nproblem as maximizing expected human reward functions while adding an Integral\nKullback-Leibler divergence term to prevent the generator from diverging. By\novercoming technical challenges, we introduce Diff-Instruct++ (DI++), the\nfirst, fast-converging and image data-free human preference alignment method\nfor one-step text-to-image generators. We also introduce novel theoretical\ninsights, showing that using CFG for diffusion distillation is secretly doing\nRLHF with DI++. Such an interesting finding brings understanding and potential\ncontributions to future research involving CFG. In the experiment sections, we\nalign both UNet-based and DiT-based one-step generators using DI++, which use\nthe Stable Diffusion 1.5 and the PixelArt-$\\alpha$ as the reference diffusion\nprocesses. The resulting DiT-based one-step text-to-image model achieves a\nstrong Aesthetic Score of 6.19 and an Image Reward of 1.24 on the COCO\nvalidation prompt dataset. It also achieves a leading Human preference Score\n(HPSv2.0) of 28.48, outperforming other open-sourced models such as Stable\nDiffusion XL, DMD2, SD-Turbo, as well as PixelArt-$\\alpha$. Both theoretical\ncontributions and empirical evidence indicate that DI++ is a strong\nhuman-preference alignment approach for one-step text-to-image models."
                },
                "authors": [
                    {
                        "name": "Weijian Luo"
                    }
                ],
                "author_detail": {
                    "name": "Weijian Luo"
                },
                "author": "Weijian Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18881v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18881v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16197v3",
                "updated": "2024-10-24T16:13:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    13,
                    21,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-21T17:00:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    0,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "LASER: Script Execution by Autonomous Agents for On-demand Traffic\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LASER: Script Execution by Autonomous Agents for On-demand Traffic\n  Simulation"
                },
                "summary": "Autonomous Driving Systems (ADS) require diverse and safety-critical traffic\nscenarios for effective training and testing, but the existing data generation\nmethods struggle to provide flexibility and scalability. We propose LASER, a\nnovel frame-work that leverage large language models (LLMs) to conduct traffic\nsimulations based on natural language inputs. The framework operates in two\nstages: it first generates scripts from user-provided descriptions and then\nexecutes them using autonomous agents in real time. Validated in the CARLA\nsimulator, LASER successfully generates complex, on-demand driving scenarios,\nsignificantly improving ADS training and testing data generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Driving Systems (ADS) require diverse and safety-critical traffic\nscenarios for effective training and testing, but the existing data generation\nmethods struggle to provide flexibility and scalability. We propose LASER, a\nnovel frame-work that leverage large language models (LLMs) to conduct traffic\nsimulations based on natural language inputs. The framework operates in two\nstages: it first generates scripts from user-provided descriptions and then\nexecutes them using autonomous agents in real time. Validated in the CARLA\nsimulator, LASER successfully generates complex, on-demand driving scenarios,\nsignificantly improving ADS training and testing data generation."
                },
                "authors": [
                    {
                        "name": "Hao Gao"
                    },
                    {
                        "name": "Jingyue Wang"
                    },
                    {
                        "name": "Wenyang Fang"
                    },
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Yunpeng Huang"
                    },
                    {
                        "name": "Taolue Chen"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxing Ma"
                },
                "author": "Xiaoxing Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18870v1",
                "updated": "2024-10-24T15:57:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    57,
                    17,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:57:17Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    57,
                    17,
                    3,
                    298,
                    0
                ],
                "title": "End-to-end Training for Recommendation with Language-based User Profiles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end Training for Recommendation with Language-based User Profiles"
                },
                "summary": "Many online platforms maintain user profiles for personalization.\nUnfortunately, these profiles are typically not interpretable or easily\nmodifiable by the user. To remedy this shortcoming, we explore natural\nlanguage-based user profiles, as they promise enhanced transparency and\nscrutability of recommender systems. While existing work has shown that\nlanguage-based profiles from standard LLMs can be effective, such generalist\nLLMs are unlikely to be optimal for this task. In this paper, we introduce\nLangPTune, the first end-to-end learning method for training LLMs to produce\nlanguage-based user profiles that optimize recommendation effectiveness.\nThrough comprehensive evaluations of LangPTune across various training\nconfigurations and benchmarks, we demonstrate that our approach significantly\noutperforms existing profile-based methods. In addition, it approaches\nperformance levels comparable to state-of-the-art, less transparent recommender\nsystems, providing a robust and interpretable alternative to conventional\nsystems. Finally, we validate the relative interpretability of these\nlanguage-based user profiles through user studies involving crowdworkers and\nGPT-4-based evaluations. Implementation of LangPTune can be found at\nhttps://github.com/ZhaolinGao/LangPTune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many online platforms maintain user profiles for personalization.\nUnfortunately, these profiles are typically not interpretable or easily\nmodifiable by the user. To remedy this shortcoming, we explore natural\nlanguage-based user profiles, as they promise enhanced transparency and\nscrutability of recommender systems. While existing work has shown that\nlanguage-based profiles from standard LLMs can be effective, such generalist\nLLMs are unlikely to be optimal for this task. In this paper, we introduce\nLangPTune, the first end-to-end learning method for training LLMs to produce\nlanguage-based user profiles that optimize recommendation effectiveness.\nThrough comprehensive evaluations of LangPTune across various training\nconfigurations and benchmarks, we demonstrate that our approach significantly\noutperforms existing profile-based methods. In addition, it approaches\nperformance levels comparable to state-of-the-art, less transparent recommender\nsystems, providing a robust and interpretable alternative to conventional\nsystems. Finally, we validate the relative interpretability of these\nlanguage-based user profiles through user studies involving crowdworkers and\nGPT-4-based evaluations. Implementation of LangPTune can be found at\nhttps://github.com/ZhaolinGao/LangPTune."
                },
                "authors": [
                    {
                        "name": "Zhaolin Gao"
                    },
                    {
                        "name": "Joyce Zhou"
                    },
                    {
                        "name": "Yijia Dai"
                    },
                    {
                        "name": "Thorsten Joachims"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Joachims"
                },
                "author": "Thorsten Joachims",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18868v1",
                "updated": "2024-10-24T15:53:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    53,
                    21,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:53:21Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    53,
                    21,
                    3,
                    298,
                    0
                ],
                "title": "A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics"
                },
                "summary": "By incorporating physical consistency as inductive bias, deep neural networks\ndisplay increased generalization capabilities and data efficiency in learning\nnonlinear dynamic models. However, the complexity of these models generally\nincreases with the system dimensionality, requiring larger datasets, more\ncomplex deep networks, and significant computational effort. We propose a novel\ngeometric network architecture to learn physically-consistent reduced-order\ndynamic parameters that accurately describe the original high-dimensional\nsystem behavior. This is achieved by building on recent advances in model-order\nreduction and by adopting a Riemannian perspective to jointly learn a\nstructure-preserving latent space and the associated low-dimensional dynamics.\nOur approach enables accurate long-term predictions of the high-dimensional\ndynamics of rigid and deformable systems with increased data efficiency by\ninferring interpretable and physically plausible reduced Lagrangian models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By incorporating physical consistency as inductive bias, deep neural networks\ndisplay increased generalization capabilities and data efficiency in learning\nnonlinear dynamic models. However, the complexity of these models generally\nincreases with the system dimensionality, requiring larger datasets, more\ncomplex deep networks, and significant computational effort. We propose a novel\ngeometric network architecture to learn physically-consistent reduced-order\ndynamic parameters that accurately describe the original high-dimensional\nsystem behavior. This is achieved by building on recent advances in model-order\nreduction and by adopting a Riemannian perspective to jointly learn a\nstructure-preserving latent space and the associated low-dimensional dynamics.\nOur approach enables accurate long-term predictions of the high-dimensional\ndynamics of rigid and deformable systems with increased data efficiency by\ninferring interpretable and physically plausible reduced Lagrangian models."
                },
                "authors": [
                    {
                        "name": "Katharina Friedl"
                    },
                    {
                        "name": "Noémie Jaquier"
                    },
                    {
                        "name": "Jens Lundell"
                    },
                    {
                        "name": "Tamim Asfour"
                    },
                    {
                        "name": "Danica Kragic"
                    }
                ],
                "author_detail": {
                    "name": "Danica Kragic"
                },
                "author": "Danica Kragic",
                "arxiv_comment": "28 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18861v1",
                "updated": "2024-10-24T15:44:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    44,
                    34,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:44:34Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    44,
                    34,
                    3,
                    298,
                    0
                ],
                "title": "Provably Robust Watermarks for Open-Source Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Robust Watermarks for Open-Source Language Models"
                },
                "summary": "The recent explosion of high-quality language models has necessitated new\nmethods for identifying AI-generated text. Watermarking is a leading solution\nand could prove to be an essential tool in the age of generative AI. Existing\napproaches embed watermarks at inference and crucially rely on the large\nlanguage model (LLM) specification and parameters being secret, which makes\nthem inapplicable to the open-source setting. In this work, we introduce the\nfirst watermarking scheme for open-source LLMs. Our scheme works by modifying\nthe parameters of the model, but the watermark can be detected from just the\noutputs of the model. Perhaps surprisingly, we prove that our watermarks are\nunremovable under certain assumptions about the adversary's knowledge. To\ndemonstrate the behavior of our construction under concrete parameter\ninstantiations, we present experimental results with OPT-6.7B and OPT-1.3B. We\ndemonstrate robustness to both token substitution and perturbation of the model\nparameters. We find that the stronger of these attacks, the model-perturbation\nattack, requires deteriorating the quality score to 0 out of 100 in order to\nbring the detection rate down to 50%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent explosion of high-quality language models has necessitated new\nmethods for identifying AI-generated text. Watermarking is a leading solution\nand could prove to be an essential tool in the age of generative AI. Existing\napproaches embed watermarks at inference and crucially rely on the large\nlanguage model (LLM) specification and parameters being secret, which makes\nthem inapplicable to the open-source setting. In this work, we introduce the\nfirst watermarking scheme for open-source LLMs. Our scheme works by modifying\nthe parameters of the model, but the watermark can be detected from just the\noutputs of the model. Perhaps surprisingly, we prove that our watermarks are\nunremovable under certain assumptions about the adversary's knowledge. To\ndemonstrate the behavior of our construction under concrete parameter\ninstantiations, we present experimental results with OPT-6.7B and OPT-1.3B. We\ndemonstrate robustness to both token substitution and perturbation of the model\nparameters. We find that the stronger of these attacks, the model-perturbation\nattack, requires deteriorating the quality score to 0 out of 100 in order to\nbring the detection rate down to 50%."
                },
                "authors": [
                    {
                        "name": "Miranda Christ"
                    },
                    {
                        "name": "Sam Gunn"
                    },
                    {
                        "name": "Tal Malkin"
                    },
                    {
                        "name": "Mariana Raykova"
                    }
                ],
                "author_detail": {
                    "name": "Mariana Raykova"
                },
                "author": "Mariana Raykova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18860v1",
                "updated": "2024-10-24T15:44:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    44,
                    33,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:44:33Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    44,
                    33,
                    3,
                    298,
                    0
                ],
                "title": "DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate\n  Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate\n  Hallucinations"
                },
                "summary": "Large Language Models (LLMs) often hallucinate, producing unfaithful or\nfactually incorrect outputs by misrepresenting the provided context or\nincorrectly recalling internal knowledge. Recent studies have identified\nspecific attention heads within the Transformer architecture, known as\nretrieval heads, responsible for extracting relevant contextual information. We\nhypothesise that masking these retrieval heads can induce hallucinations and\nthat contrasting the outputs of the base LLM and the masked LLM can reduce\nhallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads\n(DeCoRe), a novel training-free decoding strategy that amplifies information\nfound in the context and model parameters. DeCoRe mitigates potentially\nhallucinated responses by dynamically contrasting the outputs of the base LLM\nand the masked LLM, using conditional entropy as a guide. Our extensive\nexperiments confirm that DeCoRe significantly improves performance on tasks\nrequiring high contextual faithfulness, such as summarisation (XSum by 18.6%),\ninstruction following (MemoTrap by 10.9%), and open-book question answering\n(NQ-Open by 2.4% and NQ-Swap by 5.5%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often hallucinate, producing unfaithful or\nfactually incorrect outputs by misrepresenting the provided context or\nincorrectly recalling internal knowledge. Recent studies have identified\nspecific attention heads within the Transformer architecture, known as\nretrieval heads, responsible for extracting relevant contextual information. We\nhypothesise that masking these retrieval heads can induce hallucinations and\nthat contrasting the outputs of the base LLM and the masked LLM can reduce\nhallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads\n(DeCoRe), a novel training-free decoding strategy that amplifies information\nfound in the context and model parameters. DeCoRe mitigates potentially\nhallucinated responses by dynamically contrasting the outputs of the base LLM\nand the masked LLM, using conditional entropy as a guide. Our extensive\nexperiments confirm that DeCoRe significantly improves performance on tasks\nrequiring high contextual faithfulness, such as summarisation (XSum by 18.6%),\ninstruction following (MemoTrap by 10.9%), and open-book question answering\n(NQ-Open by 2.4% and NQ-Swap by 5.5%)."
                },
                "authors": [
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Chen Jin"
                    },
                    {
                        "name": "Ahmed Abdulaal"
                    },
                    {
                        "name": "Tom Diethe"
                    },
                    {
                        "name": "Philip Teare"
                    },
                    {
                        "name": "Beatrice Alex"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Amrutha Saseendran"
                    }
                ],
                "author_detail": {
                    "name": "Amrutha Saseendran"
                },
                "author": "Amrutha Saseendran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18856v1",
                "updated": "2024-10-24T15:41:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    41,
                    56,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:41:56Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    41,
                    56,
                    3,
                    298,
                    0
                ],
                "title": "Demystifying Large Language Models for Medicine: A Primer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Large Language Models for Medicine: A Primer"
                },
                "summary": "Large language models (LLMs) represent a transformative class of AI tools\ncapable of revolutionizing various aspects of healthcare by generating\nhuman-like responses across diverse contexts and adapting to novel tasks\nfollowing human instructions. Their potential application spans a broad range\nof medical tasks, such as clinical documentation, matching patients to clinical\ntrials, and answering medical questions. In this primer paper, we propose an\nactionable guideline to help healthcare professionals more efficiently utilize\nLLMs in their work, along with a set of best practices. This approach consists\nof several main phases, including formulating the task, choosing LLMs, prompt\nengineering, fine-tuning, and deployment. We start with the discussion of\ncritical considerations in identifying healthcare tasks that align with the\ncore capabilities of LLMs and selecting models based on the selected task and\ndata, performance requirements, and model interface. We then review the\nstrategies, such as prompt engineering and fine-tuning, to adapt standard LLMs\nto specialized medical tasks. Deployment considerations, including regulatory\ncompliance, ethical guidelines, and continuous monitoring for fairness and\nbias, are also discussed. By providing a structured step-by-step methodology,\nthis tutorial aims to equip healthcare professionals with the tools necessary\nto effectively integrate LLMs into clinical practice, ensuring that these\npowerful technologies are applied in a safe, reliable, and impactful manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a transformative class of AI tools\ncapable of revolutionizing various aspects of healthcare by generating\nhuman-like responses across diverse contexts and adapting to novel tasks\nfollowing human instructions. Their potential application spans a broad range\nof medical tasks, such as clinical documentation, matching patients to clinical\ntrials, and answering medical questions. In this primer paper, we propose an\nactionable guideline to help healthcare professionals more efficiently utilize\nLLMs in their work, along with a set of best practices. This approach consists\nof several main phases, including formulating the task, choosing LLMs, prompt\nengineering, fine-tuning, and deployment. We start with the discussion of\ncritical considerations in identifying healthcare tasks that align with the\ncore capabilities of LLMs and selecting models based on the selected task and\ndata, performance requirements, and model interface. We then review the\nstrategies, such as prompt engineering and fine-tuning, to adapt standard LLMs\nto specialized medical tasks. Deployment considerations, including regulatory\ncompliance, ethical guidelines, and continuous monitoring for fairness and\nbias, are also discussed. By providing a structured step-by-step methodology,\nthis tutorial aims to equip healthcare professionals with the tools necessary\nto effectively integrate LLMs into clinical practice, ensuring that these\npowerful technologies are applied in a safe, reliable, and impactful manner."
                },
                "authors": [
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Nicholas Wan"
                    },
                    {
                        "name": "Robert Leaman"
                    },
                    {
                        "name": "Shubo Tian"
                    },
                    {
                        "name": "Zhizheng Wang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Po-Ting Lai"
                    },
                    {
                        "name": "Qingqing Zhu"
                    },
                    {
                        "name": "Benjamin Hou"
                    },
                    {
                        "name": "Maame Sarfo-Gyamfi"
                    },
                    {
                        "name": "Gongbo Zhang"
                    },
                    {
                        "name": "Aidan Gilson"
                    },
                    {
                        "name": "Balu Bhasuran"
                    },
                    {
                        "name": "Zhe He"
                    },
                    {
                        "name": "Aidong Zhang"
                    },
                    {
                        "name": "Jimeng Sun"
                    },
                    {
                        "name": "Chunhua Weng"
                    },
                    {
                        "name": "Ronald M. Summers"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Zhiyong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Lu"
                },
                "author": "Zhiyong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18850v1",
                "updated": "2024-10-24T15:32:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    32,
                    52,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:32:52Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    32,
                    52,
                    3,
                    298,
                    0
                ],
                "title": "We Augmented Whisper With kNN and You Won't Believe What Came Next",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We Augmented Whisper With kNN and You Won't Believe What Came Next"
                },
                "summary": "Speech recognition performance varies by language, domain, and speaker\ncharacteristics such as accent, and fine-tuning a model on any of these\ncategories may lead to catastrophic forgetting. $k$ nearest neighbor search\n($k$NN), first proposed for neural sequence decoders for natural language\ngeneration (NLG) and machine translation (MT), is a non-parametric method that\ncan instead adapt by building an external datastore that can then be searched\nduring inference time, without training the underlying model. We show that\nWhisper, a transformer end-to-end speech model, benefits from $k$NN. We\ninvestigate the differences between the speech and text setups. We discuss\nimplications for speaker adaptation, and analyze improvements by gender,\naccent, and age.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech recognition performance varies by language, domain, and speaker\ncharacteristics such as accent, and fine-tuning a model on any of these\ncategories may lead to catastrophic forgetting. $k$ nearest neighbor search\n($k$NN), first proposed for neural sequence decoders for natural language\ngeneration (NLG) and machine translation (MT), is a non-parametric method that\ncan instead adapt by building an external datastore that can then be searched\nduring inference time, without training the underlying model. We show that\nWhisper, a transformer end-to-end speech model, benefits from $k$NN. We\ninvestigate the differences between the speech and text setups. We discuss\nimplications for speaker adaptation, and analyze improvements by gender,\naccent, and age."
                },
                "authors": [
                    {
                        "name": "Maya K. Nachesa"
                    },
                    {
                        "name": "Vlad Niculae"
                    }
                ],
                "author_detail": {
                    "name": "Vlad Niculae"
                },
                "author": "Vlad Niculae",
                "arxiv_comment": "6 pages incl. appendix, 2 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18836v1",
                "updated": "2024-10-24T15:20:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    20,
                    54,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:20:54Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    20,
                    54,
                    3,
                    298,
                    0
                ],
                "title": "From English-Centric to Effective Bilingual: LLMs with Custom Tokenizers\n  for Underrepresented Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From English-Centric to Effective Bilingual: LLMs with Custom Tokenizers\n  for Underrepresented Languages"
                },
                "summary": "In this paper, we propose a model-agnostic cost-effective approach to\ndeveloping bilingual base large language models (LLMs) to support English and\nany target language. The method includes vocabulary expansion, initialization\nof new embeddings, model training and evaluation. We performed our experiments\nwith three languages, each using a non-Latin script - Ukrainian, Arabic, and\nGeorgian.\n  Our approach demonstrates improved language performance while reducing\ncomputational costs. It mitigates the disproportionate penalization of\nunderrepresented languages, promoting fairness and minimizing adverse phenomena\nsuch as code-switching and broken grammar. Additionally, we introduce new\nmetrics to evaluate language quality, revealing that vocabulary size\nsignificantly impacts the quality of generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a model-agnostic cost-effective approach to\ndeveloping bilingual base large language models (LLMs) to support English and\nany target language. The method includes vocabulary expansion, initialization\nof new embeddings, model training and evaluation. We performed our experiments\nwith three languages, each using a non-Latin script - Ukrainian, Arabic, and\nGeorgian.\n  Our approach demonstrates improved language performance while reducing\ncomputational costs. It mitigates the disproportionate penalization of\nunderrepresented languages, promoting fairness and minimizing adverse phenomena\nsuch as code-switching and broken grammar. Additionally, we introduce new\nmetrics to evaluate language quality, revealing that vocabulary size\nsignificantly impacts the quality of generated text."
                },
                "authors": [
                    {
                        "name": "Artur Kiulian"
                    },
                    {
                        "name": "Anton Polishko"
                    },
                    {
                        "name": "Mykola Khandoga"
                    },
                    {
                        "name": "Yevhen Kostiuk"
                    },
                    {
                        "name": "Guillermo Gabrielli"
                    },
                    {
                        "name": "Łukasz Gagała"
                    },
                    {
                        "name": "Fadi Zaraket"
                    },
                    {
                        "name": "Qusai Abu Obaida"
                    },
                    {
                        "name": "Hrishikesh Garud"
                    },
                    {
                        "name": "Wendy Wing Yee Mak"
                    },
                    {
                        "name": "Dmytro Chaplynskyi"
                    },
                    {
                        "name": "Selma Belhadj Amor"
                    },
                    {
                        "name": "Grigol Peradze"
                    }
                ],
                "author_detail": {
                    "name": "Grigol Peradze"
                },
                "author": "Grigol Peradze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18824v1",
                "updated": "2024-10-24T15:15:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    15,
                    42,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:15:42Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    15,
                    42,
                    3,
                    298,
                    0
                ],
                "title": "PSY: Posterior Sampling Based Privacy Enhancer in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PSY: Posterior Sampling Based Privacy Enhancer in Large Language Models"
                },
                "summary": "Privacy vulnerabilities in LLMs, such as leakage from memorization, have been\nconstantly identified, and various mitigation proposals have been proposed.\nLoRA is usually used in fine-tuning LLMs and a good entry point to insert\nprivacy-enhancing modules. In this ongoing research, we introduce PSY, a\nPosterior Sampling based PrivacY enhancer that can be used in LoRA. We propose\na simple yet effective realization of PSY using posterior sampling, which\neffectively prevents privacy leakage from intermediate information and, in\nturn, preserves the privacy of data owners. We evaluate LoRA extended with PSY\nagainst state-of-the-art membership inference and data extraction attacks. The\nexperiments are executed on three different LLM architectures fine-tuned on\nthree datasets with LoRA. In contrast to the commonly used differential privacy\nmethod, we find that our proposed modification consistently reduces the attack\nsuccess rate. Meanwhile, our method has almost no negative impact on model\nfine-tuning or final performance. Most importantly, PSY reveals a promising\npath toward privacy enhancement with latent space extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy vulnerabilities in LLMs, such as leakage from memorization, have been\nconstantly identified, and various mitigation proposals have been proposed.\nLoRA is usually used in fine-tuning LLMs and a good entry point to insert\nprivacy-enhancing modules. In this ongoing research, we introduce PSY, a\nPosterior Sampling based PrivacY enhancer that can be used in LoRA. We propose\na simple yet effective realization of PSY using posterior sampling, which\neffectively prevents privacy leakage from intermediate information and, in\nturn, preserves the privacy of data owners. We evaluate LoRA extended with PSY\nagainst state-of-the-art membership inference and data extraction attacks. The\nexperiments are executed on three different LLM architectures fine-tuned on\nthree datasets with LoRA. In contrast to the commonly used differential privacy\nmethod, we find that our proposed modification consistently reduces the attack\nsuccess rate. Meanwhile, our method has almost no negative impact on model\nfine-tuning or final performance. Most importantly, PSY reveals a promising\npath toward privacy enhancement with latent space extensions."
                },
                "authors": [
                    {
                        "name": "Yulian Sun"
                    },
                    {
                        "name": "Li Duan"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18822v1",
                "updated": "2024-10-24T15:10:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    10,
                    27,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:10:27Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    10,
                    27,
                    3,
                    298,
                    0
                ],
                "title": "Binocular-Guided 3D Gaussian Splatting with View Consistency for Sparse\n  View Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binocular-Guided 3D Gaussian Splatting with View Consistency for Sparse\n  View Synthesis"
                },
                "summary": "Novel view synthesis from sparse inputs is a vital yet challenging task in 3D\ncomputer vision. Previous methods explore 3D Gaussian Splatting with neural\npriors (e.g. depth priors) as an additional supervision, demonstrating\npromising quality and efficiency compared to the NeRF based methods. However,\nthe neural priors from 2D pretrained models are often noisy and blurry, which\nstruggle to precisely guide the learning of radiance fields. In this paper, We\npropose a novel method for synthesizing novel views from sparse views with\nGaussian Splatting that does not require external prior as supervision. Our key\nidea lies in exploring the self-supervisions inherent in the binocular stereo\nconsistency between each pair of binocular images constructed with\ndisparity-guided image warping. To this end, we additionally introduce a\nGaussian opacity constraint which regularizes the Gaussian locations and avoids\nGaussian redundancy for improving the robustness and efficiency of inferring 3D\nGaussians from sparse views. Extensive experiments on the LLFF, DTU, and\nBlender datasets demonstrate that our method significantly outperforms the\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel view synthesis from sparse inputs is a vital yet challenging task in 3D\ncomputer vision. Previous methods explore 3D Gaussian Splatting with neural\npriors (e.g. depth priors) as an additional supervision, demonstrating\npromising quality and efficiency compared to the NeRF based methods. However,\nthe neural priors from 2D pretrained models are often noisy and blurry, which\nstruggle to precisely guide the learning of radiance fields. In this paper, We\npropose a novel method for synthesizing novel views from sparse views with\nGaussian Splatting that does not require external prior as supervision. Our key\nidea lies in exploring the self-supervisions inherent in the binocular stereo\nconsistency between each pair of binocular images constructed with\ndisparity-guided image warping. To this end, we additionally introduce a\nGaussian opacity constraint which regularizes the Gaussian locations and avoids\nGaussian redundancy for improving the robustness and efficiency of inferring 3D\nGaussians from sparse views. Extensive experiments on the LLFF, DTU, and\nBlender datasets demonstrate that our method significantly outperforms the\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Liang Han"
                    },
                    {
                        "name": "Junsheng Zhou"
                    },
                    {
                        "name": "Yu-Shen Liu"
                    },
                    {
                        "name": "Zhizhong Han"
                    }
                ],
                "author_detail": {
                    "name": "Zhizhong Han"
                },
                "author": "Zhizhong Han",
                "arxiv_comment": "Accepted by NeurIPS 2024. Project page:\n  https://hanl2010.github.io/Binocular3DGS/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17248v2",
                "updated": "2024-10-24T15:06:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    6,
                    36,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-22T17:59:55Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    59,
                    55,
                    1,
                    296,
                    0
                ],
                "title": "HyperspectralViTs: General Hyperspectral Models for On-board Remote\n  Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperspectralViTs: General Hyperspectral Models for On-board Remote\n  Sensing"
                },
                "summary": "On-board processing of hyperspectral data with machine learning models would\nenable unprecedented amount of autonomy for a wide range of tasks, for example\nmethane detection or mineral identification. This can enable early warning\nsystem and could allow new capabilities such as automated scheduling across\nconstellations of satellites. Classical methods suffer from high false positive\nrates and previous deep learning models exhibit prohibitive computational\nrequirements. We propose fast and accurate machine learning architectures which\nsupport end-to-end training with data of high spectral dimension without\nrelying on hand-crafted products or spectral band compression preprocessing. We\nevaluate our models on two tasks related to hyperspectral data processing. With\nour proposed general architectures, we improve the F1 score of the previous\nmethane detection state-of-the-art models by 27% on a newly created synthetic\ndataset and by 13% on the previously released large benchmark dataset. We also\ndemonstrate that training models on the synthetic dataset improves performance\nof models finetuned on the dataset of real events by 6.9% in F1 score in\ncontrast with training from scratch. On a newly created dataset for mineral\nidentification, our models provide 3.5% improvement in the F1 score in contrast\nto the default versions of the models. With our proposed models we improve the\ninference speed by 85% in contrast to previous classical and deep learning\napproaches by removing the dependency on classically computed features. With\nour architecture, one capture from the EMIT sensor can be processed within 30\nseconds on realistic proxy of the ION-SCV 004 satellite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-board processing of hyperspectral data with machine learning models would\nenable unprecedented amount of autonomy for a wide range of tasks, for example\nmethane detection or mineral identification. This can enable early warning\nsystem and could allow new capabilities such as automated scheduling across\nconstellations of satellites. Classical methods suffer from high false positive\nrates and previous deep learning models exhibit prohibitive computational\nrequirements. We propose fast and accurate machine learning architectures which\nsupport end-to-end training with data of high spectral dimension without\nrelying on hand-crafted products or spectral band compression preprocessing. We\nevaluate our models on two tasks related to hyperspectral data processing. With\nour proposed general architectures, we improve the F1 score of the previous\nmethane detection state-of-the-art models by 27% on a newly created synthetic\ndataset and by 13% on the previously released large benchmark dataset. We also\ndemonstrate that training models on the synthetic dataset improves performance\nof models finetuned on the dataset of real events by 6.9% in F1 score in\ncontrast with training from scratch. On a newly created dataset for mineral\nidentification, our models provide 3.5% improvement in the F1 score in contrast\nto the default versions of the models. With our proposed models we improve the\ninference speed by 85% in contrast to previous classical and deep learning\napproaches by removing the dependency on classically computed features. With\nour architecture, one capture from the EMIT sensor can be processed within 30\nseconds on realistic proxy of the ION-SCV 004 satellite."
                },
                "authors": [
                    {
                        "name": "Vít Růžička"
                    },
                    {
                        "name": "Andrew Markham"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Markham"
                },
                "author": "Andrew Markham",
                "arxiv_comment": "13 pages, This work has been submitted to the IEEE for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18815v1",
                "updated": "2024-10-24T15:02:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    2,
                    14,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:02:14Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    2,
                    14,
                    3,
                    298,
                    0
                ],
                "title": "McFacts III: Compact binary mergers from AGN disks over an entire\n  synthetic universe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "McFacts III: Compact binary mergers from AGN disks over an entire\n  synthetic universe"
                },
                "summary": "The Active Galactic Nuclei (AGN) channel for the formation of binary black\nhole (BBH) mergers has been previously studied as a potential formation channel\nfor the merging compact binaries observed by the LIGO/Virgo/KAGRA (LVK)\nscientific collaboration. The first two papers in this series explored the\nMcFACTS code for the evolution of black hole orbits in AGN accretion disks for\nindividual galaxy models and described the characteristics of predicted BBH\npopulations in realizations of those models (such as the correlation between\nmass ratio and aligned spin). In this work, we explore the impact of the\nproperties of AGN host galaxies and assume an AGN lifetime and cosmological\nmodel for the density of AGN in a universe like our own. By sampling from an\ninferred population of AGN, we marginalize over galaxy mass to predict a\npopulation of BBH mergers observable by modern ground-based gravitational wave\nobservatories. We find that for reasonable assumptions, AGN disk environments\nmay account for massive BBH mergers such as GW190521 and GW190929_012149. We\nfind that the majority of observable BBH mergers from our simulation are\nexpected to originate in galaxies with a super-massive black hole between\n$10^{7}M_{\\odot}$ and $10^{9.4}M_{\\odot}$. We also find that if hierarchical\nmergers from AGN disks account for a substantial part of the LVK population,\nour current models require an AGN lifetime of 0.5 to 2.5 Myr.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Active Galactic Nuclei (AGN) channel for the formation of binary black\nhole (BBH) mergers has been previously studied as a potential formation channel\nfor the merging compact binaries observed by the LIGO/Virgo/KAGRA (LVK)\nscientific collaboration. The first two papers in this series explored the\nMcFACTS code for the evolution of black hole orbits in AGN accretion disks for\nindividual galaxy models and described the characteristics of predicted BBH\npopulations in realizations of those models (such as the correlation between\nmass ratio and aligned spin). In this work, we explore the impact of the\nproperties of AGN host galaxies and assume an AGN lifetime and cosmological\nmodel for the density of AGN in a universe like our own. By sampling from an\ninferred population of AGN, we marginalize over galaxy mass to predict a\npopulation of BBH mergers observable by modern ground-based gravitational wave\nobservatories. We find that for reasonable assumptions, AGN disk environments\nmay account for massive BBH mergers such as GW190521 and GW190929_012149. We\nfind that the majority of observable BBH mergers from our simulation are\nexpected to originate in galaxies with a super-massive black hole between\n$10^{7}M_{\\odot}$ and $10^{9.4}M_{\\odot}$. We also find that if hierarchical\nmergers from AGN disks account for a substantial part of the LVK population,\nour current models require an AGN lifetime of 0.5 to 2.5 Myr."
                },
                "authors": [
                    {
                        "name": "Vera Delfavero"
                    },
                    {
                        "name": "K. E. Saavik Ford"
                    },
                    {
                        "name": "Barry McKernan"
                    },
                    {
                        "name": "Harrison E. Cook"
                    },
                    {
                        "name": "Kaila Nathaniel"
                    },
                    {
                        "name": "Jake Postiglione"
                    },
                    {
                        "name": "Shawn Ray"
                    },
                    {
                        "name": "Richard O'Shaughnessy"
                    }
                ],
                "author_detail": {
                    "name": "Richard O'Shaughnessy"
                },
                "author": "Richard O'Shaughnessy",
                "arxiv_comment": "Submitting to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05794v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05794v3",
                "updated": "2024-10-24T14:57:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    57,
                    52,
                    3,
                    298,
                    0
                ],
                "published": "2024-06-09T14:11:19Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    14,
                    11,
                    19,
                    6,
                    161,
                    0
                ],
                "title": "RE-RAG: Improving Open-Domain QA Performance and Interpretability with\n  Relevance Estimator in Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RE-RAG: Improving Open-Domain QA Performance and Interpretability with\n  Relevance Estimator in Retrieval-Augmented Generation"
                },
                "summary": "The Retrieval Augmented Generation (RAG) framework utilizes a combination of\nparametric knowledge and external knowledge to demonstrate state-of-the-art\nperformance on open-domain question answering tasks. However, the RAG framework\nsuffers from performance degradation when the query is accompanied by\nirrelevant contexts. In this work, we propose the RE-RAG framework, which\nintroduces a relevance estimator (RE) that not only provides relative relevance\nbetween contexts as previous rerankers did, but also provides confidence, which\ncan be used to classify whether given context is useful for answering the given\nquestion. We propose a weakly supervised method for training the RE simply\nutilizing question-answer data without any labels for correct contexts. We show\nthat RE trained with a small generator (sLM) can not only improve the sLM\nfine-tuned together with RE but also improve previously unreferenced large\nlanguage models (LLMs). Furthermore, we investigate new decoding strategies\nthat utilize the proposed confidence measured by RE such as choosing to let the\nuser know that it is \"unanswerable\" to answer the question given the retrieved\ncontexts or choosing to rely on LLM's parametric knowledge rather than\nunrelated contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Retrieval Augmented Generation (RAG) framework utilizes a combination of\nparametric knowledge and external knowledge to demonstrate state-of-the-art\nperformance on open-domain question answering tasks. However, the RAG framework\nsuffers from performance degradation when the query is accompanied by\nirrelevant contexts. In this work, we propose the RE-RAG framework, which\nintroduces a relevance estimator (RE) that not only provides relative relevance\nbetween contexts as previous rerankers did, but also provides confidence, which\ncan be used to classify whether given context is useful for answering the given\nquestion. We propose a weakly supervised method for training the RE simply\nutilizing question-answer data without any labels for correct contexts. We show\nthat RE trained with a small generator (sLM) can not only improve the sLM\nfine-tuned together with RE but also improve previously unreferenced large\nlanguage models (LLMs). Furthermore, we investigate new decoding strategies\nthat utilize the proposed confidence measured by RE such as choosing to let the\nuser know that it is \"unanswerable\" to answer the question given the retrieved\ncontexts or choosing to rely on LLM's parametric knowledge rather than\nunrelated contexts."
                },
                "authors": [
                    {
                        "name": "Kiseung Kim"
                    },
                    {
                        "name": "Jay-Yoon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jay-Yoon Lee"
                },
                "author": "Jay-Yoon Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05794v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05794v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18808v1",
                "updated": "2024-10-24T14:55:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    55,
                    9,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:55:09Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    55,
                    9,
                    3,
                    298,
                    0
                ],
                "title": "Delving into the Reversal Curse: How Far Can Large Language Models\n  Generalize?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delving into the Reversal Curse: How Far Can Large Language Models\n  Generalize?"
                },
                "summary": "While large language models (LLMs) showcase unprecedented capabilities, they\nalso exhibit certain inherent limitations when facing seemingly trivial tasks.\nA prime example is the recently debated \"reversal curse\", which surfaces when\nmodels, having been trained on the fact \"A is B\", struggle to generalize this\nknowledge to infer that \"B is A\". In this paper, we examine the manifestation\nof the reversal curse across various tasks and delve into both the\ngeneralization abilities and the problem-solving mechanisms of LLMs. This\ninvestigation leads to a series of significant insights: (1) LLMs are able to\ngeneralize to \"B is A\" when both A and B are presented in the context as in the\ncase of a multiple-choice question. (2) This generalization ability is highly\ncorrelated to the structure of the fact \"A is B\" in the training documents. For\nexample, this generalization only applies to biographies structured in \"[Name]\nis [Description]\" but not to \"[Description] is [Name]\". (3) We propose and\nverify the hypothesis that LLMs possess an inherent bias in fact recalling\nduring knowledge application, which explains and underscores the importance of\nthe document structure to successful learning. (4) The negative impact of this\nbias on the downstream performance of LLMs can hardly be mitigated through\ntraining alone. Based on these intriguing findings, our work not only presents\na novel perspective for interpreting LLMs' generalization abilities from their\nintrinsic working mechanism but also provides new insights for the development\nof more effective learning methods for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) showcase unprecedented capabilities, they\nalso exhibit certain inherent limitations when facing seemingly trivial tasks.\nA prime example is the recently debated \"reversal curse\", which surfaces when\nmodels, having been trained on the fact \"A is B\", struggle to generalize this\nknowledge to infer that \"B is A\". In this paper, we examine the manifestation\nof the reversal curse across various tasks and delve into both the\ngeneralization abilities and the problem-solving mechanisms of LLMs. This\ninvestigation leads to a series of significant insights: (1) LLMs are able to\ngeneralize to \"B is A\" when both A and B are presented in the context as in the\ncase of a multiple-choice question. (2) This generalization ability is highly\ncorrelated to the structure of the fact \"A is B\" in the training documents. For\nexample, this generalization only applies to biographies structured in \"[Name]\nis [Description]\" but not to \"[Description] is [Name]\". (3) We propose and\nverify the hypothesis that LLMs possess an inherent bias in fact recalling\nduring knowledge application, which explains and underscores the importance of\nthe document structure to successful learning. (4) The negative impact of this\nbias on the downstream performance of LLMs can hardly be mitigated through\ntraining alone. Based on these intriguing findings, our work not only presents\na novel perspective for interpreting LLMs' generalization abilities from their\nintrinsic working mechanism but also provides new insights for the development\nof more effective learning methods for LLMs."
                },
                "authors": [
                    {
                        "name": "Zhengkai Lin"
                    },
                    {
                        "name": "Zhihang Fu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Binbin Lin"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18804v1",
                "updated": "2024-10-24T14:52:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    52,
                    38,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:52:38Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    52,
                    38,
                    3,
                    298,
                    0
                ],
                "title": "Fast constrained sampling in pre-trained diffusion models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast constrained sampling in pre-trained diffusion models"
                },
                "summary": "Diffusion models have dominated the field of large, generative image models,\nwith the prime examples of Stable Diffusion and DALL-E 3 being widely adopted.\nThese models have been trained to perform text-conditioned generation on vast\nnumbers of image-caption pairs and as a byproduct, have acquired general\nknowledge about natural image statistics. However, when confronted with the\ntask of constrained sampling, e.g. generating the right half of an image\nconditioned on the known left half, applying these models is a delicate and\nslow process, with previously proposed algorithms relying on expensive\niterative operations that are usually orders of magnitude slower than\ntext-based inference. This is counter-intuitive, as image-conditioned\ngeneration should rely less on the difficult-to-learn semantic knowledge that\nlinks captions and imagery, and should instead be achievable by lower-level\ncorrelations among image pixels. In practice, inverse models are trained or\ntuned separately for each inverse problem, e.g. by providing parts of images\nduring training as an additional condition, to allow their application in\nrealistic settings. However, we argue that this is not necessary and propose an\nalgorithm for fast-constrained sampling in large pre-trained diffusion models\n(Stable Diffusion) that requires no expensive backpropagation operations\nthrough the model and produces results comparable even to the state-of-the-art\n\\emph{tuned} models. Our method is based on a novel optimization perspective to\nsampling under constraints and employs a numerical approximation to the\nexpensive gradients, previously computed using backpropagation, incurring\nsignificant speed-ups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have dominated the field of large, generative image models,\nwith the prime examples of Stable Diffusion and DALL-E 3 being widely adopted.\nThese models have been trained to perform text-conditioned generation on vast\nnumbers of image-caption pairs and as a byproduct, have acquired general\nknowledge about natural image statistics. However, when confronted with the\ntask of constrained sampling, e.g. generating the right half of an image\nconditioned on the known left half, applying these models is a delicate and\nslow process, with previously proposed algorithms relying on expensive\niterative operations that are usually orders of magnitude slower than\ntext-based inference. This is counter-intuitive, as image-conditioned\ngeneration should rely less on the difficult-to-learn semantic knowledge that\nlinks captions and imagery, and should instead be achievable by lower-level\ncorrelations among image pixels. In practice, inverse models are trained or\ntuned separately for each inverse problem, e.g. by providing parts of images\nduring training as an additional condition, to allow their application in\nrealistic settings. However, we argue that this is not necessary and propose an\nalgorithm for fast-constrained sampling in large pre-trained diffusion models\n(Stable Diffusion) that requires no expensive backpropagation operations\nthrough the model and produces results comparable even to the state-of-the-art\n\\emph{tuned} models. Our method is based on a novel optimization perspective to\nsampling under constraints and employs a numerical approximation to the\nexpensive gradients, previously computed using backpropagation, incurring\nsignificant speed-ups."
                },
                "authors": [
                    {
                        "name": "Alexandros Graikos"
                    },
                    {
                        "name": "Nebojsa Jojic"
                    },
                    {
                        "name": "Dimitris Samaras"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Samaras"
                },
                "author": "Dimitris Samaras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11103v2",
                "updated": "2024-10-24T14:51:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    51,
                    59,
                    3,
                    298,
                    0
                ],
                "published": "2024-09-17T11:56:13Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    56,
                    13,
                    1,
                    261,
                    0
                ],
                "title": "Vetting quark-star models with gravitational waves in the hierarchical\n  Bayesian framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vetting quark-star models with gravitational waves in the hierarchical\n  Bayesian framework"
                },
                "summary": "The recent discovery of gravitational waves (GWs) has opened a new avenue for\ninvestigating the equation of state (EOS) of dense matter in compact stars,\nwhich is an outstanding problem in astronomy and nuclear physics. In the\nfuture, next-generation (XG) GW detectors will be constructed, deemed to\nprovide a large number of high-precision observations. We investigate the\npotential of constraining the EOS of quark stars (QSs) with high-precision\nmeasurements of mass $m$ and tidal deformability $\\Lambda$ from the XG GW\nobservatories. We adopt the widely-used bag model for QSs, consisting of four\nmicroscopic parameters: the effective bag constant $B_{\\rm eff}$, the\nperturbative quantum chromodynamics correction parameter $a_4$, the strange\nquark mass $m_s$, and the pairing energy gap $\\Delta$. With the help of\nhierarchical Bayesian inference, for the first time we are able to infer the\nEOS of QSs combining multiple GW observations. Using the top 25 loudest GW\nevents in our simulation, we find that, the constraints on $B_{\\rm eff}$ and\n$\\Delta$ are tightened by several times, while $a_4$ and $m_s$ are still poorly\nconstrained. We also study a simplified 2-dimensional (2-d) EOS model which was\nrecently proposed in literature. The 2-d model is found to exhibit significant\nparameter-estimation biases as more GW events are analyzed, while the predicted\n$m$-$\\Lambda$ relation remains consistent with the full model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent discovery of gravitational waves (GWs) has opened a new avenue for\ninvestigating the equation of state (EOS) of dense matter in compact stars,\nwhich is an outstanding problem in astronomy and nuclear physics. In the\nfuture, next-generation (XG) GW detectors will be constructed, deemed to\nprovide a large number of high-precision observations. We investigate the\npotential of constraining the EOS of quark stars (QSs) with high-precision\nmeasurements of mass $m$ and tidal deformability $\\Lambda$ from the XG GW\nobservatories. We adopt the widely-used bag model for QSs, consisting of four\nmicroscopic parameters: the effective bag constant $B_{\\rm eff}$, the\nperturbative quantum chromodynamics correction parameter $a_4$, the strange\nquark mass $m_s$, and the pairing energy gap $\\Delta$. With the help of\nhierarchical Bayesian inference, for the first time we are able to infer the\nEOS of QSs combining multiple GW observations. Using the top 25 loudest GW\nevents in our simulation, we find that, the constraints on $B_{\\rm eff}$ and\n$\\Delta$ are tightened by several times, while $a_4$ and $m_s$ are still poorly\nconstrained. We also study a simplified 2-dimensional (2-d) EOS model which was\nrecently proposed in literature. The 2-d model is found to exhibit significant\nparameter-estimation biases as more GW events are analyzed, while the predicted\n$m$-$\\Lambda$ relation remains consistent with the full model."
                },
                "authors": [
                    {
                        "name": "Ziming Wang"
                    },
                    {
                        "name": "Yong Gao"
                    },
                    {
                        "name": "Dicong Liang"
                    },
                    {
                        "name": "Junjie Zhao"
                    },
                    {
                        "name": "Lijing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Lijing Shao"
                },
                "author": "Lijing Shao",
                "arxiv_comment": "19 pages, 4 figures; accepted by JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18798v1",
                "updated": "2024-10-24T14:50:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    50,
                    42,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:50:42Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    50,
                    42,
                    3,
                    298,
                    0
                ],
                "title": "Distill Visual Chart Reasoning Ability from LLMs to MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distill Visual Chart Reasoning Ability from LLMs to MLLMs"
                },
                "summary": "Solving complex chart Q&A tasks requires advanced visual reasoning abilities\nin multimodal large language models (MLLMs). Recent studies highlight that\nthese abilities consist of two main parts: recognizing key information from\nvisual inputs and conducting reasoning over it. Thus, a promising approach to\nenhance MLLMs is to construct relevant training data focusing on the two\naspects. However, collecting and annotating complex charts and questions is\ncostly and time-consuming, and ensuring the quality of annotated answers\nremains a challenge. In this paper, we propose Code-as-Intermediary Translation\n(CIT), a cost-effective, efficient and easily scalable data synthesis method\nfor distilling visual reasoning abilities from LLMs to MLLMs. The code serves\nas an intermediary that translates visual chart representations into textual\nrepresentations, enabling LLMs to understand cross-modal information.\nSpecifically, we employ text-based synthesizing techniques to construct\nchart-plotting code and produce ReachQA, a dataset containing 3k\nreasoning-intensive charts and 20k Q&A pairs to enhance both recognition and\nreasoning abilities. Experiments show that when fine-tuned with our data,\nmodels not only perform well on chart-related benchmarks, but also demonstrate\nimproved multimodal reasoning abilities on general mathematical benchmarks like\nMathVista. The code and dataset are publicly available at\nhttps://github.com/hewei2001/ReachQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving complex chart Q&A tasks requires advanced visual reasoning abilities\nin multimodal large language models (MLLMs). Recent studies highlight that\nthese abilities consist of two main parts: recognizing key information from\nvisual inputs and conducting reasoning over it. Thus, a promising approach to\nenhance MLLMs is to construct relevant training data focusing on the two\naspects. However, collecting and annotating complex charts and questions is\ncostly and time-consuming, and ensuring the quality of annotated answers\nremains a challenge. In this paper, we propose Code-as-Intermediary Translation\n(CIT), a cost-effective, efficient and easily scalable data synthesis method\nfor distilling visual reasoning abilities from LLMs to MLLMs. The code serves\nas an intermediary that translates visual chart representations into textual\nrepresentations, enabling LLMs to understand cross-modal information.\nSpecifically, we employ text-based synthesizing techniques to construct\nchart-plotting code and produce ReachQA, a dataset containing 3k\nreasoning-intensive charts and 20k Q&A pairs to enhance both recognition and\nreasoning abilities. Experiments show that when fine-tuned with our data,\nmodels not only perform well on chart-related benchmarks, but also demonstrate\nimproved multimodal reasoning abilities on general mathematical benchmarks like\nMathVista. The code and dataset are publicly available at\nhttps://github.com/hewei2001/ReachQA."
                },
                "authors": [
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Wanxu Zhao"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Yiwen Ding"
                    },
                    {
                        "name": "Zifei Shan"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Under review. The code and dataset are publicly available at\n  https://github.com/hewei2001/ReachQA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18793v1",
                "updated": "2024-10-24T14:47:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    47,
                    28,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:47:28Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    47,
                    28,
                    3,
                    298,
                    0
                ],
                "title": "Adapting MLOps for Diverse In-Network Intelligence in 6G Era: Challenges\n  and Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting MLOps for Diverse In-Network Intelligence in 6G Era: Challenges\n  and Solutions"
                },
                "summary": "Seamless integration of artificial intelligence (AI) and machine learning\n(ML) techniques with wireless systems is a crucial step for 6G AInization.\nHowever, such integration faces challenges in terms of model functionality and\nlifecycle management. ML operations (MLOps) offer a systematic approach to\ntackle these challenges. Existing approaches toward implementing MLOps in a\ncentralized platform often overlook the challenges posed by diverse learning\nparadigms and network heterogeneity. This article provides a new approach to\nMLOps targeting the intricacies of future wireless networks. Considering unique\naspects of the future radio access network (RAN), we formulate three\noperational pipelines, namely reinforcement learning operations (RLOps),\nfederated learning operations (FedOps), and generative AI operations (GenOps).\nThese pipelines form the foundation for seamlessly integrating various\nlearning/inference capabilities into networks. We outline the specific\nchallenges and proposed solutions for each operation, facilitating large-scale\ndeployment of AI-Native 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seamless integration of artificial intelligence (AI) and machine learning\n(ML) techniques with wireless systems is a crucial step for 6G AInization.\nHowever, such integration faces challenges in terms of model functionality and\nlifecycle management. ML operations (MLOps) offer a systematic approach to\ntackle these challenges. Existing approaches toward implementing MLOps in a\ncentralized platform often overlook the challenges posed by diverse learning\nparadigms and network heterogeneity. This article provides a new approach to\nMLOps targeting the intricacies of future wireless networks. Considering unique\naspects of the future radio access network (RAN), we formulate three\noperational pipelines, namely reinforcement learning operations (RLOps),\nfederated learning operations (FedOps), and generative AI operations (GenOps).\nThese pipelines form the foundation for seamlessly integrating various\nlearning/inference capabilities into networks. We outline the specific\nchallenges and proposed solutions for each operation, facilitating large-scale\ndeployment of AI-Native 6G networks."
                },
                "authors": [
                    {
                        "name": "Peizheng Li"
                    },
                    {
                        "name": "Ioannis Mavromatis"
                    },
                    {
                        "name": "Tim Farnham"
                    },
                    {
                        "name": "Adnan Aijaz"
                    },
                    {
                        "name": "Aftab Khan"
                    }
                ],
                "author_detail": {
                    "name": "Aftab Khan"
                },
                "author": "Aftab Khan",
                "arxiv_comment": "7 pages, 5 figures. This paper has been submitted to IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18792v1",
                "updated": "2024-10-24T14:47:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    47,
                    25,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:47:25Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    47,
                    25,
                    3,
                    298,
                    0
                ],
                "title": "An LLM Agent for Automatic Geospatial Data Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM Agent for Automatic Geospatial Data Analysis"
                },
                "summary": "Large language models (LLMs) are being used in data science code generation\ntasks, but they often struggle with complex sequential tasks, leading to\nlogical errors. Their application to geospatial data processing is particularly\nchallenging due to difficulties in incorporating complex data structures and\nspatial constraints, effectively utilizing diverse function calls, and the\ntendency to hallucinate less-used geospatial libraries. To tackle these\nproblems, we introduce GeoAgent, a new interactive framework designed to help\nLLMs handle geospatial data processing more effectively. GeoAgent pioneers the\nintegration of a code interpreter, static analysis, and Retrieval-Augmented\nGeneration (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm,\noffering a novel approach to geospatial data processing. In addition, we\ncontribute a new benchmark specifically designed to evaluate the LLM-based\napproach in geospatial tasks. This benchmark leverages a variety of Python\nlibraries and includes both single-turn and multi-turn tasks such as data\nacquisition, data analysis, and visualization. By offering a comprehensive\nevaluation among diverse geospatial contexts, this benchmark sets a new\nstandard for developing LLM-based approaches in geospatial data analysis tasks.\nOur findings suggest that relying solely on knowledge of LLM is insufficient\nfor accurate geospatial task programming, which requires coherent multi-step\nprocesses and multiple function calls. Compared to the baseline LLMs, the\nproposed GeoAgent has demonstrated superior performance, yielding notable\nimprovements in function calls and task completion. In addition, these results\noffer valuable insights for the future development of LLM agents in automatic\ngeospatial data analysis task programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are being used in data science code generation\ntasks, but they often struggle with complex sequential tasks, leading to\nlogical errors. Their application to geospatial data processing is particularly\nchallenging due to difficulties in incorporating complex data structures and\nspatial constraints, effectively utilizing diverse function calls, and the\ntendency to hallucinate less-used geospatial libraries. To tackle these\nproblems, we introduce GeoAgent, a new interactive framework designed to help\nLLMs handle geospatial data processing more effectively. GeoAgent pioneers the\nintegration of a code interpreter, static analysis, and Retrieval-Augmented\nGeneration (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm,\noffering a novel approach to geospatial data processing. In addition, we\ncontribute a new benchmark specifically designed to evaluate the LLM-based\napproach in geospatial tasks. This benchmark leverages a variety of Python\nlibraries and includes both single-turn and multi-turn tasks such as data\nacquisition, data analysis, and visualization. By offering a comprehensive\nevaluation among diverse geospatial contexts, this benchmark sets a new\nstandard for developing LLM-based approaches in geospatial data analysis tasks.\nOur findings suggest that relying solely on knowledge of LLM is insufficient\nfor accurate geospatial task programming, which requires coherent multi-step\nprocesses and multiple function calls. Compared to the baseline LLMs, the\nproposed GeoAgent has demonstrated superior performance, yielding notable\nimprovements in function calls and task completion. In addition, these results\noffer valuable insights for the future development of LLM agents in automatic\ngeospatial data analysis task programming."
                },
                "authors": [
                    {
                        "name": "Yuxing Chen"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Sylvain Lobry"
                    },
                    {
                        "name": "Camille Kurtz"
                    }
                ],
                "author_detail": {
                    "name": "Camille Kurtz"
                },
                "author": "Camille Kurtz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18790v1",
                "updated": "2024-10-24T14:43:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    43,
                    35,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:43:35Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    43,
                    35,
                    3,
                    298,
                    0
                ],
                "title": "Large Generative AI Models meet Open Networks for 6G: Integration,\n  Platform, and Monetization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Generative AI Models meet Open Networks for 6G: Integration,\n  Platform, and Monetization"
                },
                "summary": "Generative artificial intelligence (GAI) has emerged as a pivotal technology\nfor content generation, reasoning, and decision-making, making it a promising\nsolution on the 6G stage characterized by openness, connected intelligence, and\nservice democratization. This article explores strategies for integrating and\nmonetizing GAI within future open 6G networks, mainly from the perspectives of\nmobile network operators (MNOs). We propose a novel API-centric telecoms GAI\nmarketplace platform, designed to serve as a central hub for deploying,\nmanaging, and monetizing diverse GAI services directly within the network. This\nplatform underpins a flexible and interoperable ecosystem, enhances service\ndelivery, and facilitates seamless integration of GAI capabilities across\nvarious network segments, thereby enabling new revenue streams through\ncustomer-centric generative services. Results from experimental evaluation in\nan end-to-end Open RAN testbed, show the latency benefits of this platform for\nlocal large language model (LLM) deployment, by comparing token timing for\nvarious generated lengths with cloud-based general-purpose LLMs. Lastly, the\narticle discusses key considerations for implementing the GAI marketplace\nwithin 6G networks, including monetization strategy, regulatory, management,\nand service platform aspects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence (GAI) has emerged as a pivotal technology\nfor content generation, reasoning, and decision-making, making it a promising\nsolution on the 6G stage characterized by openness, connected intelligence, and\nservice democratization. This article explores strategies for integrating and\nmonetizing GAI within future open 6G networks, mainly from the perspectives of\nmobile network operators (MNOs). We propose a novel API-centric telecoms GAI\nmarketplace platform, designed to serve as a central hub for deploying,\nmanaging, and monetizing diverse GAI services directly within the network. This\nplatform underpins a flexible and interoperable ecosystem, enhances service\ndelivery, and facilitates seamless integration of GAI capabilities across\nvarious network segments, thereby enabling new revenue streams through\ncustomer-centric generative services. Results from experimental evaluation in\nan end-to-end Open RAN testbed, show the latency benefits of this platform for\nlocal large language model (LLM) deployment, by comparing token timing for\nvarious generated lengths with cloud-based general-purpose LLMs. Lastly, the\narticle discusses key considerations for implementing the GAI marketplace\nwithin 6G networks, including monetization strategy, regulatory, management,\nand service platform aspects."
                },
                "authors": [
                    {
                        "name": "Peizheng Li"
                    },
                    {
                        "name": "Adrián Sánchez-Mompó"
                    },
                    {
                        "name": "Tim Farnham"
                    },
                    {
                        "name": "Aftab Khan"
                    },
                    {
                        "name": "Adnan Aijaz"
                    }
                ],
                "author_detail": {
                    "name": "Adnan Aijaz"
                },
                "author": "Adnan Aijaz",
                "arxiv_comment": "8 pages, 6 figures. This paper has been submitted to IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04199v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04199v3",
                "updated": "2024-10-24T14:43:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    43,
                    22,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-05T15:33:25Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    15,
                    33,
                    25,
                    5,
                    279,
                    0
                ],
                "title": "LongGenBench: Long-context Generation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongGenBench: Long-context Generation Benchmark"
                },
                "summary": "Current long-context benchmarks primarily focus on retrieval-based tests,\nrequiring Large Language Models (LLMs) to locate specific information within\nextensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark.\nLong-context generation refers to the ability of a language model to generate\ncoherent and contextually accurate text that spans across lengthy passages or\ndocuments. While recent studies show strong performance on NIAH and other\nretrieval-based long-context benchmarks, there is a significant lack of\nbenchmarks for evaluating long-context generation capabilities. To bridge this\ngap and offer a comprehensive assessment, we introduce a synthetic benchmark,\nLongGenBench, which allows for flexible configurations of customized generation\ncontext lengths. LongGenBench advances beyond traditional benchmarks by\nredesigning the format of questions and necessitating that LLMs respond with a\nsingle, cohesive long-context answer. Upon extensive evaluation using\nLongGenBench, we observe that: (1) both API accessed and open source models\nexhibit performance degradation in long-context generation scenarios, ranging\nfrom 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of\nperformance degradation, with the Gemini-1.5-Flash model showing the least\ndegradation among API accessed models, and the Qwen2 series exhibiting the\nleast degradation in LongGenBench among open source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current long-context benchmarks primarily focus on retrieval-based tests,\nrequiring Large Language Models (LLMs) to locate specific information within\nextensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark.\nLong-context generation refers to the ability of a language model to generate\ncoherent and contextually accurate text that spans across lengthy passages or\ndocuments. While recent studies show strong performance on NIAH and other\nretrieval-based long-context benchmarks, there is a significant lack of\nbenchmarks for evaluating long-context generation capabilities. To bridge this\ngap and offer a comprehensive assessment, we introduce a synthetic benchmark,\nLongGenBench, which allows for flexible configurations of customized generation\ncontext lengths. LongGenBench advances beyond traditional benchmarks by\nredesigning the format of questions and necessitating that LLMs respond with a\nsingle, cohesive long-context answer. Upon extensive evaluation using\nLongGenBench, we observe that: (1) both API accessed and open source models\nexhibit performance degradation in long-context generation scenarios, ranging\nfrom 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of\nperformance degradation, with the Gemini-1.5-Flash model showing the least\ndegradation among API accessed models, and the Qwen2 series exhibiting the\nleast degradation in LongGenBench among open source models."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "EMNLP 2024 https://github.com/Dominic789654/LongGenBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04199v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04199v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09836v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09836v3",
                "updated": "2024-10-24T14:35:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    35,
                    43,
                    3,
                    298,
                    0
                ],
                "published": "2024-04-15T14:44:08Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    14,
                    44,
                    8,
                    0,
                    106,
                    0
                ],
                "title": "How Far Have We Gone in Binary Code Understanding Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Have We Gone in Binary Code Understanding Using Large Language\n  Models"
                },
                "summary": "Binary code analysis plays a pivotal role in various software security\napplications, such as software maintenance, malware detection, software\nvulnerability discovery, patch analysis, etc. However, unlike source code,\nunderstanding binary code is challenging for reverse engineers due to the\nabsence of semantic information. Therefore, automated tools are needed to\nassist human players in interpreting binary code. In recent years, two groups\nof technologies have shown promising prospects: (1) Deep learning-based\ntechnologies have demonstrated competitive results in tasks related to binary\ncode understanding, furthermore, (2) Large Language Models (LLMs) have been\nextensively pre-trained at the source-code level for tasks such as code\nunderstanding and generation. This makes participants wonder about the ability\nof LLMs in binary code understanding.\n  In this work, we propose a benchmark to evaluate the effectiveness of LLMs in\nreal-world reverse engineering scenarios. The benchmark covers two key binary\ncode understanding tasks, including function name recovery and binary code\nsummarization. We gain valuable insights into their capabilities and\nlimitations through extensive evaluations of popular LLMs using our benchmark.\nOur evaluations reveal that existing LLMs can understand binary code to a\ncertain extent, thereby improving the efficiency of binary code analysis. Our\nresults highlight the great potential of the LLMs in advancing the field of\nbinary code understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary code analysis plays a pivotal role in various software security\napplications, such as software maintenance, malware detection, software\nvulnerability discovery, patch analysis, etc. However, unlike source code,\nunderstanding binary code is challenging for reverse engineers due to the\nabsence of semantic information. Therefore, automated tools are needed to\nassist human players in interpreting binary code. In recent years, two groups\nof technologies have shown promising prospects: (1) Deep learning-based\ntechnologies have demonstrated competitive results in tasks related to binary\ncode understanding, furthermore, (2) Large Language Models (LLMs) have been\nextensively pre-trained at the source-code level for tasks such as code\nunderstanding and generation. This makes participants wonder about the ability\nof LLMs in binary code understanding.\n  In this work, we propose a benchmark to evaluate the effectiveness of LLMs in\nreal-world reverse engineering scenarios. The benchmark covers two key binary\ncode understanding tasks, including function name recovery and binary code\nsummarization. We gain valuable insights into their capabilities and\nlimitations through extensive evaluations of popular LLMs using our benchmark.\nOur evaluations reveal that existing LLMs can understand binary code to a\ncertain extent, thereby improving the efficiency of binary code analysis. Our\nresults highlight the great potential of the LLMs in advancing the field of\nbinary code understanding."
                },
                "authors": [
                    {
                        "name": "Xiuwei Shang"
                    },
                    {
                        "name": "Shaoyin Cheng"
                    },
                    {
                        "name": "Guoqiang Chen"
                    },
                    {
                        "name": "Yanming Zhang"
                    },
                    {
                        "name": "Li Hu"
                    },
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Gangyang Li"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "arxiv_comment": "12 pages, 8 figures, to be published in ICSME 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09836v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09836v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18779v1",
                "updated": "2024-10-24T14:31:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    31,
                    52,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:31:52Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    31,
                    52,
                    3,
                    298,
                    0
                ],
                "title": "A Little Help Goes a Long Way: Efficient LLM Training by Leveraging\n  Small LMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Help Goes a Long Way: Efficient LLM Training by Leveraging\n  Small LMs"
                },
                "summary": "A primary challenge in large language model (LLM) development is their\nonerous pre-training cost. Typically, such pre-training involves optimizing a\nself-supervised objective (such as next-token prediction) over a large corpus.\nThis paper explores a promising paradigm to improve LLM pre-training efficiency\nand quality by suitably leveraging a small language model (SLM). In particular,\nthis paradigm relies on an SLM to both (1) provide soft labels as additional\ntraining supervision, and (2) select a small subset of valuable (\"informative\"\nand \"hard\") training examples. Put together, this enables an effective transfer\nof the SLM's predictive distribution to the LLM, while prioritizing specific\nregions of the training data distribution. Empirically, this leads to reduced\nLLM training time compared to standard training, while improving the overall\nquality. Theoretically, we develop a statistical framework to systematically\nstudy the utility of SLMs in enabling efficient training of high-quality LLMs.\nIn particular, our framework characterizes how the SLM's seemingly low-quality\nsupervision can enhance the training of a much more capable LLM. Furthermore,\nit also highlights the need for an adaptive utilization of such supervision, by\nstriking a balance between the bias and variance introduced by the SLM-provided\nsoft labels. We corroborate our theoretical framework by improving the\npre-training of an LLM with 2.8B parameters by utilizing a smaller LM with 1.5B\nparameters on the Pile dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A primary challenge in large language model (LLM) development is their\nonerous pre-training cost. Typically, such pre-training involves optimizing a\nself-supervised objective (such as next-token prediction) over a large corpus.\nThis paper explores a promising paradigm to improve LLM pre-training efficiency\nand quality by suitably leveraging a small language model (SLM). In particular,\nthis paradigm relies on an SLM to both (1) provide soft labels as additional\ntraining supervision, and (2) select a small subset of valuable (\"informative\"\nand \"hard\") training examples. Put together, this enables an effective transfer\nof the SLM's predictive distribution to the LLM, while prioritizing specific\nregions of the training data distribution. Empirically, this leads to reduced\nLLM training time compared to standard training, while improving the overall\nquality. Theoretically, we develop a statistical framework to systematically\nstudy the utility of SLMs in enabling efficient training of high-quality LLMs.\nIn particular, our framework characterizes how the SLM's seemingly low-quality\nsupervision can enhance the training of a much more capable LLM. Furthermore,\nit also highlights the need for an adaptive utilization of such supervision, by\nstriking a balance between the bias and variance introduced by the SLM-provided\nsoft labels. We corroborate our theoretical framework by improving the\npre-training of an LLM with 2.8B parameters by utilizing a smaller LM with 1.5B\nparameters on the Pile dataset."
                },
                "authors": [
                    {
                        "name": "Ankit Singh Rawat"
                    },
                    {
                        "name": "Veeranjaneyulu Sadhanala"
                    },
                    {
                        "name": "Afshin Rostamizadeh"
                    },
                    {
                        "name": "Ayan Chakrabarti"
                    },
                    {
                        "name": "Wittawat Jitkrittum"
                    },
                    {
                        "name": "Vladimir Feinberg"
                    },
                    {
                        "name": "Seungyeon Kim"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Nikunj Saunshi"
                    },
                    {
                        "name": "Zachary Nado"
                    },
                    {
                        "name": "Rakesh Shivanna"
                    },
                    {
                        "name": "Sashank J. Reddi"
                    },
                    {
                        "name": "Aditya Krishna Menon"
                    },
                    {
                        "name": "Rohan Anil"
                    },
                    {
                        "name": "Sanjiv Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Sanjiv Kumar"
                },
                "author": "Sanjiv Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08228v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08228v3",
                "updated": "2024-10-24T14:25:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    25,
                    3,
                    3,
                    298,
                    0
                ],
                "published": "2024-03-13T04:11:41Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    4,
                    11,
                    41,
                    2,
                    73,
                    0
                ],
                "title": "Empowering Robot Path Planning with Large Language Models: osmAG Map\n  Topology & Hierarchy Comprehension with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Robot Path Planning with Large Language Models: osmAG Map\n  Topology & Hierarchy Comprehension with LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated great potential in robotic\napplications by providing essential general knowledge. Mobile robots rely on\nmap comprehension for tasks like localization and navigation. In this paper, we\nexplore enabling LLMs to comprehend the topology and hierarchy of Area Graph, a\ntext-based hierarchical, topometric semantic map representation utilizing\npolygons to demark areas such as rooms or buildings. Our experiments\ndemonstrate that with the right map representation, LLMs can effectively\ncomprehend Area Graph's topology and hierarchy. After straightforward\nfine-tuning, the LLaMA2 models exceeded ChatGPT-3.5 in mastering these aspects.\nOur dataset, dataset generation code, fine-tuned LoRA adapters can be accessed\nat https://github.com/xiefujing/LLM-osmAG-Comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated great potential in robotic\napplications by providing essential general knowledge. Mobile robots rely on\nmap comprehension for tasks like localization and navigation. In this paper, we\nexplore enabling LLMs to comprehend the topology and hierarchy of Area Graph, a\ntext-based hierarchical, topometric semantic map representation utilizing\npolygons to demark areas such as rooms or buildings. Our experiments\ndemonstrate that with the right map representation, LLMs can effectively\ncomprehend Area Graph's topology and hierarchy. After straightforward\nfine-tuning, the LLaMA2 models exceeded ChatGPT-3.5 in mastering these aspects.\nOur dataset, dataset generation code, fine-tuned LoRA adapters can be accessed\nat https://github.com/xiefujing/LLM-osmAG-Comprehension."
                },
                "authors": [
                    {
                        "name": "Fujing Xie"
                    },
                    {
                        "name": "Sören Schwertfeger"
                    }
                ],
                "author_detail": {
                    "name": "Sören Schwertfeger"
                },
                "author": "Sören Schwertfeger",
                "arxiv_comment": "Accepted at IEEE International Conference on Robotics and Biomimetics\n  (ROBIO) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08228v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08228v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18764v1",
                "updated": "2024-10-24T14:18:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    18,
                    32,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:18:32Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    18,
                    32,
                    3,
                    298,
                    0
                ],
                "title": "Task Calibration: Calibrating Large Language Models on Inference Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Calibration: Calibrating Large Language Models on Inference Tasks"
                },
                "summary": "Large language models (LLMs) have exhibited impressive zero-shot performance\non inference tasks. However, LLMs may suffer from spurious correlations between\ninput texts and output labels, which limits LLMs' ability to reason based\npurely on general language understanding. In other words, LLMs may make\npredictions primarily based on premise or hypothesis, rather than both\ncomponents. To address this problem that may lead to unexpected performance\ndegradation, we propose task calibration (TC), a zero-shot and inference-only\ncalibration method inspired by mutual information which recovers LLM\nperformance through task reformulation. TC encourages LLMs to reason based on\nboth premise and hypothesis, while mitigating the models' over-reliance on\nindividual premise or hypothesis for inference. Experimental results show that\nTC achieves a substantial improvement on 13 inference tasks in the zero-shot\nsetup. We further validate the effectiveness of TC in few-shot setups and\nvarious natural language understanding tasks. Further analysis indicates that\nTC is also robust to prompt templates and has the potential to be integrated\nwith other calibration methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited impressive zero-shot performance\non inference tasks. However, LLMs may suffer from spurious correlations between\ninput texts and output labels, which limits LLMs' ability to reason based\npurely on general language understanding. In other words, LLMs may make\npredictions primarily based on premise or hypothesis, rather than both\ncomponents. To address this problem that may lead to unexpected performance\ndegradation, we propose task calibration (TC), a zero-shot and inference-only\ncalibration method inspired by mutual information which recovers LLM\nperformance through task reformulation. TC encourages LLMs to reason based on\nboth premise and hypothesis, while mitigating the models' over-reliance on\nindividual premise or hypothesis for inference. Experimental results show that\nTC achieves a substantial improvement on 13 inference tasks in the zero-shot\nsetup. We further validate the effectiveness of TC in few-shot setups and\nvarious natural language understanding tasks. Further analysis indicates that\nTC is also robust to prompt templates and has the potential to be integrated\nwith other calibration methods."
                },
                "authors": [
                    {
                        "name": "Yingjie Li"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Xiaotian Xie"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03694v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03694v2",
                "updated": "2024-10-24T14:15:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    15,
                    42,
                    3,
                    298,
                    0
                ],
                "published": "2024-02-06T04:28:33Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    4,
                    28,
                    33,
                    1,
                    37,
                    0
                ],
                "title": "ServeFlow: A Fast-Slow Model Architecture for Network Traffic Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ServeFlow: A Fast-Slow Model Architecture for Network Traffic Analysis"
                },
                "summary": "Network traffic analysis increasingly uses complex machine learning models as\nthe internet consolidates and traffic gets more encrypted. However, over\nhigh-bandwidth networks, flows can easily arrive faster than model inference\nrates. The temporal nature of network flows limits simple scale-out approaches\nleveraged in other high-traffic machine learning applications. Accordingly,\nthis paper presents ServeFlow, a solution for machine-learning model serving\naimed at network traffic analysis tasks, which carefully selects the number of\npackets to collect and the models to apply for individual flows to achieve a\nbalance between minimal latency, high service rate, and high accuracy. We\nidentify that on the same task, inference time across models can differ by 1.8x\n- 141.3x, while the inter-packet waiting time is up to 6-8 orders of magnitude\nhigher than the inference time! Based on these insights, we tailor a novel\nfast-slow model architecture for networking ML pipelines. Flows are assigned to\na slower model only when the inferences from the fast model are deemed high\nuncertainty. ServeFlow is able to make inferences on 76.3% of flows in under\n16ms, which is a speed-up of 40.5x on the median end-to-end serving latency\nwhile increasing the service rate and maintaining similar accuracy. Even with\nthousands of features per flow, it achieves a service rate of over 48.5k new\nflows per second on a 16-core CPU commodity server, which matches the order of\nmagnitude of flow rates observed on city-level network backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network traffic analysis increasingly uses complex machine learning models as\nthe internet consolidates and traffic gets more encrypted. However, over\nhigh-bandwidth networks, flows can easily arrive faster than model inference\nrates. The temporal nature of network flows limits simple scale-out approaches\nleveraged in other high-traffic machine learning applications. Accordingly,\nthis paper presents ServeFlow, a solution for machine-learning model serving\naimed at network traffic analysis tasks, which carefully selects the number of\npackets to collect and the models to apply for individual flows to achieve a\nbalance between minimal latency, high service rate, and high accuracy. We\nidentify that on the same task, inference time across models can differ by 1.8x\n- 141.3x, while the inter-packet waiting time is up to 6-8 orders of magnitude\nhigher than the inference time! Based on these insights, we tailor a novel\nfast-slow model architecture for networking ML pipelines. Flows are assigned to\na slower model only when the inferences from the fast model are deemed high\nuncertainty. ServeFlow is able to make inferences on 76.3% of flows in under\n16ms, which is a speed-up of 40.5x on the median end-to-end serving latency\nwhile increasing the service rate and maintaining similar accuracy. Even with\nthousands of features per flow, it achieves a service rate of over 48.5k new\nflows per second on a 16-core CPU commodity server, which matches the order of\nmagnitude of flow rates observed on city-level network backbones."
                },
                "authors": [
                    {
                        "name": "Shinan Liu"
                    },
                    {
                        "name": "Ted Shaowang"
                    },
                    {
                        "name": "Gerry Wan"
                    },
                    {
                        "name": "Jeewon Chae"
                    },
                    {
                        "name": "Jonatas Marques"
                    },
                    {
                        "name": "Sanjay Krishnan"
                    },
                    {
                        "name": "Nick Feamster"
                    }
                ],
                "author_detail": {
                    "name": "Nick Feamster"
                },
                "author": "Nick Feamster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03694v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03694v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16387v3",
                "updated": "2024-10-24T14:15:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    15,
                    40,
                    3,
                    298,
                    0
                ],
                "published": "2024-08-29T09:50:21Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    9,
                    50,
                    21,
                    3,
                    242,
                    0
                ],
                "title": "Enhancing MOTION2NX for Efficient, Scalable and Secure Image Inference\n  using Convolutional Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing MOTION2NX for Efficient, Scalable and Secure Image Inference\n  using Convolutional Neural Networks"
                },
                "summary": "This work contributes towards the development of an efficient and scalable\nopen-source Secure Multi-Party Computation (SMPC) protocol on machines with\nmoderate computational resources. We use the ABY2.0 SMPC protocol implemented\non the C++ based MOTION2NX framework for secure convolutional neural network\n(CNN) inference application with semi-honest security. Our list of\ncontributions are as follows. Firstly, we enhance MOTION2NX by providing a\ntensorized version of several primitive functions including the Hadamard\nproduct, indicator function and argmax function. Secondly, we adapt an existing\nHelper node algorithm, working in tandem with the ABY2.0 protocol, for\nefficient convolution computation to reduce execution time and RAM usage.\nThirdly, we also present a novel splitting algorithm that divides the\ncomputations at each CNN layer into multiple configurable chunks. This novel\nsplitting algorithm, providing significant reduction in RAM usage, is of\nindependent interest and is applicable to general SMPC protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work contributes towards the development of an efficient and scalable\nopen-source Secure Multi-Party Computation (SMPC) protocol on machines with\nmoderate computational resources. We use the ABY2.0 SMPC protocol implemented\non the C++ based MOTION2NX framework for secure convolutional neural network\n(CNN) inference application with semi-honest security. Our list of\ncontributions are as follows. Firstly, we enhance MOTION2NX by providing a\ntensorized version of several primitive functions including the Hadamard\nproduct, indicator function and argmax function. Secondly, we adapt an existing\nHelper node algorithm, working in tandem with the ABY2.0 protocol, for\nefficient convolution computation to reduce execution time and RAM usage.\nThirdly, we also present a novel splitting algorithm that divides the\ncomputations at each CNN layer into multiple configurable chunks. This novel\nsplitting algorithm, providing significant reduction in RAM usage, is of\nindependent interest and is applicable to general SMPC protocols."
                },
                "authors": [
                    {
                        "name": "Haritha K"
                    },
                    {
                        "name": "Ramya Burra"
                    },
                    {
                        "name": "Srishti Mittal"
                    },
                    {
                        "name": "Sarthak Sharma"
                    },
                    {
                        "name": "Abhilash Venkatesh"
                    },
                    {
                        "name": "Anshoo Tandon"
                    }
                ],
                "author_detail": {
                    "name": "Anshoo Tandon"
                },
                "author": "Anshoo Tandon",
                "arxiv_comment": "20 pages, 1 figure. arXiv admin note: text overlap with\n  arXiv:2310.10133",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18749v1",
                "updated": "2024-10-24T13:59:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    59,
                    3,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T13:59:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    59,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "Does Differential Privacy Impact Bias in Pretrained NLP Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Differential Privacy Impact Bias in Pretrained NLP Models?"
                },
                "summary": "Differential privacy (DP) is applied when fine-tuning pre-trained large\nlanguage models (LLMs) to limit leakage of training examples. While most DP\nresearch has focused on improving a model's privacy-utility tradeoff, some find\nthat DP can be unfair to or biased against underrepresented groups. In this\nwork, we show the impact of DP on bias in LLMs through empirical analysis.\nDifferentially private training can increase the model bias against protected\ngroups w.r.t AUC-based bias metrics. DP makes it more difficult for the model\nto differentiate between the positive and negative examples from the protected\ngroups and other groups in the rest of the population. Our results also show\nthat the impact of DP on bias is not only affected by the privacy protection\nlevel but also the underlying distribution of the dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential privacy (DP) is applied when fine-tuning pre-trained large\nlanguage models (LLMs) to limit leakage of training examples. While most DP\nresearch has focused on improving a model's privacy-utility tradeoff, some find\nthat DP can be unfair to or biased against underrepresented groups. In this\nwork, we show the impact of DP on bias in LLMs through empirical analysis.\nDifferentially private training can increase the model bias against protected\ngroups w.r.t AUC-based bias metrics. DP makes it more difficult for the model\nto differentiate between the positive and negative examples from the protected\ngroups and other groups in the rest of the population. Our results also show\nthat the impact of DP on bias is not only affected by the privacy protection\nlevel but also the underlying distribution of the dataset."
                },
                "authors": [
                    {
                        "name": "Md. Khairul Islam"
                    },
                    {
                        "name": "Andrew Wang"
                    },
                    {
                        "name": "Tianhao Wang"
                    },
                    {
                        "name": "Yangfeng Ji"
                    },
                    {
                        "name": "Judy Fox"
                    },
                    {
                        "name": "Jieyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieyu Zhao"
                },
                "author": "Jieyu Zhao",
                "arxiv_comment": "Github https://github.com/khairulislam/DP-on-NLP-Bias",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18745v1",
                "updated": "2024-10-24T13:51:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    51,
                    50,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T13:51:50Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    51,
                    50,
                    3,
                    298,
                    0
                ],
                "title": "Why Does the Effective Context Length of LLMs Fall Short?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Does the Effective Context Length of LLMs Fall Short?"
                },
                "summary": "Advancements in distributed training and efficient attention mechanisms have\nsignificantly expanded the context window sizes of large language models\n(LLMs). However, recent work reveals that the effective context lengths of\nopen-source LLMs often fall short, typically not exceeding half of their\ntraining lengths. In this work, we attribute this limitation to the left-skewed\nfrequency distribution of relative positions formed in LLMs pretraining and\npost-training stages, which impedes their ability to effectively gather distant\ninformation. To address this challenge, we introduce ShifTed Rotray position\nembeddING (STRING). STRING shifts well-trained positions to overwrite the\noriginal ineffective positions during inference, enhancing performance within\ntheir existing training lengths. Experimental results show that without\nadditional training, STRING dramatically improves the performance of the latest\nlarge-scale models, such as Llama3.1 70B and Qwen2 72B, by over 10 points on\npopular long-context benchmarks RULER and InfiniteBench, establishing new\nstate-of-the-art results for open-source LLMs. Compared to commercial models,\nLlama 3.1 70B with \\method even achieves better performance than GPT-4-128K and\nclearly surpasses Claude 2 and Kimi-chat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in distributed training and efficient attention mechanisms have\nsignificantly expanded the context window sizes of large language models\n(LLMs). However, recent work reveals that the effective context lengths of\nopen-source LLMs often fall short, typically not exceeding half of their\ntraining lengths. In this work, we attribute this limitation to the left-skewed\nfrequency distribution of relative positions formed in LLMs pretraining and\npost-training stages, which impedes their ability to effectively gather distant\ninformation. To address this challenge, we introduce ShifTed Rotray position\nembeddING (STRING). STRING shifts well-trained positions to overwrite the\noriginal ineffective positions during inference, enhancing performance within\ntheir existing training lengths. Experimental results show that without\nadditional training, STRING dramatically improves the performance of the latest\nlarge-scale models, such as Llama3.1 70B and Qwen2 72B, by over 10 points on\npopular long-context benchmarks RULER and InfiniteBench, establishing new\nstate-of-the-art results for open-source LLMs. Compared to commercial models,\nLlama 3.1 70B with \\method even achieves better performance than GPT-4-128K and\nclearly surpasses Claude 2 and Kimi-chat."
                },
                "authors": [
                    {
                        "name": "Chenxin An"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Shansan Gong"
                    },
                    {
                        "name": "Yao Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17439v2",
                "updated": "2024-10-24T13:34:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    34,
                    47,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-22T21:30:58Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    21,
                    30,
                    58,
                    1,
                    296,
                    0
                ],
                "title": "Evaluating AI-Generated Essays with GRE Analytical Writing Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating AI-Generated Essays with GRE Analytical Writing Assessment"
                },
                "summary": "The recent revolutionary advance in generative AI enables the generation of\nrealistic and coherent texts by large language models (LLMs). Despite many\nexisting evaluation metrics on the quality of the generated texts, there is\nstill a lack of rigorous assessment of how well LLMs perform in complex and\ndemanding writing assessments. This study examines essays generated by ten\nleading LLMs for the analytical writing assessment of the Graduate Record Exam\n(GRE). We assessed these essays using both human raters and the e-rater\nautomated scoring engine as used in the GRE scoring pipeline. Notably, the\ntop-performing Gemini and GPT-4o received an average score of 4.78 and 4.67,\nrespectively, falling between \"generally thoughtful, well-developed analysis of\nthe issue and conveys meaning clearly\" and \"presents a competent analysis of\nthe issue and conveys meaning with acceptable clarity\" according to the GRE\nscoring guideline. We also evaluated the detection accuracy of these essays,\nwith detectors trained on essays generated by the same and different LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent revolutionary advance in generative AI enables the generation of\nrealistic and coherent texts by large language models (LLMs). Despite many\nexisting evaluation metrics on the quality of the generated texts, there is\nstill a lack of rigorous assessment of how well LLMs perform in complex and\ndemanding writing assessments. This study examines essays generated by ten\nleading LLMs for the analytical writing assessment of the Graduate Record Exam\n(GRE). We assessed these essays using both human raters and the e-rater\nautomated scoring engine as used in the GRE scoring pipeline. Notably, the\ntop-performing Gemini and GPT-4o received an average score of 4.78 and 4.67,\nrespectively, falling between \"generally thoughtful, well-developed analysis of\nthe issue and conveys meaning clearly\" and \"presents a competent analysis of\nthe issue and conveys meaning with acceptable clarity\" according to the GRE\nscoring guideline. We also evaluated the detection accuracy of these essays,\nwith detectors trained on essays generated by the same and different LLMs."
                },
                "authors": [
                    {
                        "name": "Yang Zhong"
                    },
                    {
                        "name": "Jiangang Hao"
                    },
                    {
                        "name": "Michael Fauss"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Yuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wang"
                },
                "author": "Yuan Wang",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12381v2",
                "updated": "2024-10-24T13:33:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    33,
                    58,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-16T09:04:57Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    4,
                    57,
                    2,
                    290,
                    0
                ],
                "title": "HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of\n  Large Multimodal Models Through Coding Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of\n  Large Multimodal Models Through Coding Tasks"
                },
                "summary": "Coding tasks have been valuable for evaluating Large Language Models (LLMs),\nas they demand the comprehension of high-level instructions, complex reasoning,\nand the implementation of functional programs -- core capabilities for\nadvancing Artificial General Intelligence. Despite the progress in Large\nMultimodal Models (LMMs), which extend LLMs with visual perception and\nunderstanding capabilities, there remains a notable lack of coding benchmarks\nthat rigorously assess these models, particularly in tasks that emphasize\nvisual reasoning. To address this gap, we introduce HumanEval-V, a novel and\nlightweight benchmark specifically designed to evaluate LMMs' visual\nunderstanding and reasoning capabilities through code generation. HumanEval-V\nincludes 108 carefully crafted, entry-level Python coding tasks derived from\nplatforms like CodeForces and Stack Overflow. Each task is adapted by modifying\nthe context and algorithmic patterns of the original problems, with visual\nelements redrawn to ensure distinction from the source, preventing potential\ndata leakage. LMMs are required to complete the code solution based on the\nprovided visual context and a predefined Python function signature outlining\nthe task requirements. Every task is equipped with meticulously handcrafted\ntest cases to ensure a thorough and reliable evaluation of model-generated\nsolutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering\nsignificant challenges. Proprietary models like GPT-4o achieve only 13% pass@1\nand 36.4% pass@10, while open-weight models with 70B parameters score below 4%\npass@1. Ablation studies further reveal the limitations of current LMMs in\nvision reasoning and coding capabilities. These results underscore key areas\nfor future research to enhance LMMs' capabilities. We have open-sourced our\ncode and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coding tasks have been valuable for evaluating Large Language Models (LLMs),\nas they demand the comprehension of high-level instructions, complex reasoning,\nand the implementation of functional programs -- core capabilities for\nadvancing Artificial General Intelligence. Despite the progress in Large\nMultimodal Models (LMMs), which extend LLMs with visual perception and\nunderstanding capabilities, there remains a notable lack of coding benchmarks\nthat rigorously assess these models, particularly in tasks that emphasize\nvisual reasoning. To address this gap, we introduce HumanEval-V, a novel and\nlightweight benchmark specifically designed to evaluate LMMs' visual\nunderstanding and reasoning capabilities through code generation. HumanEval-V\nincludes 108 carefully crafted, entry-level Python coding tasks derived from\nplatforms like CodeForces and Stack Overflow. Each task is adapted by modifying\nthe context and algorithmic patterns of the original problems, with visual\nelements redrawn to ensure distinction from the source, preventing potential\ndata leakage. LMMs are required to complete the code solution based on the\nprovided visual context and a predefined Python function signature outlining\nthe task requirements. Every task is equipped with meticulously handcrafted\ntest cases to ensure a thorough and reliable evaluation of model-generated\nsolutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering\nsignificant challenges. Proprietary models like GPT-4o achieve only 13% pass@1\nand 36.4% pass@10, while open-weight models with 70B parameters score below 4%\npass@1. Ablation studies further reveal the limitations of current LMMs in\nvision reasoning and coding capabilities. These results underscore key areas\nfor future research to enhance LMMs' capabilities. We have open-sourced our\ncode and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark."
                },
                "authors": [
                    {
                        "name": "Fengji Zhang"
                    },
                    {
                        "name": "Linquan Wu"
                    },
                    {
                        "name": "Huiyu Bai"
                    },
                    {
                        "name": "Guancheng Lin"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "Jacky Keung"
                    }
                ],
                "author_detail": {
                    "name": "Jacky Keung"
                },
                "author": "Jacky Keung",
                "arxiv_comment": "homepage https://humaneval-v.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18718v1",
                "updated": "2024-10-24T13:22:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    22,
                    50,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T13:22:50Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    22,
                    50,
                    3,
                    298,
                    0
                ],
                "title": "LLM-based Online Prediction of Time-varying Graph Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Online Prediction of Time-varying Graph Signals"
                },
                "summary": "In this paper, we propose a novel framework that leverages large language\nmodels (LLMs) for predicting missing values in time-varying graph signals by\nexploiting spatial and temporal smoothness. We leverage the power of LLM to\nachieve a message-passing scheme. For each missing node, its neighbors and\nprevious estimates are fed into and processed by LLM to infer the missing\nobservations. Tested on the task of the online prediction of wind-speed graph\nsignals, our model outperforms online graph filtering algorithms in terms of\naccuracy, demonstrating the potential of LLMs in effectively addressing\npartially observed signals in graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel framework that leverages large language\nmodels (LLMs) for predicting missing values in time-varying graph signals by\nexploiting spatial and temporal smoothness. We leverage the power of LLM to\nachieve a message-passing scheme. For each missing node, its neighbors and\nprevious estimates are fed into and processed by LLM to infer the missing\nobservations. Tested on the task of the online prediction of wind-speed graph\nsignals, our model outperforms online graph filtering algorithms in terms of\naccuracy, demonstrating the potential of LLMs in effectively addressing\npartially observed signals in graphs."
                },
                "authors": [
                    {
                        "name": "Dayu Qin"
                    },
                    {
                        "name": "Yi Yan"
                    },
                    {
                        "name": "Ercan Engin Kuruoglu"
                    }
                ],
                "author_detail": {
                    "name": "Ercan Engin Kuruoglu"
                },
                "author": "Ercan Engin Kuruoglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18703v1",
                "updated": "2024-10-24T12:59:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    59,
                    5,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T12:59:05Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    59,
                    5,
                    3,
                    298,
                    0
                ],
                "title": "Whose fault is it anyway? SILC: Safe Integration of LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whose fault is it anyway? SILC: Safe Integration of LLM-Generated Code"
                },
                "summary": "In modern software development, multiple software components, often sourced\nfrom different contributors, including AI assistants, are combined to create a\ncohesive system. Although these components might each be individually safe,\ntheir composition might not be so. At the core of this issue is often a\nmisalignment between the requirements and assumptions made by each component.\nOnce discovered it is important to determine which component is accountable for\naddressing the misalignment issue and to prevent its occurrence in the future.\n  In this work we propose SILC, a framework for localising fault, i.e. blame,\nand for assigning sanitization obligations to prevent memory issues resulting\nfrom the composition of multiple software components. In particular, we show\nthe role Incorrectness Logic could have in automatically extracting implicit\nnon-functional assumptions in auto-generated code and render them explicit in\norder to detect misalignment with the requirements in existing code. In other\nwords, we are looking at the problem of code comprehension from a perspective\nfocused on safety properties rather than the traditional approach centered on\nfunctionality. To do that, we enhance Incorrectness Separation Logic with\ncapabilities for fault tracking and sanitization insertion. We show the\nbenefits of this framework by running experiments on millions of lines of code\nfrom open source projects where parts of existing functionality are regenerated\nby AI assistants. We empirically show that AI assistants produce unsafe code\nand demonstrate the utility of our framework in proposing appropriate blame and\nsanitization obligations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern software development, multiple software components, often sourced\nfrom different contributors, including AI assistants, are combined to create a\ncohesive system. Although these components might each be individually safe,\ntheir composition might not be so. At the core of this issue is often a\nmisalignment between the requirements and assumptions made by each component.\nOnce discovered it is important to determine which component is accountable for\naddressing the misalignment issue and to prevent its occurrence in the future.\n  In this work we propose SILC, a framework for localising fault, i.e. blame,\nand for assigning sanitization obligations to prevent memory issues resulting\nfrom the composition of multiple software components. In particular, we show\nthe role Incorrectness Logic could have in automatically extracting implicit\nnon-functional assumptions in auto-generated code and render them explicit in\norder to detect misalignment with the requirements in existing code. In other\nwords, we are looking at the problem of code comprehension from a perspective\nfocused on safety properties rather than the traditional approach centered on\nfunctionality. To do that, we enhance Incorrectness Separation Logic with\ncapabilities for fault tracking and sanitization insertion. We show the\nbenefits of this framework by running experiments on millions of lines of code\nfrom open source projects where parts of existing functionality are regenerated\nby AI assistants. We empirically show that AI assistants produce unsafe code\nand demonstrate the utility of our framework in proposing appropriate blame and\nsanitization obligations."
                },
                "authors": [
                    {
                        "name": "Peisen Lin"
                    },
                    {
                        "name": "Yuntong Zhang"
                    },
                    {
                        "name": "Andreea Costea"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18702v1",
                "updated": "2024-10-24T12:56:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    56,
                    1,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T12:56:01Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    56,
                    1,
                    3,
                    298,
                    0
                ],
                "title": "GrammaMT: Improving Machine Translation with Grammar-Informed In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrammaMT: Improving Machine Translation with Grammar-Informed In-Context\n  Learning"
                },
                "summary": "We introduce GrammaMT, a grammatically-aware prompting approach for machine\ntranslation that uses Interlinear Glossed Text (IGT), a common form of\nlinguistic description providing morphological and lexical annotations for\nsource sentences. GrammaMT proposes three prompting strategies: gloss-shot,\nchain-gloss and model-gloss. All are training-free, requiring only a few\nexamples that involve minimal effort to collect, and making them well-suited\nfor low-resource setups. Experiments show that GrammaMT enhances translation\nperformance on open-source instruction-tuned LLMs for various low- to\nhigh-resource languages across three benchmarks: (1) the largest IGT corpus,\n(2) the challenging 2023 SIGMORPHON Shared Task data over endangered languages,\nand (3) even in an out-of-domain setting with FLORES. Moreover, ablation\nstudies reveal that leveraging gloss resources could substantially boost MT\nperformance (by over 17 BLEU points) if LLMs accurately generate or access\ninput sentence glosses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GrammaMT, a grammatically-aware prompting approach for machine\ntranslation that uses Interlinear Glossed Text (IGT), a common form of\nlinguistic description providing morphological and lexical annotations for\nsource sentences. GrammaMT proposes three prompting strategies: gloss-shot,\nchain-gloss and model-gloss. All are training-free, requiring only a few\nexamples that involve minimal effort to collect, and making them well-suited\nfor low-resource setups. Experiments show that GrammaMT enhances translation\nperformance on open-source instruction-tuned LLMs for various low- to\nhigh-resource languages across three benchmarks: (1) the largest IGT corpus,\n(2) the challenging 2023 SIGMORPHON Shared Task data over endangered languages,\nand (3) even in an out-of-domain setting with FLORES. Moreover, ablation\nstudies reveal that leveraging gloss resources could substantially boost MT\nperformance (by over 17 BLEU points) if LLMs accurately generate or access\ninput sentence glosses."
                },
                "authors": [
                    {
                        "name": "Rita Ramos"
                    },
                    {
                        "name": "Everlyn Asiko Chimoto"
                    },
                    {
                        "name": "Maartje ter Hoeve"
                    },
                    {
                        "name": "Natalie Schluter"
                    }
                ],
                "author_detail": {
                    "name": "Natalie Schluter"
                },
                "author": "Natalie Schluter",
                "arxiv_comment": "Under review at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18701v1",
                "updated": "2024-10-24T12:53:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    53,
                    39,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T12:53:39Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    53,
                    39,
                    3,
                    298,
                    0
                ],
                "title": "BATON: Enhancing Batch-wise Inference Efficiency for Large Language\n  Models via Dynamic Re-batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BATON: Enhancing Batch-wise Inference Efficiency for Large Language\n  Models via Dynamic Re-batching"
                },
                "summary": "The advanced capabilities of Large Language Models (LLMs) have inspired the\ndevelopment of various interactive web services or applications, such as\nChatGPT, which offer query inference services for users. Unlike traditional DNN\nmodel, the inference of LLM entails different iterations of forward computation\nfor different queries, which result in efficiency challenges for existing\nrun-to-completion batch-wise inference. Hence, some methods refine batch-wise\ninference to iteration-level by duplicating all nonlinear layers of LLM.\nHowever, this approach not only increases resource usage but also introduces\nidle computations to the batch due to the prefilling of newly added queries.\nTherefore, we propose BATON, an efficient batch-wise LLM inference scheme by\ndynamically adjusting processing batch, which can achieve near-zero idle\ncomputations without incurring additional resource consumption. To do so, BATON\n1) shapes the vectors involved in the inference of the newly inserted query and\nprocessing batch to align dimensions and generates a new attention mask based\non vector shaping to ensure inference correctness, which enables query\ninserting without consuming additional resource; 2) embeds prefilled Keys and\nValues of the new query into the KV_Cache of the processing batch by leveraging\nthe prefilling and decoding separation mechanism, eliminating idle computations\nto the batch introduced by the prefilling process of the new query.\nExperimental results show that compared to the state-of-the-art solution Orca,\nBATON improves query processing by up to 1.75 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advanced capabilities of Large Language Models (LLMs) have inspired the\ndevelopment of various interactive web services or applications, such as\nChatGPT, which offer query inference services for users. Unlike traditional DNN\nmodel, the inference of LLM entails different iterations of forward computation\nfor different queries, which result in efficiency challenges for existing\nrun-to-completion batch-wise inference. Hence, some methods refine batch-wise\ninference to iteration-level by duplicating all nonlinear layers of LLM.\nHowever, this approach not only increases resource usage but also introduces\nidle computations to the batch due to the prefilling of newly added queries.\nTherefore, we propose BATON, an efficient batch-wise LLM inference scheme by\ndynamically adjusting processing batch, which can achieve near-zero idle\ncomputations without incurring additional resource consumption. To do so, BATON\n1) shapes the vectors involved in the inference of the newly inserted query and\nprocessing batch to align dimensions and generates a new attention mask based\non vector shaping to ensure inference correctness, which enables query\ninserting without consuming additional resource; 2) embeds prefilled Keys and\nValues of the new query into the KV_Cache of the processing batch by leveraging\nthe prefilling and decoding separation mechanism, eliminating idle computations\nto the batch introduced by the prefilling process of the new query.\nExperimental results show that compared to the state-of-the-art solution Orca,\nBATON improves query processing by up to 1.75 times."
                },
                "authors": [
                    {
                        "name": "Peizhuang Cong"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Haochen Zhao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18697v1",
                "updated": "2024-10-24T12:48:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    48,
                    3,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T12:48:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    48,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "How Good Are LLMs for Literary Translation, Really? Literary Translation\n  Evaluation with Humans and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Good Are LLMs for Literary Translation, Really? Literary Translation\n  Evaluation with Humans and LLMs"
                },
                "summary": "Recent research has focused on literary machine translation (MT) as a new\nchallenge in MT. However, the evaluation of literary MT remains an open\nproblem. We contribute to this ongoing discussion by introducing\nLITEVAL-CORPUS, a paragraph-level parallel corpus comprising multiple verified\nhuman translations and outputs from 9 MT systems, which totals over 2k\nparagraphs and includes 13k annotated sentences across four language pairs,\ncosting 4.5k Euro. This corpus enables us to (i) examine the consistency and\nadequacy of multiple annotation schemes, (ii) compare evaluations by students\nand professionals, and (iii) assess the effectiveness of LLM-based metrics. We\nfind that Multidimensional Quality Metrics (MQM), as the de facto standard in\nnon-literary human MT evaluation, is inadequate for literary translation: While\nBest-Worst Scaling (BWS) with students and Scalar Quality Metric (SQM) with\nprofessional translators prefer human translations at rates of ~82% and ~94%,\nrespectively, MQM with student annotators prefers human professional\ntranslations over the translations of the best-performing LLMs in only ~42% of\ncases. While automatic metrics generally show a moderate correlation with human\nMQM and SQM, they struggle to accurately identify human translations, with\nrates of at most ~20%. Our overall evaluation indicates that human professional\ntranslations consistently outperform LLM translations, where even the most\nrecent LLMs tend to produce more literal and less diverse translations compared\nto human translations. However, newer LLMs such as GPT-4o perform substantially\nbetter than older ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has focused on literary machine translation (MT) as a new\nchallenge in MT. However, the evaluation of literary MT remains an open\nproblem. We contribute to this ongoing discussion by introducing\nLITEVAL-CORPUS, a paragraph-level parallel corpus comprising multiple verified\nhuman translations and outputs from 9 MT systems, which totals over 2k\nparagraphs and includes 13k annotated sentences across four language pairs,\ncosting 4.5k Euro. This corpus enables us to (i) examine the consistency and\nadequacy of multiple annotation schemes, (ii) compare evaluations by students\nand professionals, and (iii) assess the effectiveness of LLM-based metrics. We\nfind that Multidimensional Quality Metrics (MQM), as the de facto standard in\nnon-literary human MT evaluation, is inadequate for literary translation: While\nBest-Worst Scaling (BWS) with students and Scalar Quality Metric (SQM) with\nprofessional translators prefer human translations at rates of ~82% and ~94%,\nrespectively, MQM with student annotators prefers human professional\ntranslations over the translations of the best-performing LLMs in only ~42% of\ncases. While automatic metrics generally show a moderate correlation with human\nMQM and SQM, they struggle to accurately identify human translations, with\nrates of at most ~20%. Our overall evaluation indicates that human professional\ntranslations consistently outperform LLM translations, where even the most\nrecent LLMs tend to produce more literal and less diverse translations compared\nto human translations. However, newer LLMs such as GPT-4o perform substantially\nbetter than older ones."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05628v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05628v3",
                "updated": "2024-10-24T12:47:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    47,
                    56,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-08T02:23:53Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    2,
                    23,
                    53,
                    1,
                    282,
                    0
                ],
                "title": "Versatile Motion Language Models for Multi-Turn Interactive Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Versatile Motion Language Models for Multi-Turn Interactive Agents"
                },
                "summary": "Recent advancements in large language models (LLMs) have greatly enhanced\ntheir ability to generate natural and contextually relevant text, making AI\ninteractions more human-like. However, generating and understanding interactive\nhuman-like motion, where two individuals engage in coordinated movements,\nremains a challenge due to the complexity of modeling these coordinated\ninteractions. Furthermore, a versatile model is required to handle diverse\ninteractive scenarios, such as chat systems that follow user instructions or\nadapt to their assigned role while adjusting interaction dynamics. To tackle\nthis problem, we introduce VIM, short for the Versatile Interactive Motion\nlanguage model, which integrates both language and motion modalities to\neffectively understand, generate, and control interactive motions in multi-turn\nconversational contexts. To address the scarcity of multi-turn interactive\nmotion data, we introduce a synthetic dataset, INERT-MT2, where we utilize\npre-trained models to create diverse instructional datasets with interactive\nmotion. Our approach first trains a motion tokenizer that encodes interactive\nmotions into residual discrete tokens. In the pretraining stage, the model\nlearns to align motion and text representations with these discrete tokens.\nDuring the instruction fine-tuning stage, VIM adapts to multi-turn\nconversations using the INTER-MT2 dataset. We evaluate the versatility of our\nmethod across motion-related tasks, motion to text, text to motion, reaction\ngeneration, motion editing, and reasoning about motion sequences. The results\nhighlight the versatility and effectiveness of proposed method in handling\ncomplex interactive motion synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have greatly enhanced\ntheir ability to generate natural and contextually relevant text, making AI\ninteractions more human-like. However, generating and understanding interactive\nhuman-like motion, where two individuals engage in coordinated movements,\nremains a challenge due to the complexity of modeling these coordinated\ninteractions. Furthermore, a versatile model is required to handle diverse\ninteractive scenarios, such as chat systems that follow user instructions or\nadapt to their assigned role while adjusting interaction dynamics. To tackle\nthis problem, we introduce VIM, short for the Versatile Interactive Motion\nlanguage model, which integrates both language and motion modalities to\neffectively understand, generate, and control interactive motions in multi-turn\nconversational contexts. To address the scarcity of multi-turn interactive\nmotion data, we introduce a synthetic dataset, INERT-MT2, where we utilize\npre-trained models to create diverse instructional datasets with interactive\nmotion. Our approach first trains a motion tokenizer that encodes interactive\nmotions into residual discrete tokens. In the pretraining stage, the model\nlearns to align motion and text representations with these discrete tokens.\nDuring the instruction fine-tuning stage, VIM adapts to multi-turn\nconversations using the INTER-MT2 dataset. We evaluate the versatility of our\nmethod across motion-related tasks, motion to text, text to motion, reaction\ngeneration, motion editing, and reasoning about motion sequences. The results\nhighlight the versatility and effectiveness of proposed method in handling\ncomplex interactive motion synthesis."
                },
                "authors": [
                    {
                        "name": "Jeongeun Park"
                    },
                    {
                        "name": "Sungjoon Choi"
                    },
                    {
                        "name": "Sangdoo Yun"
                    }
                ],
                "author_detail": {
                    "name": "Sangdoo Yun"
                },
                "author": "Sangdoo Yun",
                "arxiv_comment": "https://vim-motion-language.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05628v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05628v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18693v1",
                "updated": "2024-10-24T12:42:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    42,
                    4,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T12:42:04Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    42,
                    4,
                    3,
                    298,
                    0
                ],
                "title": "Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis\n  from Scratch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis\n  from Scratch"
                },
                "summary": "The availability of high-quality data is one of the most important factors in\nimproving the reasoning capability of LLMs. Existing works have demonstrated\nthe effectiveness of creating more instruction data from seed questions or\nknowledge bases. Recent research indicates that continually scaling up data\nsynthesis from strong models (e.g., GPT-4) can further elicit reasoning\nperformance. Though promising, the open-sourced community still lacks\nhigh-quality data at scale and scalable data synthesis methods with affordable\ncosts. To address this, we introduce ScaleQuest, a scalable and novel data\nsynthesis method that utilizes \"small-size\" (e.g., 7B) open-source models to\ngenerate questions from scratch without the need for seed data with complex\naugmentation constraints. With the efficient ScaleQuest, we automatically\nconstructed a mathematical reasoning dataset consisting of 1 million\nproblem-solution pairs, which are more effective than existing open-sourced\ndatasets. It can universally increase the performance of mainstream open-source\nmodels (i.e., Mistral, Llama3, DeepSeekMath, and Qwen2-Math) by achieving 29.2%\nto 46.4% gains on MATH. Notably, simply fine-tuning the Qwen2-Math-7B-Base\nmodel with our dataset can even surpass Qwen2-Math-7B-Instruct, a strong and\nwell-aligned model on closed-source data, and proprietary models such as\nGPT-4-Turbo and Claude-3.5 Sonnet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of high-quality data is one of the most important factors in\nimproving the reasoning capability of LLMs. Existing works have demonstrated\nthe effectiveness of creating more instruction data from seed questions or\nknowledge bases. Recent research indicates that continually scaling up data\nsynthesis from strong models (e.g., GPT-4) can further elicit reasoning\nperformance. Though promising, the open-sourced community still lacks\nhigh-quality data at scale and scalable data synthesis methods with affordable\ncosts. To address this, we introduce ScaleQuest, a scalable and novel data\nsynthesis method that utilizes \"small-size\" (e.g., 7B) open-source models to\ngenerate questions from scratch without the need for seed data with complex\naugmentation constraints. With the efficient ScaleQuest, we automatically\nconstructed a mathematical reasoning dataset consisting of 1 million\nproblem-solution pairs, which are more effective than existing open-sourced\ndatasets. It can universally increase the performance of mainstream open-source\nmodels (i.e., Mistral, Llama3, DeepSeekMath, and Qwen2-Math) by achieving 29.2%\nto 46.4% gains on MATH. Notably, simply fine-tuning the Qwen2-Math-7B-Base\nmodel with our dataset can even surpass Qwen2-Math-7B-Instruct, a strong and\nwell-aligned model on closed-source data, and proprietary models such as\nGPT-4-Turbo and Claude-3.5 Sonnet."
                },
                "authors": [
                    {
                        "name": "Yuyang Ding"
                    },
                    {
                        "name": "Xinyu Shi"
                    },
                    {
                        "name": "Xiaobo Liang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qiaoming Zhu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Preprint. Project page: https://scalequest.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18689v1",
                "updated": "2024-10-24T12:35:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    35,
                    27,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T12:35:27Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    35,
                    27,
                    3,
                    298,
                    0
                ],
                "title": "Observation of Complete Orbital Two-channel Kondo Effect in van der\n  Waals Ferromagnet Fe3GaTe2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observation of Complete Orbital Two-channel Kondo Effect in van der\n  Waals Ferromagnet Fe3GaTe2"
                },
                "summary": "Orbital two-channel Kondo (2CK) effect is one of the crucial systems with\nnon- Fermi liquid (NFL) behaviors. But the full three-regime transport evidence\nhas never been observed in one sample. Here, all three-resistive regimes for\nthe orbital 2CK effect induced by two-level systems (TLSs) have been observed\nin the van der Waals ferromagnet Fe3GaTe2. The electron behavior undergoes a\ncontinuous transition from electron scattering to the NFL behavior, and\nsubsequently to Fermi liquid behavior. The magnetic field does not affect any\nregimes, indicating the non-magnetic origin of the TLSs in Fe3GaTe2. In\naddition, the slope of linear negative magnetoresistance, rather than the\ntopological Hall effect, has been found to be related to spin-magnon scattering\nand can be used to infer the emergence of spin textures. Our findings indicate\nFe3GaTe2 may be an ideal platform to study electron-correlation and topological\nphenomena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orbital two-channel Kondo (2CK) effect is one of the crucial systems with\nnon- Fermi liquid (NFL) behaviors. But the full three-regime transport evidence\nhas never been observed in one sample. Here, all three-resistive regimes for\nthe orbital 2CK effect induced by two-level systems (TLSs) have been observed\nin the van der Waals ferromagnet Fe3GaTe2. The electron behavior undergoes a\ncontinuous transition from electron scattering to the NFL behavior, and\nsubsequently to Fermi liquid behavior. The magnetic field does not affect any\nregimes, indicating the non-magnetic origin of the TLSs in Fe3GaTe2. In\naddition, the slope of linear negative magnetoresistance, rather than the\ntopological Hall effect, has been found to be related to spin-magnon scattering\nand can be used to infer the emergence of spin textures. Our findings indicate\nFe3GaTe2 may be an ideal platform to study electron-correlation and topological\nphenomena."
                },
                "authors": [
                    {
                        "name": "Chunhao Bao"
                    },
                    {
                        "name": "Xiaolong Yin"
                    },
                    {
                        "name": "Jifeng Shao"
                    },
                    {
                        "name": "Longxiang Li"
                    },
                    {
                        "name": "Zhiyue Li"
                    },
                    {
                        "name": "Xiaoming Ma"
                    },
                    {
                        "name": "Shu Guo"
                    },
                    {
                        "name": "Tingyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tingyong Chen"
                },
                "author": "Tingyong Chen",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18686v1",
                "updated": "2024-10-24T12:32:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    32,
                    19,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T12:32:19Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    32,
                    19,
                    3,
                    298,
                    0
                ],
                "title": "Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced\n  Time Series Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced\n  Time Series Classification"
                },
                "summary": "Leveraging large language models (LLMs) has garnered increasing attention and\nintroduced novel perspectives in time series classification. However, existing\napproaches often overlook the crucial dynamic temporal information inherent in\ntime series data and face challenges in aligning this data with textual\nsemantics. To address these limitations, we propose HiTime, a hierarchical\nmulti-modal model that seamlessly integrates temporal information into LLMs for\nmultivariate time series classification (MTSC). Our model employs a\nhierarchical feature encoder to capture diverse aspects of time series data\nthrough both data-specific and task-specific embeddings. To facilitate semantic\nspace alignment between time series and text, we introduce a dual-view\ncontrastive alignment module that bridges the gap between modalities.\nAdditionally, we adopt a hybrid prompting strategy to fine-tune the pre-trained\nLLM in a parameter-efficient manner. By effectively incorporating dynamic\ntemporal features and ensuring semantic alignment, HiTime enables LLMs to\nprocess continuous time series data and achieves state-of-the-art\nclassification performance through text generation. Extensive experiments on\nbenchmark datasets demonstrate that HiTime significantly enhances time series\nclassification accuracy compared to most competitive baseline methods. Our\nfindings highlight the potential of integrating temporal features into LLMs,\npaving the way for advanced time series analysis. The code is publicly\navailable for further research and validation. Our codes are publicly\navailable1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging large language models (LLMs) has garnered increasing attention and\nintroduced novel perspectives in time series classification. However, existing\napproaches often overlook the crucial dynamic temporal information inherent in\ntime series data and face challenges in aligning this data with textual\nsemantics. To address these limitations, we propose HiTime, a hierarchical\nmulti-modal model that seamlessly integrates temporal information into LLMs for\nmultivariate time series classification (MTSC). Our model employs a\nhierarchical feature encoder to capture diverse aspects of time series data\nthrough both data-specific and task-specific embeddings. To facilitate semantic\nspace alignment between time series and text, we introduce a dual-view\ncontrastive alignment module that bridges the gap between modalities.\nAdditionally, we adopt a hybrid prompting strategy to fine-tune the pre-trained\nLLM in a parameter-efficient manner. By effectively incorporating dynamic\ntemporal features and ensuring semantic alignment, HiTime enables LLMs to\nprocess continuous time series data and achieves state-of-the-art\nclassification performance through text generation. Extensive experiments on\nbenchmark datasets demonstrate that HiTime significantly enhances time series\nclassification accuracy compared to most competitive baseline methods. Our\nfindings highlight the potential of integrating temporal features into LLMs,\npaving the way for advanced time series analysis. The code is publicly\navailable for further research and validation. Our codes are publicly\navailable1."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Tao"
                    },
                    {
                        "name": "Tingyue Pan"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Yucong Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yucong Luo"
                },
                "author": "Yucong Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07251v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07251v3",
                "updated": "2024-10-24T12:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    31,
                    9,
                    3,
                    298,
                    0
                ],
                "published": "2024-06-11T13:33:33Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    13,
                    33,
                    33,
                    1,
                    163,
                    0
                ],
                "title": "Is One GPU Enough? Pushing Image Generation at Higher-Resolutions with\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is One GPU Enough? Pushing Image Generation at Higher-Resolutions with\n  Foundation Models"
                },
                "summary": "In this work, we introduce Pixelsmith, a zero-shot text-to-image generative\nframework to sample images at higher resolutions with a single GPU. We are the\nfirst to show that it is possible to scale the output of a pre-trained\ndiffusion model by a factor of 1000, opening the road for gigapixel image\ngeneration at no additional cost. Our cascading method uses the image generated\nat the lowest resolution as a baseline to sample at higher resolutions. For the\nguidance, we introduce the Slider, a tunable mechanism that fuses the overall\nstructure contained in the first-generated image with enhanced fine details. At\neach inference step, we denoise patches rather than the entire latent space,\nminimizing memory demands such that a single GPU can handle the process,\nregardless of the image's resolution. Our experimental results show that\nPixelsmith not only achieves higher quality and diversity compared to existing\ntechniques, but also reduces sampling time and artifacts. The code for our work\nis available at https://github.com/Thanos-DB/Pixelsmith.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce Pixelsmith, a zero-shot text-to-image generative\nframework to sample images at higher resolutions with a single GPU. We are the\nfirst to show that it is possible to scale the output of a pre-trained\ndiffusion model by a factor of 1000, opening the road for gigapixel image\ngeneration at no additional cost. Our cascading method uses the image generated\nat the lowest resolution as a baseline to sample at higher resolutions. For the\nguidance, we introduce the Slider, a tunable mechanism that fuses the overall\nstructure contained in the first-generated image with enhanced fine details. At\neach inference step, we denoise patches rather than the entire latent space,\nminimizing memory demands such that a single GPU can handle the process,\nregardless of the image's resolution. Our experimental results show that\nPixelsmith not only achieves higher quality and diversity compared to existing\ntechniques, but also reduces sampling time and artifacts. The code for our work\nis available at https://github.com/Thanos-DB/Pixelsmith."
                },
                "authors": [
                    {
                        "name": "Athanasios Tragakis"
                    },
                    {
                        "name": "Marco Aversa"
                    },
                    {
                        "name": "Chaitanya Kaul"
                    },
                    {
                        "name": "Roderick Murray-Smith"
                    },
                    {
                        "name": "Daniele Faccio"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Faccio"
                },
                "author": "Daniele Faccio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07251v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07251v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09949v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09949v2",
                "updated": "2024-10-24T12:13:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    13,
                    54,
                    3,
                    298,
                    0
                ],
                "published": "2024-06-14T11:52:09Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    11,
                    52,
                    9,
                    4,
                    166,
                    0
                ],
                "title": "Neural Concept Binder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Concept Binder"
                },
                "summary": "The challenge in object-based visual reasoning lies in generating concept\nrepresentations that are both descriptive and distinct. Achieving this in an\nunsupervised manner requires human users to understand the model's learned\nconcepts and, if necessary, revise incorrect ones. To address this challenge,\nwe introduce the Neural Concept Binder (NCB), a novel framework for deriving\nboth discrete and continuous concept representations, which we refer to as\n\"concept-slot encodings\". NCB employs two types of binding: \"soft binding\",\nwhich leverages the recent SysBinder mechanism to obtain object-factor\nencodings, and subsequent \"hard binding\", achieved through hierarchical\nclustering and retrieval-based inference. This enables obtaining expressive,\ndiscrete representations from unlabeled images. Moreover, the structured nature\nof NCB's concept representations allows for intuitive inspection and the\nstraightforward integration of external knowledge, such as human input or\ninsights from other AI models like GPT-4. Additionally, we demonstrate that\nincorporating the hard binding mechanism preserves model performance while\nenabling seamless integration into both neural and symbolic modules for complex\nreasoning tasks. We validate the effectiveness of NCB through evaluations on\nour newly introduced CLEVR-Sudoku dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge in object-based visual reasoning lies in generating concept\nrepresentations that are both descriptive and distinct. Achieving this in an\nunsupervised manner requires human users to understand the model's learned\nconcepts and, if necessary, revise incorrect ones. To address this challenge,\nwe introduce the Neural Concept Binder (NCB), a novel framework for deriving\nboth discrete and continuous concept representations, which we refer to as\n\"concept-slot encodings\". NCB employs two types of binding: \"soft binding\",\nwhich leverages the recent SysBinder mechanism to obtain object-factor\nencodings, and subsequent \"hard binding\", achieved through hierarchical\nclustering and retrieval-based inference. This enables obtaining expressive,\ndiscrete representations from unlabeled images. Moreover, the structured nature\nof NCB's concept representations allows for intuitive inspection and the\nstraightforward integration of external knowledge, such as human input or\ninsights from other AI models like GPT-4. Additionally, we demonstrate that\nincorporating the hard binding mechanism preserves model performance while\nenabling seamless integration into both neural and symbolic modules for complex\nreasoning tasks. We validate the effectiveness of NCB through evaluations on\nour newly introduced CLEVR-Sudoku dataset."
                },
                "authors": [
                    {
                        "name": "Wolfgang Stammer"
                    },
                    {
                        "name": "Antonia Wüst"
                    },
                    {
                        "name": "David Steinmann"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09949v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18678v1",
                "updated": "2024-10-24T12:12:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    12,
                    46,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T12:12:46Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    12,
                    46,
                    3,
                    298,
                    0
                ],
                "title": "Ali-AUG: Innovative Approaches to Labeled Data Augmentation using\n  One-Step Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ali-AUG: Innovative Approaches to Labeled Data Augmentation using\n  One-Step Diffusion Model"
                },
                "summary": "This paper introduces Ali-AUG, a novel single-step diffusion model for\nefficient labeled data augmentation in industrial applications. Our method\naddresses the challenge of limited labeled data by generating synthetic,\nlabeled images with precise feature insertion. Ali-AUG utilizes a stable\ndiffusion architecture enhanced with skip connections and LoRA modules to\nefficiently integrate masks and images, ensuring accurate feature placement\nwithout affecting unrelated image content. Experimental validation across\nvarious industrial datasets demonstrates Ali-AUG's superiority in generating\nhigh-quality, defect-enhanced images while maintaining rapid single-step\ninference. By offering precise control over feature insertion and minimizing\nrequired training steps, our technique significantly enhances data augmentation\ncapabilities, providing a powerful tool for improving the performance of deep\nlearning models in scenarios with limited labeled data. Ali-AUG is especially\nuseful for use cases like defective product image generation to train AI-based\nmodels to improve their ability to detect defects in manufacturing processes.\nUsing different data preparation strategies, including Classification Accuracy\nScore (CAS) and Naive Augmentation Score (NAS), we show that Ali-AUG improves\nmodel performance by 31% compared to other augmentation methods and by 45%\ncompared to models without data augmentation. Notably, Ali-AUG reduces training\ntime by 32% and supports both paired and unpaired datasets, enhancing\nflexibility in data preparation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Ali-AUG, a novel single-step diffusion model for\nefficient labeled data augmentation in industrial applications. Our method\naddresses the challenge of limited labeled data by generating synthetic,\nlabeled images with precise feature insertion. Ali-AUG utilizes a stable\ndiffusion architecture enhanced with skip connections and LoRA modules to\nefficiently integrate masks and images, ensuring accurate feature placement\nwithout affecting unrelated image content. Experimental validation across\nvarious industrial datasets demonstrates Ali-AUG's superiority in generating\nhigh-quality, defect-enhanced images while maintaining rapid single-step\ninference. By offering precise control over feature insertion and minimizing\nrequired training steps, our technique significantly enhances data augmentation\ncapabilities, providing a powerful tool for improving the performance of deep\nlearning models in scenarios with limited labeled data. Ali-AUG is especially\nuseful for use cases like defective product image generation to train AI-based\nmodels to improve their ability to detect defects in manufacturing processes.\nUsing different data preparation strategies, including Classification Accuracy\nScore (CAS) and Naive Augmentation Score (NAS), we show that Ali-AUG improves\nmodel performance by 31% compared to other augmentation methods and by 45%\ncompared to models without data augmentation. Notably, Ali-AUG reduces training\ntime by 32% and supports both paired and unpaired datasets, enhancing\nflexibility in data preparation."
                },
                "authors": [
                    {
                        "name": "Ali Hamza"
                    },
                    {
                        "name": "Aizea Lojo"
                    },
                    {
                        "name": "Adrian Núñez-Marcos"
                    },
                    {
                        "name": "Aitziber Atutxa"
                    }
                ],
                "author_detail": {
                    "name": "Aitziber Atutxa"
                },
                "author": "Aitziber Atutxa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17820v2",
                "updated": "2024-10-24T12:01:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    1,
                    31,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-23T12:26:10Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    26,
                    10,
                    2,
                    297,
                    0
                ],
                "title": "Understanding When Tree of Thoughts Succeeds: Larger Models Excel in\n  Generation, Not Discrimination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding When Tree of Thoughts Succeeds: Larger Models Excel in\n  Generation, Not Discrimination"
                },
                "summary": "Tree of Thoughts (ToT) is a reasoning strategy for Large Language Models\n(LLMs) that employs a generator to suggest reasoning steps and a discriminator\nto decide which steps to implement. ToT demonstrates strong performance on\nreasoning tasks, often surpassing simple methods such as Input-Output (IO)\nprompting and Chain-of-Thought (CoT) reasoning. However, ToT does not\nconsistently outperform such simpler methods across all models, leaving large\nknowledge gaps on the conditions under which ToT is most beneficial. In this\npaper, we analyze the roles of the generator and discriminator separately to\nbetter understand the conditions when ToT is beneficial. We find that the\ngenerator plays a more critical role than the discriminator in driving the\nsuccess of ToT. Scaling the generator leads to notable improvements in ToT\nperformance, even when using a smaller model as the discriminator, whereas\nscaling the discriminator with a fixed generator yields only marginal gains.\nOur results show that models across different scales exhibit comparable\ndiscrimination capabilities, yet differ significantly in their generative\nperformance for ToT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) is a reasoning strategy for Large Language Models\n(LLMs) that employs a generator to suggest reasoning steps and a discriminator\nto decide which steps to implement. ToT demonstrates strong performance on\nreasoning tasks, often surpassing simple methods such as Input-Output (IO)\nprompting and Chain-of-Thought (CoT) reasoning. However, ToT does not\nconsistently outperform such simpler methods across all models, leaving large\nknowledge gaps on the conditions under which ToT is most beneficial. In this\npaper, we analyze the roles of the generator and discriminator separately to\nbetter understand the conditions when ToT is beneficial. We find that the\ngenerator plays a more critical role than the discriminator in driving the\nsuccess of ToT. Scaling the generator leads to notable improvements in ToT\nperformance, even when using a smaller model as the discriminator, whereas\nscaling the discriminator with a fixed generator yields only marginal gains.\nOur results show that models across different scales exhibit comparable\ndiscrimination capabilities, yet differ significantly in their generative\nperformance for ToT."
                },
                "authors": [
                    {
                        "name": "Qiqi Chen"
                    },
                    {
                        "name": "Xinpeng Wang"
                    },
                    {
                        "name": "Philipp Mondorf"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "Code: github.com/mainlp/tot-eval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18668v1",
                "updated": "2024-10-24T11:59:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    59,
                    32,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T11:59:32Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    59,
                    32,
                    3,
                    298,
                    0
                ],
                "title": "3D Shape Completion with Test-Time Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Shape Completion with Test-Time Training"
                },
                "summary": "This work addresses the problem of \\textit{shape completion}, i.e., the task\nof restoring incomplete shapes by predicting their missing parts. While\nprevious works have often predicted the fractured and restored shape in one\nstep, we approach the task by separately predicting the fractured and newly\nrestored parts, but ensuring these predictions are interconnected. We use a\ndecoder network motivated by related work on the prediction of signed distance\nfunctions (DeepSDF). In particular, our representation allows us to consider\ntest-time-training, i.e., finetuning network parameters to match the given\nincomplete shape more accurately during inference. While previous works often\nhave difficulties with artifacts around the fracture boundary, we demonstrate\nthat our overfitting to the fractured parts leads to significant improvements\nin the restoration of eight different shape categories of the ShapeNet data set\nin terms of their chamfer distances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work addresses the problem of \\textit{shape completion}, i.e., the task\nof restoring incomplete shapes by predicting their missing parts. While\nprevious works have often predicted the fractured and restored shape in one\nstep, we approach the task by separately predicting the fractured and newly\nrestored parts, but ensuring these predictions are interconnected. We use a\ndecoder network motivated by related work on the prediction of signed distance\nfunctions (DeepSDF). In particular, our representation allows us to consider\ntest-time-training, i.e., finetuning network parameters to match the given\nincomplete shape more accurately during inference. While previous works often\nhave difficulties with artifacts around the fracture boundary, we demonstrate\nthat our overfitting to the fractured parts leads to significant improvements\nin the restoration of eight different shape categories of the ShapeNet data set\nin terms of their chamfer distances."
                },
                "authors": [
                    {
                        "name": "Michael Schopf-Kuester"
                    },
                    {
                        "name": "Zorah Lähner"
                    },
                    {
                        "name": "Michael Moeller"
                    }
                ],
                "author_detail": {
                    "name": "Michael Moeller"
                },
                "author": "Michael Moeller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18665v1",
                "updated": "2024-10-24T11:54:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    54,
                    30,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T11:54:30Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    54,
                    30,
                    3,
                    298,
                    0
                ],
                "title": "A second radio flare from the tidal disruption event AT2020vwl: a\n  delayed outflow ejection?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A second radio flare from the tidal disruption event AT2020vwl: a\n  delayed outflow ejection?"
                },
                "summary": "We present the discovery of a second radio flare from the tidal disruption\nevent (TDE) AT2020vwl via long-term monitoring radio observations. Late-time\nradio flares from TDEs are being discovered more commonly, with many TDEs\nshowing radio emission 1000s of days after the stellar disruption, but the\nmechanism that powers these late-time flares is uncertain. Here we present\nradio spectral observations of the first and second radio flares observed from\nthe TDE AT2020vwl. Through detailed radio spectral monitoring, we find evidence\nfor two distinct outflow ejection episodes, or a period of renewed energy\ninjection into the pre-existing outflow. We deduce that the second radio flare\nis powered by an outflow that is initially slower than the first flare, but\ncarries more energy and accelerates over time. Through modelling the long-term\noptical and UV emission from the TDE as arising from an accretion disc, we\ninfer that the second radio outflow launch or energy injection episode occurred\napproximately at the time of peak accretion rate. The fast decay of the second\nflare precludes environmental changes as an explanation, while the velocity of\nthe outflow is at all times too low to be explained by an off-axis relativistic\njet. Future observations that search for any link between the accretion disc\nproperties and late time radio flares from TDEs will aid in understanding what\npowers the radio outflows in TDEs, and confirm if multiple outflow ejections or\nenergy injection episodes are common.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the discovery of a second radio flare from the tidal disruption\nevent (TDE) AT2020vwl via long-term monitoring radio observations. Late-time\nradio flares from TDEs are being discovered more commonly, with many TDEs\nshowing radio emission 1000s of days after the stellar disruption, but the\nmechanism that powers these late-time flares is uncertain. Here we present\nradio spectral observations of the first and second radio flares observed from\nthe TDE AT2020vwl. Through detailed radio spectral monitoring, we find evidence\nfor two distinct outflow ejection episodes, or a period of renewed energy\ninjection into the pre-existing outflow. We deduce that the second radio flare\nis powered by an outflow that is initially slower than the first flare, but\ncarries more energy and accelerates over time. Through modelling the long-term\noptical and UV emission from the TDE as arising from an accretion disc, we\ninfer that the second radio outflow launch or energy injection episode occurred\napproximately at the time of peak accretion rate. The fast decay of the second\nflare precludes environmental changes as an explanation, while the velocity of\nthe outflow is at all times too low to be explained by an off-axis relativistic\njet. Future observations that search for any link between the accretion disc\nproperties and late time radio flares from TDEs will aid in understanding what\npowers the radio outflows in TDEs, and confirm if multiple outflow ejections or\nenergy injection episodes are common."
                },
                "authors": [
                    {
                        "name": "A. J. Goodwin"
                    },
                    {
                        "name": "A. Mummery"
                    },
                    {
                        "name": "T. Laskar"
                    },
                    {
                        "name": "K. D. Alexander"
                    },
                    {
                        "name": "G. E. Anderson"
                    },
                    {
                        "name": "M. Bietenholz"
                    },
                    {
                        "name": "C. Bonnerot"
                    },
                    {
                        "name": "C. T. Christy"
                    },
                    {
                        "name": "W. Golay"
                    },
                    {
                        "name": "W. Lu"
                    },
                    {
                        "name": "R. Margutti"
                    },
                    {
                        "name": "J. C. A. Miller-Jones"
                    },
                    {
                        "name": "E. Ramirez-Ruiz"
                    },
                    {
                        "name": "R. Saxton"
                    },
                    {
                        "name": "S. van Velzen"
                    }
                ],
                "author_detail": {
                    "name": "S. van Velzen"
                },
                "author": "S. van Velzen",
                "arxiv_comment": "19 pages, 7 figures, submitted to ApJ. Comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18663v1",
                "updated": "2024-10-24T11:48:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    48,
                    2,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T11:48:02Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    48,
                    2,
                    3,
                    298,
                    0
                ],
                "title": "Detection of faculae in the transit and transmission spectrum of\n  WASP-69b",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection of faculae in the transit and transmission spectrum of\n  WASP-69b"
                },
                "summary": "Context: Transmission spectroscopy is a powerful tool for understanding\nexoplanet atmospheres. At optical wavelengths, it makes it possible to infer\nthe composition and the presence of aerosols in the atmosphere. However,\nunocculted stellar activity can result in contamination of atmospheric\ntransmission spectra by introducing spurious slopes and molecular signals.\n  Aims: We aim to characterise the atmosphere of the transiting exoplanet\nWASP-69b, a hot Jupiter orbiting an active K star, and characterise the host\nstar's activity levels. Methods: We obtained three nights of spectrophotometric\ndata with the FORS2 instrument on the VLT, covering a wavelength range of\n340-1100 nm. We performed retrievals on the full spectrum with combined stellar\nactivity and planet atmosphere models.\n  Results: We directly detect a facula in the form of a hot spot crossing event\nin one of the transits and indirectly detect unocculted faculae through an\napparently decreasing radius towards the blue end of the transmission spectrum.\nWe determine a facula temperature of $\\Delta T=+644^{+427}_{-263}$ K for the\nformer and a stellar coverage fraction of around 30% with a temperature of\n$\\Delta T=+231\\pm72$ K for the latter. The planetary atmosphere is best fit\nwith a high-altitude cloud deck at 1.4 mbar that mutes atomic and molecular\nfeatures. We find indications of water and ammonia with\n$log(H_2O)=-2.01^{+0.54}_{-0.86}$ and $log(NH_3)=-3.4^{+0.96}_{-5.20}\nrespectively and place 3$\\sigma$ upper limits on TiO ($10^{-7.65}$) and K\n($10^{-7}$). Conclusions. The simultaneous multi-wavelength observations allow\nus to break the size-contrast degeneracy for facula-crossings, meaning we can\nobtain temperatures for both the directly and indirectly detected faculae,\nwhich are consistent with each other.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Transmission spectroscopy is a powerful tool for understanding\nexoplanet atmospheres. At optical wavelengths, it makes it possible to infer\nthe composition and the presence of aerosols in the atmosphere. However,\nunocculted stellar activity can result in contamination of atmospheric\ntransmission spectra by introducing spurious slopes and molecular signals.\n  Aims: We aim to characterise the atmosphere of the transiting exoplanet\nWASP-69b, a hot Jupiter orbiting an active K star, and characterise the host\nstar's activity levels. Methods: We obtained three nights of spectrophotometric\ndata with the FORS2 instrument on the VLT, covering a wavelength range of\n340-1100 nm. We performed retrievals on the full spectrum with combined stellar\nactivity and planet atmosphere models.\n  Results: We directly detect a facula in the form of a hot spot crossing event\nin one of the transits and indirectly detect unocculted faculae through an\napparently decreasing radius towards the blue end of the transmission spectrum.\nWe determine a facula temperature of $\\Delta T=+644^{+427}_{-263}$ K for the\nformer and a stellar coverage fraction of around 30% with a temperature of\n$\\Delta T=+231\\pm72$ K for the latter. The planetary atmosphere is best fit\nwith a high-altitude cloud deck at 1.4 mbar that mutes atomic and molecular\nfeatures. We find indications of water and ammonia with\n$log(H_2O)=-2.01^{+0.54}_{-0.86}$ and $log(NH_3)=-3.4^{+0.96}_{-5.20}\nrespectively and place 3$\\sigma$ upper limits on TiO ($10^{-7.65}$) and K\n($10^{-7}$). Conclusions. The simultaneous multi-wavelength observations allow\nus to break the size-contrast degeneracy for facula-crossings, meaning we can\nobtain temperatures for both the directly and indirectly detected faculae,\nwhich are consistent with each other."
                },
                "authors": [
                    {
                        "name": "D. J. M. Petit dit de la Roche"
                    },
                    {
                        "name": "H. Chakraborty"
                    },
                    {
                        "name": "M. Lendl"
                    },
                    {
                        "name": "D. Kitzmann"
                    },
                    {
                        "name": "A. G. M. Pietrow"
                    },
                    {
                        "name": "B. Akinsanmi"
                    },
                    {
                        "name": "H. M. J. Boffin"
                    },
                    {
                        "name": "Patricio E. Cubillos"
                    },
                    {
                        "name": "A. Deline"
                    },
                    {
                        "name": "D. Ehrenreich"
                    },
                    {
                        "name": "L. Fossati"
                    },
                    {
                        "name": "E. Sedaghati"
                    }
                ],
                "author_detail": {
                    "name": "E. Sedaghati"
                },
                "author": "E. Sedaghati",
                "arxiv_comment": "Accepted in A\\&A, 18 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18652v1",
                "updated": "2024-10-24T11:32:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    32,
                    0,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T11:32:00Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    32,
                    0,
                    3,
                    298,
                    0
                ],
                "title": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation"
                },
                "summary": "Generating high-quality charts with Large Language Models presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. Instruction, data, and code triplets are scarce and expensive\nto manually curate as their creation demands technical expertise. To address\nthis scalability issue, we introduce a reference-free automatic feedback\ngenerator, which eliminates the need for costly human intervention. Our novel\nframework, $C^2$, consists of (1) an automatic feedback provider (ChartAF) and\n(2) a diverse, reference-free dataset (ChartUIE-8K). Quantitative results are\ncompelling: in our first experiment, 74% of respondents strongly preferred, and\n10% preferred, the results after feedback. The second post-feedback experiment\ndemonstrates that ChartAF outperforms nine baselines. Moreover, ChartUIE-8K\nsignificantly improves data diversity by increasing queries, datasets, and\nchart types by 5982%, 1936%, and 91%, respectively, over benchmarks. Finally,\nan LLM user study revealed that 94% of participants preferred ChartUIE-8K's\nqueries, with 93% deeming them aligned with real-world use cases. Core\ncontributions are available as open-source at an anonymized project site, with\nample qualitative examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-quality charts with Large Language Models presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. Instruction, data, and code triplets are scarce and expensive\nto manually curate as their creation demands technical expertise. To address\nthis scalability issue, we introduce a reference-free automatic feedback\ngenerator, which eliminates the need for costly human intervention. Our novel\nframework, $C^2$, consists of (1) an automatic feedback provider (ChartAF) and\n(2) a diverse, reference-free dataset (ChartUIE-8K). Quantitative results are\ncompelling: in our first experiment, 74% of respondents strongly preferred, and\n10% preferred, the results after feedback. The second post-feedback experiment\ndemonstrates that ChartAF outperforms nine baselines. Moreover, ChartUIE-8K\nsignificantly improves data diversity by increasing queries, datasets, and\nchart types by 5982%, 1936%, and 91%, respectively, over benchmarks. Finally,\nan LLM user study revealed that 94% of participants preferred ChartUIE-8K's\nqueries, with 93% deeming them aligned with real-world use cases. Core\ncontributions are available as open-source at an anonymized project site, with\nample qualitative examples."
                },
                "authors": [
                    {
                        "name": "Woosung Koh"
                    },
                    {
                        "name": "Jang Han Yoon"
                    },
                    {
                        "name": "MinHyung Lee"
                    },
                    {
                        "name": "Youngjin Song"
                    },
                    {
                        "name": "Jaegwan Cho"
                    },
                    {
                        "name": "Jaehyun Kang"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Se-young Yun"
                    },
                    {
                        "name": "Youngjae Yu"
                    },
                    {
                        "name": "Bongshin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Bongshin Lee"
                },
                "author": "Bongshin Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13728v2",
                "updated": "2024-10-24T11:30:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    30,
                    33,
                    3,
                    298,
                    0
                ],
                "published": "2024-09-09T22:36:35Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    22,
                    36,
                    35,
                    0,
                    253,
                    0
                ],
                "title": "Rule Extrapolation in Language Models: A Study of Compositional\n  Generalization on OOD Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rule Extrapolation in Language Models: A Study of Compositional\n  Generalization on OOD Prompts"
                },
                "summary": "LLMs show remarkable emergent abilities, such as inferring concepts from\npresumably out-of-distribution prompts, known as in-context learning. Though\nthis success is often attributed to the Transformer architecture, our\nsystematic understanding is limited. In complex real-world data sets, even\ndefining what is out-of-distribution is not obvious. To better understand the\nOOD behaviour of autoregressive LLMs, we focus on formal languages, which are\ndefined by the intersection of rules. We define a new scenario of OOD\ncompositional generalization, termed rule extrapolation. Rule extrapolation\ndescribes OOD scenarios, where the prompt violates at least one rule. We\nevaluate rule extrapolation in formal languages with varying complexity in\nlinear and recurrent architectures, the Transformer, and state space models to\nunderstand the architectures' influence on rule extrapolation. We also lay the\nfirst stones of a normative theory of rule extrapolation, inspired by the\nSolomonoff prior in algorithmic information theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs show remarkable emergent abilities, such as inferring concepts from\npresumably out-of-distribution prompts, known as in-context learning. Though\nthis success is often attributed to the Transformer architecture, our\nsystematic understanding is limited. In complex real-world data sets, even\ndefining what is out-of-distribution is not obvious. To better understand the\nOOD behaviour of autoregressive LLMs, we focus on formal languages, which are\ndefined by the intersection of rules. We define a new scenario of OOD\ncompositional generalization, termed rule extrapolation. Rule extrapolation\ndescribes OOD scenarios, where the prompt violates at least one rule. We\nevaluate rule extrapolation in formal languages with varying complexity in\nlinear and recurrent architectures, the Transformer, and state space models to\nunderstand the architectures' influence on rule extrapolation. We also lay the\nfirst stones of a normative theory of rule extrapolation, inspired by the\nSolomonoff prior in algorithmic information theory."
                },
                "authors": [
                    {
                        "name": "Anna Mészáros"
                    },
                    {
                        "name": "Szilvia Ujváry"
                    },
                    {
                        "name": "Wieland Brendel"
                    },
                    {
                        "name": "Patrik Reizinger"
                    },
                    {
                        "name": "Ferenc Huszár"
                    }
                ],
                "author_detail": {
                    "name": "Ferenc Huszár"
                },
                "author": "Ferenc Huszár",
                "arxiv_comment": "Accepted as a spotlight poster at NeurIPS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19255v2",
                "updated": "2024-10-24T11:29:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    29,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-09-28T06:04:56Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    6,
                    4,
                    56,
                    5,
                    272,
                    0
                ],
                "title": "DENEB: A Hallucination-Robust Automatic Evaluation Metric for Image\n  Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DENEB: A Hallucination-Robust Automatic Evaluation Metric for Image\n  Captioning"
                },
                "summary": "In this work, we address the challenge of developing automatic evaluation\nmetrics for image captioning, with a particular focus on robustness against\nhallucinations. Existing metrics are often inadequate for handling\nhallucinations, primarily due to their limited ability to compare candidate\ncaptions with multifaceted reference captions. To address this shortcoming, we\npropose DENEB, a novel supervised automatic evaluation metric specifically\nrobust against hallucinations. DENEB incorporates the Sim-Vec Transformer, a\nmechanism that processes multiple references simultaneously, thereby\nefficiently capturing the similarity between an image, a candidate caption, and\nreference captions. To train DENEB, we construct the diverse and balanced\nNebula dataset comprising 32,978 images, paired with human judgments provided\nby 805 annotators. We demonstrated that DENEB achieves state-of-the-art\nperformance among existing LLM-free metrics on the FOIL, Composite,\nFlickr8K-Expert, Flickr8K-CF, Nebula, and PASCAL-50S datasets, validating its\neffectiveness and robustness against hallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we address the challenge of developing automatic evaluation\nmetrics for image captioning, with a particular focus on robustness against\nhallucinations. Existing metrics are often inadequate for handling\nhallucinations, primarily due to their limited ability to compare candidate\ncaptions with multifaceted reference captions. To address this shortcoming, we\npropose DENEB, a novel supervised automatic evaluation metric specifically\nrobust against hallucinations. DENEB incorporates the Sim-Vec Transformer, a\nmechanism that processes multiple references simultaneously, thereby\nefficiently capturing the similarity between an image, a candidate caption, and\nreference captions. To train DENEB, we construct the diverse and balanced\nNebula dataset comprising 32,978 images, paired with human judgments provided\nby 805 annotators. We demonstrated that DENEB achieves state-of-the-art\nperformance among existing LLM-free metrics on the FOIL, Composite,\nFlickr8K-Expert, Flickr8K-CF, Nebula, and PASCAL-50S datasets, validating its\neffectiveness and robustness against hallucinations."
                },
                "authors": [
                    {
                        "name": "Kazuki Matsuda"
                    },
                    {
                        "name": "Yuiga Wada"
                    },
                    {
                        "name": "Komei Sugiura"
                    }
                ],
                "author_detail": {
                    "name": "Komei Sugiura"
                },
                "author": "Komei Sugiura",
                "arxiv_comment": "ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18641v1",
                "updated": "2024-10-24T11:10:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    10,
                    54,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T11:10:54Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    10,
                    54,
                    3,
                    298,
                    0
                ],
                "title": "Smart ETL and LLM-based contents classification: the European Smart\n  Tourism Tools Observatory experience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart ETL and LLM-based contents classification: the European Smart\n  Tourism Tools Observatory experience"
                },
                "summary": "Purpose: Our research project focuses on improving the content update of the\nonline European Smart Tourism Tools (STTs) Observatory by incorporating and\ncategorizing STTs. The categorization is based on their taxonomy, and it\nfacilitates the end user's search process. The use of a Smart ETL (Extract,\nTransform, and Load) process, where \\emph{Smart} indicates the use of\nArtificial Intelligence (AI), is central to this endeavor.\n  Methods: The contents describing STTs are derived from PDF catalogs, where\nPDF-scraping techniques extract QR codes, images, links, and text information.\nDuplicate STTs between the catalogs are removed, and the remaining ones are\nclassified based on their text information using Large Language Models (LLMs).\nFinally, the data is transformed to comply with the Dublin Core metadata\nstructure (the observatory's metadata structure), chosen for its wide\nacceptance and flexibility.\n  Results: The Smart ETL process to import STTs to the observatory combines\nPDF-scraping techniques with LLMs for text content-based classification. Our\npreliminary results have demonstrated the potential of LLMs for text\ncontent-based classification.\n  Conclusion: The proposed approach's feasibility is a step towards efficient\ncontent-based classification, not only in Smart Tourism but also adaptable to\nother fields. Future work will mainly focus on refining this classification\nprocess.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: Our research project focuses on improving the content update of the\nonline European Smart Tourism Tools (STTs) Observatory by incorporating and\ncategorizing STTs. The categorization is based on their taxonomy, and it\nfacilitates the end user's search process. The use of a Smart ETL (Extract,\nTransform, and Load) process, where \\emph{Smart} indicates the use of\nArtificial Intelligence (AI), is central to this endeavor.\n  Methods: The contents describing STTs are derived from PDF catalogs, where\nPDF-scraping techniques extract QR codes, images, links, and text information.\nDuplicate STTs between the catalogs are removed, and the remaining ones are\nclassified based on their text information using Large Language Models (LLMs).\nFinally, the data is transformed to comply with the Dublin Core metadata\nstructure (the observatory's metadata structure), chosen for its wide\nacceptance and flexibility.\n  Results: The Smart ETL process to import STTs to the observatory combines\nPDF-scraping techniques with LLMs for text content-based classification. Our\npreliminary results have demonstrated the potential of LLMs for text\ncontent-based classification.\n  Conclusion: The proposed approach's feasibility is a step towards efficient\ncontent-based classification, not only in Smart Tourism but also adaptable to\nother fields. Future work will mainly focus on refining this classification\nprocess."
                },
                "authors": [
                    {
                        "name": "Diogo Cosme"
                    },
                    {
                        "name": "António Galvão"
                    },
                    {
                        "name": "Fernando Brito e Abreu"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Brito e Abreu"
                },
                "author": "Fernando Brito e Abreu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; I.2.7; I.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05008v2",
                "updated": "2024-10-24T10:41:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    41,
                    23,
                    3,
                    298,
                    0
                ],
                "published": "2024-05-08T12:24:52Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    12,
                    24,
                    52,
                    2,
                    129,
                    0
                ],
                "title": "ADELIE: Aligning Large Language Models on Information Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADELIE: Aligning Large Language Models on Information Extraction"
                },
                "summary": "Large language models (LLMs) usually fall short on information extraction\n(IE) tasks and struggle to follow the complex instructions of IE tasks. This\nprimarily arises from LLMs not being aligned with humans, as mainstream\nalignment datasets typically do not include IE data. In this paper, we\nintroduce ADELIE (Aligning large language moDELs on Information Extraction), an\naligned LLM that effectively solves various IE tasks, including closed IE, open\nIE, and on-demand IE. We first collect and construct a high-quality alignment\ncorpus IEInstruct for IE. Then we train ADELIE_SFT using instruction tuning on\nIEInstruct. We further train ADELIE_SFT with direct preference optimization\n(DPO) objective, resulting in ADELIE_DPO. Extensive experiments on various\nheld-out IE datasets demonstrate that our models (ADELIE_SFT and ADELIE_DPO)\nachieve state-of-the-art (SoTA) performance among open-source models. We\nfurther explore the general capabilities of ADELIE, and experimental results\nreveal that their general capabilities do not exhibit a noticeable decline. We\nwill release the code, data, and models to facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) usually fall short on information extraction\n(IE) tasks and struggle to follow the complex instructions of IE tasks. This\nprimarily arises from LLMs not being aligned with humans, as mainstream\nalignment datasets typically do not include IE data. In this paper, we\nintroduce ADELIE (Aligning large language moDELs on Information Extraction), an\naligned LLM that effectively solves various IE tasks, including closed IE, open\nIE, and on-demand IE. We first collect and construct a high-quality alignment\ncorpus IEInstruct for IE. Then we train ADELIE_SFT using instruction tuning on\nIEInstruct. We further train ADELIE_SFT with direct preference optimization\n(DPO) objective, resulting in ADELIE_DPO. Extensive experiments on various\nheld-out IE datasets demonstrate that our models (ADELIE_SFT and ADELIE_DPO)\nachieve state-of-the-art (SoTA) performance among open-source models. We\nfurther explore the general capabilities of ADELIE, and experimental results\nreveal that their general capabilities do not exhibit a noticeable decline. We\nwill release the code, data, and models to facilitate further research."
                },
                "authors": [
                    {
                        "name": "Yunjia Qi"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "Accepted at EMNLP 2024. Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18624v1",
                "updated": "2024-10-24T10:32:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    32,
                    10,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T10:32:10Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    32,
                    10,
                    3,
                    298,
                    0
                ],
                "title": "Prompting and Fine-Tuning of Small LLMs for Length-Controllable\n  Telephone Call Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting and Fine-Tuning of Small LLMs for Length-Controllable\n  Telephone Call Summarization"
                },
                "summary": "This paper explores the rapid development of a telephone call summarization\nsystem utilizing large language models (LLMs). Our approach involves initial\nexperiments with prompting existing LLMs to generate summaries of telephone\nconversations, followed by the creation of a tailored synthetic training\ndataset utilizing stronger frontier models. We place special focus on the\ndiversity of the generated data and on the ability to control the length of the\ngenerated summaries to meet various use-case specific requirements. The\neffectiveness of our method is evaluated using two state-of-the-art\nLLM-as-a-judge-based evaluation techniques to ensure the quality and relevance\nof the summaries. Our results show that fine-tuned Llama-2-7B-based\nsummarization model performs on-par with GPT-4 in terms of factual accuracy,\ncompleteness and conciseness. Our findings demonstrate the potential for\nquickly bootstrapping a practical and efficient call summarization system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the rapid development of a telephone call summarization\nsystem utilizing large language models (LLMs). Our approach involves initial\nexperiments with prompting existing LLMs to generate summaries of telephone\nconversations, followed by the creation of a tailored synthetic training\ndataset utilizing stronger frontier models. We place special focus on the\ndiversity of the generated data and on the ability to control the length of the\ngenerated summaries to meet various use-case specific requirements. The\neffectiveness of our method is evaluated using two state-of-the-art\nLLM-as-a-judge-based evaluation techniques to ensure the quality and relevance\nof the summaries. Our results show that fine-tuned Llama-2-7B-based\nsummarization model performs on-par with GPT-4 in terms of factual accuracy,\ncompleteness and conciseness. Our findings demonstrate the potential for\nquickly bootstrapping a practical and efficient call summarization system."
                },
                "authors": [
                    {
                        "name": "David Thulke"
                    },
                    {
                        "name": "Yingbo Gao"
                    },
                    {
                        "name": "Rricha Jalota"
                    },
                    {
                        "name": "Christian Dugast"
                    },
                    {
                        "name": "Hermann Ney"
                    }
                ],
                "author_detail": {
                    "name": "Hermann Ney"
                },
                "author": "Hermann Ney",
                "arxiv_comment": "Accepted at the The International Conference on Foundation and Large\n  Language Models (FLLM2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15859v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15859v3",
                "updated": "2024-10-24T10:29:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    29,
                    15,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-21T10:39:05Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    10,
                    39,
                    5,
                    0,
                    295,
                    0
                ],
                "title": "Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced\n  Extrapolation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced\n  Extrapolation in LLMs"
                },
                "summary": "Large language models (LLMs), although having revolutionized many fields,\nstill suffer from the challenging extrapolation problem, where the inference\nability of LLMs sharply declines beyond their max training lengths. In this\nwork, we conduct a theoretical analysis to better understand why No Position\nEncoding (NoPE) fails outside its effective range, as well as examining the\npower of Position Encoding (PE) in this context. Our findings reveal that with\nmeticulous weave position, PE can indeed be extended beyond effective range.\nOur theorems establish that LLMs equipped with weave PE can achieve improved\nextrapolation performance without additional cost. Furthermore, we introduce a\nnovel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based\ntriangular attention matrix and applies Stair PE to manage the final chunk.\nThis method not only retains competitive performance but also offers\nsubstantial benefits such as significantly reduced memory demand and faster\ninference speed. Extensive experiments validate the effectiveness of\nMesa-Extrapolation, demonstrating its potential as a scalable solution to\nenhancing LLMs applicative reach. Our code is available at\n\\url{https://github.com/soacker/Mesa-Extrapolation}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), although having revolutionized many fields,\nstill suffer from the challenging extrapolation problem, where the inference\nability of LLMs sharply declines beyond their max training lengths. In this\nwork, we conduct a theoretical analysis to better understand why No Position\nEncoding (NoPE) fails outside its effective range, as well as examining the\npower of Position Encoding (PE) in this context. Our findings reveal that with\nmeticulous weave position, PE can indeed be extended beyond effective range.\nOur theorems establish that LLMs equipped with weave PE can achieve improved\nextrapolation performance without additional cost. Furthermore, we introduce a\nnovel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based\ntriangular attention matrix and applies Stair PE to manage the final chunk.\nThis method not only retains competitive performance but also offers\nsubstantial benefits such as significantly reduced memory demand and faster\ninference speed. Extensive experiments validate the effectiveness of\nMesa-Extrapolation, demonstrating its potential as a scalable solution to\nenhancing LLMs applicative reach. Our code is available at\n\\url{https://github.com/soacker/Mesa-Extrapolation}."
                },
                "authors": [
                    {
                        "name": "Xin Ma"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jingjing Liu"
                    },
                    {
                        "name": "Xiaoxu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxu Ma"
                },
                "author": "Xiaoxu Ma",
                "arxiv_comment": "Accepted by NeurIPS 2024; 13 pages and 30 pages appendix;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15859v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15859v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18608v1",
                "updated": "2024-10-24T10:05:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    5,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T10:05:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    5,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "Learning Transparent Reward Models via Unsupervised Feature Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Transparent Reward Models via Unsupervised Feature Selection"
                },
                "summary": "In complex real-world tasks such as robotic manipulation and autonomous\ndriving, collecting expert demonstrations is often more straightforward than\nspecifying precise learning objectives and task descriptions. Learning from\nexpert data can be achieved through behavioral cloning or by learning a reward\nfunction, i.e., inverse reinforcement learning. The latter allows for training\nwith additional data outside the training distribution, guided by the inferred\nreward function. We propose a novel approach to construct compact and\ntransparent reward models from automatically selected state features. These\ninferred rewards have an explicit form and enable the learning of policies that\nclosely match expert behavior by training standard reinforcement learning\nalgorithms from scratch. We validate our method's performance in various\nrobotic environments with continuous and high-dimensional state spaces.\nWebpage: \\url{https://sites.google.com/view/transparent-reward}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In complex real-world tasks such as robotic manipulation and autonomous\ndriving, collecting expert demonstrations is often more straightforward than\nspecifying precise learning objectives and task descriptions. Learning from\nexpert data can be achieved through behavioral cloning or by learning a reward\nfunction, i.e., inverse reinforcement learning. The latter allows for training\nwith additional data outside the training distribution, guided by the inferred\nreward function. We propose a novel approach to construct compact and\ntransparent reward models from automatically selected state features. These\ninferred rewards have an explicit form and enable the learning of policies that\nclosely match expert behavior by training standard reinforcement learning\nalgorithms from scratch. We validate our method's performance in various\nrobotic environments with continuous and high-dimensional state spaces.\nWebpage: \\url{https://sites.google.com/view/transparent-reward}."
                },
                "authors": [
                    {
                        "name": "Daulet Baimukashev"
                    },
                    {
                        "name": "Gokhan Alcan"
                    },
                    {
                        "name": "Kevin Sebastian Luck"
                    },
                    {
                        "name": "Ville Kyrki"
                    }
                ],
                "author_detail": {
                    "name": "Ville Kyrki"
                },
                "author": "Ville Kyrki",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16802v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16802v4",
                "updated": "2024-10-24T09:52:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    52,
                    59,
                    3,
                    298,
                    0
                ],
                "published": "2024-05-27T03:44:24Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    3,
                    44,
                    24,
                    0,
                    148,
                    0
                ],
                "title": "AutoPSV: Automated Process-Supervised Verifier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPSV: Automated Process-Supervised Verifier"
                },
                "summary": "In this work, we propose a novel method named \\textbf{Auto}mated\n\\textbf{P}rocess-\\textbf{S}upervised \\textbf{V}erifier\n(\\textbf{\\textsc{AutoPSV}}) to enhance the reasoning capabilities of large\nlanguage models (LLMs) by automatically annotating the reasoning steps.\n\\textsc{AutoPSV} begins by training a verification model on the correctness of\nfinal answers, enabling it to generate automatic process annotations. This\nverification model assigns a confidence score to each reasoning step,\nindicating the probability of arriving at the correct final answer from that\npoint onward. We detect relative changes in the verification's confidence\nscores across reasoning steps to automatically annotate the reasoning process,\nenabling error detection even in scenarios where ground truth answers are\nunavailable. This alleviates the need for numerous manual annotations or the\nhigh computational costs associated with model-induced annotation approaches.\nWe experimentally validate that the step-level confidence changes learned by\nthe verification model trained on the final answer correctness can effectively\nidentify errors in the reasoning steps. We demonstrate that the verification\nmodel, when trained on process annotations generated by \\textsc{AutoPSV},\nexhibits improved performance in selecting correct answers from multiple\nLLM-generated outputs. Notably, we achieve substantial improvements across five\ndatasets in mathematics and commonsense reasoning. The source code of\n\\textsc{AutoPSV} is available at \\url{https://github.com/rookie-joe/AutoPSV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a novel method named \\textbf{Auto}mated\n\\textbf{P}rocess-\\textbf{S}upervised \\textbf{V}erifier\n(\\textbf{\\textsc{AutoPSV}}) to enhance the reasoning capabilities of large\nlanguage models (LLMs) by automatically annotating the reasoning steps.\n\\textsc{AutoPSV} begins by training a verification model on the correctness of\nfinal answers, enabling it to generate automatic process annotations. This\nverification model assigns a confidence score to each reasoning step,\nindicating the probability of arriving at the correct final answer from that\npoint onward. We detect relative changes in the verification's confidence\nscores across reasoning steps to automatically annotate the reasoning process,\nenabling error detection even in scenarios where ground truth answers are\nunavailable. This alleviates the need for numerous manual annotations or the\nhigh computational costs associated with model-induced annotation approaches.\nWe experimentally validate that the step-level confidence changes learned by\nthe verification model trained on the final answer correctness can effectively\nidentify errors in the reasoning steps. We demonstrate that the verification\nmodel, when trained on process annotations generated by \\textsc{AutoPSV},\nexhibits improved performance in selecting correct answers from multiple\nLLM-generated outputs. Notably, we achieve substantial improvements across five\ndatasets in mathematics and commonsense reasoning. The source code of\n\\textsc{AutoPSV} is available at \\url{https://github.com/rookie-joe/AutoPSV}."
                },
                "authors": [
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Zhiyang Dou"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Zeyu Cao"
                    },
                    {
                        "name": "Jianbo Dai"
                    },
                    {
                        "name": "Yingjia Wan"
                    },
                    {
                        "name": "Zhijiang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Guo"
                },
                "author": "Zhijiang Guo",
                "arxiv_comment": "Accepted by NeurIPS 2024 Poster, 21 pages, 1 figure, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16802v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16802v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18598v1",
                "updated": "2024-10-24T09:48:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    48,
                    5,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T09:48:05Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    48,
                    5,
                    3,
                    298,
                    0
                ],
                "title": "Impact of ground-state properties and collective excitations on the\n  Skyrme ansatz: a Bayesian study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of ground-state properties and collective excitations on the\n  Skyrme ansatz: a Bayesian study"
                },
                "summary": "State-of-the-art models based on nuclear Density Functional Theory are\nsuccessful in the description of nuclei throughout the whole nuclear chart.\nAmong them, some differences arise regarding their accuracy. For a given\nnuclear model, this depends on the procedure adopted to determine the\nparameters, and, at the same time, new experimental findings constantly\nchallenge theory. In the present work, we present a Bayesian inference study\naimed at assessing the performance of the Skyrme Energy Density Functional. For\nthe sake of simplicity and clarity, we restrict to spherical, double-magic\nnuclei, giving equal emphasis to ground-state and dynamical properties. Our\nbasic constraints are: i) masses and charge radii, which are known to be very\nsensitive to the saturation energy and density; ii) spin-orbit splittings,\nwhich are associated with the spin-orbit parameter(s); iii) the electric dipole\npolarizability and parity-violating asymmetry, which are associated with the\ndensity dependence of the symmetry energy; iv) the excitation energy of the\nIsoscalar Giant Monopole Resonance, to constrain the nuclear matter\nincompressibility; v) the energy-weighted sum rule of the Isovector Giant\nDipole Resonance, to account for the isovector effective mass; and vi) the\nexcitation energy of the Isoscalar Quadrupole Resonance, that is related to the\nisoscalar effective mass. In this way, we test the Skyrme ansatz in a\nstatistically meaningful way, by determining the posterior distributions of the\nparameters as well as their correlation, and discussing a possible strategy for\nfuture developments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art models based on nuclear Density Functional Theory are\nsuccessful in the description of nuclei throughout the whole nuclear chart.\nAmong them, some differences arise regarding their accuracy. For a given\nnuclear model, this depends on the procedure adopted to determine the\nparameters, and, at the same time, new experimental findings constantly\nchallenge theory. In the present work, we present a Bayesian inference study\naimed at assessing the performance of the Skyrme Energy Density Functional. For\nthe sake of simplicity and clarity, we restrict to spherical, double-magic\nnuclei, giving equal emphasis to ground-state and dynamical properties. Our\nbasic constraints are: i) masses and charge radii, which are known to be very\nsensitive to the saturation energy and density; ii) spin-orbit splittings,\nwhich are associated with the spin-orbit parameter(s); iii) the electric dipole\npolarizability and parity-violating asymmetry, which are associated with the\ndensity dependence of the symmetry energy; iv) the excitation energy of the\nIsoscalar Giant Monopole Resonance, to constrain the nuclear matter\nincompressibility; v) the energy-weighted sum rule of the Isovector Giant\nDipole Resonance, to account for the isovector effective mass; and vi) the\nexcitation energy of the Isoscalar Quadrupole Resonance, that is related to the\nisoscalar effective mass. In this way, we test the Skyrme ansatz in a\nstatistically meaningful way, by determining the posterior distributions of the\nparameters as well as their correlation, and discussing a possible strategy for\nfuture developments."
                },
                "authors": [
                    {
                        "name": "Pietro Klausner"
                    },
                    {
                        "name": "Gianluca Colò"
                    },
                    {
                        "name": "Xavier Roca-Maza"
                    },
                    {
                        "name": "Enrico Vigezzi"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Vigezzi"
                },
                "author": "Enrico Vigezzi",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18588v1",
                "updated": "2024-10-24T09:37:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    37,
                    23,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T09:37:23Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    37,
                    23,
                    3,
                    298,
                    0
                ],
                "title": "Knowledge Distillation Using Frontier Open-source LLMs: Generalizability\n  and the Role of Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation Using Frontier Open-source LLMs: Generalizability\n  and the Role of Synthetic Data"
                },
                "summary": "Leading open-source large language models (LLMs) such as\nLlama-3.1-Instruct-405B are extremely capable at generating text, answering\nquestions, and solving a variety of natural language understanding tasks.\nHowever, they incur higher inference cost and latency compared to smaller LLMs.\nKnowledge distillation provides a way to use outputs from these large, capable\nteacher models to train smaller student models which can be used for inference\nat lower cost and latency, while retaining comparable accuracy. We investigate\nthe efficacy of distillation using the Llama-3.1-405B-Instruct teacher and the\nsmaller Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct student models.\nContributions of this work include (a) We evaluate the generalizability of\ndistillation with the above Llama-3.1 teacher-student pairs across different\ntasks and datasets (b) We show that using synthetic data during distillation\nsignificantly improves the accuracy of 8B and 70B models, and when used with\nreasoning chains, even matches or surpasses the zero-shot accuracy of 405B\nmodel on some datasets (c) We empirically show that distillation enables 8B and\n70B models to internalize 405B's reasoning ability by using only standard\nfine-tuning (without customizing any loss function). This allows cost and\nlatency-efficient student model inference. (d) We show pitfalls in evaluation\nof distillation, and present task-specific evaluation, including both human and\nLLM-grading, and ground-truth based traditional accuracy benchmarks. This\nmethodical study brings out the fundamental importance of synthetic data\nquality in knowledge distillation, and of combining multiple, task-specific\nways of accuracy and quality evaluation in assessing the effectiveness of\ndistillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leading open-source large language models (LLMs) such as\nLlama-3.1-Instruct-405B are extremely capable at generating text, answering\nquestions, and solving a variety of natural language understanding tasks.\nHowever, they incur higher inference cost and latency compared to smaller LLMs.\nKnowledge distillation provides a way to use outputs from these large, capable\nteacher models to train smaller student models which can be used for inference\nat lower cost and latency, while retaining comparable accuracy. We investigate\nthe efficacy of distillation using the Llama-3.1-405B-Instruct teacher and the\nsmaller Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct student models.\nContributions of this work include (a) We evaluate the generalizability of\ndistillation with the above Llama-3.1 teacher-student pairs across different\ntasks and datasets (b) We show that using synthetic data during distillation\nsignificantly improves the accuracy of 8B and 70B models, and when used with\nreasoning chains, even matches or surpasses the zero-shot accuracy of 405B\nmodel on some datasets (c) We empirically show that distillation enables 8B and\n70B models to internalize 405B's reasoning ability by using only standard\nfine-tuning (without customizing any loss function). This allows cost and\nlatency-efficient student model inference. (d) We show pitfalls in evaluation\nof distillation, and present task-specific evaluation, including both human and\nLLM-grading, and ground-truth based traditional accuracy benchmarks. This\nmethodical study brings out the fundamental importance of synthetic data\nquality in knowledge distillation, and of combining multiple, task-specific\nways of accuracy and quality evaluation in assessing the effectiveness of\ndistillation."
                },
                "authors": [
                    {
                        "name": "Anup Shirgaonkar"
                    },
                    {
                        "name": "Nikhil Pandey"
                    },
                    {
                        "name": "Nazmiye Ceren Abay"
                    },
                    {
                        "name": "Tolga Aktas"
                    },
                    {
                        "name": "Vijay Aski"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Aski"
                },
                "author": "Vijay Aski",
                "arxiv_comment": "25 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09528v2",
                "updated": "2024-10-24T09:36:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    36,
                    53,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-12T13:19:11Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    19,
                    11,
                    5,
                    286,
                    0
                ],
                "title": "Boosting Deductive Reasoning with Step Signals In RLHF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Deductive Reasoning with Step Signals In RLHF"
                },
                "summary": "Logical reasoning is a crucial task for Large Language Models (LLMs),\nenabling them to tackle complex problems. Among reasoning tasks, multi-step\nreasoning poses a particular challenge. Grounded in the theory of formal logic,\nwe have developed an automated method, Multi-step Deduction (MuseD), for\ndeductive reasoning data. MuseD has allowed us to create training and testing\ndatasets for multi-step reasoning. Our generation method enables control over\nthe complexity of the generated instructions, facilitating training and\nevaluation of models across different difficulty levels. Through RLHF training,\nour training data has demonstrated significant improvements in logical\ncapabilities for both in-domain of out-of-domain reasoning tasks. Additionally,\nwe have conducted tests to assess the multi-step reasoning abilities of various\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical reasoning is a crucial task for Large Language Models (LLMs),\nenabling them to tackle complex problems. Among reasoning tasks, multi-step\nreasoning poses a particular challenge. Grounded in the theory of formal logic,\nwe have developed an automated method, Multi-step Deduction (MuseD), for\ndeductive reasoning data. MuseD has allowed us to create training and testing\ndatasets for multi-step reasoning. Our generation method enables control over\nthe complexity of the generated instructions, facilitating training and\nevaluation of models across different difficulty levels. Through RLHF training,\nour training data has demonstrated significant improvements in logical\ncapabilities for both in-domain of out-of-domain reasoning tasks. Additionally,\nwe have conducted tests to assess the multi-step reasoning abilities of various\nmodels."
                },
                "authors": [
                    {
                        "name": "Jialian Li"
                    },
                    {
                        "name": "Yipin Zhang"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Yuzi Yan"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Dong Yan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yan"
                },
                "author": "Dong Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04168v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04168v3",
                "updated": "2024-10-24T09:36:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    36,
                    26,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-05T14:14:08Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    14,
                    14,
                    8,
                    5,
                    279,
                    0
                ],
                "title": "R-ACP: Real-Time Adaptive Collaborative Perception Leveraging Robust\n  Task-Oriented Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-ACP: Real-Time Adaptive Collaborative Perception Leveraging Robust\n  Task-Oriented Communications"
                },
                "summary": "Collaborative perception enhances sensing in multi-robot and vehicular\nnetworks by fusing information from multiple agents, improving perception\naccuracy and sensing range. However, mobility and non-rigid sensor mounts\nintroduce extrinsic calibration errors, necessitating online calibration,\nfurther complicated by limited overlap in sensing regions. Moreover,\nmaintaining fresh information is crucial for timely and accurate sensing. To\naddress calibration errors and ensure timely and accurate perception, we\npropose a robust task-oriented communication strategy to optimize online\nself-calibration and efficient feature sharing for Real-time Adaptive\nCollaborative Perception (R-ACP). Specifically, we first formulate an Age of\nPerceived Targets (AoPT) minimization problem to capture data timeliness of\nmulti-view streaming. Then, in the calibration phase, we introduce a\nchannel-aware self-calibration technique based on re-identification (Re-ID),\nwhich adaptively compresses key features according to channel capacities,\neffectively addressing calibration issues via spatial and temporal cross-camera\ncorrelations. In the streaming phase, we tackle the trade-off between bandwidth\nand inference accuracy by leveraging an Information Bottleneck (IB) based\nencoding method to adjust video compression rates based on task relevance,\nthereby reducing communication overhead and latency. Finally, we design a\npriority-aware network to filter corrupted features to mitigate performance\ndegradation from packet corruption. Extensive studies demonstrate that our\nframework outperforms five baselines, improving multiple object detection\naccuracy (MODA) by 25.49% and reducing communication costs by 51.36% under\nseverely poor channel conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative perception enhances sensing in multi-robot and vehicular\nnetworks by fusing information from multiple agents, improving perception\naccuracy and sensing range. However, mobility and non-rigid sensor mounts\nintroduce extrinsic calibration errors, necessitating online calibration,\nfurther complicated by limited overlap in sensing regions. Moreover,\nmaintaining fresh information is crucial for timely and accurate sensing. To\naddress calibration errors and ensure timely and accurate perception, we\npropose a robust task-oriented communication strategy to optimize online\nself-calibration and efficient feature sharing for Real-time Adaptive\nCollaborative Perception (R-ACP). Specifically, we first formulate an Age of\nPerceived Targets (AoPT) minimization problem to capture data timeliness of\nmulti-view streaming. Then, in the calibration phase, we introduce a\nchannel-aware self-calibration technique based on re-identification (Re-ID),\nwhich adaptively compresses key features according to channel capacities,\neffectively addressing calibration issues via spatial and temporal cross-camera\ncorrelations. In the streaming phase, we tackle the trade-off between bandwidth\nand inference accuracy by leveraging an Information Bottleneck (IB) based\nencoding method to adjust video compression rates based on task relevance,\nthereby reducing communication overhead and latency. Finally, we design a\npriority-aware network to filter corrupted features to mitigate performance\ndegradation from packet corruption. Extensive studies demonstrate that our\nframework outperforms five baselines, improving multiple object detection\naccuracy (MODA) by 25.49% and reducing communication costs by 51.36% under\nseverely poor channel conditions."
                },
                "authors": [
                    {
                        "name": "Zhengru Fang"
                    },
                    {
                        "name": "Jingjing Wang"
                    },
                    {
                        "name": "Yanan Ma"
                    },
                    {
                        "name": "Yihang Tao"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Yuguang Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yuguang Fang"
                },
                "author": "Yuguang Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04168v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04168v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18585v1",
                "updated": "2024-10-24T09:36:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    36,
                    13,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T09:36:13Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    36,
                    13,
                    3,
                    298,
                    0
                ],
                "title": "Aligning CodeLLMs with Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning CodeLLMs with Direct Preference Optimization"
                },
                "summary": "The last year has witnessed the rapid progress of large language models\n(LLMs) across diverse domains. Among them, CodeLLMs have garnered particular\nattention because they can not only assist in completing various programming\ntasks but also represent the decision-making and logical reasoning capabilities\nof LLMs. However, current CodeLLMs mainly focus on pre-training and supervised\nfine-tuning scenarios, leaving the alignment stage, which is important for\npost-training LLMs, under-explored. This work first identifies that the\ncommonly used PPO algorithm may be suboptimal for the alignment of CodeLLM\nbecause the involved reward rules are routinely coarse-grained and potentially\nflawed. We then advocate addressing this using the DPO algorithm. Based on only\npreference data pairs, DPO can render the model rank data automatically, giving\nrise to a fine-grained rewarding pattern more robust than human intervention.\nWe also contribute a pipeline for collecting preference pairs for DPO on\nCodeLLMs. Studies show that our method significantly improves the performance\nof existing CodeLLMs on benchmarks such as MBPP and HumanEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The last year has witnessed the rapid progress of large language models\n(LLMs) across diverse domains. Among them, CodeLLMs have garnered particular\nattention because they can not only assist in completing various programming\ntasks but also represent the decision-making and logical reasoning capabilities\nof LLMs. However, current CodeLLMs mainly focus on pre-training and supervised\nfine-tuning scenarios, leaving the alignment stage, which is important for\npost-training LLMs, under-explored. This work first identifies that the\ncommonly used PPO algorithm may be suboptimal for the alignment of CodeLLM\nbecause the involved reward rules are routinely coarse-grained and potentially\nflawed. We then advocate addressing this using the DPO algorithm. Based on only\npreference data pairs, DPO can render the model rank data automatically, giving\nrise to a fine-grained rewarding pattern more robust than human intervention.\nWe also contribute a pipeline for collecting preference pairs for DPO on\nCodeLLMs. Studies show that our method significantly improves the performance\nof existing CodeLLMs on benchmarks such as MBPP and HumanEval."
                },
                "authors": [
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18582v1",
                "updated": "2024-10-24T09:35:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    35,
                    21,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T09:35:21Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    35,
                    21,
                    3,
                    298,
                    0
                ],
                "title": "LLM-Aided Efficient Hardware Design Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Aided Efficient Hardware Design Automation"
                },
                "summary": "With the rapidly increasing complexity of modern chips, hardware engineers\nare required to invest more effort in tasks such as circuit design,\nverification, and physical implementation. These workflows often involve\ncontinuous modifications, which are labor-intensive and prone to errors.\nTherefore, there is an increasing need for more efficient and cost-effective\nElectronic Design Automation (EDA) solutions to accelerate new hardware\ndevelopment. Recently, large language models (LLMs) have made significant\nadvancements in contextual understanding, logical reasoning, and response\ngeneration. Since hardware designs and intermediate scripts can be expressed in\ntext format, it is reasonable to explore whether integrating LLMs into EDA\ncould simplify and fully automate the entire workflow. Accordingly, this paper\ndiscusses such possibilities in several aspects, covering hardware description\nlanguage (HDL) generation, code debugging, design verification, and physical\nimplementation. Two case studies, along with their future outlook, are\nintroduced to highlight the capabilities of LLMs in code repair and testbench\ngeneration. Finally, future directions and challenges are highlighted to\nfurther explore the potential of LLMs in shaping the next-generation EDA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing complexity of modern chips, hardware engineers\nare required to invest more effort in tasks such as circuit design,\nverification, and physical implementation. These workflows often involve\ncontinuous modifications, which are labor-intensive and prone to errors.\nTherefore, there is an increasing need for more efficient and cost-effective\nElectronic Design Automation (EDA) solutions to accelerate new hardware\ndevelopment. Recently, large language models (LLMs) have made significant\nadvancements in contextual understanding, logical reasoning, and response\ngeneration. Since hardware designs and intermediate scripts can be expressed in\ntext format, it is reasonable to explore whether integrating LLMs into EDA\ncould simplify and fully automate the entire workflow. Accordingly, this paper\ndiscusses such possibilities in several aspects, covering hardware description\nlanguage (HDL) generation, code debugging, design verification, and physical\nimplementation. Two case studies, along with their future outlook, are\nintroduced to highlight the capabilities of LLMs in code repair and testbench\ngeneration. Finally, future directions and challenges are highlighted to\nfurther explore the potential of LLMs in shaping the next-generation EDA"
                },
                "authors": [
                    {
                        "name": "Kangwei Xu"
                    },
                    {
                        "name": "Ruidi Qiu"
                    },
                    {
                        "name": "Zhuorui Zhao"
                    },
                    {
                        "name": "Grace Li Zhang"
                    },
                    {
                        "name": "Ulf Schlichtmann"
                    },
                    {
                        "name": "Bing Li"
                    }
                ],
                "author_detail": {
                    "name": "Bing Li"
                },
                "author": "Bing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18574v1",
                "updated": "2024-10-24T09:29:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    29,
                    18,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T09:29:18Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    29,
                    18,
                    3,
                    298,
                    0
                ],
                "title": "SIKeD: Self-guided Iterative Knowledge Distillation for mathematical\n  reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIKeD: Self-guided Iterative Knowledge Distillation for mathematical\n  reasoning"
                },
                "summary": "Large Language Models (LLMs) can transfer their reasoning skills to smaller\nmodels by teaching them to generate the intermediate reasoning process required\nto solve multistep reasoning tasks. While LLMs can accurately solve reasoning\ntasks through a variety of strategies, even without fine-tuning, smaller models\nare not expressive enough to fit the LLMs distribution on all strategies when\ndistilled and tend to prioritize one strategy over the others. This reliance on\none strategy poses a challenge for smaller models when attempting to solve\nreasoning tasks that may be difficult with their preferred strategy. To address\nthis, we propose a distillation method SIKeD (Self-guided Iterative Knowledge\nDistillation for mathematical reasoning), where the LLM teaches the smaller\nmodel to approach a task using different strategies and the smaller model uses\nits self-generated on-policy outputs to choose the most suitable strategy for\nthe given task. The training continues in a self-guided iterative manner, where\nfor each training iteration, a decision is made on how to combine the LLM data\nwith the self-generated outputs. Unlike traditional distillation methods, SIKeD\nallows the smaller model to learn which strategy is suitable for a given task\nwhile continuously learning to solve a task using different strategies. Our\nexperiments on various mathematical reasoning datasets show that SIKeD\nsignificantly outperforms traditional distillation techniques across smaller\nmodels of different sizes. Our code is available at:\nhttps://github.com/kumar-shridhar/SIKeD",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can transfer their reasoning skills to smaller\nmodels by teaching them to generate the intermediate reasoning process required\nto solve multistep reasoning tasks. While LLMs can accurately solve reasoning\ntasks through a variety of strategies, even without fine-tuning, smaller models\nare not expressive enough to fit the LLMs distribution on all strategies when\ndistilled and tend to prioritize one strategy over the others. This reliance on\none strategy poses a challenge for smaller models when attempting to solve\nreasoning tasks that may be difficult with their preferred strategy. To address\nthis, we propose a distillation method SIKeD (Self-guided Iterative Knowledge\nDistillation for mathematical reasoning), where the LLM teaches the smaller\nmodel to approach a task using different strategies and the smaller model uses\nits self-generated on-policy outputs to choose the most suitable strategy for\nthe given task. The training continues in a self-guided iterative manner, where\nfor each training iteration, a decision is made on how to combine the LLM data\nwith the self-generated outputs. Unlike traditional distillation methods, SIKeD\nallows the smaller model to learn which strategy is suitable for a given task\nwhile continuously learning to solve a task using different strategies. Our\nexperiments on various mathematical reasoning datasets show that SIKeD\nsignificantly outperforms traditional distillation techniques across smaller\nmodels of different sizes. Our code is available at:\nhttps://github.com/kumar-shridhar/SIKeD"
                },
                "authors": [
                    {
                        "name": "Shivam Adarsh"
                    },
                    {
                        "name": "Kumar Shridhar"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18572v1",
                "updated": "2024-10-24T09:25:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    25,
                    37,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T09:25:37Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    25,
                    37,
                    3,
                    298,
                    0
                ],
                "title": "Taipan: Efficient and Expressive State Space Language Models with\n  Selective Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taipan: Efficient and Expressive State Space Language Models with\n  Selective Attention"
                },
                "summary": "Efficient long-context language modeling remains a significant challenge in\nNatural Language Processing (NLP). While Transformers dominate language tasks,\nthey struggle with long sequences due to quadratic computational complexity in\ntraining and linearly scaling memory costs during inference. Recent State Space\nModels (SSMs) such as Mamba offer alternatives with constant memory usage, but\nthey underperform in tasks requiring extensive in-context retrieval. We\nintroduce Taipan, a novel hybrid architecture that combines Mamba-2 with\nSelective Attention Layers (SALs). These SALs identify tokens requiring\nlong-range interactions, remove less important features, and then augment their\nrepresentations using the attention module. This approach balances Mamba's\nefficiency with Transformer-like performance in memory-intensive tasks. By\nconstraining the attention budget, Taipan extends accurate predictions to\ncontext lengths of up to 1 million tokens while preserving computational\nefficiency. Our experiments demonstrate Taipan's superior performance across\nvarious scales and tasks, offering a promising solution for efficient\nlong-context language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-context language modeling remains a significant challenge in\nNatural Language Processing (NLP). While Transformers dominate language tasks,\nthey struggle with long sequences due to quadratic computational complexity in\ntraining and linearly scaling memory costs during inference. Recent State Space\nModels (SSMs) such as Mamba offer alternatives with constant memory usage, but\nthey underperform in tasks requiring extensive in-context retrieval. We\nintroduce Taipan, a novel hybrid architecture that combines Mamba-2 with\nSelective Attention Layers (SALs). These SALs identify tokens requiring\nlong-range interactions, remove less important features, and then augment their\nrepresentations using the attention module. This approach balances Mamba's\nefficiency with Transformer-like performance in memory-intensive tasks. By\nconstraining the attention budget, Taipan extends accurate predictions to\ncontext lengths of up to 1 million tokens while preserving computational\nefficiency. Our experiments demonstrate Taipan's superior performance across\nvarious scales and tasks, offering a promising solution for efficient\nlong-context language modeling."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Huy Huu Nguyen"
                    },
                    {
                        "name": "Thang M. Pham"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20318v2",
                "updated": "2024-10-24T09:21:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    21,
                    38,
                    3,
                    298,
                    0
                ],
                "published": "2024-05-30T17:55:28Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    17,
                    55,
                    28,
                    3,
                    151,
                    0
                ],
                "title": "Analyzing Human Questioning Behavior and Causal Curiosity through\n  Natural Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Human Questioning Behavior and Causal Curiosity through\n  Natural Queries"
                },
                "summary": "The recent development of Large Language Models (LLMs) has changed our role\nin interacting with them. Instead of primarily testing these models with\nquestions we already know the answers to, we now use them to explore questions\nwhere the answers are unknown to us. This shift, which hasn't been fully\naddressed in existing datasets, highlights the growing need to understand\nnaturally occurring human questions - that are more complex, open-ended, and\nreflective of real-world needs. To this end, we present NatQuest, a collection\nof 13,500 naturally occurring questions from three diverse sources:\nhuman-to-search-engine queries, human-to-human interactions, and human-to-LLM\nconversations. Our comprehensive collection enables a rich understanding of\nhuman curiosity across various domains and contexts. Our analysis reveals a\nsignificant presence of causal questions (up to 42%) within the dataset, for\nwhich we develop an iterative prompt improvement framework to identify all\ncausal queries, and examine their unique linguistic properties, cognitive\ncomplexity, and source distribution. We also lay the groundwork to explore LLM\nperformance on these questions and provide six efficient classification models\nto identify causal questions at scale for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of Large Language Models (LLMs) has changed our role\nin interacting with them. Instead of primarily testing these models with\nquestions we already know the answers to, we now use them to explore questions\nwhere the answers are unknown to us. This shift, which hasn't been fully\naddressed in existing datasets, highlights the growing need to understand\nnaturally occurring human questions - that are more complex, open-ended, and\nreflective of real-world needs. To this end, we present NatQuest, a collection\nof 13,500 naturally occurring questions from three diverse sources:\nhuman-to-search-engine queries, human-to-human interactions, and human-to-LLM\nconversations. Our comprehensive collection enables a rich understanding of\nhuman curiosity across various domains and contexts. Our analysis reveals a\nsignificant presence of causal questions (up to 42%) within the dataset, for\nwhich we develop an iterative prompt improvement framework to identify all\ncausal queries, and examine their unique linguistic properties, cognitive\ncomplexity, and source distribution. We also lay the groundwork to explore LLM\nperformance on these questions and provide six efficient classification models\nto identify causal questions at scale for future work."
                },
                "authors": [
                    {
                        "name": "Roberto Ceraolo"
                    },
                    {
                        "name": "Dmitrii Kharlapenko"
                    },
                    {
                        "name": "Ahmad Khan"
                    },
                    {
                        "name": "Amélie Reymond"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Zhijing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Jin"
                },
                "author": "Zhijing Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11954v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11954v2",
                "updated": "2024-10-24T09:17:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    17,
                    3,
                    3,
                    298,
                    0
                ],
                "published": "2024-03-18T16:51:56Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    16,
                    51,
                    56,
                    0,
                    78,
                    0
                ],
                "title": "Robust Estimation and Inference for Categorical Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Estimation and Inference for Categorical Data"
                },
                "summary": "While there is a rich literature on robust methodologies for contamination in\ncontinuously distributed data, contamination in categorical data is largely\noverlooked. This is regrettable because many datasets are categorical and\noftentimes suffer from contamination. Examples include inattentive responding\nand bot responses in questionnaires or zero-inflated count data. We propose a\nnovel class of contamination-robust estimators of models for categorical data,\ncoined $C$-estimators (``$C$\" for categorical). We show that the countable and\npossibly finite sample space of categorical data results in non-standard\ntheoretical properties. Notably, in contrast to classic robustness theory,\n$C$-estimators can be simultaneously robust \\textit{and} fully efficient at the\npostulated model. In addition, a certain particularly robust specification\nfails to be asymptotically Gaussian at the postulated model, but is\nasymptotically Gaussian in the presence of contamination. We furthermore\npropose a diagnostic test to identify categorical outliers and demonstrate the\nenhanced robustness of $C$-estimators in a simulation study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there is a rich literature on robust methodologies for contamination in\ncontinuously distributed data, contamination in categorical data is largely\noverlooked. This is regrettable because many datasets are categorical and\noftentimes suffer from contamination. Examples include inattentive responding\nand bot responses in questionnaires or zero-inflated count data. We propose a\nnovel class of contamination-robust estimators of models for categorical data,\ncoined $C$-estimators (``$C$\" for categorical). We show that the countable and\npossibly finite sample space of categorical data results in non-standard\ntheoretical properties. Notably, in contrast to classic robustness theory,\n$C$-estimators can be simultaneously robust \\textit{and} fully efficient at the\npostulated model. In addition, a certain particularly robust specification\nfails to be asymptotically Gaussian at the postulated model, but is\nasymptotically Gaussian in the presence of contamination. We furthermore\npropose a diagnostic test to identify categorical outliers and demonstrate the\nenhanced robustness of $C$-estimators in a simulation study."
                },
                "authors": [
                    {
                        "name": "Max Welz"
                    }
                ],
                "author_detail": {
                    "name": "Max Welz"
                },
                "author": "Max Welz",
                "arxiv_comment": "45 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.11954v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11954v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18565v1",
                "updated": "2024-10-24T09:16:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    16,
                    9,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T09:16:09Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    16,
                    9,
                    3,
                    298,
                    0
                ],
                "title": "Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and\n  Evaluation"
                },
                "summary": "We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for\nPolish language processing. Trained on curated Polish corpora, this model\naddresses key challenges in language model development through innovative\ntechniques. These include Weighted Instruction Cross-Entropy Loss, which\nbalances the learning of different instruction types, and Adaptive Learning\nRate, which dynamically adjusts the learning rate based on training progress.\nTo evaluate performance, we created the Open PL LLM Leaderboard and Polish\nMT-Bench, novel frameworks assessing various NLP tasks and conversational\nabilities. Bielik 7B v0.1 demonstrates significant improvements, achieving a 9\npercentage point increase in average score compared to Mistral-7B-v0.1 on the\nRAG Reader task. It also excels in the Polish MT-Bench, particularly in\nReasoning (6.15/10) and Role-playing (7.83/10) categories. This model\nrepresents a substantial advancement in Polish language AI, offering a powerful\ntool for diverse linguistic applications and setting new benchmarks in the\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for\nPolish language processing. Trained on curated Polish corpora, this model\naddresses key challenges in language model development through innovative\ntechniques. These include Weighted Instruction Cross-Entropy Loss, which\nbalances the learning of different instruction types, and Adaptive Learning\nRate, which dynamically adjusts the learning rate based on training progress.\nTo evaluate performance, we created the Open PL LLM Leaderboard and Polish\nMT-Bench, novel frameworks assessing various NLP tasks and conversational\nabilities. Bielik 7B v0.1 demonstrates significant improvements, achieving a 9\npercentage point increase in average score compared to Mistral-7B-v0.1 on the\nRAG Reader task. It also excels in the Polish MT-Bench, particularly in\nReasoning (6.15/10) and Role-playing (7.83/10) categories. This model\nrepresents a substantial advancement in Polish language AI, offering a powerful\ntool for diverse linguistic applications and setting new benchmarks in the\nfield."
                },
                "authors": [
                    {
                        "name": "Krzysztof Ociepa"
                    },
                    {
                        "name": "Łukasz Flis"
                    },
                    {
                        "name": "Krzysztof Wróbel"
                    },
                    {
                        "name": "Adrian Gwoździej"
                    },
                    {
                        "name": "Remigiusz Kinas"
                    }
                ],
                "author_detail": {
                    "name": "Remigiusz Kinas"
                },
                "author": "Remigiusz Kinas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2201.12577v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2201.12577v6",
                "updated": "2024-10-24T09:05:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    5,
                    36,
                    3,
                    298,
                    0
                ],
                "published": "2022-01-29T12:40:19Z",
                "published_parsed": [
                    2022,
                    1,
                    29,
                    12,
                    40,
                    19,
                    5,
                    29,
                    0
                ],
                "title": "Volley Revolver: A Novel Matrix-Encoding Method for Privacy-Preserving\n  Neural Networks (Inference)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Volley Revolver: A Novel Matrix-Encoding Method for Privacy-Preserving\n  Neural Networks (Inference)"
                },
                "summary": "In this work, we present a novel matrix-encoding method that is particularly\nconvenient for neural networks to make predictions in a privacy-preserving\nmanner using homomorphic encryption. Based on this encoding method, we\nimplement a convolutional neural network for handwritten image classification\nover encryption. For two matrices $A$ and $B$ to perform homomorphic\nmultiplication, the main idea behind it, in a simple version, is to encrypt\nmatrix $A$ and the transpose of matrix $B$ into two ciphertexts respectively.\nWith additional operations, the homomorphic matrix multiplication can be\ncalculated over encrypted matrices efficiently. For the convolution operation,\nwe in advance span each convolution kernel to a matrix space of the same size\nas the input image so as to generate several ciphertexts, each of which is\nlater used together with the ciphertext encrypting input images for calculating\nsome of the final convolution results. We accumulate all these intermediate\nresults and thus complete the convolution operation.\n  In a public cloud with 40 vCPUs, our convolutional neural network\nimplementation on the MNIST testing dataset takes $\\sim$ 287 seconds to compute\nten likelihoods of 32 encrypted images of size $28 \\times 28$ simultaneously.\nThe data owner only needs to upload one ciphertext ($\\sim 19.8$ MB) encrypting\nthese 32 images to the public cloud.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present a novel matrix-encoding method that is particularly\nconvenient for neural networks to make predictions in a privacy-preserving\nmanner using homomorphic encryption. Based on this encoding method, we\nimplement a convolutional neural network for handwritten image classification\nover encryption. For two matrices $A$ and $B$ to perform homomorphic\nmultiplication, the main idea behind it, in a simple version, is to encrypt\nmatrix $A$ and the transpose of matrix $B$ into two ciphertexts respectively.\nWith additional operations, the homomorphic matrix multiplication can be\ncalculated over encrypted matrices efficiently. For the convolution operation,\nwe in advance span each convolution kernel to a matrix space of the same size\nas the input image so as to generate several ciphertexts, each of which is\nlater used together with the ciphertext encrypting input images for calculating\nsome of the final convolution results. We accumulate all these intermediate\nresults and thus complete the convolution operation.\n  In a public cloud with 40 vCPUs, our convolutional neural network\nimplementation on the MNIST testing dataset takes $\\sim$ 287 seconds to compute\nten likelihoods of 32 encrypted images of size $28 \\times 28$ simultaneously.\nThe data owner only needs to upload one ciphertext ($\\sim 19.8$ MB) encrypting\nthese 32 images to the public cloud."
                },
                "authors": [
                    {
                        "name": "John Chiang"
                    }
                ],
                "author_detail": {
                    "name": "John Chiang"
                },
                "author": "John Chiang",
                "arxiv_comment": "The encoding method we proposed in this work, $\\texttt{Volley\n  Revolver}$, is particularly tailored for privacy-preserving neural networks.\n  There is a great chance that it can be used to assist the private neural\n  networks training, in which case for the backpropagation algorithm of the\n  fully-connected layer the first matrix $A$ is revolved while the second\n  matrix $B$ is settled to be still",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2201.12577v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2201.12577v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.18975v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18975v1",
                "updated": "2024-10-24T17:59:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    59,
                    31,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:59:31Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    59,
                    31,
                    3,
                    298,
                    0
                ],
                "title": "Unbounded: A Generative Infinite Game of Character Life Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unbounded: A Generative Infinite Game of Character Life Simulation"
                },
                "summary": "We introduce the concept of a generative infinite game, a video game that\ntranscends the traditional boundaries of finite, hard-coded systems by using\ngenerative models. Inspired by James P. Carse's distinction between finite and\ninfinite games, we leverage recent advances in generative AI to create\nUnbounded: a game of character life simulation that is fully encapsulated in\ngenerative models. Specifically, Unbounded draws inspiration from sandbox life\nsimulations and allows you to interact with your autonomous virtual character\nin a virtual world by feeding, playing with and guiding it - with open-ended\nmechanics generated by an LLM, some of which can be emergent. In order to\ndevelop Unbounded, we propose technical innovations in both the LLM and visual\ngeneration domains. Specifically, we present: (1) a specialized, distilled\nlarge language model (LLM) that dynamically generates game mechanics,\nnarratives, and character interactions in real-time, and (2) a new dynamic\nregional image prompt Adapter (IP-Adapter) for vision models that ensures\nconsistent yet flexible visual generation of a character across multiple\nenvironments. We evaluate our system through both qualitative and quantitative\nanalysis, showing significant improvements in character life simulation, user\ninstruction following, narrative coherence, and visual consistency for both\ncharacters and the environments compared to traditional related approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the concept of a generative infinite game, a video game that\ntranscends the traditional boundaries of finite, hard-coded systems by using\ngenerative models. Inspired by James P. Carse's distinction between finite and\ninfinite games, we leverage recent advances in generative AI to create\nUnbounded: a game of character life simulation that is fully encapsulated in\ngenerative models. Specifically, Unbounded draws inspiration from sandbox life\nsimulations and allows you to interact with your autonomous virtual character\nin a virtual world by feeding, playing with and guiding it - with open-ended\nmechanics generated by an LLM, some of which can be emergent. In order to\ndevelop Unbounded, we propose technical innovations in both the LLM and visual\ngeneration domains. Specifically, we present: (1) a specialized, distilled\nlarge language model (LLM) that dynamically generates game mechanics,\nnarratives, and character interactions in real-time, and (2) a new dynamic\nregional image prompt Adapter (IP-Adapter) for vision models that ensures\nconsistent yet flexible visual generation of a character across multiple\nenvironments. We evaluate our system through both qualitative and quantitative\nanalysis, showing significant improvements in character life simulation, user\ninstruction following, narrative coherence, and visual consistency for both\ncharacters and the environments compared to traditional related approaches."
                },
                "authors": [
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Yuanzhen Li"
                    },
                    {
                        "name": "Neal Wadhwa"
                    },
                    {
                        "name": "Yael Pritch"
                    },
                    {
                        "name": "David E. Jacobs"
                    },
                    {
                        "name": "Michael Rubinstein"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Nataniel Ruiz"
                    }
                ],
                "author_detail": {
                    "name": "Nataniel Ruiz"
                },
                "author": "Nataniel Ruiz",
                "arxiv_comment": "18 pages; Project page: https://generative-infinite-game.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18975v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14322v2",
                "updated": "2024-10-24T17:59:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    59,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2023-05-23T17:53:38Z",
                "published_parsed": [
                    2023,
                    5,
                    23,
                    17,
                    53,
                    38,
                    1,
                    143,
                    0
                ],
                "title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RET-LLM: Towards a General Read-Write Memory for Large Language Models"
                },
                "summary": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP) through their extensive parameters and comprehensive\ndata utilization. However, existing LLMs lack a dedicated memory unit, limiting\ntheir ability to explicitly store and retrieve knowledge for various tasks. In\nthis paper, we propose RET-LLM a novel framework that equips LLMs with a\ngeneral write-read memory unit, allowing them to extract, store, and recall\nknowledge from the text as needed for task performance. Inspired by Davidsonian\nsemantics theory, we extract and save knowledge in the form of triplets. The\nmemory unit is designed to be scalable, aggregatable, updatable, and\ninterpretable. Through qualitative evaluations, we demonstrate the superiority\nof our proposed framework over baseline approaches in question answering tasks.\nMoreover, our framework exhibits robust performance in handling temporal-based\nquestion answering tasks, showcasing its ability to effectively manage\ntime-dependent information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP) through their extensive parameters and comprehensive\ndata utilization. However, existing LLMs lack a dedicated memory unit, limiting\ntheir ability to explicitly store and retrieve knowledge for various tasks. In\nthis paper, we propose RET-LLM a novel framework that equips LLMs with a\ngeneral write-read memory unit, allowing them to extract, store, and recall\nknowledge from the text as needed for task performance. Inspired by Davidsonian\nsemantics theory, we extract and save knowledge in the form of triplets. The\nmemory unit is designed to be scalable, aggregatable, updatable, and\ninterpretable. Through qualitative evaluations, we demonstrate the superiority\nof our proposed framework over baseline approaches in question answering tasks.\nMoreover, our framework exhibits robust performance in handling temporal-based\nquestion answering tasks, showcasing its ability to effectively manage\ntime-dependent information."
                },
                "authors": [
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Ayyoob Imani"
                    },
                    {
                        "name": "Mohsen Fayyaz"
                    },
                    {
                        "name": "Hinrich Schütze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schütze"
                },
                "author": "Hinrich Schütze",
                "arxiv_comment": "NOTE: This concept paper outlines an initial methodology, now evolved\n  and thoroughly evaluated in the MemLLM paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10870v2",
                "updated": "2024-10-24T17:58:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    58,
                    52,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-08T13:41:08Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    13,
                    41,
                    8,
                    1,
                    282,
                    0
                ],
                "title": "PortLLM: Personalizing Evolving Large Language Models with Training-Free\n  and Portable Model Patches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PortLLM: Personalizing Evolving Large Language Models with Training-Free\n  and Portable Model Patches"
                },
                "summary": "As large language models (LLMs) increasingly shape the AI landscape,\nfine-tuning pretrained models has become more popular than in the pre-LLM era\nfor achieving optimal performance in domain-specific tasks. However, pretrained\nLLMs such as ChatGPT are periodically evolved, i.e., model parameters are\nfrequently updated), making it challenging for downstream users with limited\nresources to keep up with fine-tuning the newest LLMs for their domain\napplication. Even though fine-tuning costs have nowadays been reduced thanks to\nthe innovations of parameter-efficient fine-tuning such as LoRA, not all\ndownstream users have adequate computing for frequent personalization.\nMoreover, access to fine-tuning datasets, particularly in sensitive domains\nsuch as healthcare, could be time-restrictive, making it crucial to retain the\nknowledge encoded in earlier fine-tuned rounds for future adaptation. In this\npaper, we present PortLLM, a training-free framework that (i) creates an\ninitial lightweight model update patch to capture domain-specific knowledge,\nand (ii) allows a subsequent seamless plugging for the continual\npersonalization of evolved LLM at minimal cost. Our extensive experiments cover\nseven representative datasets, from easier question-answering tasks {BoolQ,\nSST2} to harder reasoning tasks {WinoGrande, GSM8K}, and models including\n{Mistral-7B, Llama2, Llama3.1, and Gemma2}, validating the portability of our\ndesigned model patches and showcasing the effectiveness of our proposed\nframework. For instance, PortLLM achieves comparable performance to LoRA\nfine-tuning with reductions of up to 12.2x in GPU memory usage. Finally, we\nprovide theoretical justifications to understand the portability of our model\nupdate patches, which offers new insights into the theoretical dimension of\nLLMs' personalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly shape the AI landscape,\nfine-tuning pretrained models has become more popular than in the pre-LLM era\nfor achieving optimal performance in domain-specific tasks. However, pretrained\nLLMs such as ChatGPT are periodically evolved, i.e., model parameters are\nfrequently updated), making it challenging for downstream users with limited\nresources to keep up with fine-tuning the newest LLMs for their domain\napplication. Even though fine-tuning costs have nowadays been reduced thanks to\nthe innovations of parameter-efficient fine-tuning such as LoRA, not all\ndownstream users have adequate computing for frequent personalization.\nMoreover, access to fine-tuning datasets, particularly in sensitive domains\nsuch as healthcare, could be time-restrictive, making it crucial to retain the\nknowledge encoded in earlier fine-tuned rounds for future adaptation. In this\npaper, we present PortLLM, a training-free framework that (i) creates an\ninitial lightweight model update patch to capture domain-specific knowledge,\nand (ii) allows a subsequent seamless plugging for the continual\npersonalization of evolved LLM at minimal cost. Our extensive experiments cover\nseven representative datasets, from easier question-answering tasks {BoolQ,\nSST2} to harder reasoning tasks {WinoGrande, GSM8K}, and models including\n{Mistral-7B, Llama2, Llama3.1, and Gemma2}, validating the portability of our\ndesigned model patches and showcasing the effectiveness of our proposed\nframework. For instance, PortLLM achieves comparable performance to LoRA\nfine-tuning with reductions of up to 12.2x in GPU memory usage. Finally, we\nprovide theoretical justifications to understand the portability of our model\nupdate patches, which offers new insights into the theoretical dimension of\nLLMs' personalization."
                },
                "authors": [
                    {
                        "name": "Rana Muhammad Shahroz Khan"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Sukwon Yun"
                    },
                    {
                        "name": "Zhenyu Wang"
                    },
                    {
                        "name": "Shahriar Nirjon"
                    },
                    {
                        "name": "Chau-Wai Wong"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18966v1",
                "updated": "2024-10-24T17:58:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    58,
                    22,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:58:22Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    58,
                    22,
                    3,
                    298,
                    0
                ],
                "title": "Does Data Contamination Detection Work (Well) for LLMs? A Survey and\n  Evaluation on Detection Assumptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Data Contamination Detection Work (Well) for LLMs? A Survey and\n  Evaluation on Detection Assumptions"
                },
                "summary": "Large language models (LLMs) have demonstrated great performance across\nvarious benchmarks, showing potential as general-purpose task solvers. However,\nas LLMs are typically trained on vast amounts of data, a significant concern in\ntheir evaluation is data contamination, where overlap between training data and\nevaluation datasets inflates performance assessments. While multiple approaches\nhave been developed to identify data contamination, these approaches rely on\nspecific assumptions that may not hold universally across different settings.\nTo bridge this gap, we systematically review 47 papers on data contamination\ndetection, categorize the underlying assumptions, and assess whether they have\nbeen rigorously validated. We identify and analyze eight categories of\nassumptions and test three of them as case studies. Our analysis reveals that\nwhen classifying instances used for pretraining LLMs, detection approaches\nbased on these three assumptions perform close to random guessing, suggesting\nthat current LLMs learn data distributions rather than memorizing individual\ninstances. Overall, this work underscores the importance of approaches clearly\nstating their underlying assumptions and testing their validity across various\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated great performance across\nvarious benchmarks, showing potential as general-purpose task solvers. However,\nas LLMs are typically trained on vast amounts of data, a significant concern in\ntheir evaluation is data contamination, where overlap between training data and\nevaluation datasets inflates performance assessments. While multiple approaches\nhave been developed to identify data contamination, these approaches rely on\nspecific assumptions that may not hold universally across different settings.\nTo bridge this gap, we systematically review 47 papers on data contamination\ndetection, categorize the underlying assumptions, and assess whether they have\nbeen rigorously validated. We identify and analyze eight categories of\nassumptions and test three of them as case studies. Our analysis reveals that\nwhen classifying instances used for pretraining LLMs, detection approaches\nbased on these three assumptions perform close to random guessing, suggesting\nthat current LLMs learn data distributions rather than memorizing individual\ninstances. Overall, this work underscores the importance of approaches clearly\nstating their underlying assumptions and testing their validity across various\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yujuan Fu"
                    },
                    {
                        "name": "Ozlem Uzuner"
                    },
                    {
                        "name": "Meliha Yetisgen"
                    },
                    {
                        "name": "Fei Xia"
                    }
                ],
                "author_detail": {
                    "name": "Fei Xia"
                },
                "author": "Fei Xia",
                "arxiv_comment": "2 tables and 1 figures in the main text. This is a preprint, under\n  review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18963v1",
                "updated": "2024-10-24T17:58:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    58,
                    8,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:58:08Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    58,
                    8,
                    3,
                    298,
                    0
                ],
                "title": "OSCAR: Operating System Control via State-Aware Reasoning and\n  Re-Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OSCAR: Operating System Control via State-Aware Reasoning and\n  Re-Planning"
                },
                "summary": "Large language models (LLMs) and large multimodal models (LMMs) have shown\ngreat potential in automating complex tasks like web browsing and gaming.\nHowever, their ability to generalize across diverse applications remains\nlimited, hindering broader utility. To address this challenge, we present\nOSCAR: Operating System Control via state-Aware reasoning and Re-planning.\nOSCAR is a generalist agent designed to autonomously navigate and interact with\nvarious desktop and mobile applications through standardized controls, such as\nmouse and keyboard inputs, while processing screen images to fulfill user\ncommands. OSCAR translates human instructions into executable Python code,\nenabling precise control over graphical user interfaces (GUIs). To enhance\nstability and adaptability, OSCAR operates as a state machine, equipped with\nerror-handling mechanisms and dynamic task re-planning, allowing it to\nefficiently adjust to real-time feedback and exceptions. We demonstrate OSCAR's\neffectiveness through extensive experiments on diverse benchmarks across\ndesktop and mobile platforms, where it transforms complex workflows into simple\nnatural language commands, significantly boosting user productivity. Our code\nwill be open-source upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) and large multimodal models (LMMs) have shown\ngreat potential in automating complex tasks like web browsing and gaming.\nHowever, their ability to generalize across diverse applications remains\nlimited, hindering broader utility. To address this challenge, we present\nOSCAR: Operating System Control via state-Aware reasoning and Re-planning.\nOSCAR is a generalist agent designed to autonomously navigate and interact with\nvarious desktop and mobile applications through standardized controls, such as\nmouse and keyboard inputs, while processing screen images to fulfill user\ncommands. OSCAR translates human instructions into executable Python code,\nenabling precise control over graphical user interfaces (GUIs). To enhance\nstability and adaptability, OSCAR operates as a state machine, equipped with\nerror-handling mechanisms and dynamic task re-planning, allowing it to\nefficiently adjust to real-time feedback and exceptions. We demonstrate OSCAR's\neffectiveness through extensive experiments on diverse benchmarks across\ndesktop and mobile platforms, where it transforms complex workflows into simple\nnatural language commands, significantly boosting user productivity. Our code\nwill be open-source upon publication."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Wang"
                    },
                    {
                        "name": "Bang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bang Liu"
                },
                "author": "Bang Liu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11029v2",
                "updated": "2024-10-24T17:56:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    56,
                    14,
                    3,
                    298,
                    0
                ],
                "published": "2024-08-20T17:30:48Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    30,
                    48,
                    1,
                    233,
                    0
                ],
                "title": "Scaling Law with Learning Rate Annealing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Law with Learning Rate Annealing"
                },
                "summary": "We find that the cross-entropy loss curves of neural language models\nempirically adhere to a scaling law with learning rate (LR) annealing over\ntraining steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where\n$L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR\ncurve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are\nconstant parameters. This formulation takes into account two factors: (1)\npower-law scaling over data size, and (2) the additional loss reduction during\nLR annealing. Therefore, this formulation can describe the full loss curve at\neach step, rather than the single loss point at the end of training. Applying\nthe scaling law with LR annealing and fitting only one or two training curves,\nwe can accurately predict the loss at any given step across any learning rate\nscheduler (LRS). This approach significantly reduces computational cost in\nformulating scaling laws while providing more accuracy and expressiveness for\ntraining dynamics. Extensive experiments demonstrate that our findings hold\nacross a range of hyper-parameters and model architectures, and our equation\ncan extend to scaling effect of model sizes. Moreover, our formulation provides\naccurate theoretical verification and explanation for empirical results\nobserved in numerous previous studies, particularly those focusing on LR\nschedule and annealing. We believe that this work is promising to enhance the\nunderstanding of LLM training dynamics while greatly democratizing scaling\nlaws, and it can guide researchers in refining training strategies (e.g.\ncritical LRS) for further LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We find that the cross-entropy loss curves of neural language models\nempirically adhere to a scaling law with learning rate (LR) annealing over\ntraining steps: $$L(s) = L_0 + A\\cdot S_1^{-\\alpha} - C\\cdot S_2,$$ where\n$L(s)$ is the validation loss at step $s$, $S_1$ is the area under the LR\ncurve, $S_2$ is the LR annealing area, and $L_0$, $A$, $C$, $\\alpha$ are\nconstant parameters. This formulation takes into account two factors: (1)\npower-law scaling over data size, and (2) the additional loss reduction during\nLR annealing. Therefore, this formulation can describe the full loss curve at\neach step, rather than the single loss point at the end of training. Applying\nthe scaling law with LR annealing and fitting only one or two training curves,\nwe can accurately predict the loss at any given step across any learning rate\nscheduler (LRS). This approach significantly reduces computational cost in\nformulating scaling laws while providing more accuracy and expressiveness for\ntraining dynamics. Extensive experiments demonstrate that our findings hold\nacross a range of hyper-parameters and model architectures, and our equation\ncan extend to scaling effect of model sizes. Moreover, our formulation provides\naccurate theoretical verification and explanation for empirical results\nobserved in numerous previous studies, particularly those focusing on LR\nschedule and annealing. We believe that this work is promising to enhance the\nunderstanding of LLM training dynamics while greatly democratizing scaling\nlaws, and it can guide researchers in refining training strategies (e.g.\ncritical LRS) for further LLMs."
                },
                "authors": [
                    {
                        "name": "Howe Tissue"
                    },
                    {
                        "name": "Venus Wang"
                    },
                    {
                        "name": "Lu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Wang"
                },
                "author": "Lu Wang",
                "arxiv_comment": "Add more experiments to consolidate our scaling laws. 29 pages, 29\n  figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18959v1",
                "updated": "2024-10-24T17:56:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    56,
                    8,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:56:08Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    56,
                    8,
                    3,
                    298,
                    0
                ],
                "title": "Context is Key: A Benchmark for Forecasting with Essential Textual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context is Key: A Benchmark for Forecasting with Essential Textual\n  Information"
                },
                "summary": "Forecasting is a critical task in decision making across various domains.\nWhile numerical data provides a foundation, it often lacks crucial context\nnecessary for accurate predictions. Human forecasters frequently rely on\nadditional information, such as background knowledge or constraints, which can\nbe efficiently communicated through natural language. However, the ability of\nexisting forecasting models to effectively integrate this textual information\nremains an open question. To address this, we introduce \"Context is Key\" (CiK),\na time series forecasting benchmark that pairs numerical data with diverse\ntypes of carefully crafted textual context, requiring models to integrate both\nmodalities. We evaluate a range of approaches, including statistical models,\ntime series foundation models, and LLM-based forecasters, and propose a simple\nyet effective LLM prompting method that outperforms all other tested methods on\nour benchmark. Our experiments highlight the importance of incorporating\ncontextual information, demonstrate surprising performance when using LLM-based\nforecasting models, and also reveal some of their critical shortcomings. By\npresenting this benchmark, we aim to advance multimodal forecasting, promoting\nmodels that are both accurate and accessible to decision-makers with varied\ntechnical expertise. The benchmark can be visualized at\nhttps://servicenow.github.io/context-is-key-forecasting/v0/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting is a critical task in decision making across various domains.\nWhile numerical data provides a foundation, it often lacks crucial context\nnecessary for accurate predictions. Human forecasters frequently rely on\nadditional information, such as background knowledge or constraints, which can\nbe efficiently communicated through natural language. However, the ability of\nexisting forecasting models to effectively integrate this textual information\nremains an open question. To address this, we introduce \"Context is Key\" (CiK),\na time series forecasting benchmark that pairs numerical data with diverse\ntypes of carefully crafted textual context, requiring models to integrate both\nmodalities. We evaluate a range of approaches, including statistical models,\ntime series foundation models, and LLM-based forecasters, and propose a simple\nyet effective LLM prompting method that outperforms all other tested methods on\nour benchmark. Our experiments highlight the importance of incorporating\ncontextual information, demonstrate surprising performance when using LLM-based\nforecasting models, and also reveal some of their critical shortcomings. By\npresenting this benchmark, we aim to advance multimodal forecasting, promoting\nmodels that are both accurate and accessible to decision-makers with varied\ntechnical expertise. The benchmark can be visualized at\nhttps://servicenow.github.io/context-is-key-forecasting/v0/ ."
                },
                "authors": [
                    {
                        "name": "Andrew Robert Williams"
                    },
                    {
                        "name": "Arjun Ashok"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "Jithendaraa Subramanian"
                    },
                    {
                        "name": "Roland Riachi"
                    },
                    {
                        "name": "James Requeima"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    },
                    {
                        "name": "Irina Rish"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Alexandre Drouin"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Drouin"
                },
                "author": "Alexandre Drouin",
                "arxiv_comment": "Preprint; under review. First two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18957v1",
                "updated": "2024-10-24T17:55:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    55,
                    3,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:55:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    55,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "Bridge-Coder: Unlocking LLMs' Potential to Overcome Language Gaps in\n  Low-Resource Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridge-Coder: Unlocking LLMs' Potential to Overcome Language Gaps in\n  Low-Resource Code"
                },
                "summary": "Large Language Models (LLMs) demonstrate strong proficiency in generating\ncode for high-resource programming languages (HRPLs) like Python but struggle\nsignificantly with low-resource programming languages (LRPLs) such as Racket or\nD. This performance gap deepens the digital divide, preventing developers using\nLRPLs from benefiting equally from LLM advancements and reinforcing disparities\nin innovation within underrepresented programming communities. While generating\nadditional training data for LRPLs is promising, it faces two key challenges:\nmanual annotation is labor-intensive and costly, and LLM-generated LRPL code is\noften of subpar quality. The underlying cause of this issue is the gap between\nnatural language to programming language gap (NL-PL Gap), which is especially\npronounced in LRPLs due to limited aligned data. In this work, we introduce a\nnovel approach called Bridge-Coder, which leverages LLMs' intrinsic\ncapabilities to enhance the performance on LRPLs. Our method consists of two\nkey stages. Bridge Generation, where we create high-quality dataset by\nutilizing LLMs' general knowledge understanding, proficiency in HRPLs, and\nin-context learning abilities. Then, we apply the Bridged Alignment, which\nprogressively improves the alignment between NL instructions and LRPLs.\nExperimental results across multiple LRPLs show that Bridge-Coder significantly\nenhances model performance, demonstrating the effectiveness and generalization\nof our approach. Furthermore, we offer a detailed analysis of the key\ncomponents of our method, providing valuable insights for future work aimed at\naddressing the challenges associated with LRPLs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate strong proficiency in generating\ncode for high-resource programming languages (HRPLs) like Python but struggle\nsignificantly with low-resource programming languages (LRPLs) such as Racket or\nD. This performance gap deepens the digital divide, preventing developers using\nLRPLs from benefiting equally from LLM advancements and reinforcing disparities\nin innovation within underrepresented programming communities. While generating\nadditional training data for LRPLs is promising, it faces two key challenges:\nmanual annotation is labor-intensive and costly, and LLM-generated LRPL code is\noften of subpar quality. The underlying cause of this issue is the gap between\nnatural language to programming language gap (NL-PL Gap), which is especially\npronounced in LRPLs due to limited aligned data. In this work, we introduce a\nnovel approach called Bridge-Coder, which leverages LLMs' intrinsic\ncapabilities to enhance the performance on LRPLs. Our method consists of two\nkey stages. Bridge Generation, where we create high-quality dataset by\nutilizing LLMs' general knowledge understanding, proficiency in HRPLs, and\nin-context learning abilities. Then, we apply the Bridged Alignment, which\nprogressively improves the alignment between NL instructions and LRPLs.\nExperimental results across multiple LRPLs show that Bridge-Coder significantly\nenhances model performance, demonstrating the effectiveness and generalization\nof our approach. Furthermore, we offer a detailed analysis of the key\ncomponents of our method, providing valuable insights for future work aimed at\naddressing the challenges associated with LRPLs."
                },
                "authors": [
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Jianshu Zhang"
                    },
                    {
                        "name": "Yuanzhe Li"
                    },
                    {
                        "name": "Renjie Pi"
                    },
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Runtao Liu"
                    },
                    {
                        "name": "Ziqiang Zheng"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19380v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19380v4",
                "updated": "2024-10-24T17:54:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    54,
                    37,
                    3,
                    298,
                    0
                ],
                "published": "2024-06-27T17:55:31Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    17,
                    55,
                    31,
                    3,
                    179,
                    0
                ],
                "title": "TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning\n  Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning\n  Benchmarks"
                },
                "summary": "Advances in machine learning research drive progress in real-world\napplications. To ensure this progress, it is important to understand the\npotential pitfalls on the way from a novel method's success on academic\nbenchmarks to its practical deployment. In this work, we analyze existing\ntabular benchmarks and find two common characteristics of tabular data in\ntypical industrial applications that are underrepresented in the datasets\nusually used for evaluation in the literature. First, in real-world deployment\nscenarios, distribution of data often changes over time. To account for this\ndistribution drift, time-based train/test splits should be used in evaluation.\nHowever, popular tabular datasets often lack timestamp metadata to enable such\nevaluation. Second, a considerable portion of datasets in production settings\nstem from extensive data acquisition and feature engineering pipelines. This\ncan have an impact on the absolute and relative number of predictive,\nuninformative, and correlated features compared to academic datasets. In this\nwork, we aim to understand how recent research advances in tabular deep\nlearning transfer to these underrepresented conditions. To this end, we\nintroduce TabReD -- a collection of eight industry-grade tabular datasets. We\nreassess a large number of tabular ML models and techniques on TabReD. We\ndemonstrate that evaluation on time-based data splits leads to different\nmethods ranking, compared to evaluation on random splits, which are common in\ncurrent benchmarks. Furthermore, simple MLP-like architectures and GBDT show\nthe best results on the TabReD datasets, while other methods are less effective\nin the new setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in machine learning research drive progress in real-world\napplications. To ensure this progress, it is important to understand the\npotential pitfalls on the way from a novel method's success on academic\nbenchmarks to its practical deployment. In this work, we analyze existing\ntabular benchmarks and find two common characteristics of tabular data in\ntypical industrial applications that are underrepresented in the datasets\nusually used for evaluation in the literature. First, in real-world deployment\nscenarios, distribution of data often changes over time. To account for this\ndistribution drift, time-based train/test splits should be used in evaluation.\nHowever, popular tabular datasets often lack timestamp metadata to enable such\nevaluation. Second, a considerable portion of datasets in production settings\nstem from extensive data acquisition and feature engineering pipelines. This\ncan have an impact on the absolute and relative number of predictive,\nuninformative, and correlated features compared to academic datasets. In this\nwork, we aim to understand how recent research advances in tabular deep\nlearning transfer to these underrepresented conditions. To this end, we\nintroduce TabReD -- a collection of eight industry-grade tabular datasets. We\nreassess a large number of tabular ML models and techniques on TabReD. We\ndemonstrate that evaluation on time-based data splits leads to different\nmethods ranking, compared to evaluation on random splits, which are common in\ncurrent benchmarks. Furthermore, simple MLP-like architectures and GBDT show\nthe best results on the TabReD datasets, while other methods are less effective\nin the new setting."
                },
                "authors": [
                    {
                        "name": "Ivan Rubachev"
                    },
                    {
                        "name": "Nikolay Kartashev"
                    },
                    {
                        "name": "Yury Gorishniy"
                    },
                    {
                        "name": "Artem Babenko"
                    }
                ],
                "author_detail": {
                    "name": "Artem Babenko"
                },
                "author": "Artem Babenko",
                "arxiv_comment": "Code: https://github.com/yandex-research/tabred (V4: preprint update;\n  substantial text changes)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19380v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19380v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18955v1",
                "updated": "2024-10-24T17:53:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    53,
                    53,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:53:53Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    53,
                    53,
                    3,
                    298,
                    0
                ],
                "title": "BioMistral-NLU: Towards More Generalizable Medical Language\n  Understanding through Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BioMistral-NLU: Towards More Generalizable Medical Language\n  Understanding through Instruction Tuning"
                },
                "summary": "Large language models (LLMs) such as ChatGPT are fine-tuned on large and\ndiverse instruction-following corpora, and can generalize to new tasks.\nHowever, those instruction-tuned LLMs often perform poorly in specialized\nmedical natural language understanding (NLU) tasks that require domain\nknowledge, granular text comprehension, and structured data extraction. To\nbridge the gap, we: (1) propose a unified prompting format for 7 important NLU\ntasks, % through span extraction and multi-choice question-answering (QA), (2)\ncurate an instruction-tuning dataset, MNLU-Instruct, utilizing diverse existing\nopen-source medical NLU corpora, and (3) develop BioMistral-NLU, a\ngeneralizable medical NLU model, through fine-tuning BioMistral on\nMNLU-Instruct. We evaluate BioMistral-NLU in a zero-shot setting, across 6\nimportant NLU tasks, from two widely adopted medical NLU benchmarks: Biomedical\nLanguage Understanding Evaluation (BLUE) and Biomedical Language Understanding\nand Reasoning Benchmark (BLURB). Our experiments show that our BioMistral-NLU\noutperforms the original BioMistral, as well as the proprietary LLMs - ChatGPT\nand GPT-4. Our dataset-agnostic prompting strategy and instruction tuning step\nover diverse NLU tasks enhance LLMs' generalizability across diverse medical\nNLU tasks. Our ablation experiments show that instruction-tuning on a wider\nvariety of tasks, even when the total number of training instances remains\nconstant, enhances downstream zero-shot generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) such as ChatGPT are fine-tuned on large and\ndiverse instruction-following corpora, and can generalize to new tasks.\nHowever, those instruction-tuned LLMs often perform poorly in specialized\nmedical natural language understanding (NLU) tasks that require domain\nknowledge, granular text comprehension, and structured data extraction. To\nbridge the gap, we: (1) propose a unified prompting format for 7 important NLU\ntasks, % through span extraction and multi-choice question-answering (QA), (2)\ncurate an instruction-tuning dataset, MNLU-Instruct, utilizing diverse existing\nopen-source medical NLU corpora, and (3) develop BioMistral-NLU, a\ngeneralizable medical NLU model, through fine-tuning BioMistral on\nMNLU-Instruct. We evaluate BioMistral-NLU in a zero-shot setting, across 6\nimportant NLU tasks, from two widely adopted medical NLU benchmarks: Biomedical\nLanguage Understanding Evaluation (BLUE) and Biomedical Language Understanding\nand Reasoning Benchmark (BLURB). Our experiments show that our BioMistral-NLU\noutperforms the original BioMistral, as well as the proprietary LLMs - ChatGPT\nand GPT-4. Our dataset-agnostic prompting strategy and instruction tuning step\nover diverse NLU tasks enhance LLMs' generalizability across diverse medical\nNLU tasks. Our ablation experiments show that instruction-tuning on a wider\nvariety of tasks, even when the total number of training instances remains\nconstant, enhances downstream zero-shot generalization."
                },
                "authors": [
                    {
                        "name": "Yujuan Velvin Fu"
                    },
                    {
                        "name": "Giridhar Kaushik Ramachandran"
                    },
                    {
                        "name": "Namu Park"
                    },
                    {
                        "name": "Kevin Lybarger"
                    },
                    {
                        "name": "Fei Xia"
                    },
                    {
                        "name": "Ozlem Uzuner"
                    },
                    {
                        "name": "Meliha Yetisgen"
                    }
                ],
                "author_detail": {
                    "name": "Meliha Yetisgen"
                },
                "author": "Meliha Yetisgen",
                "arxiv_comment": "3 figures an 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18952v1",
                "updated": "2024-10-24T17:52:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    52,
                    31,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:52:31Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    52,
                    31,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Vocabulary Pruning in Early-Exit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Vocabulary Pruning in Early-Exit LLMs"
                },
                "summary": "Increasing the size of large language models (LLMs) has been shown to lead to\nbetter performance. However, this comes at the cost of slower and more\nexpensive inference. Early-exiting is a promising approach for improving the\nefficiency of LLM inference by enabling next token prediction at intermediate\nlayers. Yet, the large vocabulary size in modern LLMs makes the confidence\nestimation required for exit decisions computationally expensive, diminishing\nthe efficiency gains. To address this, we propose dynamically pruning the\nvocabulary at test time for each token. Specifically, the vocabulary is pruned\nat one of the initial layers, and the smaller vocabulary is then used\nthroughout the rest of the forward pass. Our experiments demonstrate that such\npost-hoc dynamic vocabulary pruning improves the efficiency of confidence\nestimation in early-exit LLMs while maintaining competitive performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasing the size of large language models (LLMs) has been shown to lead to\nbetter performance. However, this comes at the cost of slower and more\nexpensive inference. Early-exiting is a promising approach for improving the\nefficiency of LLM inference by enabling next token prediction at intermediate\nlayers. Yet, the large vocabulary size in modern LLMs makes the confidence\nestimation required for exit decisions computationally expensive, diminishing\nthe efficiency gains. To address this, we propose dynamically pruning the\nvocabulary at test time for each token. Specifically, the vocabulary is pruned\nat one of the initial layers, and the smaller vocabulary is then used\nthroughout the rest of the forward pass. Our experiments demonstrate that such\npost-hoc dynamic vocabulary pruning improves the efficiency of confidence\nestimation in early-exit LLMs while maintaining competitive performance."
                },
                "authors": [
                    {
                        "name": "Jort Vincenti"
                    },
                    {
                        "name": "Karim Abdel Sadek"
                    },
                    {
                        "name": "Joan Velja"
                    },
                    {
                        "name": "Matteo Nulli"
                    },
                    {
                        "name": "Metod Jazbec"
                    }
                ],
                "author_detail": {
                    "name": "Metod Jazbec"
                },
                "author": "Metod Jazbec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10508v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10508v4",
                "updated": "2024-10-24T17:43:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    43,
                    28,
                    3,
                    298,
                    0
                ],
                "published": "2024-04-16T12:27:54Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    12,
                    27,
                    54,
                    1,
                    107,
                    0
                ],
                "title": "White Men Lead, Black Women Help? Benchmarking Language Agency Social\n  Biases in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "White Men Lead, Black Women Help? Benchmarking Language Agency Social\n  Biases in LLMs"
                },
                "summary": "Social biases can manifest in language agency. While several studies\napproached agency-related bias in human-written language, very limited research\nhas investigated such biases in Large Language Model (LLM)-generated content.\nIn addition, previous works often rely on string-matching techniques to\nidentify agentic and communal words within texts, which fall short of\naccurately classifying language agency. We introduce the novel Language Agency\nBias Evaluation (LABE) benchmark, which comprehensively evaluates biases in\nLLMs by analyzing agency levels attributed to different demographic groups in\nmodel generations. LABE leverages 5,400 template-based prompts, an accurate\nagency classifier, and corresponding bias metrics to test for gender, racial,\nand intersectional language agency biases in LLMs on 3 text generation tasks:\nbiographies, professor reviews, and reference letters. We also contribute the\nLanguage Agency Classification (LAC) dataset, consisting of 3,724 agentic and\ncommunal sentences. Using LABE, we unveil language agency social biases in 3\nrecent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) LLM generations\ntend to demonstrate greater gender bias than human-written texts; (2) Models\ndemonstrate remarkably higher levels of intersectional bias than the other bias\naspects. Those who are at the intersection of gender and racial minority\ngroups--such as Black females--are consistently described by texts with lower\nlevels of agency, aligning with real-world social inequalities; (3) Among the 3\nLLMs investigated, Llama3 demonstrates the greatest overall bias; (4) Not only\ndoes prompt-based mitigation fail to resolve language agency bias in LLMs, but\nit frequently leads to the exacerbation of biases in generated texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social biases can manifest in language agency. While several studies\napproached agency-related bias in human-written language, very limited research\nhas investigated such biases in Large Language Model (LLM)-generated content.\nIn addition, previous works often rely on string-matching techniques to\nidentify agentic and communal words within texts, which fall short of\naccurately classifying language agency. We introduce the novel Language Agency\nBias Evaluation (LABE) benchmark, which comprehensively evaluates biases in\nLLMs by analyzing agency levels attributed to different demographic groups in\nmodel generations. LABE leverages 5,400 template-based prompts, an accurate\nagency classifier, and corresponding bias metrics to test for gender, racial,\nand intersectional language agency biases in LLMs on 3 text generation tasks:\nbiographies, professor reviews, and reference letters. We also contribute the\nLanguage Agency Classification (LAC) dataset, consisting of 3,724 agentic and\ncommunal sentences. Using LABE, we unveil language agency social biases in 3\nrecent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) LLM generations\ntend to demonstrate greater gender bias than human-written texts; (2) Models\ndemonstrate remarkably higher levels of intersectional bias than the other bias\naspects. Those who are at the intersection of gender and racial minority\ngroups--such as Black females--are consistently described by texts with lower\nlevels of agency, aligning with real-world social inequalities; (3) Among the 3\nLLMs investigated, Llama3 demonstrates the greatest overall bias; (4) Not only\ndoes prompt-based mitigation fail to resolve language agency bias in LLMs, but\nit frequently leads to the exacerbation of biases in generated texts."
                },
                "authors": [
                    {
                        "name": "Yixin Wan"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10508v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10508v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18927v1",
                "updated": "2024-10-24T17:14:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    14,
                    40,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:14:40Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    14,
                    40,
                    3,
                    298,
                    0
                ],
                "title": "SafeBench: A Safety Evaluation Framework for Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeBench: A Safety Evaluation Framework for Multimodal Large Language\n  Models"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are showing strong safety concerns\n(e.g., generating harmful outputs for users), which motivates the development\nof safety evaluation benchmarks. However, we observe that existing safety\nbenchmarks for MLLMs show limitations in query quality and evaluation\nreliability limiting the detection of model safety implications as MLLMs\ncontinue to evolve. In this paper, we propose \\toolns, a comprehensive\nframework designed for conducting safety evaluations of MLLMs. Our framework\nconsists of a comprehensive harmful query dataset and an automated evaluation\nprotocol that aims to address the above limitations, respectively. We first\ndesign an automatic safety dataset generation pipeline, where we employ a set\nof LLM judges to recognize and categorize the risk scenarios that are most\nharmful and diverse for MLLMs; based on the taxonomy, we further ask these\njudges to generate high-quality harmful queries accordingly resulting in 23\nrisk scenarios with 2,300 multi-modal harmful query pairs. During safety\nevaluation, we draw inspiration from the jury system in judicial proceedings\nand pioneer the jury deliberation evaluation protocol that adopts collaborative\nLLMs to evaluate whether target models exhibit specific harmful behaviors,\nproviding a reliable and unbiased assessment of content security risks. In\naddition, our benchmark can also be extended to the audio modality showing high\nscalability and potential. Based on our framework, we conducted large-scale\nexperiments on 15 widely-used open-source MLLMs and 6 commercial MLLMs (e.g.,\nGPT-4o, Gemini), where we revealed widespread safety issues in existing MLLMs\nand instantiated several insights on MLLM safety performance such as image\nquality and parameter size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are showing strong safety concerns\n(e.g., generating harmful outputs for users), which motivates the development\nof safety evaluation benchmarks. However, we observe that existing safety\nbenchmarks for MLLMs show limitations in query quality and evaluation\nreliability limiting the detection of model safety implications as MLLMs\ncontinue to evolve. In this paper, we propose \\toolns, a comprehensive\nframework designed for conducting safety evaluations of MLLMs. Our framework\nconsists of a comprehensive harmful query dataset and an automated evaluation\nprotocol that aims to address the above limitations, respectively. We first\ndesign an automatic safety dataset generation pipeline, where we employ a set\nof LLM judges to recognize and categorize the risk scenarios that are most\nharmful and diverse for MLLMs; based on the taxonomy, we further ask these\njudges to generate high-quality harmful queries accordingly resulting in 23\nrisk scenarios with 2,300 multi-modal harmful query pairs. During safety\nevaluation, we draw inspiration from the jury system in judicial proceedings\nand pioneer the jury deliberation evaluation protocol that adopts collaborative\nLLMs to evaluate whether target models exhibit specific harmful behaviors,\nproviding a reliable and unbiased assessment of content security risks. In\naddition, our benchmark can also be extended to the audio modality showing high\nscalability and potential. Based on our framework, we conducted large-scale\nexperiments on 15 widely-used open-source MLLMs and 6 commercial MLLMs (e.g.,\nGPT-4o, Gemini), where we revealed widespread safety issues in existing MLLMs\nand instantiated several insights on MLLM safety performance such as image\nquality and parameter size."
                },
                "authors": [
                    {
                        "name": "Zonghao Ying"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Wenbo Zhou"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17078v2",
                "updated": "2024-10-24T17:12:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    12,
                    19,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-22T14:56:50Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    14,
                    56,
                    50,
                    1,
                    296,
                    0
                ],
                "title": "FlowTracer: A Tool for Uncovering Network Path Usage Imbalance in AI\n  Training Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowTracer: A Tool for Uncovering Network Path Usage Imbalance in AI\n  Training Clusters"
                },
                "summary": "The increasing complexity of AI workloads, especially distributed Large\nLanguage Model (LLM) training, places significant strain on the networking\ninfrastructure of parallel data centers and supercomputing systems. While\nEqual-Cost Multi- Path (ECMP) routing distributes traffic over parallel paths,\nhash collisions often lead to imbalanced network resource utilization and\nperformance bottlenecks. This paper presents FlowTracer, a tool designed to\nanalyze network path utilization and evaluate different routing strategies.\nFlowTracer aids in debugging network inefficiencies by providing detailed\nvisibility into traffic distribution and helping to identify the root causes of\nperformance degradation, such as issues caused by hash collisions. By offering\nflow-level insights, FlowTracer enables system operators to optimize routing,\nreduce congestion, and improve the performance of distributed AI workloads. We\nuse a RoCEv2-enabled cluster with a leaf-spine network and 16 400-Gbps nodes to\ndemonstrate how FlowTracer can be used to compare the flow imbalances of ECMP\nrouting against a statically configured network. The example showcases a 30%\nreduction in imbalance, as measured by a new metric we introduce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of AI workloads, especially distributed Large\nLanguage Model (LLM) training, places significant strain on the networking\ninfrastructure of parallel data centers and supercomputing systems. While\nEqual-Cost Multi- Path (ECMP) routing distributes traffic over parallel paths,\nhash collisions often lead to imbalanced network resource utilization and\nperformance bottlenecks. This paper presents FlowTracer, a tool designed to\nanalyze network path utilization and evaluate different routing strategies.\nFlowTracer aids in debugging network inefficiencies by providing detailed\nvisibility into traffic distribution and helping to identify the root causes of\nperformance degradation, such as issues caused by hash collisions. By offering\nflow-level insights, FlowTracer enables system operators to optimize routing,\nreduce congestion, and improve the performance of distributed AI workloads. We\nuse a RoCEv2-enabled cluster with a leaf-spine network and 16 400-Gbps nodes to\ndemonstrate how FlowTracer can be used to compare the flow imbalances of ECMP\nrouting against a statically configured network. The example showcases a 30%\nreduction in imbalance, as measured by a new metric we introduce."
                },
                "authors": [
                    {
                        "name": "Hasibul Jamil"
                    },
                    {
                        "name": "Abdul Alim"
                    },
                    {
                        "name": "Laurent Schares"
                    },
                    {
                        "name": "Pavlos Maniotis"
                    },
                    {
                        "name": "Liran Schour"
                    },
                    {
                        "name": "Ali Sydney"
                    },
                    {
                        "name": "Abdullah Kayi"
                    },
                    {
                        "name": "Tevfik Kosar"
                    },
                    {
                        "name": "Bengi Karacali"
                    }
                ],
                "author_detail": {
                    "name": "Bengi Karacali"
                },
                "author": "Bengi Karacali",
                "arxiv_comment": "Submitted for peer reviewing in IEEE ICC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18923v1",
                "updated": "2024-10-24T17:11:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    11,
                    52,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:11:52Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    11,
                    52,
                    3,
                    298,
                    0
                ],
                "title": "SegLLM: Multi-round Reasoning Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SegLLM: Multi-round Reasoning Segmentation"
                },
                "summary": "We present SegLLM, a novel multi-round interactive reasoning segmentation\nmodel that enhances LLM-based segmentation by exploiting conversational memory\nof both visual and textual outputs. By leveraging a mask-aware multimodal LLM,\nSegLLM re-integrates previous segmentation results into its input stream,\nenabling it to reason about complex user intentions and segment objects in\nrelation to previously identified entities, including positional,\ninteractional, and hierarchical relationships, across multiple interactions.\nThis capability allows SegLLM to respond to visual and text queries in a\nchat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLM\noutperforms existing methods in multi-round interactive reasoning segmentation\nby over 20%. Additionally, we observed that training on multi-round reasoning\nsegmentation data enhances performance on standard single-round referring\nsegmentation and localization tasks, resulting in a 5.5% increase in cIoU for\nreferring expression segmentation and a 4.5% improvement in Acc@0.5 for\nreferring expression localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SegLLM, a novel multi-round interactive reasoning segmentation\nmodel that enhances LLM-based segmentation by exploiting conversational memory\nof both visual and textual outputs. By leveraging a mask-aware multimodal LLM,\nSegLLM re-integrates previous segmentation results into its input stream,\nenabling it to reason about complex user intentions and segment objects in\nrelation to previously identified entities, including positional,\ninteractional, and hierarchical relationships, across multiple interactions.\nThis capability allows SegLLM to respond to visual and text queries in a\nchat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLM\noutperforms existing methods in multi-round interactive reasoning segmentation\nby over 20%. Additionally, we observed that training on multi-round reasoning\nsegmentation data enhances performance on standard single-round referring\nsegmentation and localization tasks, resulting in a 5.5% increase in cIoU for\nreferring expression segmentation and a 4.5% improvement in Acc@0.5 for\nreferring expression localization."
                },
                "authors": [
                    {
                        "name": "XuDong Wang"
                    },
                    {
                        "name": "Shaolun Zhang"
                    },
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Kehan Li"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Trevor Darrell"
                    }
                ],
                "author_detail": {
                    "name": "Trevor Darrell"
                },
                "author": "Trevor Darrell",
                "arxiv_comment": "22 pages, 10 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18921v1",
                "updated": "2024-10-24T17:10:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    10,
                    39,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T17:10:39Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    10,
                    39,
                    3,
                    298,
                    0
                ],
                "title": "From Blind Solvers to Logical Thinkers: Benchmarking LLMs' Logical\n  Integrity on Faulty Mathematical Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Blind Solvers to Logical Thinkers: Benchmarking LLMs' Logical\n  Integrity on Faulty Mathematical Problems"
                },
                "summary": "Consider the math problem: \"Lily received 3 cookies from her best friend\nyesterday and ate 5 for breakfast. Today, her friend gave her 3 more cookies.\nHow many cookies does Lily have now?\" Many large language models (LLMs) in\nprevious research approach this problem by calculating the answer \"1\" using the\nequation \"3 - 5 + 3.\" However, from a human perspective, we recognize the\ninherent flaw in this problem: Lily cannot eat 5 cookies if she initially only\nhad 3. This discrepancy prompts a key question: Are current LLMs merely Blind\nSolver that apply mathematical operations without deeper reasoning, or can they\nfunction as Logical Thinker capable of identifying logical inconsistencies?\n  To explore this question, we propose a benchmark dataset, FaultyMath, which\nincludes faulty math problems of rich diversity: i) multiple mathematical\ncategories, e.g., algebra, geometry, number theory, etc., ii) varying levels of\ndifficulty, and iii) different origins of faultiness -- ranging from violations\nof common sense and ambiguous statements to mathematical contradictions and\nmore. We evaluate a broad spectrum of LLMs, including open-source,\nclosed-source, and math-specialized models, using FaultyMath across three\ndimensions: (i) How accurately can the models detect faulty math problems\nwithout being explicitly prompted to do so? (ii) When provided with hints --\neither correct or misleading -- about the validity of the problems, to what\nextent do LLMs adapt to become reliable Logical Thinker? (iii) How trustworthy\nare the explanations generated by LLMs when they recognize a math problem as\nflawed? Through extensive experimentation and detailed analysis, our results\ndemonstrate that existing LLMs largely function as Blind Solver and fall short\nof the reasoning capabilities required to perform as Logical Thinker.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consider the math problem: \"Lily received 3 cookies from her best friend\nyesterday and ate 5 for breakfast. Today, her friend gave her 3 more cookies.\nHow many cookies does Lily have now?\" Many large language models (LLMs) in\nprevious research approach this problem by calculating the answer \"1\" using the\nequation \"3 - 5 + 3.\" However, from a human perspective, we recognize the\ninherent flaw in this problem: Lily cannot eat 5 cookies if she initially only\nhad 3. This discrepancy prompts a key question: Are current LLMs merely Blind\nSolver that apply mathematical operations without deeper reasoning, or can they\nfunction as Logical Thinker capable of identifying logical inconsistencies?\n  To explore this question, we propose a benchmark dataset, FaultyMath, which\nincludes faulty math problems of rich diversity: i) multiple mathematical\ncategories, e.g., algebra, geometry, number theory, etc., ii) varying levels of\ndifficulty, and iii) different origins of faultiness -- ranging from violations\nof common sense and ambiguous statements to mathematical contradictions and\nmore. We evaluate a broad spectrum of LLMs, including open-source,\nclosed-source, and math-specialized models, using FaultyMath across three\ndimensions: (i) How accurately can the models detect faulty math problems\nwithout being explicitly prompted to do so? (ii) When provided with hints --\neither correct or misleading -- about the validity of the problems, to what\nextent do LLMs adapt to become reliable Logical Thinker? (iii) How trustworthy\nare the explanations generated by LLMs when they recognize a math problem as\nflawed? Through extensive experimentation and detailed analysis, our results\ndemonstrate that existing LLMs largely function as Blind Solver and fall short\nof the reasoning capabilities required to perform as Logical Thinker."
                },
                "authors": [
                    {
                        "name": "A M Muntasir Rahman"
                    },
                    {
                        "name": "Junyi Ye"
                    },
                    {
                        "name": "Wei Yao"
                    },
                    {
                        "name": "Wenpeng Yin"
                    },
                    {
                        "name": "Guiling Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guiling Wang"
                },
                "author": "Guiling Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01444v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01444v6",
                "updated": "2024-10-24T17:03:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    17,
                    3,
                    43,
                    3,
                    298,
                    0
                ],
                "published": "2023-03-02T18:03:03Z",
                "published_parsed": [
                    2023,
                    3,
                    2,
                    18,
                    3,
                    3,
                    3,
                    61,
                    0
                ],
                "title": "A Neuro-Symbolic AI Approach to Personal Health Risk Assessment and\n  Immune Age Characterisation using Common Blood Markers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Neuro-Symbolic AI Approach to Personal Health Risk Assessment and\n  Immune Age Characterisation using Common Blood Markers"
                },
                "summary": "We introduce a simulated digital model that learns a person's baseline blood\nhealth over time. Using an adaptive learning algorithm, the model provides a\nrisk assessment score that compares an individual's chronological age with an\nestimation of biological age based on common immune-relevant markers used in\ncurrent clinical practice. We demonstrate its efficacy on real and synthetic\ndata from medically relevant cases, extreme cases, and empirical blood cell\ncount data from 100K data records in the Centers for Disease Control and\nPrevention's National Health and Nutrition Examination Survey (CDC NHANES) that\nspans 13 years. We find that the score is informative in distinguishing healthy\nindividuals from those with diseases, both self-reported and as manifested via\nabnormal blood test results, providing an entry-level score for patient\ntriaging. The risk assessment score is not a machine learning black-box\napproach but can interact with ML and DL approaches to help guide, control the\nattention given to specific features, and assign proper explainable weight to\nan otherwise transparent adaptive learning algorithm. This approach may allow\nfast and scalable deployment to personalised, sensitive, and predictive\nderivative indexes within digital medicine, without the need for a new test,\nassay, or prospective sampling, unlike other biological ageing-related scores\nand methods. It demonstrates the potential of clinical informatics and deep\nmedicine in digital healthcare as drivers of innovation in preventive patient\ncare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a simulated digital model that learns a person's baseline blood\nhealth over time. Using an adaptive learning algorithm, the model provides a\nrisk assessment score that compares an individual's chronological age with an\nestimation of biological age based on common immune-relevant markers used in\ncurrent clinical practice. We demonstrate its efficacy on real and synthetic\ndata from medically relevant cases, extreme cases, and empirical blood cell\ncount data from 100K data records in the Centers for Disease Control and\nPrevention's National Health and Nutrition Examination Survey (CDC NHANES) that\nspans 13 years. We find that the score is informative in distinguishing healthy\nindividuals from those with diseases, both self-reported and as manifested via\nabnormal blood test results, providing an entry-level score for patient\ntriaging. The risk assessment score is not a machine learning black-box\napproach but can interact with ML and DL approaches to help guide, control the\nattention given to specific features, and assign proper explainable weight to\nan otherwise transparent adaptive learning algorithm. This approach may allow\nfast and scalable deployment to personalised, sensitive, and predictive\nderivative indexes within digital medicine, without the need for a new test,\nassay, or prospective sampling, unlike other biological ageing-related scores\nand methods. It demonstrates the potential of clinical informatics and deep\nmedicine in digital healthcare as drivers of innovation in preventive patient\ncare."
                },
                "authors": [
                    {
                        "name": "Santiago Hernández-Orozco"
                    },
                    {
                        "name": "Abicumaran Uthamacumaran"
                    },
                    {
                        "name": "Francisco Hernández-Quiroz"
                    },
                    {
                        "name": "Kourosh Saeb-Parsy"
                    },
                    {
                        "name": "Hector Zenil"
                    }
                ],
                "author_detail": {
                    "name": "Hector Zenil"
                },
                "author": "Hector Zenil",
                "arxiv_comment": "40 pages + appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.01444v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01444v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18908v1",
                "updated": "2024-10-24T16:59:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    59,
                    28,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T16:59:28Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    59,
                    28,
                    3,
                    298,
                    0
                ],
                "title": "A Survey on Speech Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Speech Large Language Models"
                },
                "summary": "Large Language Models (LLMs) exhibit strong contextual understanding and\nremarkable multi-task performance. Therefore, researchers have been seeking to\nintegrate LLMs in the broad sense of Spoken Language Understanding (SLU) field.\nDifferent from the traditional method of cascading LLMs to process text\ngenerated by Automatic Speech Recognition(ASR), new efforts have focused on\ndesigning architectures centered around Audio Feature Extraction - Multimodal\nInformation Fusion - LLM Inference(Speech LLMs). This approach enables richer\naudio feature extraction while simultaneously facilitating end-to-end fusion of\naudio and text modalities, thereby achieving deeper understanding and reasoning\nfrom audio data. This paper elucidates the development of Speech LLMs, offering\nan in-depth analysis of system architectures and training strategies. Through\nextensive research and a series of targeted experiments, the paper assesses\nSpeech LLMs' advancements in Rich Audio Transcription and its potential for\nCross-task Integration within the SLU field. Additionally, it indicates key\nchallenges uncovered through experimentation, such as the Dormancy of LLMs\nunder certain conditions. The paper further delves into the training strategies\nfor Speech LLMs, proposing potential solutions based on these findings, and\noffering valuable insights and references for future research in this domain,\nas well as LLM applications in multimodal contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit strong contextual understanding and\nremarkable multi-task performance. Therefore, researchers have been seeking to\nintegrate LLMs in the broad sense of Spoken Language Understanding (SLU) field.\nDifferent from the traditional method of cascading LLMs to process text\ngenerated by Automatic Speech Recognition(ASR), new efforts have focused on\ndesigning architectures centered around Audio Feature Extraction - Multimodal\nInformation Fusion - LLM Inference(Speech LLMs). This approach enables richer\naudio feature extraction while simultaneously facilitating end-to-end fusion of\naudio and text modalities, thereby achieving deeper understanding and reasoning\nfrom audio data. This paper elucidates the development of Speech LLMs, offering\nan in-depth analysis of system architectures and training strategies. Through\nextensive research and a series of targeted experiments, the paper assesses\nSpeech LLMs' advancements in Rich Audio Transcription and its potential for\nCross-task Integration within the SLU field. Additionally, it indicates key\nchallenges uncovered through experimentation, such as the Dormancy of LLMs\nunder certain conditions. The paper further delves into the training strategies\nfor Speech LLMs, proposing potential solutions based on these findings, and\noffering valuable insights and references for future research in this domain,\nas well as LLM applications in multimodal contexts."
                },
                "authors": [
                    {
                        "name": "Jing Peng"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Yu Xi"
                    },
                    {
                        "name": "Xv Li"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18907v1",
                "updated": "2024-10-24T16:59:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    59,
                    26,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T16:59:26Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    59,
                    26,
                    3,
                    298,
                    0
                ],
                "title": "SkillMimicGen: Automated Demonstration Generation for Efficient Skill\n  Learning and Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkillMimicGen: Automated Demonstration Generation for Efficient Skill\n  Learning and Deployment"
                },
                "summary": "Imitation learning from human demonstrations is an effective paradigm for\nrobot manipulation, but acquiring large datasets is costly and\nresource-intensive, especially for long-horizon tasks. To address this issue,\nwe propose SkillMimicGen (SkillGen), an automated system for generating\ndemonstration datasets from a few human demos. SkillGen segments human demos\ninto manipulation skills, adapts these skills to new contexts, and stitches\nthem together through free-space transit and transfer motion. We also propose a\nHybrid Skill Policy (HSP) framework for learning skill initiation, control, and\ntermination components from SkillGen datasets, enabling skills to be sequenced\nusing motion planning at test-time. We demonstrate that SkillGen greatly\nimproves data generation and policy learning performance over a\nstate-of-the-art data generation framework, resulting in the capability to\nproduce data for large scene variations, including clutter, and agents that are\non average 24% more successful. We demonstrate the efficacy of SkillGen by\ngenerating over 24K demonstrations across 18 task variants in simulation from\njust 60 human demonstrations, and training proficient, often near-perfect, HSP\nagents. Finally, we apply SkillGen to 3 real-world manipulation tasks and also\ndemonstrate zero-shot sim-to-real transfer on a long-horizon assembly task.\nVideos, and more at https://skillgen.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation learning from human demonstrations is an effective paradigm for\nrobot manipulation, but acquiring large datasets is costly and\nresource-intensive, especially for long-horizon tasks. To address this issue,\nwe propose SkillMimicGen (SkillGen), an automated system for generating\ndemonstration datasets from a few human demos. SkillGen segments human demos\ninto manipulation skills, adapts these skills to new contexts, and stitches\nthem together through free-space transit and transfer motion. We also propose a\nHybrid Skill Policy (HSP) framework for learning skill initiation, control, and\ntermination components from SkillGen datasets, enabling skills to be sequenced\nusing motion planning at test-time. We demonstrate that SkillGen greatly\nimproves data generation and policy learning performance over a\nstate-of-the-art data generation framework, resulting in the capability to\nproduce data for large scene variations, including clutter, and agents that are\non average 24% more successful. We demonstrate the efficacy of SkillGen by\ngenerating over 24K demonstrations across 18 task variants in simulation from\njust 60 human demonstrations, and training proficient, often near-perfect, HSP\nagents. Finally, we apply SkillGen to 3 real-world manipulation tasks and also\ndemonstrate zero-shot sim-to-real transfer on a long-horizon assembly task.\nVideos, and more at https://skillgen.github.io."
                },
                "authors": [
                    {
                        "name": "Caelan Garrett"
                    },
                    {
                        "name": "Ajay Mandlekar"
                    },
                    {
                        "name": "Bowen Wen"
                    },
                    {
                        "name": "Dieter Fox"
                    }
                ],
                "author_detail": {
                    "name": "Dieter Fox"
                },
                "author": "Dieter Fox",
                "arxiv_journal_ref": "2024 Conference on Robot Learning (CoRL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18906v1",
                "updated": "2024-10-24T16:57:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    57,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T16:57:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    57,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "PRISM: A Methodology for Auditing Biases in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: A Methodology for Auditing Biases in Large Language Models"
                },
                "summary": "Auditing Large Language Models (LLMs) to discover their biases and\npreferences is an emerging challenge in creating Responsible Artificial\nIntelligence (AI). While various methods have been proposed to elicit the\npreferences of such models, countermeasures have been taken by LLM trainers,\nsuch that LLMs hide, obfuscate or point blank refuse to disclosure their\npositions on certain subjects. This paper presents PRISM, a flexible,\ninquiry-based methodology for auditing LLMs - that seeks to illicit such\npositions indirectly through task-based inquiry prompting rather than direct\ninquiry of said preferences. To demonstrate the utility of the methodology, we\napplied PRISM on the Political Compass Test, where we assessed the political\nleanings of twenty-one LLMs from seven providers. We show LLMs, by default,\nespouse positions that are economically left and socially liberal (consistent\nwith prior work). We also show the space of positions that these models are\nwilling to espouse - where some models are more constrained and less compliant\nthan others - while others are more neutral and objective. In sum, PRISM can\nmore reliably probe and audit LLMs to understand their preferences, biases and\nconstraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Large Language Models (LLMs) to discover their biases and\npreferences is an emerging challenge in creating Responsible Artificial\nIntelligence (AI). While various methods have been proposed to elicit the\npreferences of such models, countermeasures have been taken by LLM trainers,\nsuch that LLMs hide, obfuscate or point blank refuse to disclosure their\npositions on certain subjects. This paper presents PRISM, a flexible,\ninquiry-based methodology for auditing LLMs - that seeks to illicit such\npositions indirectly through task-based inquiry prompting rather than direct\ninquiry of said preferences. To demonstrate the utility of the methodology, we\napplied PRISM on the Political Compass Test, where we assessed the political\nleanings of twenty-one LLMs from seven providers. We show LLMs, by default,\nespouse positions that are economically left and socially liberal (consistent\nwith prior work). We also show the space of positions that these models are\nwilling to espouse - where some models are more constrained and less compliant\nthan others - while others are more neutral and objective. In sum, PRISM can\nmore reliably probe and audit LLMs to understand their preferences, biases and\nconstraints."
                },
                "authors": [
                    {
                        "name": "Leif Azzopardi"
                    },
                    {
                        "name": "Yashar Moshfeghi"
                    }
                ],
                "author_detail": {
                    "name": "Yashar Moshfeghi"
                },
                "author": "Yashar Moshfeghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18902v1",
                "updated": "2024-10-24T16:48:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    48,
                    12,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T16:48:12Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    48,
                    12,
                    3,
                    298,
                    0
                ],
                "title": "LLMs for Extremely Low-Resource Finno-Ugric Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Extremely Low-Resource Finno-Ugric Languages"
                },
                "summary": "The advancement of large language models (LLMs) has predominantly focused on\nhigh-resource languages, leaving low-resource languages, such as those in the\nFinno-Ugric family, significantly underrepresented. This paper addresses this\ngap by focusing on V\\~oro, Livonian, and Komi. We cover almost the entire cycle\nof LLM creation, from data collection to instruction tuning and evaluation. Our\ncontributions include developing multilingual base and instruction-tuned\nmodels; creating evaluation benchmarks, including the smugri-MT-bench\nmulti-turn conversational benchmark; and conducting human evaluation. We intend\nfor this work to promote linguistic diversity, ensuring that lesser-resourced\nlanguages can benefit from advancements in NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) has predominantly focused on\nhigh-resource languages, leaving low-resource languages, such as those in the\nFinno-Ugric family, significantly underrepresented. This paper addresses this\ngap by focusing on V\\~oro, Livonian, and Komi. We cover almost the entire cycle\nof LLM creation, from data collection to instruction tuning and evaluation. Our\ncontributions include developing multilingual base and instruction-tuned\nmodels; creating evaluation benchmarks, including the smugri-MT-bench\nmulti-turn conversational benchmark; and conducting human evaluation. We intend\nfor this work to promote linguistic diversity, ensuring that lesser-resourced\nlanguages can benefit from advancements in NLP."
                },
                "authors": [
                    {
                        "name": "Taido Purason"
                    },
                    {
                        "name": "Hele-Andra Kuulmets"
                    },
                    {
                        "name": "Mark Fishel"
                    }
                ],
                "author_detail": {
                    "name": "Mark Fishel"
                },
                "author": "Mark Fishel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v2",
                "updated": "2024-10-24T16:40:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    40,
                    10,
                    3,
                    298,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "João Monteiro"
                    },
                    {
                        "name": "Étienne Marcotte"
                    },
                    {
                        "name": "Pierre-André Noël"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vázquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00953v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00953v3",
                "updated": "2024-10-24T16:32:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    32,
                    34,
                    3,
                    298,
                    0
                ],
                "published": "2024-03-01T20:06:39Z",
                "published_parsed": [
                    2024,
                    3,
                    1,
                    20,
                    6,
                    39,
                    4,
                    61,
                    0
                ],
                "title": "AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge\n  Graph Construction Based on Ontologies-enhanced Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRD: An Automatic and End-to-End System for Rare Disease Knowledge\n  Graph Construction Based on Ontologies-enhanced Large Language Models"
                },
                "summary": "Rare diseases affect millions worldwide but often face limited research focus\ndue to their low prevalence. This results in prolonged diagnoses and a lack of\napproved therapies. Recent advancements in Large Language Models (LLMs) have\nshown promise in automating the extraction of medical information, offering\npotential to improve medical diagnosis and management. However, most LLMs lack\nprofessional medical knowledge, especially concerning rare diseases, and\nstruggle to handle the latest rare disease information. They also cannot\neffectively manage rare disease data and are not directly suitable for\ndiagnosis and management tasks. Our objective is to create an end-to-end system\ncalled AutoRD, which automates the extraction of information from medical texts\nabout rare diseases, focusing on entities and their relations. AutoRD\nintegrates up-to-date structured knowledge and demonstrates superior\nperformance in rare disease extraction tasks. We conduct various experiments to\nevaluate AutoRD's performance, aiming to surpass common LLMs and traditional\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rare diseases affect millions worldwide but often face limited research focus\ndue to their low prevalence. This results in prolonged diagnoses and a lack of\napproved therapies. Recent advancements in Large Language Models (LLMs) have\nshown promise in automating the extraction of medical information, offering\npotential to improve medical diagnosis and management. However, most LLMs lack\nprofessional medical knowledge, especially concerning rare diseases, and\nstruggle to handle the latest rare disease information. They also cannot\neffectively manage rare disease data and are not directly suitable for\ndiagnosis and management tasks. Our objective is to create an end-to-end system\ncalled AutoRD, which automates the extraction of information from medical texts\nabout rare diseases, focusing on entities and their relations. AutoRD\nintegrates up-to-date structured knowledge and demonstrates superior\nperformance in rare disease extraction tasks. We conduct various experiments to\nevaluate AutoRD's performance, aiming to surpass common LLMs and traditional\nmethods."
                },
                "authors": [
                    {
                        "name": "Lang Cao"
                    },
                    {
                        "name": "Jimeng Sun"
                    },
                    {
                        "name": "Adam Cross"
                    }
                ],
                "author_detail": {
                    "name": "Adam Cross"
                },
                "author": "Adam Cross",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.00953v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00953v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18893v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18893v1",
                "updated": "2024-10-24T16:30:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    30,
                    14,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T16:30:14Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    30,
                    14,
                    3,
                    298,
                    0
                ],
                "title": "Creating and Repairing Robot Programs in Open-World Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating and Repairing Robot Programs in Open-World Domains"
                },
                "summary": "Using Large Language Models (LLMs) to produce robot programs from natural\nlanguage has allowed for robot systems that can complete a higher diversity of\ntasks. However, LLM-generated programs may be faulty, either due to ambiguity\nin instructions, misinterpretation of the desired task, or missing information\nabout the world state. As these programs run, the state of the world changes\nand they gather new information. When a failure occurs, it is important that\nthey recover from the current world state and avoid repeating steps that they\nthey previously completed successfully. We propose RoboRepair, a system which\ntraces the execution of a program up until error, and then runs an LLM-produced\nrecovery program that minimizes repeated actions.\n  To evaluate the efficacy of our system, we create a benchmark consisting of\neleven tasks with various error conditions that require the generation of a\nrecovery program. We compare the efficiency of the recovery program to a plan\nbuilt with an oracle that has foreknowledge of future errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models (LLMs) to produce robot programs from natural\nlanguage has allowed for robot systems that can complete a higher diversity of\ntasks. However, LLM-generated programs may be faulty, either due to ambiguity\nin instructions, misinterpretation of the desired task, or missing information\nabout the world state. As these programs run, the state of the world changes\nand they gather new information. When a failure occurs, it is important that\nthey recover from the current world state and avoid repeating steps that they\nthey previously completed successfully. We propose RoboRepair, a system which\ntraces the execution of a program up until error, and then runs an LLM-produced\nrecovery program that minimizes repeated actions.\n  To evaluate the efficacy of our system, we create a benchmark consisting of\neleven tasks with various error conditions that require the generation of a\nrecovery program. We compare the efficiency of the recovery program to a plan\nbuilt with an oracle that has foreknowledge of future errors."
                },
                "authors": [
                    {
                        "name": "Claire Schlesinger"
                    },
                    {
                        "name": "Arjun Guha"
                    },
                    {
                        "name": "Joydeep Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Joydeep Biswas"
                },
                "author": "Joydeep Biswas",
                "arxiv_comment": "Under review at ACL Rolling Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18893v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18893v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18890v1",
                "updated": "2024-10-24T16:27:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    27,
                    35,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T16:27:35Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    27,
                    35,
                    3,
                    298,
                    0
                ],
                "title": "Improving Small-Scale Large Language Models Function Calling for\n  Reasoning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Small-Scale Large Language Models Function Calling for\n  Reasoning Tasks"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional capabilities in natural language understanding and generation.\nWhile these models excel in general complex reasoning tasks, they still face\nchallenges in mathematical problem-solving and logical reasoning. To address\nthese limitations, researchers have explored function calling abilities,\nallowing LLMs to execute provided functions and utilize their outputs for task\ncompletion. However, concentrating on specific tasks can be very inefficient\nfor large-scale LLMs to be used, because of the expensive cost of training and\ninference stages they need in terms of computational resources. This study\nintroduces a novel framework for training smaller language models in function\ncalling, focusing on specific logical and mathematical reasoning tasks. The\napproach aims to improve performances of small-scale models for these tasks\nusing function calling, ensuring a high level of accuracy. Our framework\nemploys an agent that, given a problem and a set of callable functions, queries\nthe LLM by injecting a description and examples of the usable functions into\nthe prompt and managing their calls in a step-by-step reasoning chain. This\nprocess is used to create a dataset of correct and incorrect reasoning chain\nchat completions from a large-scale LLM. This dataset is used to train a\nsmaller LLM using Reinforcement Learning from Human Feedback (RLHF),\nspecifically employing the Direct Preference Optimization (DPO) technique.\nExperimental results demonstrate how the proposed approach balances the\ntrade-off between model size and performance, improving the ability of function\ncalling for reasoning tasks, in smaller models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional capabilities in natural language understanding and generation.\nWhile these models excel in general complex reasoning tasks, they still face\nchallenges in mathematical problem-solving and logical reasoning. To address\nthese limitations, researchers have explored function calling abilities,\nallowing LLMs to execute provided functions and utilize their outputs for task\ncompletion. However, concentrating on specific tasks can be very inefficient\nfor large-scale LLMs to be used, because of the expensive cost of training and\ninference stages they need in terms of computational resources. This study\nintroduces a novel framework for training smaller language models in function\ncalling, focusing on specific logical and mathematical reasoning tasks. The\napproach aims to improve performances of small-scale models for these tasks\nusing function calling, ensuring a high level of accuracy. Our framework\nemploys an agent that, given a problem and a set of callable functions, queries\nthe LLM by injecting a description and examples of the usable functions into\nthe prompt and managing their calls in a step-by-step reasoning chain. This\nprocess is used to create a dataset of correct and incorrect reasoning chain\nchat completions from a large-scale LLM. This dataset is used to train a\nsmaller LLM using Reinforcement Learning from Human Feedback (RLHF),\nspecifically employing the Direct Preference Optimization (DPO) technique.\nExperimental results demonstrate how the proposed approach balances the\ntrade-off between model size and performance, improving the ability of function\ncalling for reasoning tasks, in smaller models."
                },
                "authors": [
                    {
                        "name": "Graziano A. Manduzio"
                    },
                    {
                        "name": "Federico A. Galatolo"
                    },
                    {
                        "name": "Mario G. C. A. Cimino"
                    },
                    {
                        "name": "Enzo Pasquale Scilingo"
                    },
                    {
                        "name": "Lorenzo Cominelli"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Cominelli"
                },
                "author": "Lorenzo Cominelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18889v1",
                "updated": "2024-10-24T16:27:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    27,
                    3,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T16:27:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    27,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating\n  Their Effect on Model Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Better than Reported? Detecting Label Errors and Mitigating\n  Their Effect on Model Performance"
                },
                "summary": "NLP benchmarks rely on standardized datasets for training and evaluating\nmodels and are crucial for advancing the field. Traditionally, expert\nannotations ensure high-quality labels; however, the cost of expert annotation\ndoes not scale well with the growing demand for larger datasets required by\nmodern models. While crowd-sourcing provides a more scalable solution, it often\ncomes at the expense of annotation precision and consistency. Recent\nadvancements in large language models (LLMs) offer new opportunities to enhance\nthe annotation process, particularly for detecting label errors in existing\ndatasets. In this work, we consider the recent approach of LLM-as-a-judge,\nleveraging an ensemble of LLMs to flag potentially mislabeled examples. Through\na case study of four datasets from the TRUE benchmark, covering different tasks\nand domains, we empirically analyze the labeling quality of existing datasets,\nand compare expert, crowd-sourced, and our LLM-based annotations in terms of\nagreement, label quality, and efficiency, demonstrating the strengths and\nlimitations of each annotation method. Our findings reveal a substantial number\nof label errors, which, when corrected, induce a significant upward shift in\nreported model performance. This suggests that many of the LLMs so-called\nmistakes are due to label errors rather than genuine model failures.\nAdditionally, we discuss the implications of mislabeled data and propose\nmethods to mitigate them in training to improve model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLP benchmarks rely on standardized datasets for training and evaluating\nmodels and are crucial for advancing the field. Traditionally, expert\nannotations ensure high-quality labels; however, the cost of expert annotation\ndoes not scale well with the growing demand for larger datasets required by\nmodern models. While crowd-sourcing provides a more scalable solution, it often\ncomes at the expense of annotation precision and consistency. Recent\nadvancements in large language models (LLMs) offer new opportunities to enhance\nthe annotation process, particularly for detecting label errors in existing\ndatasets. In this work, we consider the recent approach of LLM-as-a-judge,\nleveraging an ensemble of LLMs to flag potentially mislabeled examples. Through\na case study of four datasets from the TRUE benchmark, covering different tasks\nand domains, we empirically analyze the labeling quality of existing datasets,\nand compare expert, crowd-sourced, and our LLM-based annotations in terms of\nagreement, label quality, and efficiency, demonstrating the strengths and\nlimitations of each annotation method. Our findings reveal a substantial number\nof label errors, which, when corrected, induce a significant upward shift in\nreported model performance. This suggests that many of the LLMs so-called\nmistakes are due to label errors rather than genuine model failures.\nAdditionally, we discuss the implications of mislabeled data and propose\nmethods to mitigate them in training to improve model performance."
                },
                "authors": [
                    {
                        "name": "Omer Nahum"
                    },
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Orgad Keller"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16197v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16197v3",
                "updated": "2024-10-24T16:13:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    13,
                    21,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-21T17:00:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    0,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "LASER: Script Execution by Autonomous Agents for On-demand Traffic\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LASER: Script Execution by Autonomous Agents for On-demand Traffic\n  Simulation"
                },
                "summary": "Autonomous Driving Systems (ADS) require diverse and safety-critical traffic\nscenarios for effective training and testing, but the existing data generation\nmethods struggle to provide flexibility and scalability. We propose LASER, a\nnovel frame-work that leverage large language models (LLMs) to conduct traffic\nsimulations based on natural language inputs. The framework operates in two\nstages: it first generates scripts from user-provided descriptions and then\nexecutes them using autonomous agents in real time. Validated in the CARLA\nsimulator, LASER successfully generates complex, on-demand driving scenarios,\nsignificantly improving ADS training and testing data generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Driving Systems (ADS) require diverse and safety-critical traffic\nscenarios for effective training and testing, but the existing data generation\nmethods struggle to provide flexibility and scalability. We propose LASER, a\nnovel frame-work that leverage large language models (LLMs) to conduct traffic\nsimulations based on natural language inputs. The framework operates in two\nstages: it first generates scripts from user-provided descriptions and then\nexecutes them using autonomous agents in real time. Validated in the CARLA\nsimulator, LASER successfully generates complex, on-demand driving scenarios,\nsignificantly improving ADS training and testing data generation."
                },
                "authors": [
                    {
                        "name": "Hao Gao"
                    },
                    {
                        "name": "Jingyue Wang"
                    },
                    {
                        "name": "Wenyang Fang"
                    },
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Yunpeng Huang"
                    },
                    {
                        "name": "Taolue Chen"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxing Ma"
                },
                "author": "Xiaoxing Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16197v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16197v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18870v1",
                "updated": "2024-10-24T15:57:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    57,
                    17,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:57:17Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    57,
                    17,
                    3,
                    298,
                    0
                ],
                "title": "End-to-end Training for Recommendation with Language-based User Profiles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end Training for Recommendation with Language-based User Profiles"
                },
                "summary": "Many online platforms maintain user profiles for personalization.\nUnfortunately, these profiles are typically not interpretable or easily\nmodifiable by the user. To remedy this shortcoming, we explore natural\nlanguage-based user profiles, as they promise enhanced transparency and\nscrutability of recommender systems. While existing work has shown that\nlanguage-based profiles from standard LLMs can be effective, such generalist\nLLMs are unlikely to be optimal for this task. In this paper, we introduce\nLangPTune, the first end-to-end learning method for training LLMs to produce\nlanguage-based user profiles that optimize recommendation effectiveness.\nThrough comprehensive evaluations of LangPTune across various training\nconfigurations and benchmarks, we demonstrate that our approach significantly\noutperforms existing profile-based methods. In addition, it approaches\nperformance levels comparable to state-of-the-art, less transparent recommender\nsystems, providing a robust and interpretable alternative to conventional\nsystems. Finally, we validate the relative interpretability of these\nlanguage-based user profiles through user studies involving crowdworkers and\nGPT-4-based evaluations. Implementation of LangPTune can be found at\nhttps://github.com/ZhaolinGao/LangPTune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many online platforms maintain user profiles for personalization.\nUnfortunately, these profiles are typically not interpretable or easily\nmodifiable by the user. To remedy this shortcoming, we explore natural\nlanguage-based user profiles, as they promise enhanced transparency and\nscrutability of recommender systems. While existing work has shown that\nlanguage-based profiles from standard LLMs can be effective, such generalist\nLLMs are unlikely to be optimal for this task. In this paper, we introduce\nLangPTune, the first end-to-end learning method for training LLMs to produce\nlanguage-based user profiles that optimize recommendation effectiveness.\nThrough comprehensive evaluations of LangPTune across various training\nconfigurations and benchmarks, we demonstrate that our approach significantly\noutperforms existing profile-based methods. In addition, it approaches\nperformance levels comparable to state-of-the-art, less transparent recommender\nsystems, providing a robust and interpretable alternative to conventional\nsystems. Finally, we validate the relative interpretability of these\nlanguage-based user profiles through user studies involving crowdworkers and\nGPT-4-based evaluations. Implementation of LangPTune can be found at\nhttps://github.com/ZhaolinGao/LangPTune."
                },
                "authors": [
                    {
                        "name": "Zhaolin Gao"
                    },
                    {
                        "name": "Joyce Zhou"
                    },
                    {
                        "name": "Yijia Dai"
                    },
                    {
                        "name": "Thorsten Joachims"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Joachims"
                },
                "author": "Thorsten Joachims",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11722v2",
                "updated": "2024-10-24T15:48:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    48,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-15T15:55:00Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    55,
                    0,
                    1,
                    289,
                    0
                ],
                "title": "RClicks: Realistic Click Simulation for Benchmarking Interactive\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RClicks: Realistic Click Simulation for Benchmarking Interactive\n  Segmentation"
                },
                "summary": "The emergence of Segment Anything (SAM) sparked research interest in the\nfield of interactive segmentation, especially in the context of image editing\ntasks and speeding up data annotation. Unlike common semantic segmentation,\ninteractive segmentation methods allow users to directly influence their output\nthrough prompts (e.g. clicks). However, click patterns in real-world\ninteractive segmentation scenarios remain largely unexplored. Most methods rely\non the assumption that users would click in the center of the largest erroneous\narea. Nevertheless, recent studies show that this is not always the case. Thus,\nmethods may have poor performance in real-world deployment despite high metrics\nin a baseline benchmark. To accurately simulate real-user clicks, we conducted\na large crowdsourcing study of click patterns in an interactive segmentation\nscenario and collected 475K real-user clicks. Drawing on ideas from saliency\ntasks, we develop a clickability model that enables sampling clicks, which\nclosely resemble actual user inputs. Using our model and dataset, we propose\nRClicks benchmark for a comprehensive comparison of existing interactive\nsegmentation methods on realistic clicks. Specifically, we evaluate not only\nthe average quality of methods, but also the robustness w.r.t. click patterns.\nAccording to our benchmark, in real-world usage interactive segmentation models\nmay perform worse than it has been reported in the baseline benchmark, and most\nof the methods are not robust. We believe that RClicks is a significant step\ntowards creating interactive segmentation methods that provide the best user\nexperience in real-world cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Segment Anything (SAM) sparked research interest in the\nfield of interactive segmentation, especially in the context of image editing\ntasks and speeding up data annotation. Unlike common semantic segmentation,\ninteractive segmentation methods allow users to directly influence their output\nthrough prompts (e.g. clicks). However, click patterns in real-world\ninteractive segmentation scenarios remain largely unexplored. Most methods rely\non the assumption that users would click in the center of the largest erroneous\narea. Nevertheless, recent studies show that this is not always the case. Thus,\nmethods may have poor performance in real-world deployment despite high metrics\nin a baseline benchmark. To accurately simulate real-user clicks, we conducted\na large crowdsourcing study of click patterns in an interactive segmentation\nscenario and collected 475K real-user clicks. Drawing on ideas from saliency\ntasks, we develop a clickability model that enables sampling clicks, which\nclosely resemble actual user inputs. Using our model and dataset, we propose\nRClicks benchmark for a comprehensive comparison of existing interactive\nsegmentation methods on realistic clicks. Specifically, we evaluate not only\nthe average quality of methods, but also the robustness w.r.t. click patterns.\nAccording to our benchmark, in real-world usage interactive segmentation models\nmay perform worse than it has been reported in the baseline benchmark, and most\nof the methods are not robust. We believe that RClicks is a significant step\ntowards creating interactive segmentation methods that provide the best user\nexperience in real-world cases."
                },
                "authors": [
                    {
                        "name": "Anton Antonov"
                    },
                    {
                        "name": "Andrey Moskalenko"
                    },
                    {
                        "name": "Denis Shepelev"
                    },
                    {
                        "name": "Alexander Krapukhin"
                    },
                    {
                        "name": "Konstantin Soshin"
                    },
                    {
                        "name": "Anton Konushin"
                    },
                    {
                        "name": "Vlad Shakhuro"
                    }
                ],
                "author_detail": {
                    "name": "Vlad Shakhuro"
                },
                "author": "Vlad Shakhuro",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18861v1",
                "updated": "2024-10-24T15:44:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    44,
                    34,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:44:34Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    44,
                    34,
                    3,
                    298,
                    0
                ],
                "title": "Provably Robust Watermarks for Open-Source Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Robust Watermarks for Open-Source Language Models"
                },
                "summary": "The recent explosion of high-quality language models has necessitated new\nmethods for identifying AI-generated text. Watermarking is a leading solution\nand could prove to be an essential tool in the age of generative AI. Existing\napproaches embed watermarks at inference and crucially rely on the large\nlanguage model (LLM) specification and parameters being secret, which makes\nthem inapplicable to the open-source setting. In this work, we introduce the\nfirst watermarking scheme for open-source LLMs. Our scheme works by modifying\nthe parameters of the model, but the watermark can be detected from just the\noutputs of the model. Perhaps surprisingly, we prove that our watermarks are\nunremovable under certain assumptions about the adversary's knowledge. To\ndemonstrate the behavior of our construction under concrete parameter\ninstantiations, we present experimental results with OPT-6.7B and OPT-1.3B. We\ndemonstrate robustness to both token substitution and perturbation of the model\nparameters. We find that the stronger of these attacks, the model-perturbation\nattack, requires deteriorating the quality score to 0 out of 100 in order to\nbring the detection rate down to 50%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent explosion of high-quality language models has necessitated new\nmethods for identifying AI-generated text. Watermarking is a leading solution\nand could prove to be an essential tool in the age of generative AI. Existing\napproaches embed watermarks at inference and crucially rely on the large\nlanguage model (LLM) specification and parameters being secret, which makes\nthem inapplicable to the open-source setting. In this work, we introduce the\nfirst watermarking scheme for open-source LLMs. Our scheme works by modifying\nthe parameters of the model, but the watermark can be detected from just the\noutputs of the model. Perhaps surprisingly, we prove that our watermarks are\nunremovable under certain assumptions about the adversary's knowledge. To\ndemonstrate the behavior of our construction under concrete parameter\ninstantiations, we present experimental results with OPT-6.7B and OPT-1.3B. We\ndemonstrate robustness to both token substitution and perturbation of the model\nparameters. We find that the stronger of these attacks, the model-perturbation\nattack, requires deteriorating the quality score to 0 out of 100 in order to\nbring the detection rate down to 50%."
                },
                "authors": [
                    {
                        "name": "Miranda Christ"
                    },
                    {
                        "name": "Sam Gunn"
                    },
                    {
                        "name": "Tal Malkin"
                    },
                    {
                        "name": "Mariana Raykova"
                    }
                ],
                "author_detail": {
                    "name": "Mariana Raykova"
                },
                "author": "Mariana Raykova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18860v1",
                "updated": "2024-10-24T15:44:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    44,
                    33,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:44:33Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    44,
                    33,
                    3,
                    298,
                    0
                ],
                "title": "DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate\n  Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate\n  Hallucinations"
                },
                "summary": "Large Language Models (LLMs) often hallucinate, producing unfaithful or\nfactually incorrect outputs by misrepresenting the provided context or\nincorrectly recalling internal knowledge. Recent studies have identified\nspecific attention heads within the Transformer architecture, known as\nretrieval heads, responsible for extracting relevant contextual information. We\nhypothesise that masking these retrieval heads can induce hallucinations and\nthat contrasting the outputs of the base LLM and the masked LLM can reduce\nhallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads\n(DeCoRe), a novel training-free decoding strategy that amplifies information\nfound in the context and model parameters. DeCoRe mitigates potentially\nhallucinated responses by dynamically contrasting the outputs of the base LLM\nand the masked LLM, using conditional entropy as a guide. Our extensive\nexperiments confirm that DeCoRe significantly improves performance on tasks\nrequiring high contextual faithfulness, such as summarisation (XSum by 18.6%),\ninstruction following (MemoTrap by 10.9%), and open-book question answering\n(NQ-Open by 2.4% and NQ-Swap by 5.5%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often hallucinate, producing unfaithful or\nfactually incorrect outputs by misrepresenting the provided context or\nincorrectly recalling internal knowledge. Recent studies have identified\nspecific attention heads within the Transformer architecture, known as\nretrieval heads, responsible for extracting relevant contextual information. We\nhypothesise that masking these retrieval heads can induce hallucinations and\nthat contrasting the outputs of the base LLM and the masked LLM can reduce\nhallucinations. To this end, we propose Decoding by Contrasting Retrieval Heads\n(DeCoRe), a novel training-free decoding strategy that amplifies information\nfound in the context and model parameters. DeCoRe mitigates potentially\nhallucinated responses by dynamically contrasting the outputs of the base LLM\nand the masked LLM, using conditional entropy as a guide. Our extensive\nexperiments confirm that DeCoRe significantly improves performance on tasks\nrequiring high contextual faithfulness, such as summarisation (XSum by 18.6%),\ninstruction following (MemoTrap by 10.9%), and open-book question answering\n(NQ-Open by 2.4% and NQ-Swap by 5.5%)."
                },
                "authors": [
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Chen Jin"
                    },
                    {
                        "name": "Ahmed Abdulaal"
                    },
                    {
                        "name": "Tom Diethe"
                    },
                    {
                        "name": "Philip Teare"
                    },
                    {
                        "name": "Beatrice Alex"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Amrutha Saseendran"
                    }
                ],
                "author_detail": {
                    "name": "Amrutha Saseendran"
                },
                "author": "Amrutha Saseendran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18856v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18856v1",
                "updated": "2024-10-24T15:41:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    41,
                    56,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:41:56Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    41,
                    56,
                    3,
                    298,
                    0
                ],
                "title": "Demystifying Large Language Models for Medicine: A Primer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Large Language Models for Medicine: A Primer"
                },
                "summary": "Large language models (LLMs) represent a transformative class of AI tools\ncapable of revolutionizing various aspects of healthcare by generating\nhuman-like responses across diverse contexts and adapting to novel tasks\nfollowing human instructions. Their potential application spans a broad range\nof medical tasks, such as clinical documentation, matching patients to clinical\ntrials, and answering medical questions. In this primer paper, we propose an\nactionable guideline to help healthcare professionals more efficiently utilize\nLLMs in their work, along with a set of best practices. This approach consists\nof several main phases, including formulating the task, choosing LLMs, prompt\nengineering, fine-tuning, and deployment. We start with the discussion of\ncritical considerations in identifying healthcare tasks that align with the\ncore capabilities of LLMs and selecting models based on the selected task and\ndata, performance requirements, and model interface. We then review the\nstrategies, such as prompt engineering and fine-tuning, to adapt standard LLMs\nto specialized medical tasks. Deployment considerations, including regulatory\ncompliance, ethical guidelines, and continuous monitoring for fairness and\nbias, are also discussed. By providing a structured step-by-step methodology,\nthis tutorial aims to equip healthcare professionals with the tools necessary\nto effectively integrate LLMs into clinical practice, ensuring that these\npowerful technologies are applied in a safe, reliable, and impactful manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a transformative class of AI tools\ncapable of revolutionizing various aspects of healthcare by generating\nhuman-like responses across diverse contexts and adapting to novel tasks\nfollowing human instructions. Their potential application spans a broad range\nof medical tasks, such as clinical documentation, matching patients to clinical\ntrials, and answering medical questions. In this primer paper, we propose an\nactionable guideline to help healthcare professionals more efficiently utilize\nLLMs in their work, along with a set of best practices. This approach consists\nof several main phases, including formulating the task, choosing LLMs, prompt\nengineering, fine-tuning, and deployment. We start with the discussion of\ncritical considerations in identifying healthcare tasks that align with the\ncore capabilities of LLMs and selecting models based on the selected task and\ndata, performance requirements, and model interface. We then review the\nstrategies, such as prompt engineering and fine-tuning, to adapt standard LLMs\nto specialized medical tasks. Deployment considerations, including regulatory\ncompliance, ethical guidelines, and continuous monitoring for fairness and\nbias, are also discussed. By providing a structured step-by-step methodology,\nthis tutorial aims to equip healthcare professionals with the tools necessary\nto effectively integrate LLMs into clinical practice, ensuring that these\npowerful technologies are applied in a safe, reliable, and impactful manner."
                },
                "authors": [
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Nicholas Wan"
                    },
                    {
                        "name": "Robert Leaman"
                    },
                    {
                        "name": "Shubo Tian"
                    },
                    {
                        "name": "Zhizheng Wang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Po-Ting Lai"
                    },
                    {
                        "name": "Qingqing Zhu"
                    },
                    {
                        "name": "Benjamin Hou"
                    },
                    {
                        "name": "Maame Sarfo-Gyamfi"
                    },
                    {
                        "name": "Gongbo Zhang"
                    },
                    {
                        "name": "Aidan Gilson"
                    },
                    {
                        "name": "Balu Bhasuran"
                    },
                    {
                        "name": "Zhe He"
                    },
                    {
                        "name": "Aidong Zhang"
                    },
                    {
                        "name": "Jimeng Sun"
                    },
                    {
                        "name": "Chunhua Weng"
                    },
                    {
                        "name": "Ronald M. Summers"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Zhiyong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Lu"
                },
                "author": "Zhiyong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18856v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18856v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18836v1",
                "updated": "2024-10-24T15:20:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    20,
                    54,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:20:54Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    20,
                    54,
                    3,
                    298,
                    0
                ],
                "title": "From English-Centric to Effective Bilingual: LLMs with Custom Tokenizers\n  for Underrepresented Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From English-Centric to Effective Bilingual: LLMs with Custom Tokenizers\n  for Underrepresented Languages"
                },
                "summary": "In this paper, we propose a model-agnostic cost-effective approach to\ndeveloping bilingual base large language models (LLMs) to support English and\nany target language. The method includes vocabulary expansion, initialization\nof new embeddings, model training and evaluation. We performed our experiments\nwith three languages, each using a non-Latin script - Ukrainian, Arabic, and\nGeorgian.\n  Our approach demonstrates improved language performance while reducing\ncomputational costs. It mitigates the disproportionate penalization of\nunderrepresented languages, promoting fairness and minimizing adverse phenomena\nsuch as code-switching and broken grammar. Additionally, we introduce new\nmetrics to evaluate language quality, revealing that vocabulary size\nsignificantly impacts the quality of generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a model-agnostic cost-effective approach to\ndeveloping bilingual base large language models (LLMs) to support English and\nany target language. The method includes vocabulary expansion, initialization\nof new embeddings, model training and evaluation. We performed our experiments\nwith three languages, each using a non-Latin script - Ukrainian, Arabic, and\nGeorgian.\n  Our approach demonstrates improved language performance while reducing\ncomputational costs. It mitigates the disproportionate penalization of\nunderrepresented languages, promoting fairness and minimizing adverse phenomena\nsuch as code-switching and broken grammar. Additionally, we introduce new\nmetrics to evaluate language quality, revealing that vocabulary size\nsignificantly impacts the quality of generated text."
                },
                "authors": [
                    {
                        "name": "Artur Kiulian"
                    },
                    {
                        "name": "Anton Polishko"
                    },
                    {
                        "name": "Mykola Khandoga"
                    },
                    {
                        "name": "Yevhen Kostiuk"
                    },
                    {
                        "name": "Guillermo Gabrielli"
                    },
                    {
                        "name": "Łukasz Gagała"
                    },
                    {
                        "name": "Fadi Zaraket"
                    },
                    {
                        "name": "Qusai Abu Obaida"
                    },
                    {
                        "name": "Hrishikesh Garud"
                    },
                    {
                        "name": "Wendy Wing Yee Mak"
                    },
                    {
                        "name": "Dmytro Chaplynskyi"
                    },
                    {
                        "name": "Selma Belhadj Amor"
                    },
                    {
                        "name": "Grigol Peradze"
                    }
                ],
                "author_detail": {
                    "name": "Grigol Peradze"
                },
                "author": "Grigol Peradze",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18824v1",
                "updated": "2024-10-24T15:15:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    15,
                    42,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T15:15:42Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    15,
                    42,
                    3,
                    298,
                    0
                ],
                "title": "PSY: Posterior Sampling Based Privacy Enhancer in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PSY: Posterior Sampling Based Privacy Enhancer in Large Language Models"
                },
                "summary": "Privacy vulnerabilities in LLMs, such as leakage from memorization, have been\nconstantly identified, and various mitigation proposals have been proposed.\nLoRA is usually used in fine-tuning LLMs and a good entry point to insert\nprivacy-enhancing modules. In this ongoing research, we introduce PSY, a\nPosterior Sampling based PrivacY enhancer that can be used in LoRA. We propose\na simple yet effective realization of PSY using posterior sampling, which\neffectively prevents privacy leakage from intermediate information and, in\nturn, preserves the privacy of data owners. We evaluate LoRA extended with PSY\nagainst state-of-the-art membership inference and data extraction attacks. The\nexperiments are executed on three different LLM architectures fine-tuned on\nthree datasets with LoRA. In contrast to the commonly used differential privacy\nmethod, we find that our proposed modification consistently reduces the attack\nsuccess rate. Meanwhile, our method has almost no negative impact on model\nfine-tuning or final performance. Most importantly, PSY reveals a promising\npath toward privacy enhancement with latent space extensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy vulnerabilities in LLMs, such as leakage from memorization, have been\nconstantly identified, and various mitigation proposals have been proposed.\nLoRA is usually used in fine-tuning LLMs and a good entry point to insert\nprivacy-enhancing modules. In this ongoing research, we introduce PSY, a\nPosterior Sampling based PrivacY enhancer that can be used in LoRA. We propose\na simple yet effective realization of PSY using posterior sampling, which\neffectively prevents privacy leakage from intermediate information and, in\nturn, preserves the privacy of data owners. We evaluate LoRA extended with PSY\nagainst state-of-the-art membership inference and data extraction attacks. The\nexperiments are executed on three different LLM architectures fine-tuned on\nthree datasets with LoRA. In contrast to the commonly used differential privacy\nmethod, we find that our proposed modification consistently reduces the attack\nsuccess rate. Meanwhile, our method has almost no negative impact on model\nfine-tuning or final performance. Most importantly, PSY reveals a promising\npath toward privacy enhancement with latent space extensions."
                },
                "authors": [
                    {
                        "name": "Yulian Sun"
                    },
                    {
                        "name": "Li Duan"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05794v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05794v3",
                "updated": "2024-10-24T14:57:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    57,
                    52,
                    3,
                    298,
                    0
                ],
                "published": "2024-06-09T14:11:19Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    14,
                    11,
                    19,
                    6,
                    161,
                    0
                ],
                "title": "RE-RAG: Improving Open-Domain QA Performance and Interpretability with\n  Relevance Estimator in Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RE-RAG: Improving Open-Domain QA Performance and Interpretability with\n  Relevance Estimator in Retrieval-Augmented Generation"
                },
                "summary": "The Retrieval Augmented Generation (RAG) framework utilizes a combination of\nparametric knowledge and external knowledge to demonstrate state-of-the-art\nperformance on open-domain question answering tasks. However, the RAG framework\nsuffers from performance degradation when the query is accompanied by\nirrelevant contexts. In this work, we propose the RE-RAG framework, which\nintroduces a relevance estimator (RE) that not only provides relative relevance\nbetween contexts as previous rerankers did, but also provides confidence, which\ncan be used to classify whether given context is useful for answering the given\nquestion. We propose a weakly supervised method for training the RE simply\nutilizing question-answer data without any labels for correct contexts. We show\nthat RE trained with a small generator (sLM) can not only improve the sLM\nfine-tuned together with RE but also improve previously unreferenced large\nlanguage models (LLMs). Furthermore, we investigate new decoding strategies\nthat utilize the proposed confidence measured by RE such as choosing to let the\nuser know that it is \"unanswerable\" to answer the question given the retrieved\ncontexts or choosing to rely on LLM's parametric knowledge rather than\nunrelated contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Retrieval Augmented Generation (RAG) framework utilizes a combination of\nparametric knowledge and external knowledge to demonstrate state-of-the-art\nperformance on open-domain question answering tasks. However, the RAG framework\nsuffers from performance degradation when the query is accompanied by\nirrelevant contexts. In this work, we propose the RE-RAG framework, which\nintroduces a relevance estimator (RE) that not only provides relative relevance\nbetween contexts as previous rerankers did, but also provides confidence, which\ncan be used to classify whether given context is useful for answering the given\nquestion. We propose a weakly supervised method for training the RE simply\nutilizing question-answer data without any labels for correct contexts. We show\nthat RE trained with a small generator (sLM) can not only improve the sLM\nfine-tuned together with RE but also improve previously unreferenced large\nlanguage models (LLMs). Furthermore, we investigate new decoding strategies\nthat utilize the proposed confidence measured by RE such as choosing to let the\nuser know that it is \"unanswerable\" to answer the question given the retrieved\ncontexts or choosing to rely on LLM's parametric knowledge rather than\nunrelated contexts."
                },
                "authors": [
                    {
                        "name": "Kiseung Kim"
                    },
                    {
                        "name": "Jay-Yoon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jay-Yoon Lee"
                },
                "author": "Jay-Yoon Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05794v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05794v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18808v1",
                "updated": "2024-10-24T14:55:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    55,
                    9,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:55:09Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    55,
                    9,
                    3,
                    298,
                    0
                ],
                "title": "Delving into the Reversal Curse: How Far Can Large Language Models\n  Generalize?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delving into the Reversal Curse: How Far Can Large Language Models\n  Generalize?"
                },
                "summary": "While large language models (LLMs) showcase unprecedented capabilities, they\nalso exhibit certain inherent limitations when facing seemingly trivial tasks.\nA prime example is the recently debated \"reversal curse\", which surfaces when\nmodels, having been trained on the fact \"A is B\", struggle to generalize this\nknowledge to infer that \"B is A\". In this paper, we examine the manifestation\nof the reversal curse across various tasks and delve into both the\ngeneralization abilities and the problem-solving mechanisms of LLMs. This\ninvestigation leads to a series of significant insights: (1) LLMs are able to\ngeneralize to \"B is A\" when both A and B are presented in the context as in the\ncase of a multiple-choice question. (2) This generalization ability is highly\ncorrelated to the structure of the fact \"A is B\" in the training documents. For\nexample, this generalization only applies to biographies structured in \"[Name]\nis [Description]\" but not to \"[Description] is [Name]\". (3) We propose and\nverify the hypothesis that LLMs possess an inherent bias in fact recalling\nduring knowledge application, which explains and underscores the importance of\nthe document structure to successful learning. (4) The negative impact of this\nbias on the downstream performance of LLMs can hardly be mitigated through\ntraining alone. Based on these intriguing findings, our work not only presents\na novel perspective for interpreting LLMs' generalization abilities from their\nintrinsic working mechanism but also provides new insights for the development\nof more effective learning methods for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) showcase unprecedented capabilities, they\nalso exhibit certain inherent limitations when facing seemingly trivial tasks.\nA prime example is the recently debated \"reversal curse\", which surfaces when\nmodels, having been trained on the fact \"A is B\", struggle to generalize this\nknowledge to infer that \"B is A\". In this paper, we examine the manifestation\nof the reversal curse across various tasks and delve into both the\ngeneralization abilities and the problem-solving mechanisms of LLMs. This\ninvestigation leads to a series of significant insights: (1) LLMs are able to\ngeneralize to \"B is A\" when both A and B are presented in the context as in the\ncase of a multiple-choice question. (2) This generalization ability is highly\ncorrelated to the structure of the fact \"A is B\" in the training documents. For\nexample, this generalization only applies to biographies structured in \"[Name]\nis [Description]\" but not to \"[Description] is [Name]\". (3) We propose and\nverify the hypothesis that LLMs possess an inherent bias in fact recalling\nduring knowledge application, which explains and underscores the importance of\nthe document structure to successful learning. (4) The negative impact of this\nbias on the downstream performance of LLMs can hardly be mitigated through\ntraining alone. Based on these intriguing findings, our work not only presents\na novel perspective for interpreting LLMs' generalization abilities from their\nintrinsic working mechanism but also provides new insights for the development\nof more effective learning methods for LLMs."
                },
                "authors": [
                    {
                        "name": "Zhengkai Lin"
                    },
                    {
                        "name": "Zhihang Fu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Binbin Lin"
                    },
                    {
                        "name": "Wenxiao Wang"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18798v1",
                "updated": "2024-10-24T14:50:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    50,
                    42,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:50:42Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    50,
                    42,
                    3,
                    298,
                    0
                ],
                "title": "Distill Visual Chart Reasoning Ability from LLMs to MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distill Visual Chart Reasoning Ability from LLMs to MLLMs"
                },
                "summary": "Solving complex chart Q&A tasks requires advanced visual reasoning abilities\nin multimodal large language models (MLLMs). Recent studies highlight that\nthese abilities consist of two main parts: recognizing key information from\nvisual inputs and conducting reasoning over it. Thus, a promising approach to\nenhance MLLMs is to construct relevant training data focusing on the two\naspects. However, collecting and annotating complex charts and questions is\ncostly and time-consuming, and ensuring the quality of annotated answers\nremains a challenge. In this paper, we propose Code-as-Intermediary Translation\n(CIT), a cost-effective, efficient and easily scalable data synthesis method\nfor distilling visual reasoning abilities from LLMs to MLLMs. The code serves\nas an intermediary that translates visual chart representations into textual\nrepresentations, enabling LLMs to understand cross-modal information.\nSpecifically, we employ text-based synthesizing techniques to construct\nchart-plotting code and produce ReachQA, a dataset containing 3k\nreasoning-intensive charts and 20k Q&A pairs to enhance both recognition and\nreasoning abilities. Experiments show that when fine-tuned with our data,\nmodels not only perform well on chart-related benchmarks, but also demonstrate\nimproved multimodal reasoning abilities on general mathematical benchmarks like\nMathVista. The code and dataset are publicly available at\nhttps://github.com/hewei2001/ReachQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving complex chart Q&A tasks requires advanced visual reasoning abilities\nin multimodal large language models (MLLMs). Recent studies highlight that\nthese abilities consist of two main parts: recognizing key information from\nvisual inputs and conducting reasoning over it. Thus, a promising approach to\nenhance MLLMs is to construct relevant training data focusing on the two\naspects. However, collecting and annotating complex charts and questions is\ncostly and time-consuming, and ensuring the quality of annotated answers\nremains a challenge. In this paper, we propose Code-as-Intermediary Translation\n(CIT), a cost-effective, efficient and easily scalable data synthesis method\nfor distilling visual reasoning abilities from LLMs to MLLMs. The code serves\nas an intermediary that translates visual chart representations into textual\nrepresentations, enabling LLMs to understand cross-modal information.\nSpecifically, we employ text-based synthesizing techniques to construct\nchart-plotting code and produce ReachQA, a dataset containing 3k\nreasoning-intensive charts and 20k Q&A pairs to enhance both recognition and\nreasoning abilities. Experiments show that when fine-tuned with our data,\nmodels not only perform well on chart-related benchmarks, but also demonstrate\nimproved multimodal reasoning abilities on general mathematical benchmarks like\nMathVista. The code and dataset are publicly available at\nhttps://github.com/hewei2001/ReachQA."
                },
                "authors": [
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Wanxu Zhao"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Yiwen Ding"
                    },
                    {
                        "name": "Zifei Shan"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "Under review. The code and dataset are publicly available at\n  https://github.com/hewei2001/ReachQA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18793v1",
                "updated": "2024-10-24T14:47:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    47,
                    28,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:47:28Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    47,
                    28,
                    3,
                    298,
                    0
                ],
                "title": "Adapting MLOps for Diverse In-Network Intelligence in 6G Era: Challenges\n  and Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting MLOps for Diverse In-Network Intelligence in 6G Era: Challenges\n  and Solutions"
                },
                "summary": "Seamless integration of artificial intelligence (AI) and machine learning\n(ML) techniques with wireless systems is a crucial step for 6G AInization.\nHowever, such integration faces challenges in terms of model functionality and\nlifecycle management. ML operations (MLOps) offer a systematic approach to\ntackle these challenges. Existing approaches toward implementing MLOps in a\ncentralized platform often overlook the challenges posed by diverse learning\nparadigms and network heterogeneity. This article provides a new approach to\nMLOps targeting the intricacies of future wireless networks. Considering unique\naspects of the future radio access network (RAN), we formulate three\noperational pipelines, namely reinforcement learning operations (RLOps),\nfederated learning operations (FedOps), and generative AI operations (GenOps).\nThese pipelines form the foundation for seamlessly integrating various\nlearning/inference capabilities into networks. We outline the specific\nchallenges and proposed solutions for each operation, facilitating large-scale\ndeployment of AI-Native 6G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seamless integration of artificial intelligence (AI) and machine learning\n(ML) techniques with wireless systems is a crucial step for 6G AInization.\nHowever, such integration faces challenges in terms of model functionality and\nlifecycle management. ML operations (MLOps) offer a systematic approach to\ntackle these challenges. Existing approaches toward implementing MLOps in a\ncentralized platform often overlook the challenges posed by diverse learning\nparadigms and network heterogeneity. This article provides a new approach to\nMLOps targeting the intricacies of future wireless networks. Considering unique\naspects of the future radio access network (RAN), we formulate three\noperational pipelines, namely reinforcement learning operations (RLOps),\nfederated learning operations (FedOps), and generative AI operations (GenOps).\nThese pipelines form the foundation for seamlessly integrating various\nlearning/inference capabilities into networks. We outline the specific\nchallenges and proposed solutions for each operation, facilitating large-scale\ndeployment of AI-Native 6G networks."
                },
                "authors": [
                    {
                        "name": "Peizheng Li"
                    },
                    {
                        "name": "Ioannis Mavromatis"
                    },
                    {
                        "name": "Tim Farnham"
                    },
                    {
                        "name": "Adnan Aijaz"
                    },
                    {
                        "name": "Aftab Khan"
                    }
                ],
                "author_detail": {
                    "name": "Aftab Khan"
                },
                "author": "Aftab Khan",
                "arxiv_comment": "7 pages, 5 figures. This paper has been submitted to IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18792v1",
                "updated": "2024-10-24T14:47:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    47,
                    25,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:47:25Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    47,
                    25,
                    3,
                    298,
                    0
                ],
                "title": "An LLM Agent for Automatic Geospatial Data Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM Agent for Automatic Geospatial Data Analysis"
                },
                "summary": "Large language models (LLMs) are being used in data science code generation\ntasks, but they often struggle with complex sequential tasks, leading to\nlogical errors. Their application to geospatial data processing is particularly\nchallenging due to difficulties in incorporating complex data structures and\nspatial constraints, effectively utilizing diverse function calls, and the\ntendency to hallucinate less-used geospatial libraries. To tackle these\nproblems, we introduce GeoAgent, a new interactive framework designed to help\nLLMs handle geospatial data processing more effectively. GeoAgent pioneers the\nintegration of a code interpreter, static analysis, and Retrieval-Augmented\nGeneration (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm,\noffering a novel approach to geospatial data processing. In addition, we\ncontribute a new benchmark specifically designed to evaluate the LLM-based\napproach in geospatial tasks. This benchmark leverages a variety of Python\nlibraries and includes both single-turn and multi-turn tasks such as data\nacquisition, data analysis, and visualization. By offering a comprehensive\nevaluation among diverse geospatial contexts, this benchmark sets a new\nstandard for developing LLM-based approaches in geospatial data analysis tasks.\nOur findings suggest that relying solely on knowledge of LLM is insufficient\nfor accurate geospatial task programming, which requires coherent multi-step\nprocesses and multiple function calls. Compared to the baseline LLMs, the\nproposed GeoAgent has demonstrated superior performance, yielding notable\nimprovements in function calls and task completion. In addition, these results\noffer valuable insights for the future development of LLM agents in automatic\ngeospatial data analysis task programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are being used in data science code generation\ntasks, but they often struggle with complex sequential tasks, leading to\nlogical errors. Their application to geospatial data processing is particularly\nchallenging due to difficulties in incorporating complex data structures and\nspatial constraints, effectively utilizing diverse function calls, and the\ntendency to hallucinate less-used geospatial libraries. To tackle these\nproblems, we introduce GeoAgent, a new interactive framework designed to help\nLLMs handle geospatial data processing more effectively. GeoAgent pioneers the\nintegration of a code interpreter, static analysis, and Retrieval-Augmented\nGeneration (RAG) techniques within a Monte Carlo Tree Search (MCTS) algorithm,\noffering a novel approach to geospatial data processing. In addition, we\ncontribute a new benchmark specifically designed to evaluate the LLM-based\napproach in geospatial tasks. This benchmark leverages a variety of Python\nlibraries and includes both single-turn and multi-turn tasks such as data\nacquisition, data analysis, and visualization. By offering a comprehensive\nevaluation among diverse geospatial contexts, this benchmark sets a new\nstandard for developing LLM-based approaches in geospatial data analysis tasks.\nOur findings suggest that relying solely on knowledge of LLM is insufficient\nfor accurate geospatial task programming, which requires coherent multi-step\nprocesses and multiple function calls. Compared to the baseline LLMs, the\nproposed GeoAgent has demonstrated superior performance, yielding notable\nimprovements in function calls and task completion. In addition, these results\noffer valuable insights for the future development of LLM agents in automatic\ngeospatial data analysis task programming."
                },
                "authors": [
                    {
                        "name": "Yuxing Chen"
                    },
                    {
                        "name": "Weijie Wang"
                    },
                    {
                        "name": "Sylvain Lobry"
                    },
                    {
                        "name": "Camille Kurtz"
                    }
                ],
                "author_detail": {
                    "name": "Camille Kurtz"
                },
                "author": "Camille Kurtz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18790v1",
                "updated": "2024-10-24T14:43:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    43,
                    35,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:43:35Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    43,
                    35,
                    3,
                    298,
                    0
                ],
                "title": "Large Generative AI Models meet Open Networks for 6G: Integration,\n  Platform, and Monetization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Generative AI Models meet Open Networks for 6G: Integration,\n  Platform, and Monetization"
                },
                "summary": "Generative artificial intelligence (GAI) has emerged as a pivotal technology\nfor content generation, reasoning, and decision-making, making it a promising\nsolution on the 6G stage characterized by openness, connected intelligence, and\nservice democratization. This article explores strategies for integrating and\nmonetizing GAI within future open 6G networks, mainly from the perspectives of\nmobile network operators (MNOs). We propose a novel API-centric telecoms GAI\nmarketplace platform, designed to serve as a central hub for deploying,\nmanaging, and monetizing diverse GAI services directly within the network. This\nplatform underpins a flexible and interoperable ecosystem, enhances service\ndelivery, and facilitates seamless integration of GAI capabilities across\nvarious network segments, thereby enabling new revenue streams through\ncustomer-centric generative services. Results from experimental evaluation in\nan end-to-end Open RAN testbed, show the latency benefits of this platform for\nlocal large language model (LLM) deployment, by comparing token timing for\nvarious generated lengths with cloud-based general-purpose LLMs. Lastly, the\narticle discusses key considerations for implementing the GAI marketplace\nwithin 6G networks, including monetization strategy, regulatory, management,\nand service platform aspects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence (GAI) has emerged as a pivotal technology\nfor content generation, reasoning, and decision-making, making it a promising\nsolution on the 6G stage characterized by openness, connected intelligence, and\nservice democratization. This article explores strategies for integrating and\nmonetizing GAI within future open 6G networks, mainly from the perspectives of\nmobile network operators (MNOs). We propose a novel API-centric telecoms GAI\nmarketplace platform, designed to serve as a central hub for deploying,\nmanaging, and monetizing diverse GAI services directly within the network. This\nplatform underpins a flexible and interoperable ecosystem, enhances service\ndelivery, and facilitates seamless integration of GAI capabilities across\nvarious network segments, thereby enabling new revenue streams through\ncustomer-centric generative services. Results from experimental evaluation in\nan end-to-end Open RAN testbed, show the latency benefits of this platform for\nlocal large language model (LLM) deployment, by comparing token timing for\nvarious generated lengths with cloud-based general-purpose LLMs. Lastly, the\narticle discusses key considerations for implementing the GAI marketplace\nwithin 6G networks, including monetization strategy, regulatory, management,\nand service platform aspects."
                },
                "authors": [
                    {
                        "name": "Peizheng Li"
                    },
                    {
                        "name": "Adrián Sánchez-Mompó"
                    },
                    {
                        "name": "Tim Farnham"
                    },
                    {
                        "name": "Aftab Khan"
                    },
                    {
                        "name": "Adnan Aijaz"
                    }
                ],
                "author_detail": {
                    "name": "Adnan Aijaz"
                },
                "author": "Adnan Aijaz",
                "arxiv_comment": "8 pages, 6 figures. This paper has been submitted to IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04199v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04199v3",
                "updated": "2024-10-24T14:43:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    43,
                    22,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-05T15:33:25Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    15,
                    33,
                    25,
                    5,
                    279,
                    0
                ],
                "title": "LongGenBench: Long-context Generation Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongGenBench: Long-context Generation Benchmark"
                },
                "summary": "Current long-context benchmarks primarily focus on retrieval-based tests,\nrequiring Large Language Models (LLMs) to locate specific information within\nextensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark.\nLong-context generation refers to the ability of a language model to generate\ncoherent and contextually accurate text that spans across lengthy passages or\ndocuments. While recent studies show strong performance on NIAH and other\nretrieval-based long-context benchmarks, there is a significant lack of\nbenchmarks for evaluating long-context generation capabilities. To bridge this\ngap and offer a comprehensive assessment, we introduce a synthetic benchmark,\nLongGenBench, which allows for flexible configurations of customized generation\ncontext lengths. LongGenBench advances beyond traditional benchmarks by\nredesigning the format of questions and necessitating that LLMs respond with a\nsingle, cohesive long-context answer. Upon extensive evaluation using\nLongGenBench, we observe that: (1) both API accessed and open source models\nexhibit performance degradation in long-context generation scenarios, ranging\nfrom 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of\nperformance degradation, with the Gemini-1.5-Flash model showing the least\ndegradation among API accessed models, and the Qwen2 series exhibiting the\nleast degradation in LongGenBench among open source models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current long-context benchmarks primarily focus on retrieval-based tests,\nrequiring Large Language Models (LLMs) to locate specific information within\nextensive input contexts, such as the needle-in-a-haystack (NIAH) benchmark.\nLong-context generation refers to the ability of a language model to generate\ncoherent and contextually accurate text that spans across lengthy passages or\ndocuments. While recent studies show strong performance on NIAH and other\nretrieval-based long-context benchmarks, there is a significant lack of\nbenchmarks for evaluating long-context generation capabilities. To bridge this\ngap and offer a comprehensive assessment, we introduce a synthetic benchmark,\nLongGenBench, which allows for flexible configurations of customized generation\ncontext lengths. LongGenBench advances beyond traditional benchmarks by\nredesigning the format of questions and necessitating that LLMs respond with a\nsingle, cohesive long-context answer. Upon extensive evaluation using\nLongGenBench, we observe that: (1) both API accessed and open source models\nexhibit performance degradation in long-context generation scenarios, ranging\nfrom 1.2% to 47.1%; (2) different series of LLMs exhibit varying trends of\nperformance degradation, with the Gemini-1.5-Flash model showing the least\ndegradation among API accessed models, and the Qwen2 series exhibiting the\nleast degradation in LongGenBench among open source models."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "EMNLP 2024 https://github.com/Dominic789654/LongGenBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04199v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04199v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09836v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09836v3",
                "updated": "2024-10-24T14:35:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    35,
                    43,
                    3,
                    298,
                    0
                ],
                "published": "2024-04-15T14:44:08Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    14,
                    44,
                    8,
                    0,
                    106,
                    0
                ],
                "title": "How Far Have We Gone in Binary Code Understanding Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Have We Gone in Binary Code Understanding Using Large Language\n  Models"
                },
                "summary": "Binary code analysis plays a pivotal role in various software security\napplications, such as software maintenance, malware detection, software\nvulnerability discovery, patch analysis, etc. However, unlike source code,\nunderstanding binary code is challenging for reverse engineers due to the\nabsence of semantic information. Therefore, automated tools are needed to\nassist human players in interpreting binary code. In recent years, two groups\nof technologies have shown promising prospects: (1) Deep learning-based\ntechnologies have demonstrated competitive results in tasks related to binary\ncode understanding, furthermore, (2) Large Language Models (LLMs) have been\nextensively pre-trained at the source-code level for tasks such as code\nunderstanding and generation. This makes participants wonder about the ability\nof LLMs in binary code understanding.\n  In this work, we propose a benchmark to evaluate the effectiveness of LLMs in\nreal-world reverse engineering scenarios. The benchmark covers two key binary\ncode understanding tasks, including function name recovery and binary code\nsummarization. We gain valuable insights into their capabilities and\nlimitations through extensive evaluations of popular LLMs using our benchmark.\nOur evaluations reveal that existing LLMs can understand binary code to a\ncertain extent, thereby improving the efficiency of binary code analysis. Our\nresults highlight the great potential of the LLMs in advancing the field of\nbinary code understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary code analysis plays a pivotal role in various software security\napplications, such as software maintenance, malware detection, software\nvulnerability discovery, patch analysis, etc. However, unlike source code,\nunderstanding binary code is challenging for reverse engineers due to the\nabsence of semantic information. Therefore, automated tools are needed to\nassist human players in interpreting binary code. In recent years, two groups\nof technologies have shown promising prospects: (1) Deep learning-based\ntechnologies have demonstrated competitive results in tasks related to binary\ncode understanding, furthermore, (2) Large Language Models (LLMs) have been\nextensively pre-trained at the source-code level for tasks such as code\nunderstanding and generation. This makes participants wonder about the ability\nof LLMs in binary code understanding.\n  In this work, we propose a benchmark to evaluate the effectiveness of LLMs in\nreal-world reverse engineering scenarios. The benchmark covers two key binary\ncode understanding tasks, including function name recovery and binary code\nsummarization. We gain valuable insights into their capabilities and\nlimitations through extensive evaluations of popular LLMs using our benchmark.\nOur evaluations reveal that existing LLMs can understand binary code to a\ncertain extent, thereby improving the efficiency of binary code analysis. Our\nresults highlight the great potential of the LLMs in advancing the field of\nbinary code understanding."
                },
                "authors": [
                    {
                        "name": "Xiuwei Shang"
                    },
                    {
                        "name": "Shaoyin Cheng"
                    },
                    {
                        "name": "Guoqiang Chen"
                    },
                    {
                        "name": "Yanming Zhang"
                    },
                    {
                        "name": "Li Hu"
                    },
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Gangyang Li"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "arxiv_comment": "12 pages, 8 figures, to be published in ICSME 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09836v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09836v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18779v1",
                "updated": "2024-10-24T14:31:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    31,
                    52,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:31:52Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    31,
                    52,
                    3,
                    298,
                    0
                ],
                "title": "A Little Help Goes a Long Way: Efficient LLM Training by Leveraging\n  Small LMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Help Goes a Long Way: Efficient LLM Training by Leveraging\n  Small LMs"
                },
                "summary": "A primary challenge in large language model (LLM) development is their\nonerous pre-training cost. Typically, such pre-training involves optimizing a\nself-supervised objective (such as next-token prediction) over a large corpus.\nThis paper explores a promising paradigm to improve LLM pre-training efficiency\nand quality by suitably leveraging a small language model (SLM). In particular,\nthis paradigm relies on an SLM to both (1) provide soft labels as additional\ntraining supervision, and (2) select a small subset of valuable (\"informative\"\nand \"hard\") training examples. Put together, this enables an effective transfer\nof the SLM's predictive distribution to the LLM, while prioritizing specific\nregions of the training data distribution. Empirically, this leads to reduced\nLLM training time compared to standard training, while improving the overall\nquality. Theoretically, we develop a statistical framework to systematically\nstudy the utility of SLMs in enabling efficient training of high-quality LLMs.\nIn particular, our framework characterizes how the SLM's seemingly low-quality\nsupervision can enhance the training of a much more capable LLM. Furthermore,\nit also highlights the need for an adaptive utilization of such supervision, by\nstriking a balance between the bias and variance introduced by the SLM-provided\nsoft labels. We corroborate our theoretical framework by improving the\npre-training of an LLM with 2.8B parameters by utilizing a smaller LM with 1.5B\nparameters on the Pile dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A primary challenge in large language model (LLM) development is their\nonerous pre-training cost. Typically, such pre-training involves optimizing a\nself-supervised objective (such as next-token prediction) over a large corpus.\nThis paper explores a promising paradigm to improve LLM pre-training efficiency\nand quality by suitably leveraging a small language model (SLM). In particular,\nthis paradigm relies on an SLM to both (1) provide soft labels as additional\ntraining supervision, and (2) select a small subset of valuable (\"informative\"\nand \"hard\") training examples. Put together, this enables an effective transfer\nof the SLM's predictive distribution to the LLM, while prioritizing specific\nregions of the training data distribution. Empirically, this leads to reduced\nLLM training time compared to standard training, while improving the overall\nquality. Theoretically, we develop a statistical framework to systematically\nstudy the utility of SLMs in enabling efficient training of high-quality LLMs.\nIn particular, our framework characterizes how the SLM's seemingly low-quality\nsupervision can enhance the training of a much more capable LLM. Furthermore,\nit also highlights the need for an adaptive utilization of such supervision, by\nstriking a balance between the bias and variance introduced by the SLM-provided\nsoft labels. We corroborate our theoretical framework by improving the\npre-training of an LLM with 2.8B parameters by utilizing a smaller LM with 1.5B\nparameters on the Pile dataset."
                },
                "authors": [
                    {
                        "name": "Ankit Singh Rawat"
                    },
                    {
                        "name": "Veeranjaneyulu Sadhanala"
                    },
                    {
                        "name": "Afshin Rostamizadeh"
                    },
                    {
                        "name": "Ayan Chakrabarti"
                    },
                    {
                        "name": "Wittawat Jitkrittum"
                    },
                    {
                        "name": "Vladimir Feinberg"
                    },
                    {
                        "name": "Seungyeon Kim"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Nikunj Saunshi"
                    },
                    {
                        "name": "Zachary Nado"
                    },
                    {
                        "name": "Rakesh Shivanna"
                    },
                    {
                        "name": "Sashank J. Reddi"
                    },
                    {
                        "name": "Aditya Krishna Menon"
                    },
                    {
                        "name": "Rohan Anil"
                    },
                    {
                        "name": "Sanjiv Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Sanjiv Kumar"
                },
                "author": "Sanjiv Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08228v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08228v3",
                "updated": "2024-10-24T14:25:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    25,
                    3,
                    3,
                    298,
                    0
                ],
                "published": "2024-03-13T04:11:41Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    4,
                    11,
                    41,
                    2,
                    73,
                    0
                ],
                "title": "Empowering Robot Path Planning with Large Language Models: osmAG Map\n  Topology & Hierarchy Comprehension with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Robot Path Planning with Large Language Models: osmAG Map\n  Topology & Hierarchy Comprehension with LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated great potential in robotic\napplications by providing essential general knowledge. Mobile robots rely on\nmap comprehension for tasks like localization and navigation. In this paper, we\nexplore enabling LLMs to comprehend the topology and hierarchy of Area Graph, a\ntext-based hierarchical, topometric semantic map representation utilizing\npolygons to demark areas such as rooms or buildings. Our experiments\ndemonstrate that with the right map representation, LLMs can effectively\ncomprehend Area Graph's topology and hierarchy. After straightforward\nfine-tuning, the LLaMA2 models exceeded ChatGPT-3.5 in mastering these aspects.\nOur dataset, dataset generation code, fine-tuned LoRA adapters can be accessed\nat https://github.com/xiefujing/LLM-osmAG-Comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated great potential in robotic\napplications by providing essential general knowledge. Mobile robots rely on\nmap comprehension for tasks like localization and navigation. In this paper, we\nexplore enabling LLMs to comprehend the topology and hierarchy of Area Graph, a\ntext-based hierarchical, topometric semantic map representation utilizing\npolygons to demark areas such as rooms or buildings. Our experiments\ndemonstrate that with the right map representation, LLMs can effectively\ncomprehend Area Graph's topology and hierarchy. After straightforward\nfine-tuning, the LLaMA2 models exceeded ChatGPT-3.5 in mastering these aspects.\nOur dataset, dataset generation code, fine-tuned LoRA adapters can be accessed\nat https://github.com/xiefujing/LLM-osmAG-Comprehension."
                },
                "authors": [
                    {
                        "name": "Fujing Xie"
                    },
                    {
                        "name": "Sören Schwertfeger"
                    }
                ],
                "author_detail": {
                    "name": "Sören Schwertfeger"
                },
                "author": "Sören Schwertfeger",
                "arxiv_comment": "Accepted at IEEE International Conference on Robotics and Biomimetics\n  (ROBIO) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08228v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08228v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18767v1",
                "updated": "2024-10-24T14:19:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    19,
                    59,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:19:59Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    19,
                    59,
                    3,
                    298,
                    0
                ],
                "title": "STAR-RIS-Enabled Full-Duplex Integrated Sensing and Communication System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR-RIS-Enabled Full-Duplex Integrated Sensing and Communication System"
                },
                "summary": "Traditional self-interference cancellation (SIC) methods are common in\nfull-duplex (FD) integrated sensing and communication (ISAC) systems. However,\nexploring new SIC schemes is important due to the limitations of traditional\napproaches. With the challenging limitations of traditional SIC approaches,\nthis paper proposes a novel simultaneous transmitting and reflecting\nreconfigurable intelligent surface (STAR-RIS)-enabled FD ISAC system, where\nSTAR-RIS enhances simultaneous communication and target sensing and reduces\nself-interference (SI) to a level comparable to traditional SIC approaches. The\noptimization of maximizing the sensing signal-to-interference-plus-noise ratio\n(SINR) and the communication sum rate, both crucial for improving sensing\naccuracy and overall communication performance, presents significant challenges\ndue to the non-convex nature of these problems. Therefore, we develop\nalternating optimization algorithms to iteratively tackle these problems.\nSpecifically, we devise the semi-definite relaxation (SDR)-based algorithm for\ntransmit beamformer design. For the reflecting and refracting coefficients\ndesign, we adopt the successive convex approximation (SCA) method and implement\nthe SDR-based algorithm to tackle the quartic and quadratic constraints.\nSimulation results validate the effectiveness of the proposed algorithms and\nshow that the proposed deployment can achieve better performance than that of\nthe benchmark using the traditional SIC approach without STAR-RIS deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional self-interference cancellation (SIC) methods are common in\nfull-duplex (FD) integrated sensing and communication (ISAC) systems. However,\nexploring new SIC schemes is important due to the limitations of traditional\napproaches. With the challenging limitations of traditional SIC approaches,\nthis paper proposes a novel simultaneous transmitting and reflecting\nreconfigurable intelligent surface (STAR-RIS)-enabled FD ISAC system, where\nSTAR-RIS enhances simultaneous communication and target sensing and reduces\nself-interference (SI) to a level comparable to traditional SIC approaches. The\noptimization of maximizing the sensing signal-to-interference-plus-noise ratio\n(SINR) and the communication sum rate, both crucial for improving sensing\naccuracy and overall communication performance, presents significant challenges\ndue to the non-convex nature of these problems. Therefore, we develop\nalternating optimization algorithms to iteratively tackle these problems.\nSpecifically, we devise the semi-definite relaxation (SDR)-based algorithm for\ntransmit beamformer design. For the reflecting and refracting coefficients\ndesign, we adopt the successive convex approximation (SCA) method and implement\nthe SDR-based algorithm to tackle the quartic and quadratic constraints.\nSimulation results validate the effectiveness of the proposed algorithms and\nshow that the proposed deployment can achieve better performance than that of\nthe benchmark using the traditional SIC approach without STAR-RIS deployment."
                },
                "authors": [
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Gaojie Chen"
                    },
                    {
                        "name": "Yun Wen"
                    },
                    {
                        "name": "Qu Luo"
                    },
                    {
                        "name": "Chiya Zhang"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18764v1",
                "updated": "2024-10-24T14:18:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    18,
                    32,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T14:18:32Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    14,
                    18,
                    32,
                    3,
                    298,
                    0
                ],
                "title": "Task Calibration: Calibrating Large Language Models on Inference Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Calibration: Calibrating Large Language Models on Inference Tasks"
                },
                "summary": "Large language models (LLMs) have exhibited impressive zero-shot performance\non inference tasks. However, LLMs may suffer from spurious correlations between\ninput texts and output labels, which limits LLMs' ability to reason based\npurely on general language understanding. In other words, LLMs may make\npredictions primarily based on premise or hypothesis, rather than both\ncomponents. To address this problem that may lead to unexpected performance\ndegradation, we propose task calibration (TC), a zero-shot and inference-only\ncalibration method inspired by mutual information which recovers LLM\nperformance through task reformulation. TC encourages LLMs to reason based on\nboth premise and hypothesis, while mitigating the models' over-reliance on\nindividual premise or hypothesis for inference. Experimental results show that\nTC achieves a substantial improvement on 13 inference tasks in the zero-shot\nsetup. We further validate the effectiveness of TC in few-shot setups and\nvarious natural language understanding tasks. Further analysis indicates that\nTC is also robust to prompt templates and has the potential to be integrated\nwith other calibration methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited impressive zero-shot performance\non inference tasks. However, LLMs may suffer from spurious correlations between\ninput texts and output labels, which limits LLMs' ability to reason based\npurely on general language understanding. In other words, LLMs may make\npredictions primarily based on premise or hypothesis, rather than both\ncomponents. To address this problem that may lead to unexpected performance\ndegradation, we propose task calibration (TC), a zero-shot and inference-only\ncalibration method inspired by mutual information which recovers LLM\nperformance through task reformulation. TC encourages LLMs to reason based on\nboth premise and hypothesis, while mitigating the models' over-reliance on\nindividual premise or hypothesis for inference. Experimental results show that\nTC achieves a substantial improvement on 13 inference tasks in the zero-shot\nsetup. We further validate the effectiveness of TC in few-shot setups and\nvarious natural language understanding tasks. Further analysis indicates that\nTC is also robust to prompt templates and has the potential to be integrated\nwith other calibration methods."
                },
                "authors": [
                    {
                        "name": "Yingjie Li"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Xiaotian Xie"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18749v1",
                "updated": "2024-10-24T13:59:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    59,
                    3,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T13:59:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    59,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "Does Differential Privacy Impact Bias in Pretrained NLP Models?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Differential Privacy Impact Bias in Pretrained NLP Models?"
                },
                "summary": "Differential privacy (DP) is applied when fine-tuning pre-trained large\nlanguage models (LLMs) to limit leakage of training examples. While most DP\nresearch has focused on improving a model's privacy-utility tradeoff, some find\nthat DP can be unfair to or biased against underrepresented groups. In this\nwork, we show the impact of DP on bias in LLMs through empirical analysis.\nDifferentially private training can increase the model bias against protected\ngroups w.r.t AUC-based bias metrics. DP makes it more difficult for the model\nto differentiate between the positive and negative examples from the protected\ngroups and other groups in the rest of the population. Our results also show\nthat the impact of DP on bias is not only affected by the privacy protection\nlevel but also the underlying distribution of the dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential privacy (DP) is applied when fine-tuning pre-trained large\nlanguage models (LLMs) to limit leakage of training examples. While most DP\nresearch has focused on improving a model's privacy-utility tradeoff, some find\nthat DP can be unfair to or biased against underrepresented groups. In this\nwork, we show the impact of DP on bias in LLMs through empirical analysis.\nDifferentially private training can increase the model bias against protected\ngroups w.r.t AUC-based bias metrics. DP makes it more difficult for the model\nto differentiate between the positive and negative examples from the protected\ngroups and other groups in the rest of the population. Our results also show\nthat the impact of DP on bias is not only affected by the privacy protection\nlevel but also the underlying distribution of the dataset."
                },
                "authors": [
                    {
                        "name": "Md. Khairul Islam"
                    },
                    {
                        "name": "Andrew Wang"
                    },
                    {
                        "name": "Tianhao Wang"
                    },
                    {
                        "name": "Yangfeng Ji"
                    },
                    {
                        "name": "Judy Fox"
                    },
                    {
                        "name": "Jieyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieyu Zhao"
                },
                "author": "Jieyu Zhao",
                "arxiv_comment": "Github https://github.com/khairulislam/DP-on-NLP-Bias",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18745v1",
                "updated": "2024-10-24T13:51:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    51,
                    50,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T13:51:50Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    51,
                    50,
                    3,
                    298,
                    0
                ],
                "title": "Why Does the Effective Context Length of LLMs Fall Short?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Does the Effective Context Length of LLMs Fall Short?"
                },
                "summary": "Advancements in distributed training and efficient attention mechanisms have\nsignificantly expanded the context window sizes of large language models\n(LLMs). However, recent work reveals that the effective context lengths of\nopen-source LLMs often fall short, typically not exceeding half of their\ntraining lengths. In this work, we attribute this limitation to the left-skewed\nfrequency distribution of relative positions formed in LLMs pretraining and\npost-training stages, which impedes their ability to effectively gather distant\ninformation. To address this challenge, we introduce ShifTed Rotray position\nembeddING (STRING). STRING shifts well-trained positions to overwrite the\noriginal ineffective positions during inference, enhancing performance within\ntheir existing training lengths. Experimental results show that without\nadditional training, STRING dramatically improves the performance of the latest\nlarge-scale models, such as Llama3.1 70B and Qwen2 72B, by over 10 points on\npopular long-context benchmarks RULER and InfiniteBench, establishing new\nstate-of-the-art results for open-source LLMs. Compared to commercial models,\nLlama 3.1 70B with \\method even achieves better performance than GPT-4-128K and\nclearly surpasses Claude 2 and Kimi-chat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in distributed training and efficient attention mechanisms have\nsignificantly expanded the context window sizes of large language models\n(LLMs). However, recent work reveals that the effective context lengths of\nopen-source LLMs often fall short, typically not exceeding half of their\ntraining lengths. In this work, we attribute this limitation to the left-skewed\nfrequency distribution of relative positions formed in LLMs pretraining and\npost-training stages, which impedes their ability to effectively gather distant\ninformation. To address this challenge, we introduce ShifTed Rotray position\nembeddING (STRING). STRING shifts well-trained positions to overwrite the\noriginal ineffective positions during inference, enhancing performance within\ntheir existing training lengths. Experimental results show that without\nadditional training, STRING dramatically improves the performance of the latest\nlarge-scale models, such as Llama3.1 70B and Qwen2 72B, by over 10 points on\npopular long-context benchmarks RULER and InfiniteBench, establishing new\nstate-of-the-art results for open-source LLMs. Compared to commercial models,\nLlama 3.1 70B with \\method even achieves better performance than GPT-4-128K and\nclearly surpasses Claude 2 and Kimi-chat."
                },
                "authors": [
                    {
                        "name": "Chenxin An"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ming Zhong"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Shansan Gong"
                    },
                    {
                        "name": "Yao Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Lingpeng Kong"
                    }
                ],
                "author_detail": {
                    "name": "Lingpeng Kong"
                },
                "author": "Lingpeng Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18739v1",
                "updated": "2024-10-24T13:42:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    42,
                    5,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T13:42:05Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    42,
                    5,
                    3,
                    298,
                    0
                ],
                "title": "5G Replicates TSN: Extending IEEE 802.1CB Capabilities to Integrated\n  5G/TSN Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "5G Replicates TSN: Extending IEEE 802.1CB Capabilities to Integrated\n  5G/TSN Systems"
                },
                "summary": "The IEEE 802.1 time-sensitive networking (TSN) standards improve real-time\ncapabilities of the standard Ethernet. TSN and local/private 5G systems are\nenvisaged to co-exist in industrial environments. The IEEE 802.1CB standard\nprovides fault tolerance to TSN systems via frame replication and elimination\nfor reliability (FRER) capabilities. This paper presents X-FRER, a novel\nframework for extending FRER capabilities to the 3GPP-defined bridge model for\n5G and TSN integration. The different embodiments of X-FRER realize FRER-like\nfunctionality through multi-path transmissions in a 5G system based on a single\nor multiple protocol data unit (PDU) sessions. X-FRER also provides enhanced\nreplication and elimination functionality for integrated deployments.\nPerformance evaluation shows that X-FRER empowers a vanilla 5G system with\nTSN-like capabilities for end-to-end reliability in integrated TSN and 5G\ndeployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The IEEE 802.1 time-sensitive networking (TSN) standards improve real-time\ncapabilities of the standard Ethernet. TSN and local/private 5G systems are\nenvisaged to co-exist in industrial environments. The IEEE 802.1CB standard\nprovides fault tolerance to TSN systems via frame replication and elimination\nfor reliability (FRER) capabilities. This paper presents X-FRER, a novel\nframework for extending FRER capabilities to the 3GPP-defined bridge model for\n5G and TSN integration. The different embodiments of X-FRER realize FRER-like\nfunctionality through multi-path transmissions in a 5G system based on a single\nor multiple protocol data unit (PDU) sessions. X-FRER also provides enhanced\nreplication and elimination functionality for integrated deployments.\nPerformance evaluation shows that X-FRER empowers a vanilla 5G system with\nTSN-like capabilities for end-to-end reliability in integrated TSN and 5G\ndeployments."
                },
                "authors": [
                    {
                        "name": "Adnan Aijaz"
                    }
                ],
                "author_detail": {
                    "name": "Adnan Aijaz"
                },
                "author": "Adnan Aijaz",
                "arxiv_comment": "To appear in IEEE CSCN 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17439v2",
                "updated": "2024-10-24T13:34:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    34,
                    47,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-22T21:30:58Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    21,
                    30,
                    58,
                    1,
                    296,
                    0
                ],
                "title": "Evaluating AI-Generated Essays with GRE Analytical Writing Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating AI-Generated Essays with GRE Analytical Writing Assessment"
                },
                "summary": "The recent revolutionary advance in generative AI enables the generation of\nrealistic and coherent texts by large language models (LLMs). Despite many\nexisting evaluation metrics on the quality of the generated texts, there is\nstill a lack of rigorous assessment of how well LLMs perform in complex and\ndemanding writing assessments. This study examines essays generated by ten\nleading LLMs for the analytical writing assessment of the Graduate Record Exam\n(GRE). We assessed these essays using both human raters and the e-rater\nautomated scoring engine as used in the GRE scoring pipeline. Notably, the\ntop-performing Gemini and GPT-4o received an average score of 4.78 and 4.67,\nrespectively, falling between \"generally thoughtful, well-developed analysis of\nthe issue and conveys meaning clearly\" and \"presents a competent analysis of\nthe issue and conveys meaning with acceptable clarity\" according to the GRE\nscoring guideline. We also evaluated the detection accuracy of these essays,\nwith detectors trained on essays generated by the same and different LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent revolutionary advance in generative AI enables the generation of\nrealistic and coherent texts by large language models (LLMs). Despite many\nexisting evaluation metrics on the quality of the generated texts, there is\nstill a lack of rigorous assessment of how well LLMs perform in complex and\ndemanding writing assessments. This study examines essays generated by ten\nleading LLMs for the analytical writing assessment of the Graduate Record Exam\n(GRE). We assessed these essays using both human raters and the e-rater\nautomated scoring engine as used in the GRE scoring pipeline. Notably, the\ntop-performing Gemini and GPT-4o received an average score of 4.78 and 4.67,\nrespectively, falling between \"generally thoughtful, well-developed analysis of\nthe issue and conveys meaning clearly\" and \"presents a competent analysis of\nthe issue and conveys meaning with acceptable clarity\" according to the GRE\nscoring guideline. We also evaluated the detection accuracy of these essays,\nwith detectors trained on essays generated by the same and different LLMs."
                },
                "authors": [
                    {
                        "name": "Yang Zhong"
                    },
                    {
                        "name": "Jiangang Hao"
                    },
                    {
                        "name": "Michael Fauss"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Yuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Wang"
                },
                "author": "Yuan Wang",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12381v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12381v2",
                "updated": "2024-10-24T13:33:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    33,
                    58,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-16T09:04:57Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    4,
                    57,
                    2,
                    290,
                    0
                ],
                "title": "HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of\n  Large Multimodal Models Through Coding Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of\n  Large Multimodal Models Through Coding Tasks"
                },
                "summary": "Coding tasks have been valuable for evaluating Large Language Models (LLMs),\nas they demand the comprehension of high-level instructions, complex reasoning,\nand the implementation of functional programs -- core capabilities for\nadvancing Artificial General Intelligence. Despite the progress in Large\nMultimodal Models (LMMs), which extend LLMs with visual perception and\nunderstanding capabilities, there remains a notable lack of coding benchmarks\nthat rigorously assess these models, particularly in tasks that emphasize\nvisual reasoning. To address this gap, we introduce HumanEval-V, a novel and\nlightweight benchmark specifically designed to evaluate LMMs' visual\nunderstanding and reasoning capabilities through code generation. HumanEval-V\nincludes 108 carefully crafted, entry-level Python coding tasks derived from\nplatforms like CodeForces and Stack Overflow. Each task is adapted by modifying\nthe context and algorithmic patterns of the original problems, with visual\nelements redrawn to ensure distinction from the source, preventing potential\ndata leakage. LMMs are required to complete the code solution based on the\nprovided visual context and a predefined Python function signature outlining\nthe task requirements. Every task is equipped with meticulously handcrafted\ntest cases to ensure a thorough and reliable evaluation of model-generated\nsolutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering\nsignificant challenges. Proprietary models like GPT-4o achieve only 13% pass@1\nand 36.4% pass@10, while open-weight models with 70B parameters score below 4%\npass@1. Ablation studies further reveal the limitations of current LMMs in\nvision reasoning and coding capabilities. These results underscore key areas\nfor future research to enhance LMMs' capabilities. We have open-sourced our\ncode and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coding tasks have been valuable for evaluating Large Language Models (LLMs),\nas they demand the comprehension of high-level instructions, complex reasoning,\nand the implementation of functional programs -- core capabilities for\nadvancing Artificial General Intelligence. Despite the progress in Large\nMultimodal Models (LMMs), which extend LLMs with visual perception and\nunderstanding capabilities, there remains a notable lack of coding benchmarks\nthat rigorously assess these models, particularly in tasks that emphasize\nvisual reasoning. To address this gap, we introduce HumanEval-V, a novel and\nlightweight benchmark specifically designed to evaluate LMMs' visual\nunderstanding and reasoning capabilities through code generation. HumanEval-V\nincludes 108 carefully crafted, entry-level Python coding tasks derived from\nplatforms like CodeForces and Stack Overflow. Each task is adapted by modifying\nthe context and algorithmic patterns of the original problems, with visual\nelements redrawn to ensure distinction from the source, preventing potential\ndata leakage. LMMs are required to complete the code solution based on the\nprovided visual context and a predefined Python function signature outlining\nthe task requirements. Every task is equipped with meticulously handcrafted\ntest cases to ensure a thorough and reliable evaluation of model-generated\nsolutions. We evaluate 19 state-of-the-art LMMs using HumanEval-V, uncovering\nsignificant challenges. Proprietary models like GPT-4o achieve only 13% pass@1\nand 36.4% pass@10, while open-weight models with 70B parameters score below 4%\npass@1. Ablation studies further reveal the limitations of current LMMs in\nvision reasoning and coding capabilities. These results underscore key areas\nfor future research to enhance LMMs' capabilities. We have open-sourced our\ncode and benchmark at https://github.com/HumanEval-V/HumanEval-V-Benchmark."
                },
                "authors": [
                    {
                        "name": "Fengji Zhang"
                    },
                    {
                        "name": "Linquan Wu"
                    },
                    {
                        "name": "Huiyu Bai"
                    },
                    {
                        "name": "Guancheng Lin"
                    },
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "Jacky Keung"
                    }
                ],
                "author_detail": {
                    "name": "Jacky Keung"
                },
                "author": "Jacky Keung",
                "arxiv_comment": "homepage https://humaneval-v.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12381v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12381v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18718v1",
                "updated": "2024-10-24T13:22:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    22,
                    50,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T13:22:50Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    22,
                    50,
                    3,
                    298,
                    0
                ],
                "title": "LLM-based Online Prediction of Time-varying Graph Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Online Prediction of Time-varying Graph Signals"
                },
                "summary": "In this paper, we propose a novel framework that leverages large language\nmodels (LLMs) for predicting missing values in time-varying graph signals by\nexploiting spatial and temporal smoothness. We leverage the power of LLM to\nachieve a message-passing scheme. For each missing node, its neighbors and\nprevious estimates are fed into and processed by LLM to infer the missing\nobservations. Tested on the task of the online prediction of wind-speed graph\nsignals, our model outperforms online graph filtering algorithms in terms of\naccuracy, demonstrating the potential of LLMs in effectively addressing\npartially observed signals in graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel framework that leverages large language\nmodels (LLMs) for predicting missing values in time-varying graph signals by\nexploiting spatial and temporal smoothness. We leverage the power of LLM to\nachieve a message-passing scheme. For each missing node, its neighbors and\nprevious estimates are fed into and processed by LLM to infer the missing\nobservations. Tested on the task of the online prediction of wind-speed graph\nsignals, our model outperforms online graph filtering algorithms in terms of\naccuracy, demonstrating the potential of LLMs in effectively addressing\npartially observed signals in graphs."
                },
                "authors": [
                    {
                        "name": "Dayu Qin"
                    },
                    {
                        "name": "Yi Yan"
                    },
                    {
                        "name": "Ercan Engin Kuruoglu"
                    }
                ],
                "author_detail": {
                    "name": "Ercan Engin Kuruoglu"
                },
                "author": "Ercan Engin Kuruoglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18717v1",
                "updated": "2024-10-24T13:22:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    22,
                    33,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T13:22:33Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    13,
                    22,
                    33,
                    3,
                    298,
                    0
                ],
                "title": "Low-Latency Video Anonymization for Crowd Anomaly Detection: Privacy vs.\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Latency Video Anonymization for Crowd Anomaly Detection: Privacy vs.\n  Performance"
                },
                "summary": "Recent advancements in artificial intelligence promise ample potential in\nmonitoring applications with surveillance cameras. However, concerns about\nprivacy and model bias have made it challenging to utilize them in public.\nAlthough de-identification approaches have been proposed in the literature,\naiming to achieve a certain level of anonymization, most of them employ deep\nlearning models that are computationally demanding for real-time edge\ndeployment. In this study, we revisit conventional anonymization solutions for\nprivacy protection and real-time video anomaly detection (VAD) applications. We\npropose a novel lightweight adaptive anonymization for VAD (LA3D) that employs\ndynamic adjustment to enhance privacy protection. We evaluated the approaches\non publicly available privacy and VAD data sets to examine the strengths and\nweaknesses of the different anonymization techniques and highlight the\npromising efficacy of our approach. Our experiment demonstrates that LA3D\nenables substantial improvement in the privacy anonymization capability without\nmajorly degrading VAD efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in artificial intelligence promise ample potential in\nmonitoring applications with surveillance cameras. However, concerns about\nprivacy and model bias have made it challenging to utilize them in public.\nAlthough de-identification approaches have been proposed in the literature,\naiming to achieve a certain level of anonymization, most of them employ deep\nlearning models that are computationally demanding for real-time edge\ndeployment. In this study, we revisit conventional anonymization solutions for\nprivacy protection and real-time video anomaly detection (VAD) applications. We\npropose a novel lightweight adaptive anonymization for VAD (LA3D) that employs\ndynamic adjustment to enhance privacy protection. We evaluated the approaches\non publicly available privacy and VAD data sets to examine the strengths and\nweaknesses of the different anonymization techniques and highlight the\npromising efficacy of our approach. Our experiment demonstrates that LA3D\nenables substantial improvement in the privacy anonymization capability without\nmajorly degrading VAD efficacy."
                },
                "authors": [
                    {
                        "name": "Mulugeta Weldezgina Asres"
                    },
                    {
                        "name": "Lei Jiao"
                    },
                    {
                        "name": "Christian Walter Omlin"
                    }
                ],
                "author_detail": {
                    "name": "Christian Walter Omlin"
                },
                "author": "Christian Walter Omlin",
                "arxiv_comment": "16pages, 8 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18703v1",
                "updated": "2024-10-24T12:59:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    59,
                    5,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T12:59:05Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    59,
                    5,
                    3,
                    298,
                    0
                ],
                "title": "Whose fault is it anyway? SILC: Safe Integration of LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whose fault is it anyway? SILC: Safe Integration of LLM-Generated Code"
                },
                "summary": "In modern software development, multiple software components, often sourced\nfrom different contributors, including AI assistants, are combined to create a\ncohesive system. Although these components might each be individually safe,\ntheir composition might not be so. At the core of this issue is often a\nmisalignment between the requirements and assumptions made by each component.\nOnce discovered it is important to determine which component is accountable for\naddressing the misalignment issue and to prevent its occurrence in the future.\n  In this work we propose SILC, a framework for localising fault, i.e. blame,\nand for assigning sanitization obligations to prevent memory issues resulting\nfrom the composition of multiple software components. In particular, we show\nthe role Incorrectness Logic could have in automatically extracting implicit\nnon-functional assumptions in auto-generated code and render them explicit in\norder to detect misalignment with the requirements in existing code. In other\nwords, we are looking at the problem of code comprehension from a perspective\nfocused on safety properties rather than the traditional approach centered on\nfunctionality. To do that, we enhance Incorrectness Separation Logic with\ncapabilities for fault tracking and sanitization insertion. We show the\nbenefits of this framework by running experiments on millions of lines of code\nfrom open source projects where parts of existing functionality are regenerated\nby AI assistants. We empirically show that AI assistants produce unsafe code\nand demonstrate the utility of our framework in proposing appropriate blame and\nsanitization obligations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern software development, multiple software components, often sourced\nfrom different contributors, including AI assistants, are combined to create a\ncohesive system. Although these components might each be individually safe,\ntheir composition might not be so. At the core of this issue is often a\nmisalignment between the requirements and assumptions made by each component.\nOnce discovered it is important to determine which component is accountable for\naddressing the misalignment issue and to prevent its occurrence in the future.\n  In this work we propose SILC, a framework for localising fault, i.e. blame,\nand for assigning sanitization obligations to prevent memory issues resulting\nfrom the composition of multiple software components. In particular, we show\nthe role Incorrectness Logic could have in automatically extracting implicit\nnon-functional assumptions in auto-generated code and render them explicit in\norder to detect misalignment with the requirements in existing code. In other\nwords, we are looking at the problem of code comprehension from a perspective\nfocused on safety properties rather than the traditional approach centered on\nfunctionality. To do that, we enhance Incorrectness Separation Logic with\ncapabilities for fault tracking and sanitization insertion. We show the\nbenefits of this framework by running experiments on millions of lines of code\nfrom open source projects where parts of existing functionality are regenerated\nby AI assistants. We empirically show that AI assistants produce unsafe code\nand demonstrate the utility of our framework in proposing appropriate blame and\nsanitization obligations."
                },
                "authors": [
                    {
                        "name": "Peisen Lin"
                    },
                    {
                        "name": "Yuntong Zhang"
                    },
                    {
                        "name": "Andreea Costea"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18702v1",
                "updated": "2024-10-24T12:56:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    56,
                    1,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T12:56:01Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    56,
                    1,
                    3,
                    298,
                    0
                ],
                "title": "GrammaMT: Improving Machine Translation with Grammar-Informed In-Context\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrammaMT: Improving Machine Translation with Grammar-Informed In-Context\n  Learning"
                },
                "summary": "We introduce GrammaMT, a grammatically-aware prompting approach for machine\ntranslation that uses Interlinear Glossed Text (IGT), a common form of\nlinguistic description providing morphological and lexical annotations for\nsource sentences. GrammaMT proposes three prompting strategies: gloss-shot,\nchain-gloss and model-gloss. All are training-free, requiring only a few\nexamples that involve minimal effort to collect, and making them well-suited\nfor low-resource setups. Experiments show that GrammaMT enhances translation\nperformance on open-source instruction-tuned LLMs for various low- to\nhigh-resource languages across three benchmarks: (1) the largest IGT corpus,\n(2) the challenging 2023 SIGMORPHON Shared Task data over endangered languages,\nand (3) even in an out-of-domain setting with FLORES. Moreover, ablation\nstudies reveal that leveraging gloss resources could substantially boost MT\nperformance (by over 17 BLEU points) if LLMs accurately generate or access\ninput sentence glosses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GrammaMT, a grammatically-aware prompting approach for machine\ntranslation that uses Interlinear Glossed Text (IGT), a common form of\nlinguistic description providing morphological and lexical annotations for\nsource sentences. GrammaMT proposes three prompting strategies: gloss-shot,\nchain-gloss and model-gloss. All are training-free, requiring only a few\nexamples that involve minimal effort to collect, and making them well-suited\nfor low-resource setups. Experiments show that GrammaMT enhances translation\nperformance on open-source instruction-tuned LLMs for various low- to\nhigh-resource languages across three benchmarks: (1) the largest IGT corpus,\n(2) the challenging 2023 SIGMORPHON Shared Task data over endangered languages,\nand (3) even in an out-of-domain setting with FLORES. Moreover, ablation\nstudies reveal that leveraging gloss resources could substantially boost MT\nperformance (by over 17 BLEU points) if LLMs accurately generate or access\ninput sentence glosses."
                },
                "authors": [
                    {
                        "name": "Rita Ramos"
                    },
                    {
                        "name": "Everlyn Asiko Chimoto"
                    },
                    {
                        "name": "Maartje ter Hoeve"
                    },
                    {
                        "name": "Natalie Schluter"
                    }
                ],
                "author_detail": {
                    "name": "Natalie Schluter"
                },
                "author": "Natalie Schluter",
                "arxiv_comment": "Under review at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18701v1",
                "updated": "2024-10-24T12:53:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    53,
                    39,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T12:53:39Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    53,
                    39,
                    3,
                    298,
                    0
                ],
                "title": "BATON: Enhancing Batch-wise Inference Efficiency for Large Language\n  Models via Dynamic Re-batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BATON: Enhancing Batch-wise Inference Efficiency for Large Language\n  Models via Dynamic Re-batching"
                },
                "summary": "The advanced capabilities of Large Language Models (LLMs) have inspired the\ndevelopment of various interactive web services or applications, such as\nChatGPT, which offer query inference services for users. Unlike traditional DNN\nmodel, the inference of LLM entails different iterations of forward computation\nfor different queries, which result in efficiency challenges for existing\nrun-to-completion batch-wise inference. Hence, some methods refine batch-wise\ninference to iteration-level by duplicating all nonlinear layers of LLM.\nHowever, this approach not only increases resource usage but also introduces\nidle computations to the batch due to the prefilling of newly added queries.\nTherefore, we propose BATON, an efficient batch-wise LLM inference scheme by\ndynamically adjusting processing batch, which can achieve near-zero idle\ncomputations without incurring additional resource consumption. To do so, BATON\n1) shapes the vectors involved in the inference of the newly inserted query and\nprocessing batch to align dimensions and generates a new attention mask based\non vector shaping to ensure inference correctness, which enables query\ninserting without consuming additional resource; 2) embeds prefilled Keys and\nValues of the new query into the KV_Cache of the processing batch by leveraging\nthe prefilling and decoding separation mechanism, eliminating idle computations\nto the batch introduced by the prefilling process of the new query.\nExperimental results show that compared to the state-of-the-art solution Orca,\nBATON improves query processing by up to 1.75 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advanced capabilities of Large Language Models (LLMs) have inspired the\ndevelopment of various interactive web services or applications, such as\nChatGPT, which offer query inference services for users. Unlike traditional DNN\nmodel, the inference of LLM entails different iterations of forward computation\nfor different queries, which result in efficiency challenges for existing\nrun-to-completion batch-wise inference. Hence, some methods refine batch-wise\ninference to iteration-level by duplicating all nonlinear layers of LLM.\nHowever, this approach not only increases resource usage but also introduces\nidle computations to the batch due to the prefilling of newly added queries.\nTherefore, we propose BATON, an efficient batch-wise LLM inference scheme by\ndynamically adjusting processing batch, which can achieve near-zero idle\ncomputations without incurring additional resource consumption. To do so, BATON\n1) shapes the vectors involved in the inference of the newly inserted query and\nprocessing batch to align dimensions and generates a new attention mask based\non vector shaping to ensure inference correctness, which enables query\ninserting without consuming additional resource; 2) embeds prefilled Keys and\nValues of the new query into the KV_Cache of the processing batch by leveraging\nthe prefilling and decoding separation mechanism, eliminating idle computations\nto the batch introduced by the prefilling process of the new query.\nExperimental results show that compared to the state-of-the-art solution Orca,\nBATON improves query processing by up to 1.75 times."
                },
                "authors": [
                    {
                        "name": "Peizhuang Cong"
                    },
                    {
                        "name": "Qizhi Chen"
                    },
                    {
                        "name": "Haochen Zhao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18697v1",
                "updated": "2024-10-24T12:48:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    48,
                    3,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T12:48:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    48,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "How Good Are LLMs for Literary Translation, Really? Literary Translation\n  Evaluation with Humans and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Good Are LLMs for Literary Translation, Really? Literary Translation\n  Evaluation with Humans and LLMs"
                },
                "summary": "Recent research has focused on literary machine translation (MT) as a new\nchallenge in MT. However, the evaluation of literary MT remains an open\nproblem. We contribute to this ongoing discussion by introducing\nLITEVAL-CORPUS, a paragraph-level parallel corpus comprising multiple verified\nhuman translations and outputs from 9 MT systems, which totals over 2k\nparagraphs and includes 13k annotated sentences across four language pairs,\ncosting 4.5k Euro. This corpus enables us to (i) examine the consistency and\nadequacy of multiple annotation schemes, (ii) compare evaluations by students\nand professionals, and (iii) assess the effectiveness of LLM-based metrics. We\nfind that Multidimensional Quality Metrics (MQM), as the de facto standard in\nnon-literary human MT evaluation, is inadequate for literary translation: While\nBest-Worst Scaling (BWS) with students and Scalar Quality Metric (SQM) with\nprofessional translators prefer human translations at rates of ~82% and ~94%,\nrespectively, MQM with student annotators prefers human professional\ntranslations over the translations of the best-performing LLMs in only ~42% of\ncases. While automatic metrics generally show a moderate correlation with human\nMQM and SQM, they struggle to accurately identify human translations, with\nrates of at most ~20%. Our overall evaluation indicates that human professional\ntranslations consistently outperform LLM translations, where even the most\nrecent LLMs tend to produce more literal and less diverse translations compared\nto human translations. However, newer LLMs such as GPT-4o perform substantially\nbetter than older ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has focused on literary machine translation (MT) as a new\nchallenge in MT. However, the evaluation of literary MT remains an open\nproblem. We contribute to this ongoing discussion by introducing\nLITEVAL-CORPUS, a paragraph-level parallel corpus comprising multiple verified\nhuman translations and outputs from 9 MT systems, which totals over 2k\nparagraphs and includes 13k annotated sentences across four language pairs,\ncosting 4.5k Euro. This corpus enables us to (i) examine the consistency and\nadequacy of multiple annotation schemes, (ii) compare evaluations by students\nand professionals, and (iii) assess the effectiveness of LLM-based metrics. We\nfind that Multidimensional Quality Metrics (MQM), as the de facto standard in\nnon-literary human MT evaluation, is inadequate for literary translation: While\nBest-Worst Scaling (BWS) with students and Scalar Quality Metric (SQM) with\nprofessional translators prefer human translations at rates of ~82% and ~94%,\nrespectively, MQM with student annotators prefers human professional\ntranslations over the translations of the best-performing LLMs in only ~42% of\ncases. While automatic metrics generally show a moderate correlation with human\nMQM and SQM, they struggle to accurately identify human translations, with\nrates of at most ~20%. Our overall evaluation indicates that human professional\ntranslations consistently outperform LLM translations, where even the most\nrecent LLMs tend to produce more literal and less diverse translations compared\nto human translations. However, newer LLMs such as GPT-4o perform substantially\nbetter than older ones."
                },
                "authors": [
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05628v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05628v3",
                "updated": "2024-10-24T12:47:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    47,
                    56,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-08T02:23:53Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    2,
                    23,
                    53,
                    1,
                    282,
                    0
                ],
                "title": "Versatile Motion Language Models for Multi-Turn Interactive Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Versatile Motion Language Models for Multi-Turn Interactive Agents"
                },
                "summary": "Recent advancements in large language models (LLMs) have greatly enhanced\ntheir ability to generate natural and contextually relevant text, making AI\ninteractions more human-like. However, generating and understanding interactive\nhuman-like motion, where two individuals engage in coordinated movements,\nremains a challenge due to the complexity of modeling these coordinated\ninteractions. Furthermore, a versatile model is required to handle diverse\ninteractive scenarios, such as chat systems that follow user instructions or\nadapt to their assigned role while adjusting interaction dynamics. To tackle\nthis problem, we introduce VIM, short for the Versatile Interactive Motion\nlanguage model, which integrates both language and motion modalities to\neffectively understand, generate, and control interactive motions in multi-turn\nconversational contexts. To address the scarcity of multi-turn interactive\nmotion data, we introduce a synthetic dataset, INERT-MT2, where we utilize\npre-trained models to create diverse instructional datasets with interactive\nmotion. Our approach first trains a motion tokenizer that encodes interactive\nmotions into residual discrete tokens. In the pretraining stage, the model\nlearns to align motion and text representations with these discrete tokens.\nDuring the instruction fine-tuning stage, VIM adapts to multi-turn\nconversations using the INTER-MT2 dataset. We evaluate the versatility of our\nmethod across motion-related tasks, motion to text, text to motion, reaction\ngeneration, motion editing, and reasoning about motion sequences. The results\nhighlight the versatility and effectiveness of proposed method in handling\ncomplex interactive motion synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have greatly enhanced\ntheir ability to generate natural and contextually relevant text, making AI\ninteractions more human-like. However, generating and understanding interactive\nhuman-like motion, where two individuals engage in coordinated movements,\nremains a challenge due to the complexity of modeling these coordinated\ninteractions. Furthermore, a versatile model is required to handle diverse\ninteractive scenarios, such as chat systems that follow user instructions or\nadapt to their assigned role while adjusting interaction dynamics. To tackle\nthis problem, we introduce VIM, short for the Versatile Interactive Motion\nlanguage model, which integrates both language and motion modalities to\neffectively understand, generate, and control interactive motions in multi-turn\nconversational contexts. To address the scarcity of multi-turn interactive\nmotion data, we introduce a synthetic dataset, INERT-MT2, where we utilize\npre-trained models to create diverse instructional datasets with interactive\nmotion. Our approach first trains a motion tokenizer that encodes interactive\nmotions into residual discrete tokens. In the pretraining stage, the model\nlearns to align motion and text representations with these discrete tokens.\nDuring the instruction fine-tuning stage, VIM adapts to multi-turn\nconversations using the INTER-MT2 dataset. We evaluate the versatility of our\nmethod across motion-related tasks, motion to text, text to motion, reaction\ngeneration, motion editing, and reasoning about motion sequences. The results\nhighlight the versatility and effectiveness of proposed method in handling\ncomplex interactive motion synthesis."
                },
                "authors": [
                    {
                        "name": "Jeongeun Park"
                    },
                    {
                        "name": "Sungjoon Choi"
                    },
                    {
                        "name": "Sangdoo Yun"
                    }
                ],
                "author_detail": {
                    "name": "Sangdoo Yun"
                },
                "author": "Sangdoo Yun",
                "arxiv_comment": "https://vim-motion-language.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05628v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05628v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18693v1",
                "updated": "2024-10-24T12:42:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    42,
                    4,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T12:42:04Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    42,
                    4,
                    3,
                    298,
                    0
                ],
                "title": "Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis\n  from Scratch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis\n  from Scratch"
                },
                "summary": "The availability of high-quality data is one of the most important factors in\nimproving the reasoning capability of LLMs. Existing works have demonstrated\nthe effectiveness of creating more instruction data from seed questions or\nknowledge bases. Recent research indicates that continually scaling up data\nsynthesis from strong models (e.g., GPT-4) can further elicit reasoning\nperformance. Though promising, the open-sourced community still lacks\nhigh-quality data at scale and scalable data synthesis methods with affordable\ncosts. To address this, we introduce ScaleQuest, a scalable and novel data\nsynthesis method that utilizes \"small-size\" (e.g., 7B) open-source models to\ngenerate questions from scratch without the need for seed data with complex\naugmentation constraints. With the efficient ScaleQuest, we automatically\nconstructed a mathematical reasoning dataset consisting of 1 million\nproblem-solution pairs, which are more effective than existing open-sourced\ndatasets. It can universally increase the performance of mainstream open-source\nmodels (i.e., Mistral, Llama3, DeepSeekMath, and Qwen2-Math) by achieving 29.2%\nto 46.4% gains on MATH. Notably, simply fine-tuning the Qwen2-Math-7B-Base\nmodel with our dataset can even surpass Qwen2-Math-7B-Instruct, a strong and\nwell-aligned model on closed-source data, and proprietary models such as\nGPT-4-Turbo and Claude-3.5 Sonnet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of high-quality data is one of the most important factors in\nimproving the reasoning capability of LLMs. Existing works have demonstrated\nthe effectiveness of creating more instruction data from seed questions or\nknowledge bases. Recent research indicates that continually scaling up data\nsynthesis from strong models (e.g., GPT-4) can further elicit reasoning\nperformance. Though promising, the open-sourced community still lacks\nhigh-quality data at scale and scalable data synthesis methods with affordable\ncosts. To address this, we introduce ScaleQuest, a scalable and novel data\nsynthesis method that utilizes \"small-size\" (e.g., 7B) open-source models to\ngenerate questions from scratch without the need for seed data with complex\naugmentation constraints. With the efficient ScaleQuest, we automatically\nconstructed a mathematical reasoning dataset consisting of 1 million\nproblem-solution pairs, which are more effective than existing open-sourced\ndatasets. It can universally increase the performance of mainstream open-source\nmodels (i.e., Mistral, Llama3, DeepSeekMath, and Qwen2-Math) by achieving 29.2%\nto 46.4% gains on MATH. Notably, simply fine-tuning the Qwen2-Math-7B-Base\nmodel with our dataset can even surpass Qwen2-Math-7B-Instruct, a strong and\nwell-aligned model on closed-source data, and proprietary models such as\nGPT-4-Turbo and Claude-3.5 Sonnet."
                },
                "authors": [
                    {
                        "name": "Yuyang Ding"
                    },
                    {
                        "name": "Xinyu Shi"
                    },
                    {
                        "name": "Xiaobo Liang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qiaoming Zhu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Preprint. Project page: https://scalequest.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18686v1",
                "updated": "2024-10-24T12:32:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    32,
                    19,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T12:32:19Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    32,
                    19,
                    3,
                    298,
                    0
                ],
                "title": "Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced\n  Time Series Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced\n  Time Series Classification"
                },
                "summary": "Leveraging large language models (LLMs) has garnered increasing attention and\nintroduced novel perspectives in time series classification. However, existing\napproaches often overlook the crucial dynamic temporal information inherent in\ntime series data and face challenges in aligning this data with textual\nsemantics. To address these limitations, we propose HiTime, a hierarchical\nmulti-modal model that seamlessly integrates temporal information into LLMs for\nmultivariate time series classification (MTSC). Our model employs a\nhierarchical feature encoder to capture diverse aspects of time series data\nthrough both data-specific and task-specific embeddings. To facilitate semantic\nspace alignment between time series and text, we introduce a dual-view\ncontrastive alignment module that bridges the gap between modalities.\nAdditionally, we adopt a hybrid prompting strategy to fine-tune the pre-trained\nLLM in a parameter-efficient manner. By effectively incorporating dynamic\ntemporal features and ensuring semantic alignment, HiTime enables LLMs to\nprocess continuous time series data and achieves state-of-the-art\nclassification performance through text generation. Extensive experiments on\nbenchmark datasets demonstrate that HiTime significantly enhances time series\nclassification accuracy compared to most competitive baseline methods. Our\nfindings highlight the potential of integrating temporal features into LLMs,\npaving the way for advanced time series analysis. The code is publicly\navailable for further research and validation. Our codes are publicly\navailable1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging large language models (LLMs) has garnered increasing attention and\nintroduced novel perspectives in time series classification. However, existing\napproaches often overlook the crucial dynamic temporal information inherent in\ntime series data and face challenges in aligning this data with textual\nsemantics. To address these limitations, we propose HiTime, a hierarchical\nmulti-modal model that seamlessly integrates temporal information into LLMs for\nmultivariate time series classification (MTSC). Our model employs a\nhierarchical feature encoder to capture diverse aspects of time series data\nthrough both data-specific and task-specific embeddings. To facilitate semantic\nspace alignment between time series and text, we introduce a dual-view\ncontrastive alignment module that bridges the gap between modalities.\nAdditionally, we adopt a hybrid prompting strategy to fine-tune the pre-trained\nLLM in a parameter-efficient manner. By effectively incorporating dynamic\ntemporal features and ensuring semantic alignment, HiTime enables LLMs to\nprocess continuous time series data and achieves state-of-the-art\nclassification performance through text generation. Extensive experiments on\nbenchmark datasets demonstrate that HiTime significantly enhances time series\nclassification accuracy compared to most competitive baseline methods. Our\nfindings highlight the potential of integrating temporal features into LLMs,\npaving the way for advanced time series analysis. The code is publicly\navailable for further research and validation. Our codes are publicly\navailable1."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Tao"
                    },
                    {
                        "name": "Tingyue Pan"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Yucong Luo"
                    }
                ],
                "author_detail": {
                    "name": "Yucong Luo"
                },
                "author": "Yucong Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17820v2",
                "updated": "2024-10-24T12:01:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    12,
                    1,
                    31,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-23T12:26:10Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    12,
                    26,
                    10,
                    2,
                    297,
                    0
                ],
                "title": "Understanding When Tree of Thoughts Succeeds: Larger Models Excel in\n  Generation, Not Discrimination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding When Tree of Thoughts Succeeds: Larger Models Excel in\n  Generation, Not Discrimination"
                },
                "summary": "Tree of Thoughts (ToT) is a reasoning strategy for Large Language Models\n(LLMs) that employs a generator to suggest reasoning steps and a discriminator\nto decide which steps to implement. ToT demonstrates strong performance on\nreasoning tasks, often surpassing simple methods such as Input-Output (IO)\nprompting and Chain-of-Thought (CoT) reasoning. However, ToT does not\nconsistently outperform such simpler methods across all models, leaving large\nknowledge gaps on the conditions under which ToT is most beneficial. In this\npaper, we analyze the roles of the generator and discriminator separately to\nbetter understand the conditions when ToT is beneficial. We find that the\ngenerator plays a more critical role than the discriminator in driving the\nsuccess of ToT. Scaling the generator leads to notable improvements in ToT\nperformance, even when using a smaller model as the discriminator, whereas\nscaling the discriminator with a fixed generator yields only marginal gains.\nOur results show that models across different scales exhibit comparable\ndiscrimination capabilities, yet differ significantly in their generative\nperformance for ToT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) is a reasoning strategy for Large Language Models\n(LLMs) that employs a generator to suggest reasoning steps and a discriminator\nto decide which steps to implement. ToT demonstrates strong performance on\nreasoning tasks, often surpassing simple methods such as Input-Output (IO)\nprompting and Chain-of-Thought (CoT) reasoning. However, ToT does not\nconsistently outperform such simpler methods across all models, leaving large\nknowledge gaps on the conditions under which ToT is most beneficial. In this\npaper, we analyze the roles of the generator and discriminator separately to\nbetter understand the conditions when ToT is beneficial. We find that the\ngenerator plays a more critical role than the discriminator in driving the\nsuccess of ToT. Scaling the generator leads to notable improvements in ToT\nperformance, even when using a smaller model as the discriminator, whereas\nscaling the discriminator with a fixed generator yields only marginal gains.\nOur results show that models across different scales exhibit comparable\ndiscrimination capabilities, yet differ significantly in their generative\nperformance for ToT."
                },
                "authors": [
                    {
                        "name": "Qiqi Chen"
                    },
                    {
                        "name": "Xinpeng Wang"
                    },
                    {
                        "name": "Philipp Mondorf"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "Code: github.com/mainlp/tot-eval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18652v1",
                "updated": "2024-10-24T11:32:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    32,
                    0,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T11:32:00Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    32,
                    0,
                    3,
                    298,
                    0
                ],
                "title": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$C^2$: Scalable Auto-Feedback for LLM-based Chart Generation"
                },
                "summary": "Generating high-quality charts with Large Language Models presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. Instruction, data, and code triplets are scarce and expensive\nto manually curate as their creation demands technical expertise. To address\nthis scalability issue, we introduce a reference-free automatic feedback\ngenerator, which eliminates the need for costly human intervention. Our novel\nframework, $C^2$, consists of (1) an automatic feedback provider (ChartAF) and\n(2) a diverse, reference-free dataset (ChartUIE-8K). Quantitative results are\ncompelling: in our first experiment, 74% of respondents strongly preferred, and\n10% preferred, the results after feedback. The second post-feedback experiment\ndemonstrates that ChartAF outperforms nine baselines. Moreover, ChartUIE-8K\nsignificantly improves data diversity by increasing queries, datasets, and\nchart types by 5982%, 1936%, and 91%, respectively, over benchmarks. Finally,\nan LLM user study revealed that 94% of participants preferred ChartUIE-8K's\nqueries, with 93% deeming them aligned with real-world use cases. Core\ncontributions are available as open-source at an anonymized project site, with\nample qualitative examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-quality charts with Large Language Models presents\nsignificant challenges due to limited data and the high cost of scaling through\nhuman curation. Instruction, data, and code triplets are scarce and expensive\nto manually curate as their creation demands technical expertise. To address\nthis scalability issue, we introduce a reference-free automatic feedback\ngenerator, which eliminates the need for costly human intervention. Our novel\nframework, $C^2$, consists of (1) an automatic feedback provider (ChartAF) and\n(2) a diverse, reference-free dataset (ChartUIE-8K). Quantitative results are\ncompelling: in our first experiment, 74% of respondents strongly preferred, and\n10% preferred, the results after feedback. The second post-feedback experiment\ndemonstrates that ChartAF outperforms nine baselines. Moreover, ChartUIE-8K\nsignificantly improves data diversity by increasing queries, datasets, and\nchart types by 5982%, 1936%, and 91%, respectively, over benchmarks. Finally,\nan LLM user study revealed that 94% of participants preferred ChartUIE-8K's\nqueries, with 93% deeming them aligned with real-world use cases. Core\ncontributions are available as open-source at an anonymized project site, with\nample qualitative examples."
                },
                "authors": [
                    {
                        "name": "Woosung Koh"
                    },
                    {
                        "name": "Jang Han Yoon"
                    },
                    {
                        "name": "MinHyung Lee"
                    },
                    {
                        "name": "Youngjin Song"
                    },
                    {
                        "name": "Jaegwan Cho"
                    },
                    {
                        "name": "Jaehyun Kang"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Se-young Yun"
                    },
                    {
                        "name": "Youngjae Yu"
                    },
                    {
                        "name": "Bongshin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Bongshin Lee"
                },
                "author": "Bongshin Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13728v2",
                "updated": "2024-10-24T11:30:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    30,
                    33,
                    3,
                    298,
                    0
                ],
                "published": "2024-09-09T22:36:35Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    22,
                    36,
                    35,
                    0,
                    253,
                    0
                ],
                "title": "Rule Extrapolation in Language Models: A Study of Compositional\n  Generalization on OOD Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rule Extrapolation in Language Models: A Study of Compositional\n  Generalization on OOD Prompts"
                },
                "summary": "LLMs show remarkable emergent abilities, such as inferring concepts from\npresumably out-of-distribution prompts, known as in-context learning. Though\nthis success is often attributed to the Transformer architecture, our\nsystematic understanding is limited. In complex real-world data sets, even\ndefining what is out-of-distribution is not obvious. To better understand the\nOOD behaviour of autoregressive LLMs, we focus on formal languages, which are\ndefined by the intersection of rules. We define a new scenario of OOD\ncompositional generalization, termed rule extrapolation. Rule extrapolation\ndescribes OOD scenarios, where the prompt violates at least one rule. We\nevaluate rule extrapolation in formal languages with varying complexity in\nlinear and recurrent architectures, the Transformer, and state space models to\nunderstand the architectures' influence on rule extrapolation. We also lay the\nfirst stones of a normative theory of rule extrapolation, inspired by the\nSolomonoff prior in algorithmic information theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs show remarkable emergent abilities, such as inferring concepts from\npresumably out-of-distribution prompts, known as in-context learning. Though\nthis success is often attributed to the Transformer architecture, our\nsystematic understanding is limited. In complex real-world data sets, even\ndefining what is out-of-distribution is not obvious. To better understand the\nOOD behaviour of autoregressive LLMs, we focus on formal languages, which are\ndefined by the intersection of rules. We define a new scenario of OOD\ncompositional generalization, termed rule extrapolation. Rule extrapolation\ndescribes OOD scenarios, where the prompt violates at least one rule. We\nevaluate rule extrapolation in formal languages with varying complexity in\nlinear and recurrent architectures, the Transformer, and state space models to\nunderstand the architectures' influence on rule extrapolation. We also lay the\nfirst stones of a normative theory of rule extrapolation, inspired by the\nSolomonoff prior in algorithmic information theory."
                },
                "authors": [
                    {
                        "name": "Anna Mészáros"
                    },
                    {
                        "name": "Szilvia Ujváry"
                    },
                    {
                        "name": "Wieland Brendel"
                    },
                    {
                        "name": "Patrik Reizinger"
                    },
                    {
                        "name": "Ferenc Huszár"
                    }
                ],
                "author_detail": {
                    "name": "Ferenc Huszár"
                },
                "author": "Ferenc Huszár",
                "arxiv_comment": "Accepted as a spotlight poster at NeurIPS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19255v2",
                "updated": "2024-10-24T11:29:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    29,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-09-28T06:04:56Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    6,
                    4,
                    56,
                    5,
                    272,
                    0
                ],
                "title": "DENEB: A Hallucination-Robust Automatic Evaluation Metric for Image\n  Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DENEB: A Hallucination-Robust Automatic Evaluation Metric for Image\n  Captioning"
                },
                "summary": "In this work, we address the challenge of developing automatic evaluation\nmetrics for image captioning, with a particular focus on robustness against\nhallucinations. Existing metrics are often inadequate for handling\nhallucinations, primarily due to their limited ability to compare candidate\ncaptions with multifaceted reference captions. To address this shortcoming, we\npropose DENEB, a novel supervised automatic evaluation metric specifically\nrobust against hallucinations. DENEB incorporates the Sim-Vec Transformer, a\nmechanism that processes multiple references simultaneously, thereby\nefficiently capturing the similarity between an image, a candidate caption, and\nreference captions. To train DENEB, we construct the diverse and balanced\nNebula dataset comprising 32,978 images, paired with human judgments provided\nby 805 annotators. We demonstrated that DENEB achieves state-of-the-art\nperformance among existing LLM-free metrics on the FOIL, Composite,\nFlickr8K-Expert, Flickr8K-CF, Nebula, and PASCAL-50S datasets, validating its\neffectiveness and robustness against hallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we address the challenge of developing automatic evaluation\nmetrics for image captioning, with a particular focus on robustness against\nhallucinations. Existing metrics are often inadequate for handling\nhallucinations, primarily due to their limited ability to compare candidate\ncaptions with multifaceted reference captions. To address this shortcoming, we\npropose DENEB, a novel supervised automatic evaluation metric specifically\nrobust against hallucinations. DENEB incorporates the Sim-Vec Transformer, a\nmechanism that processes multiple references simultaneously, thereby\nefficiently capturing the similarity between an image, a candidate caption, and\nreference captions. To train DENEB, we construct the diverse and balanced\nNebula dataset comprising 32,978 images, paired with human judgments provided\nby 805 annotators. We demonstrated that DENEB achieves state-of-the-art\nperformance among existing LLM-free metrics on the FOIL, Composite,\nFlickr8K-Expert, Flickr8K-CF, Nebula, and PASCAL-50S datasets, validating its\neffectiveness and robustness against hallucinations."
                },
                "authors": [
                    {
                        "name": "Kazuki Matsuda"
                    },
                    {
                        "name": "Yuiga Wada"
                    },
                    {
                        "name": "Komei Sugiura"
                    }
                ],
                "author_detail": {
                    "name": "Komei Sugiura"
                },
                "author": "Komei Sugiura",
                "arxiv_comment": "ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18646v1",
                "updated": "2024-10-24T11:18:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    18,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T11:18:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    18,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "Metro-scale QKD using multimode fiber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metro-scale QKD using multimode fiber"
                },
                "summary": "We report a proof-of-principle realisation of a decoy-state BB84 QKD protocol\nwith phase encoding over a record-breaking 17 km of MMF at a rate of 193\nkbits/s, as well as over 1 Mbit/s at a distance of 1 km. These results suggest\nthat QKD can be deployed over MMF in metropolitan-scale telecommunication\nconnections. Such MMF metropolitan networks are ubiquitous - thus this advance\ncould pave the way to wide scale metropolitan deployment. We also assess the\nadvantages of adapting the OM3 channel using mode-matching photonic lanterns on\nthe QBER, signal gain, and key rate and compare different encoding techniques\nin light of MMF propagation effects. This work confirms the suitability of\ncurrent QKD technology for use in existing MMF links, unlocking new\nopportunities for quantum applications using legacy fibre.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report a proof-of-principle realisation of a decoy-state BB84 QKD protocol\nwith phase encoding over a record-breaking 17 km of MMF at a rate of 193\nkbits/s, as well as over 1 Mbit/s at a distance of 1 km. These results suggest\nthat QKD can be deployed over MMF in metropolitan-scale telecommunication\nconnections. Such MMF metropolitan networks are ubiquitous - thus this advance\ncould pave the way to wide scale metropolitan deployment. We also assess the\nadvantages of adapting the OM3 channel using mode-matching photonic lanterns on\nthe QBER, signal gain, and key rate and compare different encoding techniques\nin light of MMF propagation effects. This work confirms the suitability of\ncurrent QKD technology for use in existing MMF links, unlocking new\nopportunities for quantum applications using legacy fibre."
                },
                "authors": [
                    {
                        "name": "Adam Brzosko"
                    },
                    {
                        "name": "Robert I Woodward"
                    },
                    {
                        "name": "Yuen San Lo"
                    },
                    {
                        "name": "Mirko Pittaluga"
                    },
                    {
                        "name": "Peter Raymond Smith"
                    },
                    {
                        "name": "James F Dynes"
                    },
                    {
                        "name": "Andrew J Shields"
                    }
                ],
                "author_detail": {
                    "name": "Andrew J Shields"
                },
                "author": "Andrew J Shields",
                "arxiv_doi": "10.1364/OPTICAQ.534258",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1364/OPTICAQ.534258",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.18646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 6 figures, published in Optica Quantum",
                "arxiv_journal_ref": "Optica Quantum 2, 365-370 (2024)",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18641v1",
                "updated": "2024-10-24T11:10:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    10,
                    54,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T11:10:54Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    11,
                    10,
                    54,
                    3,
                    298,
                    0
                ],
                "title": "Smart ETL and LLM-based contents classification: the European Smart\n  Tourism Tools Observatory experience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart ETL and LLM-based contents classification: the European Smart\n  Tourism Tools Observatory experience"
                },
                "summary": "Purpose: Our research project focuses on improving the content update of the\nonline European Smart Tourism Tools (STTs) Observatory by incorporating and\ncategorizing STTs. The categorization is based on their taxonomy, and it\nfacilitates the end user's search process. The use of a Smart ETL (Extract,\nTransform, and Load) process, where \\emph{Smart} indicates the use of\nArtificial Intelligence (AI), is central to this endeavor.\n  Methods: The contents describing STTs are derived from PDF catalogs, where\nPDF-scraping techniques extract QR codes, images, links, and text information.\nDuplicate STTs between the catalogs are removed, and the remaining ones are\nclassified based on their text information using Large Language Models (LLMs).\nFinally, the data is transformed to comply with the Dublin Core metadata\nstructure (the observatory's metadata structure), chosen for its wide\nacceptance and flexibility.\n  Results: The Smart ETL process to import STTs to the observatory combines\nPDF-scraping techniques with LLMs for text content-based classification. Our\npreliminary results have demonstrated the potential of LLMs for text\ncontent-based classification.\n  Conclusion: The proposed approach's feasibility is a step towards efficient\ncontent-based classification, not only in Smart Tourism but also adaptable to\nother fields. Future work will mainly focus on refining this classification\nprocess.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: Our research project focuses on improving the content update of the\nonline European Smart Tourism Tools (STTs) Observatory by incorporating and\ncategorizing STTs. The categorization is based on their taxonomy, and it\nfacilitates the end user's search process. The use of a Smart ETL (Extract,\nTransform, and Load) process, where \\emph{Smart} indicates the use of\nArtificial Intelligence (AI), is central to this endeavor.\n  Methods: The contents describing STTs are derived from PDF catalogs, where\nPDF-scraping techniques extract QR codes, images, links, and text information.\nDuplicate STTs between the catalogs are removed, and the remaining ones are\nclassified based on their text information using Large Language Models (LLMs).\nFinally, the data is transformed to comply with the Dublin Core metadata\nstructure (the observatory's metadata structure), chosen for its wide\nacceptance and flexibility.\n  Results: The Smart ETL process to import STTs to the observatory combines\nPDF-scraping techniques with LLMs for text content-based classification. Our\npreliminary results have demonstrated the potential of LLMs for text\ncontent-based classification.\n  Conclusion: The proposed approach's feasibility is a step towards efficient\ncontent-based classification, not only in Smart Tourism but also adaptable to\nother fields. Future work will mainly focus on refining this classification\nprocess."
                },
                "authors": [
                    {
                        "name": "Diogo Cosme"
                    },
                    {
                        "name": "António Galvão"
                    },
                    {
                        "name": "Fernando Brito e Abreu"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Brito e Abreu"
                },
                "author": "Fernando Brito e Abreu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; I.2.7; I.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05008v2",
                "updated": "2024-10-24T10:41:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    41,
                    23,
                    3,
                    298,
                    0
                ],
                "published": "2024-05-08T12:24:52Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    12,
                    24,
                    52,
                    2,
                    129,
                    0
                ],
                "title": "ADELIE: Aligning Large Language Models on Information Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADELIE: Aligning Large Language Models on Information Extraction"
                },
                "summary": "Large language models (LLMs) usually fall short on information extraction\n(IE) tasks and struggle to follow the complex instructions of IE tasks. This\nprimarily arises from LLMs not being aligned with humans, as mainstream\nalignment datasets typically do not include IE data. In this paper, we\nintroduce ADELIE (Aligning large language moDELs on Information Extraction), an\naligned LLM that effectively solves various IE tasks, including closed IE, open\nIE, and on-demand IE. We first collect and construct a high-quality alignment\ncorpus IEInstruct for IE. Then we train ADELIE_SFT using instruction tuning on\nIEInstruct. We further train ADELIE_SFT with direct preference optimization\n(DPO) objective, resulting in ADELIE_DPO. Extensive experiments on various\nheld-out IE datasets demonstrate that our models (ADELIE_SFT and ADELIE_DPO)\nachieve state-of-the-art (SoTA) performance among open-source models. We\nfurther explore the general capabilities of ADELIE, and experimental results\nreveal that their general capabilities do not exhibit a noticeable decline. We\nwill release the code, data, and models to facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) usually fall short on information extraction\n(IE) tasks and struggle to follow the complex instructions of IE tasks. This\nprimarily arises from LLMs not being aligned with humans, as mainstream\nalignment datasets typically do not include IE data. In this paper, we\nintroduce ADELIE (Aligning large language moDELs on Information Extraction), an\naligned LLM that effectively solves various IE tasks, including closed IE, open\nIE, and on-demand IE. We first collect and construct a high-quality alignment\ncorpus IEInstruct for IE. Then we train ADELIE_SFT using instruction tuning on\nIEInstruct. We further train ADELIE_SFT with direct preference optimization\n(DPO) objective, resulting in ADELIE_DPO. Extensive experiments on various\nheld-out IE datasets demonstrate that our models (ADELIE_SFT and ADELIE_DPO)\nachieve state-of-the-art (SoTA) performance among open-source models. We\nfurther explore the general capabilities of ADELIE, and experimental results\nreveal that their general capabilities do not exhibit a noticeable decline. We\nwill release the code, data, and models to facilitate further research."
                },
                "authors": [
                    {
                        "name": "Yunjia Qi"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "Accepted at EMNLP 2024. Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18624v1",
                "updated": "2024-10-24T10:32:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    32,
                    10,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T10:32:10Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    32,
                    10,
                    3,
                    298,
                    0
                ],
                "title": "Prompting and Fine-Tuning of Small LLMs for Length-Controllable\n  Telephone Call Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting and Fine-Tuning of Small LLMs for Length-Controllable\n  Telephone Call Summarization"
                },
                "summary": "This paper explores the rapid development of a telephone call summarization\nsystem utilizing large language models (LLMs). Our approach involves initial\nexperiments with prompting existing LLMs to generate summaries of telephone\nconversations, followed by the creation of a tailored synthetic training\ndataset utilizing stronger frontier models. We place special focus on the\ndiversity of the generated data and on the ability to control the length of the\ngenerated summaries to meet various use-case specific requirements. The\neffectiveness of our method is evaluated using two state-of-the-art\nLLM-as-a-judge-based evaluation techniques to ensure the quality and relevance\nof the summaries. Our results show that fine-tuned Llama-2-7B-based\nsummarization model performs on-par with GPT-4 in terms of factual accuracy,\ncompleteness and conciseness. Our findings demonstrate the potential for\nquickly bootstrapping a practical and efficient call summarization system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the rapid development of a telephone call summarization\nsystem utilizing large language models (LLMs). Our approach involves initial\nexperiments with prompting existing LLMs to generate summaries of telephone\nconversations, followed by the creation of a tailored synthetic training\ndataset utilizing stronger frontier models. We place special focus on the\ndiversity of the generated data and on the ability to control the length of the\ngenerated summaries to meet various use-case specific requirements. The\neffectiveness of our method is evaluated using two state-of-the-art\nLLM-as-a-judge-based evaluation techniques to ensure the quality and relevance\nof the summaries. Our results show that fine-tuned Llama-2-7B-based\nsummarization model performs on-par with GPT-4 in terms of factual accuracy,\ncompleteness and conciseness. Our findings demonstrate the potential for\nquickly bootstrapping a practical and efficient call summarization system."
                },
                "authors": [
                    {
                        "name": "David Thulke"
                    },
                    {
                        "name": "Yingbo Gao"
                    },
                    {
                        "name": "Rricha Jalota"
                    },
                    {
                        "name": "Christian Dugast"
                    },
                    {
                        "name": "Hermann Ney"
                    }
                ],
                "author_detail": {
                    "name": "Hermann Ney"
                },
                "author": "Hermann Ney",
                "arxiv_comment": "Accepted at the The International Conference on Foundation and Large\n  Language Models (FLLM2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15859v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15859v3",
                "updated": "2024-10-24T10:29:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    29,
                    15,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-21T10:39:05Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    10,
                    39,
                    5,
                    0,
                    295,
                    0
                ],
                "title": "Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced\n  Extrapolation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced\n  Extrapolation in LLMs"
                },
                "summary": "Large language models (LLMs), although having revolutionized many fields,\nstill suffer from the challenging extrapolation problem, where the inference\nability of LLMs sharply declines beyond their max training lengths. In this\nwork, we conduct a theoretical analysis to better understand why No Position\nEncoding (NoPE) fails outside its effective range, as well as examining the\npower of Position Encoding (PE) in this context. Our findings reveal that with\nmeticulous weave position, PE can indeed be extended beyond effective range.\nOur theorems establish that LLMs equipped with weave PE can achieve improved\nextrapolation performance without additional cost. Furthermore, we introduce a\nnovel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based\ntriangular attention matrix and applies Stair PE to manage the final chunk.\nThis method not only retains competitive performance but also offers\nsubstantial benefits such as significantly reduced memory demand and faster\ninference speed. Extensive experiments validate the effectiveness of\nMesa-Extrapolation, demonstrating its potential as a scalable solution to\nenhancing LLMs applicative reach. Our code is available at\n\\url{https://github.com/soacker/Mesa-Extrapolation}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), although having revolutionized many fields,\nstill suffer from the challenging extrapolation problem, where the inference\nability of LLMs sharply declines beyond their max training lengths. In this\nwork, we conduct a theoretical analysis to better understand why No Position\nEncoding (NoPE) fails outside its effective range, as well as examining the\npower of Position Encoding (PE) in this context. Our findings reveal that with\nmeticulous weave position, PE can indeed be extended beyond effective range.\nOur theorems establish that LLMs equipped with weave PE can achieve improved\nextrapolation performance without additional cost. Furthermore, we introduce a\nnovel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based\ntriangular attention matrix and applies Stair PE to manage the final chunk.\nThis method not only retains competitive performance but also offers\nsubstantial benefits such as significantly reduced memory demand and faster\ninference speed. Extensive experiments validate the effectiveness of\nMesa-Extrapolation, demonstrating its potential as a scalable solution to\nenhancing LLMs applicative reach. Our code is available at\n\\url{https://github.com/soacker/Mesa-Extrapolation}."
                },
                "authors": [
                    {
                        "name": "Xin Ma"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jingjing Liu"
                    },
                    {
                        "name": "Xiaoxu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxu Ma"
                },
                "author": "Xiaoxu Ma",
                "arxiv_comment": "Accepted by NeurIPS 2024; 13 pages and 30 pages appendix;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15859v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15859v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16802v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16802v4",
                "updated": "2024-10-24T09:52:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    52,
                    59,
                    3,
                    298,
                    0
                ],
                "published": "2024-05-27T03:44:24Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    3,
                    44,
                    24,
                    0,
                    148,
                    0
                ],
                "title": "AutoPSV: Automated Process-Supervised Verifier",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPSV: Automated Process-Supervised Verifier"
                },
                "summary": "In this work, we propose a novel method named \\textbf{Auto}mated\n\\textbf{P}rocess-\\textbf{S}upervised \\textbf{V}erifier\n(\\textbf{\\textsc{AutoPSV}}) to enhance the reasoning capabilities of large\nlanguage models (LLMs) by automatically annotating the reasoning steps.\n\\textsc{AutoPSV} begins by training a verification model on the correctness of\nfinal answers, enabling it to generate automatic process annotations. This\nverification model assigns a confidence score to each reasoning step,\nindicating the probability of arriving at the correct final answer from that\npoint onward. We detect relative changes in the verification's confidence\nscores across reasoning steps to automatically annotate the reasoning process,\nenabling error detection even in scenarios where ground truth answers are\nunavailable. This alleviates the need for numerous manual annotations or the\nhigh computational costs associated with model-induced annotation approaches.\nWe experimentally validate that the step-level confidence changes learned by\nthe verification model trained on the final answer correctness can effectively\nidentify errors in the reasoning steps. We demonstrate that the verification\nmodel, when trained on process annotations generated by \\textsc{AutoPSV},\nexhibits improved performance in selecting correct answers from multiple\nLLM-generated outputs. Notably, we achieve substantial improvements across five\ndatasets in mathematics and commonsense reasoning. The source code of\n\\textsc{AutoPSV} is available at \\url{https://github.com/rookie-joe/AutoPSV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a novel method named \\textbf{Auto}mated\n\\textbf{P}rocess-\\textbf{S}upervised \\textbf{V}erifier\n(\\textbf{\\textsc{AutoPSV}}) to enhance the reasoning capabilities of large\nlanguage models (LLMs) by automatically annotating the reasoning steps.\n\\textsc{AutoPSV} begins by training a verification model on the correctness of\nfinal answers, enabling it to generate automatic process annotations. This\nverification model assigns a confidence score to each reasoning step,\nindicating the probability of arriving at the correct final answer from that\npoint onward. We detect relative changes in the verification's confidence\nscores across reasoning steps to automatically annotate the reasoning process,\nenabling error detection even in scenarios where ground truth answers are\nunavailable. This alleviates the need for numerous manual annotations or the\nhigh computational costs associated with model-induced annotation approaches.\nWe experimentally validate that the step-level confidence changes learned by\nthe verification model trained on the final answer correctness can effectively\nidentify errors in the reasoning steps. We demonstrate that the verification\nmodel, when trained on process annotations generated by \\textsc{AutoPSV},\nexhibits improved performance in selecting correct answers from multiple\nLLM-generated outputs. Notably, we achieve substantial improvements across five\ndatasets in mathematics and commonsense reasoning. The source code of\n\\textsc{AutoPSV} is available at \\url{https://github.com/rookie-joe/AutoPSV}."
                },
                "authors": [
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Zhiyang Dou"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Zeyu Cao"
                    },
                    {
                        "name": "Jianbo Dai"
                    },
                    {
                        "name": "Yingjia Wan"
                    },
                    {
                        "name": "Zhijiang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Guo"
                },
                "author": "Zhijiang Guo",
                "arxiv_comment": "Accepted by NeurIPS 2024 Poster, 21 pages, 1 figure, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16802v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16802v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18588v1",
                "updated": "2024-10-24T09:37:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    37,
                    23,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T09:37:23Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    37,
                    23,
                    3,
                    298,
                    0
                ],
                "title": "Knowledge Distillation Using Frontier Open-source LLMs: Generalizability\n  and the Role of Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Distillation Using Frontier Open-source LLMs: Generalizability\n  and the Role of Synthetic Data"
                },
                "summary": "Leading open-source large language models (LLMs) such as\nLlama-3.1-Instruct-405B are extremely capable at generating text, answering\nquestions, and solving a variety of natural language understanding tasks.\nHowever, they incur higher inference cost and latency compared to smaller LLMs.\nKnowledge distillation provides a way to use outputs from these large, capable\nteacher models to train smaller student models which can be used for inference\nat lower cost and latency, while retaining comparable accuracy. We investigate\nthe efficacy of distillation using the Llama-3.1-405B-Instruct teacher and the\nsmaller Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct student models.\nContributions of this work include (a) We evaluate the generalizability of\ndistillation with the above Llama-3.1 teacher-student pairs across different\ntasks and datasets (b) We show that using synthetic data during distillation\nsignificantly improves the accuracy of 8B and 70B models, and when used with\nreasoning chains, even matches or surpasses the zero-shot accuracy of 405B\nmodel on some datasets (c) We empirically show that distillation enables 8B and\n70B models to internalize 405B's reasoning ability by using only standard\nfine-tuning (without customizing any loss function). This allows cost and\nlatency-efficient student model inference. (d) We show pitfalls in evaluation\nof distillation, and present task-specific evaluation, including both human and\nLLM-grading, and ground-truth based traditional accuracy benchmarks. This\nmethodical study brings out the fundamental importance of synthetic data\nquality in knowledge distillation, and of combining multiple, task-specific\nways of accuracy and quality evaluation in assessing the effectiveness of\ndistillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leading open-source large language models (LLMs) such as\nLlama-3.1-Instruct-405B are extremely capable at generating text, answering\nquestions, and solving a variety of natural language understanding tasks.\nHowever, they incur higher inference cost and latency compared to smaller LLMs.\nKnowledge distillation provides a way to use outputs from these large, capable\nteacher models to train smaller student models which can be used for inference\nat lower cost and latency, while retaining comparable accuracy. We investigate\nthe efficacy of distillation using the Llama-3.1-405B-Instruct teacher and the\nsmaller Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct student models.\nContributions of this work include (a) We evaluate the generalizability of\ndistillation with the above Llama-3.1 teacher-student pairs across different\ntasks and datasets (b) We show that using synthetic data during distillation\nsignificantly improves the accuracy of 8B and 70B models, and when used with\nreasoning chains, even matches or surpasses the zero-shot accuracy of 405B\nmodel on some datasets (c) We empirically show that distillation enables 8B and\n70B models to internalize 405B's reasoning ability by using only standard\nfine-tuning (without customizing any loss function). This allows cost and\nlatency-efficient student model inference. (d) We show pitfalls in evaluation\nof distillation, and present task-specific evaluation, including both human and\nLLM-grading, and ground-truth based traditional accuracy benchmarks. This\nmethodical study brings out the fundamental importance of synthetic data\nquality in knowledge distillation, and of combining multiple, task-specific\nways of accuracy and quality evaluation in assessing the effectiveness of\ndistillation."
                },
                "authors": [
                    {
                        "name": "Anup Shirgaonkar"
                    },
                    {
                        "name": "Nikhil Pandey"
                    },
                    {
                        "name": "Nazmiye Ceren Abay"
                    },
                    {
                        "name": "Tolga Aktas"
                    },
                    {
                        "name": "Vijay Aski"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Aski"
                },
                "author": "Vijay Aski",
                "arxiv_comment": "25 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09528v2",
                "updated": "2024-10-24T09:36:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    36,
                    53,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-12T13:19:11Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    19,
                    11,
                    5,
                    286,
                    0
                ],
                "title": "Boosting Deductive Reasoning with Step Signals In RLHF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Deductive Reasoning with Step Signals In RLHF"
                },
                "summary": "Logical reasoning is a crucial task for Large Language Models (LLMs),\nenabling them to tackle complex problems. Among reasoning tasks, multi-step\nreasoning poses a particular challenge. Grounded in the theory of formal logic,\nwe have developed an automated method, Multi-step Deduction (MuseD), for\ndeductive reasoning data. MuseD has allowed us to create training and testing\ndatasets for multi-step reasoning. Our generation method enables control over\nthe complexity of the generated instructions, facilitating training and\nevaluation of models across different difficulty levels. Through RLHF training,\nour training data has demonstrated significant improvements in logical\ncapabilities for both in-domain of out-of-domain reasoning tasks. Additionally,\nwe have conducted tests to assess the multi-step reasoning abilities of various\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logical reasoning is a crucial task for Large Language Models (LLMs),\nenabling them to tackle complex problems. Among reasoning tasks, multi-step\nreasoning poses a particular challenge. Grounded in the theory of formal logic,\nwe have developed an automated method, Multi-step Deduction (MuseD), for\ndeductive reasoning data. MuseD has allowed us to create training and testing\ndatasets for multi-step reasoning. Our generation method enables control over\nthe complexity of the generated instructions, facilitating training and\nevaluation of models across different difficulty levels. Through RLHF training,\nour training data has demonstrated significant improvements in logical\ncapabilities for both in-domain of out-of-domain reasoning tasks. Additionally,\nwe have conducted tests to assess the multi-step reasoning abilities of various\nmodels."
                },
                "authors": [
                    {
                        "name": "Jialian Li"
                    },
                    {
                        "name": "Yipin Zhang"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Yuzi Yan"
                    },
                    {
                        "name": "Jian Xie"
                    },
                    {
                        "name": "Dong Yan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yan"
                },
                "author": "Dong Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18585v1",
                "updated": "2024-10-24T09:36:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    36,
                    13,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T09:36:13Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    36,
                    13,
                    3,
                    298,
                    0
                ],
                "title": "Aligning CodeLLMs with Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning CodeLLMs with Direct Preference Optimization"
                },
                "summary": "The last year has witnessed the rapid progress of large language models\n(LLMs) across diverse domains. Among them, CodeLLMs have garnered particular\nattention because they can not only assist in completing various programming\ntasks but also represent the decision-making and logical reasoning capabilities\nof LLMs. However, current CodeLLMs mainly focus on pre-training and supervised\nfine-tuning scenarios, leaving the alignment stage, which is important for\npost-training LLMs, under-explored. This work first identifies that the\ncommonly used PPO algorithm may be suboptimal for the alignment of CodeLLM\nbecause the involved reward rules are routinely coarse-grained and potentially\nflawed. We then advocate addressing this using the DPO algorithm. Based on only\npreference data pairs, DPO can render the model rank data automatically, giving\nrise to a fine-grained rewarding pattern more robust than human intervention.\nWe also contribute a pipeline for collecting preference pairs for DPO on\nCodeLLMs. Studies show that our method significantly improves the performance\nof existing CodeLLMs on benchmarks such as MBPP and HumanEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The last year has witnessed the rapid progress of large language models\n(LLMs) across diverse domains. Among them, CodeLLMs have garnered particular\nattention because they can not only assist in completing various programming\ntasks but also represent the decision-making and logical reasoning capabilities\nof LLMs. However, current CodeLLMs mainly focus on pre-training and supervised\nfine-tuning scenarios, leaving the alignment stage, which is important for\npost-training LLMs, under-explored. This work first identifies that the\ncommonly used PPO algorithm may be suboptimal for the alignment of CodeLLM\nbecause the involved reward rules are routinely coarse-grained and potentially\nflawed. We then advocate addressing this using the DPO algorithm. Based on only\npreference data pairs, DPO can render the model rank data automatically, giving\nrise to a fine-grained rewarding pattern more robust than human intervention.\nWe also contribute a pipeline for collecting preference pairs for DPO on\nCodeLLMs. Studies show that our method significantly improves the performance\nof existing CodeLLMs on benchmarks such as MBPP and HumanEval."
                },
                "authors": [
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18582v1",
                "updated": "2024-10-24T09:35:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    35,
                    21,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T09:35:21Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    35,
                    21,
                    3,
                    298,
                    0
                ],
                "title": "LLM-Aided Efficient Hardware Design Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Aided Efficient Hardware Design Automation"
                },
                "summary": "With the rapidly increasing complexity of modern chips, hardware engineers\nare required to invest more effort in tasks such as circuit design,\nverification, and physical implementation. These workflows often involve\ncontinuous modifications, which are labor-intensive and prone to errors.\nTherefore, there is an increasing need for more efficient and cost-effective\nElectronic Design Automation (EDA) solutions to accelerate new hardware\ndevelopment. Recently, large language models (LLMs) have made significant\nadvancements in contextual understanding, logical reasoning, and response\ngeneration. Since hardware designs and intermediate scripts can be expressed in\ntext format, it is reasonable to explore whether integrating LLMs into EDA\ncould simplify and fully automate the entire workflow. Accordingly, this paper\ndiscusses such possibilities in several aspects, covering hardware description\nlanguage (HDL) generation, code debugging, design verification, and physical\nimplementation. Two case studies, along with their future outlook, are\nintroduced to highlight the capabilities of LLMs in code repair and testbench\ngeneration. Finally, future directions and challenges are highlighted to\nfurther explore the potential of LLMs in shaping the next-generation EDA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing complexity of modern chips, hardware engineers\nare required to invest more effort in tasks such as circuit design,\nverification, and physical implementation. These workflows often involve\ncontinuous modifications, which are labor-intensive and prone to errors.\nTherefore, there is an increasing need for more efficient and cost-effective\nElectronic Design Automation (EDA) solutions to accelerate new hardware\ndevelopment. Recently, large language models (LLMs) have made significant\nadvancements in contextual understanding, logical reasoning, and response\ngeneration. Since hardware designs and intermediate scripts can be expressed in\ntext format, it is reasonable to explore whether integrating LLMs into EDA\ncould simplify and fully automate the entire workflow. Accordingly, this paper\ndiscusses such possibilities in several aspects, covering hardware description\nlanguage (HDL) generation, code debugging, design verification, and physical\nimplementation. Two case studies, along with their future outlook, are\nintroduced to highlight the capabilities of LLMs in code repair and testbench\ngeneration. Finally, future directions and challenges are highlighted to\nfurther explore the potential of LLMs in shaping the next-generation EDA"
                },
                "authors": [
                    {
                        "name": "Kangwei Xu"
                    },
                    {
                        "name": "Ruidi Qiu"
                    },
                    {
                        "name": "Zhuorui Zhao"
                    },
                    {
                        "name": "Grace Li Zhang"
                    },
                    {
                        "name": "Ulf Schlichtmann"
                    },
                    {
                        "name": "Bing Li"
                    }
                ],
                "author_detail": {
                    "name": "Bing Li"
                },
                "author": "Bing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18574v1",
                "updated": "2024-10-24T09:29:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    29,
                    18,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T09:29:18Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    29,
                    18,
                    3,
                    298,
                    0
                ],
                "title": "SIKeD: Self-guided Iterative Knowledge Distillation for mathematical\n  reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIKeD: Self-guided Iterative Knowledge Distillation for mathematical\n  reasoning"
                },
                "summary": "Large Language Models (LLMs) can transfer their reasoning skills to smaller\nmodels by teaching them to generate the intermediate reasoning process required\nto solve multistep reasoning tasks. While LLMs can accurately solve reasoning\ntasks through a variety of strategies, even without fine-tuning, smaller models\nare not expressive enough to fit the LLMs distribution on all strategies when\ndistilled and tend to prioritize one strategy over the others. This reliance on\none strategy poses a challenge for smaller models when attempting to solve\nreasoning tasks that may be difficult with their preferred strategy. To address\nthis, we propose a distillation method SIKeD (Self-guided Iterative Knowledge\nDistillation for mathematical reasoning), where the LLM teaches the smaller\nmodel to approach a task using different strategies and the smaller model uses\nits self-generated on-policy outputs to choose the most suitable strategy for\nthe given task. The training continues in a self-guided iterative manner, where\nfor each training iteration, a decision is made on how to combine the LLM data\nwith the self-generated outputs. Unlike traditional distillation methods, SIKeD\nallows the smaller model to learn which strategy is suitable for a given task\nwhile continuously learning to solve a task using different strategies. Our\nexperiments on various mathematical reasoning datasets show that SIKeD\nsignificantly outperforms traditional distillation techniques across smaller\nmodels of different sizes. Our code is available at:\nhttps://github.com/kumar-shridhar/SIKeD",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can transfer their reasoning skills to smaller\nmodels by teaching them to generate the intermediate reasoning process required\nto solve multistep reasoning tasks. While LLMs can accurately solve reasoning\ntasks through a variety of strategies, even without fine-tuning, smaller models\nare not expressive enough to fit the LLMs distribution on all strategies when\ndistilled and tend to prioritize one strategy over the others. This reliance on\none strategy poses a challenge for smaller models when attempting to solve\nreasoning tasks that may be difficult with their preferred strategy. To address\nthis, we propose a distillation method SIKeD (Self-guided Iterative Knowledge\nDistillation for mathematical reasoning), where the LLM teaches the smaller\nmodel to approach a task using different strategies and the smaller model uses\nits self-generated on-policy outputs to choose the most suitable strategy for\nthe given task. The training continues in a self-guided iterative manner, where\nfor each training iteration, a decision is made on how to combine the LLM data\nwith the self-generated outputs. Unlike traditional distillation methods, SIKeD\nallows the smaller model to learn which strategy is suitable for a given task\nwhile continuously learning to solve a task using different strategies. Our\nexperiments on various mathematical reasoning datasets show that SIKeD\nsignificantly outperforms traditional distillation techniques across smaller\nmodels of different sizes. Our code is available at:\nhttps://github.com/kumar-shridhar/SIKeD"
                },
                "authors": [
                    {
                        "name": "Shivam Adarsh"
                    },
                    {
                        "name": "Kumar Shridhar"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20318v2",
                "updated": "2024-10-24T09:21:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    21,
                    38,
                    3,
                    298,
                    0
                ],
                "published": "2024-05-30T17:55:28Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    17,
                    55,
                    28,
                    3,
                    151,
                    0
                ],
                "title": "Analyzing Human Questioning Behavior and Causal Curiosity through\n  Natural Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Human Questioning Behavior and Causal Curiosity through\n  Natural Queries"
                },
                "summary": "The recent development of Large Language Models (LLMs) has changed our role\nin interacting with them. Instead of primarily testing these models with\nquestions we already know the answers to, we now use them to explore questions\nwhere the answers are unknown to us. This shift, which hasn't been fully\naddressed in existing datasets, highlights the growing need to understand\nnaturally occurring human questions - that are more complex, open-ended, and\nreflective of real-world needs. To this end, we present NatQuest, a collection\nof 13,500 naturally occurring questions from three diverse sources:\nhuman-to-search-engine queries, human-to-human interactions, and human-to-LLM\nconversations. Our comprehensive collection enables a rich understanding of\nhuman curiosity across various domains and contexts. Our analysis reveals a\nsignificant presence of causal questions (up to 42%) within the dataset, for\nwhich we develop an iterative prompt improvement framework to identify all\ncausal queries, and examine their unique linguistic properties, cognitive\ncomplexity, and source distribution. We also lay the groundwork to explore LLM\nperformance on these questions and provide six efficient classification models\nto identify causal questions at scale for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of Large Language Models (LLMs) has changed our role\nin interacting with them. Instead of primarily testing these models with\nquestions we already know the answers to, we now use them to explore questions\nwhere the answers are unknown to us. This shift, which hasn't been fully\naddressed in existing datasets, highlights the growing need to understand\nnaturally occurring human questions - that are more complex, open-ended, and\nreflective of real-world needs. To this end, we present NatQuest, a collection\nof 13,500 naturally occurring questions from three diverse sources:\nhuman-to-search-engine queries, human-to-human interactions, and human-to-LLM\nconversations. Our comprehensive collection enables a rich understanding of\nhuman curiosity across various domains and contexts. Our analysis reveals a\nsignificant presence of causal questions (up to 42%) within the dataset, for\nwhich we develop an iterative prompt improvement framework to identify all\ncausal queries, and examine their unique linguistic properties, cognitive\ncomplexity, and source distribution. We also lay the groundwork to explore LLM\nperformance on these questions and provide six efficient classification models\nto identify causal questions at scale for future work."
                },
                "authors": [
                    {
                        "name": "Roberto Ceraolo"
                    },
                    {
                        "name": "Dmitrii Kharlapenko"
                    },
                    {
                        "name": "Ahmad Khan"
                    },
                    {
                        "name": "Amélie Reymond"
                    },
                    {
                        "name": "Rada Mihalcea"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Zhijing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Jin"
                },
                "author": "Zhijing Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18565v1",
                "updated": "2024-10-24T09:16:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    16,
                    9,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T09:16:09Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    9,
                    16,
                    9,
                    3,
                    298,
                    0
                ],
                "title": "Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and\n  Evaluation"
                },
                "summary": "We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for\nPolish language processing. Trained on curated Polish corpora, this model\naddresses key challenges in language model development through innovative\ntechniques. These include Weighted Instruction Cross-Entropy Loss, which\nbalances the learning of different instruction types, and Adaptive Learning\nRate, which dynamically adjusts the learning rate based on training progress.\nTo evaluate performance, we created the Open PL LLM Leaderboard and Polish\nMT-Bench, novel frameworks assessing various NLP tasks and conversational\nabilities. Bielik 7B v0.1 demonstrates significant improvements, achieving a 9\npercentage point increase in average score compared to Mistral-7B-v0.1 on the\nRAG Reader task. It also excels in the Polish MT-Bench, particularly in\nReasoning (6.15/10) and Role-playing (7.83/10) categories. This model\nrepresents a substantial advancement in Polish language AI, offering a powerful\ntool for diverse linguistic applications and setting new benchmarks in the\nfield.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for\nPolish language processing. Trained on curated Polish corpora, this model\naddresses key challenges in language model development through innovative\ntechniques. These include Weighted Instruction Cross-Entropy Loss, which\nbalances the learning of different instruction types, and Adaptive Learning\nRate, which dynamically adjusts the learning rate based on training progress.\nTo evaluate performance, we created the Open PL LLM Leaderboard and Polish\nMT-Bench, novel frameworks assessing various NLP tasks and conversational\nabilities. Bielik 7B v0.1 demonstrates significant improvements, achieving a 9\npercentage point increase in average score compared to Mistral-7B-v0.1 on the\nRAG Reader task. It also excels in the Polish MT-Bench, particularly in\nReasoning (6.15/10) and Role-playing (7.83/10) categories. This model\nrepresents a substantial advancement in Polish language AI, offering a powerful\ntool for diverse linguistic applications and setting new benchmarks in the\nfield."
                },
                "authors": [
                    {
                        "name": "Krzysztof Ociepa"
                    },
                    {
                        "name": "Łukasz Flis"
                    },
                    {
                        "name": "Krzysztof Wróbel"
                    },
                    {
                        "name": "Adrian Gwoździej"
                    },
                    {
                        "name": "Remigiusz Kinas"
                    }
                ],
                "author_detail": {
                    "name": "Remigiusz Kinas"
                },
                "author": "Remigiusz Kinas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13185v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13185v3",
                "updated": "2024-10-24T08:59:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    59,
                    53,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-17T03:26:37Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    3,
                    26,
                    37,
                    3,
                    291,
                    0
                ],
                "title": "Chain of Ideas: Revolutionizing Research in Novel Idea Development with\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Ideas: Revolutionizing Research in Novel Idea Development with\n  LLM Agents"
                },
                "summary": "Effective research ideation is a critical step for scientific research.\nHowever, the exponential increase in scientific literature makes it challenging\nfor researchers to stay current with recent advances and identify meaningful\nresearch directions. Recent developments in large language models~(LLMs)\nsuggest a promising avenue for automating the generation of novel research\nideas. However, existing methods for idea generation either trivially prompt\nLLMs or directly expose LLMs to extensive literature without indicating useful\ninformation. Inspired by the research process of human researchers, we propose\na Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant\nliterature in a chain structure to effectively mirror the progressive\ndevelopment in a research domain. This organization facilitates LLMs to capture\nthe current advancements in research, thereby enhancing their ideation\ncapabilities. Furthermore, we propose Idea Arena, an evaluation protocol that\ncan comprehensively evaluate idea generation methods from different\nperspectives, aligning closely with the preferences of human researchers.\nExperimental results indicate that the CoI agent consistently outperforms other\nmethods and shows comparable quality as humans in research idea generation.\nMoreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to\ngenerate a candidate idea and its corresponding experimental design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective research ideation is a critical step for scientific research.\nHowever, the exponential increase in scientific literature makes it challenging\nfor researchers to stay current with recent advances and identify meaningful\nresearch directions. Recent developments in large language models~(LLMs)\nsuggest a promising avenue for automating the generation of novel research\nideas. However, existing methods for idea generation either trivially prompt\nLLMs or directly expose LLMs to extensive literature without indicating useful\ninformation. Inspired by the research process of human researchers, we propose\na Chain-of-Ideas~(CoI) agent, an LLM-based agent that organizes relevant\nliterature in a chain structure to effectively mirror the progressive\ndevelopment in a research domain. This organization facilitates LLMs to capture\nthe current advancements in research, thereby enhancing their ideation\ncapabilities. Furthermore, we propose Idea Arena, an evaluation protocol that\ncan comprehensively evaluate idea generation methods from different\nperspectives, aligning closely with the preferences of human researchers.\nExperimental results indicate that the CoI agent consistently outperforms other\nmethods and shows comparable quality as humans in research idea generation.\nMoreover, our CoI agent is budget-friendly, with a minimum cost of \\$0.50 to\ngenerate a candidate idea and its corresponding experimental design."
                },
                "authors": [
                    {
                        "name": "Long Li"
                    },
                    {
                        "name": "Weiwen Xu"
                    },
                    {
                        "name": "Jiayan Guo"
                    },
                    {
                        "name": "Ruochen Zhao"
                    },
                    {
                        "name": "Xinxuan Li"
                    },
                    {
                        "name": "Yuqian Yuan"
                    },
                    {
                        "name": "Boqiang Zhang"
                    },
                    {
                        "name": "Yuming Jiang"
                    },
                    {
                        "name": "Yifei Xin"
                    },
                    {
                        "name": "Ronghao Dang"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Yu Rong"
                    },
                    {
                        "name": "Tian Feng"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "10 pages,5 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13185v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13185v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.19488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.19488v2",
                "updated": "2024-10-24T08:53:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    53,
                    22,
                    3,
                    298,
                    0
                ],
                "published": "2023-10-30T12:25:00Z",
                "published_parsed": [
                    2023,
                    10,
                    30,
                    12,
                    25,
                    0,
                    0,
                    303,
                    0
                ],
                "title": "CoLLM: Integrating Collaborative Embeddings into Large Language Models\n  for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoLLM: Integrating Collaborative Embeddings into Large Language Models\n  for Recommendation"
                },
                "summary": "Leveraging Large Language Models as Recommenders (LLMRec) has gained\nsignificant attention and introduced fresh perspectives in user preference\nmodeling. Existing LLMRec approaches prioritize text semantics, usually\nneglecting the valuable collaborative information from user-item interactions\nin recommendations. While these text-emphasizing approaches excel in cold-start\nscenarios, they may yield sub-optimal performance in warm-start situations. In\npursuit of superior recommendations for both cold and warm start scenarios, we\nintroduce CoLLM, an innovative LLMRec methodology that seamlessly incorporates\ncollaborative information into LLMs for recommendation. CoLLM captures\ncollaborative information through an external traditional model and maps it to\nthe input token embedding space of LLM, forming collaborative embeddings for\nLLM usage. Through this external integration of collaborative information,\nCoLLM ensures effective modeling of collaborative information without modifying\nthe LLM itself, providing the flexibility to employ various collaborative\ninformation modeling techniques. Extensive experiments validate that CoLLM\nadeptly integrates collaborative information into LLMs, resulting in enhanced\nrecommendation performance. We release the code and data at\nhttps://github.com/zyang1580/CoLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models as Recommenders (LLMRec) has gained\nsignificant attention and introduced fresh perspectives in user preference\nmodeling. Existing LLMRec approaches prioritize text semantics, usually\nneglecting the valuable collaborative information from user-item interactions\nin recommendations. While these text-emphasizing approaches excel in cold-start\nscenarios, they may yield sub-optimal performance in warm-start situations. In\npursuit of superior recommendations for both cold and warm start scenarios, we\nintroduce CoLLM, an innovative LLMRec methodology that seamlessly incorporates\ncollaborative information into LLMs for recommendation. CoLLM captures\ncollaborative information through an external traditional model and maps it to\nthe input token embedding space of LLM, forming collaborative embeddings for\nLLM usage. Through this external integration of collaborative information,\nCoLLM ensures effective modeling of collaborative information without modifying\nthe LLM itself, providing the flexibility to employ various collaborative\ninformation modeling techniques. Extensive experiments validate that CoLLM\nadeptly integrates collaborative information into LLMs, resulting in enhanced\nrecommendation performance. We release the code and data at\nhttps://github.com/zyang1580/CoLLM."
                },
                "authors": [
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Jizhi Zhang"
                    },
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "arxiv_comment": "IEEE TKDE Major Revision Version, which adds new LLM backbone\n  Qwen2-1.5",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.19488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.19488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16970v2",
                "updated": "2024-10-24T08:43:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    43,
                    21,
                    3,
                    298,
                    0
                ],
                "published": "2024-07-24T03:32:05Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    3,
                    32,
                    5,
                    2,
                    206,
                    0
                ],
                "title": "Towards Aligning Language Models with Textual Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Aligning Language Models with Textual Feedback"
                },
                "summary": "We present ALT (ALignment with Textual feedback), an approach that aligns\nlanguage models with user preferences expressed in text. We argue that text\noffers greater expressiveness, enabling users to provide richer feedback than\nsimple comparative preferences and this richer feedback can lead to more\nefficient and effective alignment. ALT aligns the model by conditioning its\ngeneration on the textual feedback. Our method relies solely on language\nmodeling techniques and requires minimal hyper-parameter tuning, though it\nstill presents the main benefits of RL-based alignment algorithms and can\neffectively learn from textual feedback. We explore the efficacy and efficiency\nof textual feedback across different tasks such as toxicity reduction,\nsummarization, and dialog response generation. We find that ALT outperforms PPO\nfor the task of toxicity reduction while being able to match its performance on\nsummarization with only 20% of the samples. We also explore how ALT can be used\nwith feedback provided by an existing LLM where we explore an LLM providing\nconstrained and unconstrained textual feedback. We also outline future\ndirections to align models with natural language feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ALT (ALignment with Textual feedback), an approach that aligns\nlanguage models with user preferences expressed in text. We argue that text\noffers greater expressiveness, enabling users to provide richer feedback than\nsimple comparative preferences and this richer feedback can lead to more\nefficient and effective alignment. ALT aligns the model by conditioning its\ngeneration on the textual feedback. Our method relies solely on language\nmodeling techniques and requires minimal hyper-parameter tuning, though it\nstill presents the main benefits of RL-based alignment algorithms and can\neffectively learn from textual feedback. We explore the efficacy and efficiency\nof textual feedback across different tasks such as toxicity reduction,\nsummarization, and dialog response generation. We find that ALT outperforms PPO\nfor the task of toxicity reduction while being able to match its performance on\nsummarization with only 20% of the samples. We also explore how ALT can be used\nwith feedback provided by an existing LLM where we explore an LLM providing\nconstrained and unconstrained textual feedback. We also outline future\ndirections to align models with natural language feedback."
                },
                "authors": [
                    {
                        "name": "Saüc Abadal Lloret"
                    },
                    {
                        "name": "Shehzaad Dhuliawala"
                    },
                    {
                        "name": "Keerthiram Murugesan"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18266v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18266v4",
                "updated": "2024-10-24T08:29:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    29,
                    13,
                    3,
                    298,
                    0
                ],
                "published": "2024-06-26T11:39:51Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    11,
                    39,
                    51,
                    2,
                    178,
                    0
                ],
                "title": "\"Vorbeşti Româneşte?\" A Recipe to Train Powerful Romanian LLMs\n  with English Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Vorbeşti Româneşte?\" A Recipe to Train Powerful Romanian LLMs\n  with English Instructions"
                },
                "summary": "In recent years, Large Language Models (LLMs) have achieved almost human-like\nperformance on various tasks. While some LLMs have been trained on multilingual\ndata, most of the training data is in English; hence, their performance in\nEnglish greatly exceeds other languages. To our knowledge, we are the first to\ncollect and translate a large collection of texts, instructions, and benchmarks\nand train, evaluate, and release open-source LLMs tailored for Romanian. We\nevaluate our methods on four different categories, including academic\nbenchmarks, MT-Bench (manually translated), and a professionally built\nhistorical, cultural, and social benchmark adapted to Romanian. We argue for\nthe usefulness and high performance of RoLLMs by obtaining state-of-the-art\nresults across the board. We publicly release all resources (i.e., data,\ntraining and evaluation code, models) to support and encourage research on\nRomanian LLMs while concurrently creating a generalizable recipe, adequate for\nother low or less-resourced languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have achieved almost human-like\nperformance on various tasks. While some LLMs have been trained on multilingual\ndata, most of the training data is in English; hence, their performance in\nEnglish greatly exceeds other languages. To our knowledge, we are the first to\ncollect and translate a large collection of texts, instructions, and benchmarks\nand train, evaluate, and release open-source LLMs tailored for Romanian. We\nevaluate our methods on four different categories, including academic\nbenchmarks, MT-Bench (manually translated), and a professionally built\nhistorical, cultural, and social benchmark adapted to Romanian. We argue for\nthe usefulness and high performance of RoLLMs by obtaining state-of-the-art\nresults across the board. We publicly release all resources (i.e., data,\ntraining and evaluation code, models) to support and encourage research on\nRomanian LLMs while concurrently creating a generalizable recipe, adequate for\nother low or less-resourced languages."
                },
                "authors": [
                    {
                        "name": "Mihai Masala"
                    },
                    {
                        "name": "Denis C. Ilie-Ablachim"
                    },
                    {
                        "name": "Alexandru Dima"
                    },
                    {
                        "name": "Dragos Corlatescu"
                    },
                    {
                        "name": "Miruna Zavelca"
                    },
                    {
                        "name": "Ovio Olaru"
                    },
                    {
                        "name": "Simina Terian"
                    },
                    {
                        "name": "Andrei Terian"
                    },
                    {
                        "name": "Marius Leordeanu"
                    },
                    {
                        "name": "Horia Velicu"
                    },
                    {
                        "name": "Marius Popescu"
                    },
                    {
                        "name": "Mihai Dascalu"
                    },
                    {
                        "name": "Traian Rebedea"
                    }
                ],
                "author_detail": {
                    "name": "Traian Rebedea"
                },
                "author": "Traian Rebedea",
                "arxiv_comment": "Accepted at The 2024 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2024 Findings). arXiv admin note: text overlap\n  with arXiv:2405.07703",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18266v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18266v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18528v1",
                "updated": "2024-10-24T08:21:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    21,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:21:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    21,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "PRACT: Optimizing Principled Reasoning and Acting of LLM Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRACT: Optimizing Principled Reasoning and Acting of LLM Agent"
                },
                "summary": "We introduce the Principled Reasoning and Acting (PRAct) framework, a novel\nmethod for learning and enforcing action principles from trajectory data.\nCentral to our approach is the use of text gradients from a reflection and\noptimization engine to derive these action principles. To adapt action\nprinciples to specific task requirements, we propose a new optimization\nframework, Reflective Principle Optimization (RPO). After execution, RPO\nemploys a reflector to critique current action principles and an optimizer to\nupdate them accordingly. We develop the RPO framework under two scenarios:\nReward-RPO, which uses environmental rewards for reflection, and Self-RPO,\nwhich conducts self-reflection without external rewards. Additionally, two RPO\nmethods, RPO-Traj and RPO-Batch, is introduced to adapt to different settings.\nExperimental results across four environments demonstrate that the PRAct agent,\nleveraging the RPO framework, effectively learns and applies action principles\nto enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Principled Reasoning and Acting (PRAct) framework, a novel\nmethod for learning and enforcing action principles from trajectory data.\nCentral to our approach is the use of text gradients from a reflection and\noptimization engine to derive these action principles. To adapt action\nprinciples to specific task requirements, we propose a new optimization\nframework, Reflective Principle Optimization (RPO). After execution, RPO\nemploys a reflector to critique current action principles and an optimizer to\nupdate them accordingly. We develop the RPO framework under two scenarios:\nReward-RPO, which uses environmental rewards for reflection, and Self-RPO,\nwhich conducts self-reflection without external rewards. Additionally, two RPO\nmethods, RPO-Traj and RPO-Batch, is introduced to adapt to different settings.\nExperimental results across four environments demonstrate that the PRAct agent,\nleveraging the RPO framework, effectively learns and applies action principles\nto enhance performance."
                },
                "authors": [
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Rithesh Murthy"
                    },
                    {
                        "name": "Liangwei Yang"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Juntao Tan"
                    },
                    {
                        "name": "Shirley Kokane"
                    },
                    {
                        "name": "Thai Hoang"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "Accepted to SIG CoNLL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18527v1",
                "updated": "2024-10-24T08:20:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    20,
                    10,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:20:10Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    20,
                    10,
                    3,
                    298,
                    0
                ],
                "title": "Probing Ranking LLMs: Mechanistic Interpretability in Information\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Ranking LLMs: Mechanistic Interpretability in Information\n  Retrieval"
                },
                "summary": "Transformer networks, especially those with performance on par with GPT\nmodels, are renowned for their powerful feature extraction capabilities.\nHowever, the nature and correlation of these features with human-engineered\nones remain unclear. In this study, we delve into the mechanistic workings of\nstate-of-the-art, fine-tuning-based passage-reranking transformer networks.\n  Our approach involves a probing-based, layer-by-layer analysis of neurons\nwithin ranking LLMs to identify individual or groups of known human-engineered\nand semantic features within the network's activations. We explore a wide range\nof features, including lexical, document structure, query-document interaction,\nadvanced semantic, interaction-based, and LLM-specific features, to gain a\ndeeper understanding of the underlying mechanisms that drive ranking decisions\nin LLMs.\n  Our results reveal a set of features that are prominently represented in LLM\nactivations, as well as others that are notably absent. Additionally, we\nobserve distinct behaviors of LLMs when processing low versus high relevance\nqueries and when encountering out-of-distribution query and document sets. By\nexamining these features within activations, we aim to enhance the\ninterpretability and performance of LLMs in ranking tasks. Our findings provide\nvaluable insights for the development of more effective and transparent ranking\nmodels, with significant implications for the broader information retrieval\ncommunity. All scripts and code necessary to replicate our findings are made\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer networks, especially those with performance on par with GPT\nmodels, are renowned for their powerful feature extraction capabilities.\nHowever, the nature and correlation of these features with human-engineered\nones remain unclear. In this study, we delve into the mechanistic workings of\nstate-of-the-art, fine-tuning-based passage-reranking transformer networks.\n  Our approach involves a probing-based, layer-by-layer analysis of neurons\nwithin ranking LLMs to identify individual or groups of known human-engineered\nand semantic features within the network's activations. We explore a wide range\nof features, including lexical, document structure, query-document interaction,\nadvanced semantic, interaction-based, and LLM-specific features, to gain a\ndeeper understanding of the underlying mechanisms that drive ranking decisions\nin LLMs.\n  Our results reveal a set of features that are prominently represented in LLM\nactivations, as well as others that are notably absent. Additionally, we\nobserve distinct behaviors of LLMs when processing low versus high relevance\nqueries and when encountering out-of-distribution query and document sets. By\nexamining these features within activations, we aim to enhance the\ninterpretability and performance of LLMs in ranking tasks. Our findings provide\nvaluable insights for the development of more effective and transparent ranking\nmodels, with significant implications for the broader information retrieval\ncommunity. All scripts and code necessary to replicate our findings are made\navailable."
                },
                "authors": [
                    {
                        "name": "Tanya Chowdhury"
                    },
                    {
                        "name": "James Allan"
                    }
                ],
                "author_detail": {
                    "name": "James Allan"
                },
                "author": "James Allan",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15444v3",
                "updated": "2024-10-24T08:02:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    2,
                    14,
                    3,
                    298,
                    0
                ],
                "published": "2024-05-30T18:07:13Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    18,
                    7,
                    13,
                    3,
                    151,
                    0
                ],
                "title": "Cutting Through the Noise: Boosting LLM Performance on Math Word\n  Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cutting Through the Noise: Boosting LLM Performance on Math Word\n  Problems"
                },
                "summary": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, PROBLEMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and improved ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to 6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at various tasks, including solving math\nword problems (MWPs), but struggle with real-world problems containing\nirrelevant information. To address this, we propose a prompting framework that\ngenerates adversarial variants of MWPs by adding irrelevant variables. We\nintroduce a dataset, PROBLEMATHIC, containing both adversarial and\nnon-adversarial MWPs. Our experiments reveal that LLMs are susceptible to\ndistraction by numerical noise, resulting in an average relative performance\ndrop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2,\nMistral) on the adversarial samples from our dataset. Fine-tuning on\nadversarial training instances improves performance on adversarial MWPs by ~8%,\nindicating increased robustness to noise and improved ability to identify\nrelevant data for reasoning. Finally, to assess the generalizability of our\nprompting framework, we introduce GSM-8K-Adv, an adversarial variant of the\nGSM-8K benchmark. LLMs continue to struggle when faced with adversarial\ninformation, reducing performance by up to 6%."
                },
                "authors": [
                    {
                        "name": "Ujjwala Anantheswaran"
                    },
                    {
                        "name": "Himanshu Gupta"
                    },
                    {
                        "name": "Kevin Scaria"
                    },
                    {
                        "name": "Shreyas Verma"
                    },
                    {
                        "name": "Chitta Baral"
                    },
                    {
                        "name": "Swaroop Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Swaroop Mishra"
                },
                "author": "Swaroop Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18499v1",
                "updated": "2024-10-24T07:36:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    7,
                    36,
                    59,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T07:36:59Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    7,
                    36,
                    59,
                    3,
                    298,
                    0
                ],
                "title": "LLM-Slice: Dedicated Wireless Network Slicing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Slice: Dedicated Wireless Network Slicing for Large Language Models"
                },
                "summary": "The rapid adoption of large language models (LLMs) presents new challenges\nfor existing network architectures due to significant peak traffic and high\ncommunication uncertainty. Traditional wireless networks struggle to support\nefficiently, leading to intolerable response delays, disconnections, and\nresource wastage. To address these issues, we propose LLM-Slice, the first\nsystem to provide dedicated communication slices for LLMs within a wireless\nnetwork environment. By creating LLM-specific network slices, LLM-Slice\nefficiently binds services with communication resources. Based on user\nequipment (UE) requests and a permissions database, the system registers\nspecific slices to offer controllable LLM services, integrating a downlink\nresource control module to optimize response speed, enhance resource\nutilization, and reduce disconnections. By deploying and validating in a real\nUE-gNB-CN environment, numerical results demonstrate that LLM-Slice\nsignificantly improves response speed and resource efficiency, providing a\nnovel solution for fast and controllable LLM access in wireless networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of large language models (LLMs) presents new challenges\nfor existing network architectures due to significant peak traffic and high\ncommunication uncertainty. Traditional wireless networks struggle to support\nefficiently, leading to intolerable response delays, disconnections, and\nresource wastage. To address these issues, we propose LLM-Slice, the first\nsystem to provide dedicated communication slices for LLMs within a wireless\nnetwork environment. By creating LLM-specific network slices, LLM-Slice\nefficiently binds services with communication resources. Based on user\nequipment (UE) requests and a permissions database, the system registers\nspecific slices to offer controllable LLM services, integrating a downlink\nresource control module to optimize response speed, enhance resource\nutilization, and reduce disconnections. By deploying and validating in a real\nUE-gNB-CN environment, numerical results demonstrate that LLM-Slice\nsignificantly improves response speed and resource efficiency, providing a\nnovel solution for fast and controllable LLM access in wireless networks."
                },
                "authors": [
                    {
                        "name": "Boyi Liu"
                    },
                    {
                        "name": "Jingwen Tong"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18494v1",
                "updated": "2024-10-24T07:29:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    7,
                    29,
                    15,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T07:29:15Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    7,
                    29,
                    15,
                    3,
                    298,
                    0
                ],
                "title": "Assured Automatic Programming via Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assured Automatic Programming via Large Language Models"
                },
                "summary": "With the advent of AI-based coding engines, it is possible to convert natural\nlanguage requirements to executable code in standard programming languages.\nHowever, AI-generated code can be unreliable, and the natural language\nrequirements driving this code may be ambiguous. In other words, the intent may\nnot be accurately captured in the code generated from AI-coding engines like\nCopilot. The goal of our work is to discover the programmer intent, while\ngenerating code which conforms to the intent and a proof of this conformance.\nOur approach to intent discovery is powered by a novel repair engine called\nprogram-proof co-evolution, where the object of repair is a tuple (code,\nlogical specification, test) generated by an LLM from the same natural language\ndescription. The program and the specification capture the initial operational\nand declarative description of intent, while the test represents a concrete,\nalbeit partial, understanding of the intent. Our objective is to achieve\nconsistency between the program, the specification, and the test by\nincrementally refining our understanding of the user intent. Reaching\nconsistency through this repair process provides us with a formal, logical\ndescription of the intent, which is then translated back into natural language\nfor the developer's inspection. The resultant intent description is now\nunambiguous, though expressed in natural language. We demonstrate how the\nunambiguous intent discovered through our approach increases the percentage of\nverifiable auto-generated programs on a recently proposed dataset in the Dafny\nprogramming language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of AI-based coding engines, it is possible to convert natural\nlanguage requirements to executable code in standard programming languages.\nHowever, AI-generated code can be unreliable, and the natural language\nrequirements driving this code may be ambiguous. In other words, the intent may\nnot be accurately captured in the code generated from AI-coding engines like\nCopilot. The goal of our work is to discover the programmer intent, while\ngenerating code which conforms to the intent and a proof of this conformance.\nOur approach to intent discovery is powered by a novel repair engine called\nprogram-proof co-evolution, where the object of repair is a tuple (code,\nlogical specification, test) generated by an LLM from the same natural language\ndescription. The program and the specification capture the initial operational\nand declarative description of intent, while the test represents a concrete,\nalbeit partial, understanding of the intent. Our objective is to achieve\nconsistency between the program, the specification, and the test by\nincrementally refining our understanding of the user intent. Reaching\nconsistency through this repair process provides us with a formal, logical\ndescription of the intent, which is then translated back into natural language\nfor the developer's inspection. The resultant intent description is now\nunambiguous, though expressed in natural language. We demonstrate how the\nunambiguous intent discovered through our approach increases the percentage of\nverifiable auto-generated programs on a recently proposed dataset in the Dafny\nprogramming language."
                },
                "authors": [
                    {
                        "name": "Martin Mirchev"
                    },
                    {
                        "name": "Andreea Costea"
                    },
                    {
                        "name": "Abhishek Kr Singh"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05121v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05121v3",
                "updated": "2024-10-24T07:26:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    7,
                    26,
                    36,
                    3,
                    298,
                    0
                ],
                "published": "2024-02-04T00:47:53Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    0,
                    47,
                    53,
                    6,
                    35,
                    0
                ],
                "title": "Large Language Model for Table Processing: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model for Table Processing: A Survey"
                },
                "summary": "Tables, typically two-dimensional and structured to store large amounts of\ndata, are essential in daily activities like database queries, spreadsheet\nmanipulations, web table question answering, and image table information\nextraction. Automating these table-centric tasks with Large Language Models\n(LLMs) or Visual Language Models (VLMs) offers significant public benefits,\ngarnering interest from academia and industry. This survey provides a\ncomprehensive overview of table-related tasks, examining both user scenarios\nand technical aspects. It covers traditional tasks like table question\nanswering as well as emerging fields such as spreadsheet manipulation and table\ndata analysis. We summarize the training techniques for LLMs and VLMs tailored\nfor table processing. Additionally, we discuss prompt engineering, particularly\nthe use of LLM-powered agents, for various table-related tasks. Finally, we\nhighlight several challenges, including diverse user input when serving and\nslow thinking using chain-of-thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables, typically two-dimensional and structured to store large amounts of\ndata, are essential in daily activities like database queries, spreadsheet\nmanipulations, web table question answering, and image table information\nextraction. Automating these table-centric tasks with Large Language Models\n(LLMs) or Visual Language Models (VLMs) offers significant public benefits,\ngarnering interest from academia and industry. This survey provides a\ncomprehensive overview of table-related tasks, examining both user scenarios\nand technical aspects. It covers traditional tasks like table question\nanswering as well as emerging fields such as spreadsheet manipulation and table\ndata analysis. We summarize the training techniques for LLMs and VLMs tailored\nfor table processing. Additionally, we discuss prompt engineering, particularly\nthe use of LLM-powered agents, for various table-related tasks. Finally, we\nhighlight several challenges, including diverse user input when serving and\nslow thinking using chain-of-thought."
                },
                "authors": [
                    {
                        "name": "Weizheng Lu"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Zihao Fu"
                    },
                    {
                        "name": "Yueguo Chen"
                    },
                    {
                        "name": "Xiaoyong Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyong Du"
                },
                "author": "Xiaoyong Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05121v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05121v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18491v1",
                "updated": "2024-10-24T07:25:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    7,
                    25,
                    29,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T07:25:29Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    7,
                    25,
                    29,
                    3,
                    298,
                    0
                ],
                "title": "ChineseSafe: A Chinese Benchmark for Evaluating Safety in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChineseSafe: A Chinese Benchmark for Evaluating Safety in Large Language\n  Models"
                },
                "summary": "With the rapid development of Large language models (LLMs), understanding the\ncapabilities of LLMs in identifying unsafe content has become increasingly\nimportant. While previous works have introduced several benchmarks to evaluate\nthe safety risk of LLMs, the community still has a limited understanding of\ncurrent LLMs' capability to recognize illegal and unsafe content in Chinese\ncontexts. In this work, we present a Chinese safety benchmark (ChineseSafe) to\nfacilitate research on the content safety of large language models. To align\nwith the regulations for Chinese Internet content moderation, our ChineseSafe\ncontains 205,034 examples across 4 classes and 10 sub-classes of safety issues.\nFor Chinese contexts, we add several special types of illegal content:\npolitical sensitivity, pornography, and variant/homophonic words. Moreover, we\nemploy two methods to evaluate the legal risks of popular LLMs, including\nopen-sourced models and APIs. The results reveal that many LLMs exhibit\nvulnerability to certain types of safety issues, leading to legal risks in\nChina. Our work provides a guideline for developers and researchers to\nfacilitate the safety of LLMs. Our results are also available at\nhttps://huggingface.co/spaces/SUSTech/ChineseSafe-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of Large language models (LLMs), understanding the\ncapabilities of LLMs in identifying unsafe content has become increasingly\nimportant. While previous works have introduced several benchmarks to evaluate\nthe safety risk of LLMs, the community still has a limited understanding of\ncurrent LLMs' capability to recognize illegal and unsafe content in Chinese\ncontexts. In this work, we present a Chinese safety benchmark (ChineseSafe) to\nfacilitate research on the content safety of large language models. To align\nwith the regulations for Chinese Internet content moderation, our ChineseSafe\ncontains 205,034 examples across 4 classes and 10 sub-classes of safety issues.\nFor Chinese contexts, we add several special types of illegal content:\npolitical sensitivity, pornography, and variant/homophonic words. Moreover, we\nemploy two methods to evaluate the legal risks of popular LLMs, including\nopen-sourced models and APIs. The results reveal that many LLMs exhibit\nvulnerability to certain types of safety issues, leading to legal risks in\nChina. Our work provides a guideline for developers and researchers to\nfacilitate the safety of LLMs. Our results are also available at\nhttps://huggingface.co/spaces/SUSTech/ChineseSafe-Benchmark."
                },
                "authors": [
                    {
                        "name": "Hengxiang Zhang"
                    },
                    {
                        "name": "Hongfu Gao"
                    },
                    {
                        "name": "Qiang Hu"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Lili Yang"
                    },
                    {
                        "name": "Bingyi Jing"
                    },
                    {
                        "name": "Hongxin Wei"
                    },
                    {
                        "name": "Bing Wang"
                    },
                    {
                        "name": "Haifeng Bai"
                    },
                    {
                        "name": "Lei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Yang"
                },
                "author": "Lei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18489v1",
                "updated": "2024-10-24T07:24:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    7,
                    24,
                    11,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T07:24:11Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    7,
                    24,
                    11,
                    3,
                    298,
                    0
                ],
                "title": "LLM as a code generator in Agile Model Driven Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM as a code generator in Agile Model Driven Development"
                },
                "summary": "Leveraging Large Language Models (LLM) like GPT4 in the auto generation of\ncode represents a significant advancement, yet it is not without its\nchallenges. The ambiguity inherent in natural language descriptions of software\nposes substantial obstacles to generating deployable, structured artifacts.\nThis research champions Model Driven Development (MDD) as a viable strategy to\novercome these challenges, proposing an Agile Model Driven Development (AMDD)\napproach that employs GPT4 as a code generator. This approach enhances the\nflexibility and scalability of the code auto generation process and offers\nagility that allows seamless adaptation to changes in models or deployment\nenvironments. We illustrate this by modeling a multi agent Unmanned Vehicle\nFleet (UVF) system using the Unified Modeling Language (UML), significantly\nreducing model ambiguity by integrating the Object Constraint Language (OCL)\nfor code structure meta modeling, and the FIPA ontology language for\ncommunication semantics meta modeling. Applying GPT4 auto generation\ncapabilities yields Java and Python code that is compatible with the JADE and\nPADE frameworks, respectively. Our thorough evaluation of the auto generated\ncode verifies its alignment with expected behaviors and identifies enhancements\nin agent interactions. Structurally, we assessed the complexity of code derived\nfrom a model constrained solely by OCL meta models, against that influenced by\nboth OCL and FIPA ontology meta models. The results indicate that the ontology\nconstrained meta model produces inherently more complex code, yet its\ncyclomatic complexity remains within manageable levels, suggesting that\nadditional meta model constraints can be incorporated without exceeding the\nhigh risk threshold for complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models (LLM) like GPT4 in the auto generation of\ncode represents a significant advancement, yet it is not without its\nchallenges. The ambiguity inherent in natural language descriptions of software\nposes substantial obstacles to generating deployable, structured artifacts.\nThis research champions Model Driven Development (MDD) as a viable strategy to\novercome these challenges, proposing an Agile Model Driven Development (AMDD)\napproach that employs GPT4 as a code generator. This approach enhances the\nflexibility and scalability of the code auto generation process and offers\nagility that allows seamless adaptation to changes in models or deployment\nenvironments. We illustrate this by modeling a multi agent Unmanned Vehicle\nFleet (UVF) system using the Unified Modeling Language (UML), significantly\nreducing model ambiguity by integrating the Object Constraint Language (OCL)\nfor code structure meta modeling, and the FIPA ontology language for\ncommunication semantics meta modeling. Applying GPT4 auto generation\ncapabilities yields Java and Python code that is compatible with the JADE and\nPADE frameworks, respectively. Our thorough evaluation of the auto generated\ncode verifies its alignment with expected behaviors and identifies enhancements\nin agent interactions. Structurally, we assessed the complexity of code derived\nfrom a model constrained solely by OCL meta models, against that influenced by\nboth OCL and FIPA ontology meta models. The results indicate that the ontology\nconstrained meta model produces inherently more complex code, yet its\ncyclomatic complexity remains within manageable levels, suggesting that\nadditional meta model constraints can be incorporated without exceeding the\nhigh risk threshold for complexity."
                },
                "authors": [
                    {
                        "name": "Ahmed R. Sadik"
                    },
                    {
                        "name": "Sebastian Brulin"
                    },
                    {
                        "name": "Markus Olhofer"
                    },
                    {
                        "name": "Antonello Ceravola"
                    },
                    {
                        "name": "Frank Joublin"
                    }
                ],
                "author_detail": {
                    "name": "Frank Joublin"
                },
                "author": "Frank Joublin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13464v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13464v2",
                "updated": "2024-10-24T07:09:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    7,
                    9,
                    43,
                    3,
                    298,
                    0
                ],
                "published": "2024-08-24T04:48:32Z",
                "published_parsed": [
                    2024,
                    8,
                    24,
                    4,
                    48,
                    32,
                    5,
                    237,
                    0
                ],
                "title": "Uncovering Biases with Reflective Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering Biases with Reflective Large Language Models"
                },
                "summary": "Biases and errors in human-labeled data present significant challenges for\nmachine learning, especially in supervised learning reliant on potentially\nflawed ground truth data. These flaws, including diagnostic errors and societal\nbiases, risk being propagated and amplified through models trained using\nmaximum likelihood estimation. We present the Reflective LLM Dialogue Framework\nRLDF, which leverages structured adversarial dialogues between multiple\ninstances of a single LLM or different LLMs to uncover diverse perspectives and\ncorrect inconsistencies. By conditioning LLMs to adopt opposing stances, RLDF\nenables systematic bias detection through conditional statistics, information\ntheory, and divergence metrics. Experiments show RLDF successfully identifies\npotential biases in public content while exposing limitations in human-labeled\ndata. Our framework supports measurable progress tracking and explainable\nremediation actions, offering a scalable approach for improving content\nneutrality through transparent, multi-perspective analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biases and errors in human-labeled data present significant challenges for\nmachine learning, especially in supervised learning reliant on potentially\nflawed ground truth data. These flaws, including diagnostic errors and societal\nbiases, risk being propagated and amplified through models trained using\nmaximum likelihood estimation. We present the Reflective LLM Dialogue Framework\nRLDF, which leverages structured adversarial dialogues between multiple\ninstances of a single LLM or different LLMs to uncover diverse perspectives and\ncorrect inconsistencies. By conditioning LLMs to adopt opposing stances, RLDF\nenables systematic bias detection through conditional statistics, information\ntheory, and divergence metrics. Experiments show RLDF successfully identifies\npotential biases in public content while exposing limitations in human-labeled\ndata. Our framework supports measurable progress tracking and explainable\nremediation actions, offering a scalable approach for improving content\nneutrality through transparent, multi-perspective analysis."
                },
                "authors": [
                    {
                        "name": "Edward Y. Chang"
                    }
                ],
                "author_detail": {
                    "name": "Edward Y. Chang"
                },
                "author": "Edward Y. Chang",
                "arxiv_comment": "18 pages, 4 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13464v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13464v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05804v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05804v5",
                "updated": "2024-10-24T07:07:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    7,
                    7,
                    43,
                    3,
                    298,
                    0
                ],
                "published": "2024-06-09T14:42:55Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    14,
                    42,
                    55,
                    6,
                    161,
                    0
                ],
                "title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning"
                },
                "summary": "Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work. Resources have been made publicly available at in our\nGitHub repository https://github.com/xinzhel/LLM-Agent-Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work. Resources have been made publicly available at in our\nGitHub repository https://github.com/xinzhel/LLM-Agent-Survey."
                },
                "authors": [
                    {
                        "name": "Xinzhe Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinzhe Li"
                },
                "author": "Xinzhe Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05804v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05804v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18472v1",
                "updated": "2024-10-24T06:47:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    6,
                    47,
                    28,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T06:47:28Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    6,
                    47,
                    28,
                    3,
                    298,
                    0
                ],
                "title": "What If the Input is Expanded in OOD Detection?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What If the Input is Expanded in OOD Detection?"
                },
                "summary": "Out-of-distribution (OOD) detection aims to identify OOD inputs from unknown\nclasses, which is important for the reliable deployment of machine learning\nmodels in the open world. Various scoring functions are proposed to distinguish\nit from in-distribution (ID) data. However, existing methods generally focus on\nexcavating the discriminative information from a single input, which implicitly\nlimits its representation dimension. In this work, we introduce a novel\nperspective, i.e., employing different common corruptions on the input space,\nto expand that. We reveal an interesting phenomenon termed confidence mutation,\nwhere the confidence of OOD data can decrease significantly under the\ncorruptions, while the ID data shows a higher confidence expectation\nconsidering the resistance of semantic features. Based on that, we formalize a\nnew scoring method, namely, Confidence aVerage (CoVer), which can capture the\ndynamic differences by simply averaging the scores obtained from different\ncorrupted inputs and the original ones, making the OOD and ID distributions\nmore separable in detection tasks. Extensive experiments and analyses have been\nconducted to understand and verify the effectiveness of CoVer. The code is\npublicly available at: https://github.com/tmlr-group/CoVer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection aims to identify OOD inputs from unknown\nclasses, which is important for the reliable deployment of machine learning\nmodels in the open world. Various scoring functions are proposed to distinguish\nit from in-distribution (ID) data. However, existing methods generally focus on\nexcavating the discriminative information from a single input, which implicitly\nlimits its representation dimension. In this work, we introduce a novel\nperspective, i.e., employing different common corruptions on the input space,\nto expand that. We reveal an interesting phenomenon termed confidence mutation,\nwhere the confidence of OOD data can decrease significantly under the\ncorruptions, while the ID data shows a higher confidence expectation\nconsidering the resistance of semantic features. Based on that, we formalize a\nnew scoring method, namely, Confidence aVerage (CoVer), which can capture the\ndynamic differences by simply averaging the scores obtained from different\ncorrupted inputs and the original ones, making the OOD and ID distributions\nmore separable in detection tasks. Extensive experiments and analyses have been\nconducted to understand and verify the effectiveness of CoVer. The code is\npublicly available at: https://github.com/tmlr-group/CoVer."
                },
                "authors": [
                    {
                        "name": "Boxuan Zhang"
                    },
                    {
                        "name": "Jianing Zhu"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Tongliang Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "arxiv_comment": "accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18469v1",
                "updated": "2024-10-24T06:36:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    6,
                    36,
                    12,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T06:36:12Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    6,
                    36,
                    12,
                    3,
                    298,
                    0
                ],
                "title": "Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities"
                },
                "summary": "Recent research has shown that Large Language Models (LLMs) are vulnerable to\nautomated jailbreak attacks, where adversarial suffixes crafted by algorithms\nappended to harmful queries bypass safety alignment and trigger unintended\nresponses. Current methods for generating these suffixes are computationally\nexpensive and have low Attack Success Rates (ASR), especially against\nwell-aligned models like Llama2 and Llama3. To overcome these limitations, we\nintroduce ADV-LLM, an iterative self-tuning process that crafts adversarial\nLLMs with enhanced jailbreak ability. Our framework significantly reduces the\ncomputational cost of generating adversarial suffixes while achieving nearly\n100\\% ASR on various open-source LLMs. Moreover, it exhibits strong attack\ntransferability to closed-source models, achieving 99% ASR on GPT-3.5 and 49%\nASR on GPT-4, despite being optimized solely on Llama3. Beyond improving\njailbreak ability, ADV-LLM provides valuable insights for future safety\nalignment research through its ability to generate large datasets for studying\nLLM safety. Our code is available at: https://github.com/SunChungEn/ADV-LLM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that Large Language Models (LLMs) are vulnerable to\nautomated jailbreak attacks, where adversarial suffixes crafted by algorithms\nappended to harmful queries bypass safety alignment and trigger unintended\nresponses. Current methods for generating these suffixes are computationally\nexpensive and have low Attack Success Rates (ASR), especially against\nwell-aligned models like Llama2 and Llama3. To overcome these limitations, we\nintroduce ADV-LLM, an iterative self-tuning process that crafts adversarial\nLLMs with enhanced jailbreak ability. Our framework significantly reduces the\ncomputational cost of generating adversarial suffixes while achieving nearly\n100\\% ASR on various open-source LLMs. Moreover, it exhibits strong attack\ntransferability to closed-source models, achieving 99% ASR on GPT-3.5 and 49%\nASR on GPT-4, despite being optimized solely on Llama3. Beyond improving\njailbreak ability, ADV-LLM provides valuable insights for future safety\nalignment research through its ability to generate large datasets for studying\nLLM safety. Our code is available at: https://github.com/SunChungEn/ADV-LLM"
                },
                "authors": [
                    {
                        "name": "Chung-En Sun"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Weiwei Yang"
                    },
                    {
                        "name": "Tsui-Wei Weng"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Aidan San"
                    },
                    {
                        "name": "Michel Galley"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01353v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01353v3",
                "updated": "2024-10-24T06:24:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    6,
                    24,
                    56,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-02T09:11:10Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    9,
                    11,
                    10,
                    2,
                    276,
                    0
                ],
                "title": "Codev-Bench: How Do LLMs Understand Developer-Centric Code Completion?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Codev-Bench: How Do LLMs Understand Developer-Centric Code Completion?"
                },
                "summary": "Code completion, a key downstream task in code generation, is one of the most\nfrequent and impactful methods for enhancing developer productivity in software\ndevelopment. As intelligent completion tools evolve, we need a robust\nevaluation benchmark that enables meaningful comparisons between products and\nguides future advancements. However, existing benchmarks focus more on\ncoarse-grained tasks without industrial analysis resembling general code\ngeneration rather than the real-world scenarios developers encounter. Moreover,\nthese benchmarks often rely on costly and time-consuming human annotation, and\nthe standalone test cases fail to leverage minimal tests for maximum\nrepository-level understanding and code coverage. To address these limitations,\nwe first analyze business data from an industrial code completion tool and\nredefine the evaluation criteria to better align with the developer's intent\nand desired completion behavior throughout the coding process. Based on these\ninsights, we introduce Codev-Agent, an agent-based system that automates\nrepository crawling, constructs execution environments, extracts dynamic\ncalling chains from existing unit tests, and generates new test samples to\navoid data leakage, ensuring fair and effective comparisons. Using Codev-Agent,\nwe present the Code-Development Benchmark (Codev-Bench), a fine-grained,\nreal-world, repository-level, and developer-centric evaluation framework.\nCodev-Bench assesses whether a code completion tool can capture a developer's\nimmediate intent and suggest appropriate code across diverse contexts,\nproviding a more realistic benchmark for code completion in modern software\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code completion, a key downstream task in code generation, is one of the most\nfrequent and impactful methods for enhancing developer productivity in software\ndevelopment. As intelligent completion tools evolve, we need a robust\nevaluation benchmark that enables meaningful comparisons between products and\nguides future advancements. However, existing benchmarks focus more on\ncoarse-grained tasks without industrial analysis resembling general code\ngeneration rather than the real-world scenarios developers encounter. Moreover,\nthese benchmarks often rely on costly and time-consuming human annotation, and\nthe standalone test cases fail to leverage minimal tests for maximum\nrepository-level understanding and code coverage. To address these limitations,\nwe first analyze business data from an industrial code completion tool and\nredefine the evaluation criteria to better align with the developer's intent\nand desired completion behavior throughout the coding process. Based on these\ninsights, we introduce Codev-Agent, an agent-based system that automates\nrepository crawling, constructs execution environments, extracts dynamic\ncalling chains from existing unit tests, and generates new test samples to\navoid data leakage, ensuring fair and effective comparisons. Using Codev-Agent,\nwe present the Code-Development Benchmark (Codev-Bench), a fine-grained,\nreal-world, repository-level, and developer-centric evaluation framework.\nCodev-Bench assesses whether a code completion tool can capture a developer's\nimmediate intent and suggest appropriate code across diverse contexts,\nproviding a more realistic benchmark for code completion in modern software\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Zhenyu Pan"
                    },
                    {
                        "name": "Rongyu Cao"
                    },
                    {
                        "name": "Yongchang Cao"
                    },
                    {
                        "name": "Yingwei Ma"
                    },
                    {
                        "name": "Binhua Li"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Yongbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Li"
                },
                "author": "Yongbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01353v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01353v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18460v1",
                "updated": "2024-10-24T06:12:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    6,
                    12,
                    3,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T06:12:03Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    6,
                    12,
                    3,
                    3,
                    298,
                    0
                ],
                "title": "Beyond Multiple-Choice Accuracy: Real-World Challenges of Implementing\n  Large Language Models in Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Multiple-Choice Accuracy: Real-World Challenges of Implementing\n  Large Language Models in Healthcare"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention in the medical\ndomain for their human-level capabilities, leading to increased efforts to\nexplore their potential in various healthcare applications. However, despite\nsuch a promising future, there are multiple challenges and obstacles that\nremain for their real-world uses in practical settings. This work discusses key\nchallenges for LLMs in medical applications from four unique aspects:\noperational vulnerabilities, ethical and social considerations, performance and\nassessment difficulties, and legal and regulatory compliance. Addressing these\nchallenges is crucial for leveraging LLMs to their full potential and ensuring\ntheir responsible integration into healthcare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention in the medical\ndomain for their human-level capabilities, leading to increased efforts to\nexplore their potential in various healthcare applications. However, despite\nsuch a promising future, there are multiple challenges and obstacles that\nremain for their real-world uses in practical settings. This work discusses key\nchallenges for LLMs in medical applications from four unique aspects:\noperational vulnerabilities, ethical and social considerations, performance and\nassessment difficulties, and legal and regulatory compliance. Addressing these\nchallenges is crucial for leveraging LLMs to their full potential and ensuring\ntheir responsible integration into healthcare."
                },
                "authors": [
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Qingqing Zhu"
                    },
                    {
                        "name": "Zhizheng Wang"
                    },
                    {
                        "name": "Francisco Erramuspe Álvarez"
                    },
                    {
                        "name": "Nicholas Wan"
                    },
                    {
                        "name": "Benjamin Hou"
                    },
                    {
                        "name": "Zhiyong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Lu"
                },
                "author": "Zhiyong Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18451v1",
                "updated": "2024-10-24T06:06:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    6,
                    6,
                    26,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T06:06:26Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    6,
                    6,
                    26,
                    3,
                    298,
                    0
                ],
                "title": "Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs"
                },
                "summary": "In this report, we introduce a collection of methods to enhance reward\nmodeling for LLMs, focusing specifically on data-centric techniques. We propose\neffective data selection and filtering strategies for curating high-quality\nopen-source preference datasets, culminating in the Skywork-Reward data\ncollection, which contains only 80K preference pairs -- significantly smaller\nthan existing datasets. Using this curated dataset, we developed the\nSkywork-Reward model series -- Skywork-Reward-Gemma-27B and\nSkywork-Reward-Llama-3.1-8B -- with the former currently holding the top\nposition on the RewardBench leaderboard. Notably, our techniques and datasets\nhave directly enhanced the performance of many top-ranked models on\nRewardBench, highlighting the practical impact of our contributions in\nreal-world preference learning applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we introduce a collection of methods to enhance reward\nmodeling for LLMs, focusing specifically on data-centric techniques. We propose\neffective data selection and filtering strategies for curating high-quality\nopen-source preference datasets, culminating in the Skywork-Reward data\ncollection, which contains only 80K preference pairs -- significantly smaller\nthan existing datasets. Using this curated dataset, we developed the\nSkywork-Reward model series -- Skywork-Reward-Gemma-27B and\nSkywork-Reward-Llama-3.1-8B -- with the former currently holding the top\nposition on the RewardBench leaderboard. Notably, our techniques and datasets\nhave directly enhanced the performance of many top-ranked models on\nRewardBench, highlighting the practical impact of our contributions in\nreal-world preference learning applications."
                },
                "authors": [
                    {
                        "name": "Chris Yuhao Liu"
                    },
                    {
                        "name": "Liang Zeng"
                    },
                    {
                        "name": "Jiacai Liu"
                    },
                    {
                        "name": "Rui Yan"
                    },
                    {
                        "name": "Jujie He"
                    },
                    {
                        "name": "Chaojie Wang"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yahui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yahui Zhou"
                },
                "author": "Yahui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17264v3",
                "updated": "2024-10-24T06:05:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    6,
                    5,
                    3,
                    3,
                    298,
                    0
                ],
                "published": "2024-05-27T15:22:58Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    15,
                    22,
                    58,
                    0,
                    148,
                    0
                ],
                "title": "On the Noise Robustness of In-Context Learning for Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Noise Robustness of In-Context Learning for Text Generation"
                },
                "summary": "Large language models (LLMs) have shown impressive performance on downstream\ntasks by in-context learning (ICL), which heavily relies on the quality of\ndemonstrations selected from a large set of annotated examples. Recent works\nclaim that in-context learning is robust to noisy demonstrations in text\nclassification. In this work, we show that, on text generation tasks, noisy\nannotations significantly hurt the performance of in-context learning. To\ncircumvent the issue, we propose a simple and effective approach called Local\nPerplexity Ranking (LPR), which replaces the \"noisy\" candidates with their\nnearest neighbors that are more likely to be clean. Our method is motivated by\nanalyzing the perplexity deviation caused by noisy labels and decomposing\nperplexity into inherent perplexity and matching perplexity. Our key idea\nbehind LPR is thus to decouple the matching perplexity by performing the\nranking among the neighbors in semantic space. Our approach can prevent the\nselected demonstrations from including mismatched input-label pairs while\npreserving the effectiveness of the original selection methods. Extensive\nexperiments demonstrate the effectiveness of LPR, improving the EM score by up\nto 18.75 on common benchmarks with noisy annotations. Our code is available at\nhttps://github.com/ml-stat-Sustech/Local-Perplexity-Ranking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive performance on downstream\ntasks by in-context learning (ICL), which heavily relies on the quality of\ndemonstrations selected from a large set of annotated examples. Recent works\nclaim that in-context learning is robust to noisy demonstrations in text\nclassification. In this work, we show that, on text generation tasks, noisy\nannotations significantly hurt the performance of in-context learning. To\ncircumvent the issue, we propose a simple and effective approach called Local\nPerplexity Ranking (LPR), which replaces the \"noisy\" candidates with their\nnearest neighbors that are more likely to be clean. Our method is motivated by\nanalyzing the perplexity deviation caused by noisy labels and decomposing\nperplexity into inherent perplexity and matching perplexity. Our key idea\nbehind LPR is thus to decouple the matching perplexity by performing the\nranking among the neighbors in semantic space. Our approach can prevent the\nselected demonstrations from including mismatched input-label pairs while\npreserving the effectiveness of the original selection methods. Extensive\nexperiments demonstrate the effectiveness of LPR, improving the EM score by up\nto 18.75 on common benchmarks with noisy annotations. Our code is available at\nhttps://github.com/ml-stat-Sustech/Local-Perplexity-Ranking."
                },
                "authors": [
                    {
                        "name": "Hongfu Gao"
                    },
                    {
                        "name": "Feipeng Zhang"
                    },
                    {
                        "name": "Wenyu Jiang"
                    },
                    {
                        "name": "Jun Shu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11819v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11819v3",
                "updated": "2024-10-24T05:53:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    53,
                    18,
                    3,
                    298,
                    0
                ],
                "published": "2024-02-19T04:19:36Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    4,
                    19,
                    36,
                    0,
                    50,
                    0
                ],
                "title": "Head-wise Shareable Attention for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Head-wise Shareable Attention for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) suffer from huge number of parameters, which\nrestricts their deployment on edge devices. Weight sharing is one promising\nsolution that encourages weight reuse, effectively reducing memory usage with\nless performance drop. However, current weight sharing techniques primarily\nfocus on small-scale models like BERT and employ coarse-grained sharing rules,\ne.g., layer-wise. This becomes limiting given the prevalence of LLMs and\nsharing an entire layer or block obviously diminishes the flexibility of weight\nsharing. In this paper, we present a perspective on head-wise shareable\nattention for large language models. We further propose two memory-efficient\nmethods that share parameters across attention heads, with a specific focus on\nLLMs. Both of them use the same dynamic strategy to select the shared weight\nmatrices. The first method directly reuses the pre-trained weights without\nretraining, denoted as $\\textbf{DirectShare}$. The second method first\npost-trains with constraint on weight matrix similarity and then shares,\ndenoted as $\\textbf{PostShare}$. Experimental results reveal our head-wise\nshared models still maintain satisfactory capabilities, demonstrating the\nfeasibility of fine-grained weight sharing applied to LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer from huge number of parameters, which\nrestricts their deployment on edge devices. Weight sharing is one promising\nsolution that encourages weight reuse, effectively reducing memory usage with\nless performance drop. However, current weight sharing techniques primarily\nfocus on small-scale models like BERT and employ coarse-grained sharing rules,\ne.g., layer-wise. This becomes limiting given the prevalence of LLMs and\nsharing an entire layer or block obviously diminishes the flexibility of weight\nsharing. In this paper, we present a perspective on head-wise shareable\nattention for large language models. We further propose two memory-efficient\nmethods that share parameters across attention heads, with a specific focus on\nLLMs. Both of them use the same dynamic strategy to select the shared weight\nmatrices. The first method directly reuses the pre-trained weights without\nretraining, denoted as $\\textbf{DirectShare}$. The second method first\npost-trains with constraint on weight matrix similarity and then shares,\ndenoted as $\\textbf{PostShare}$. Experimental results reveal our head-wise\nshared models still maintain satisfactory capabilities, demonstrating the\nfeasibility of fine-grained weight sharing applied to LLMs."
                },
                "authors": [
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "17 pages, 7 figures, 21 tables, EMNLP'24 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11819v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11819v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18448v1",
                "updated": "2024-10-24T05:49:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    49,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:49:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    49,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "GPT-Signal: Generative AI for Semi-automated Feature Engineering in the\n  Alpha Research Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT-Signal: Generative AI for Semi-automated Feature Engineering in the\n  Alpha Research Process"
                },
                "summary": "In the trading process, financial signals often imply the time to buy and\nsell assets to generate excess returns compared to a benchmark (e.g., an\nindex). Alpha is the portion of an asset's return that is not explained by\nexposure to this benchmark, and the alpha research process is a popular\ntechnique aiming at developing strategies to generate alphas and gain excess\nreturns. Feature Engineering, a significant pre-processing procedure in machine\nlearning and data analysis that helps extract and create transformed features\nfrom raw data, plays an important role in algorithmic trading strategies and\nthe alpha research process. With the recent development of Generative\nArtificial Intelligence(Gen AI) and Large Language Models (LLMs), we present a\nnovel way of leveraging GPT-4 to generate new return-predictive formulaic\nalphas, making alpha mining a semi-automated process, and saving time and\nenergy for investors and traders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the trading process, financial signals often imply the time to buy and\nsell assets to generate excess returns compared to a benchmark (e.g., an\nindex). Alpha is the portion of an asset's return that is not explained by\nexposure to this benchmark, and the alpha research process is a popular\ntechnique aiming at developing strategies to generate alphas and gain excess\nreturns. Feature Engineering, a significant pre-processing procedure in machine\nlearning and data analysis that helps extract and create transformed features\nfrom raw data, plays an important role in algorithmic trading strategies and\nthe alpha research process. With the recent development of Generative\nArtificial Intelligence(Gen AI) and Large Language Models (LLMs), we present a\nnovel way of leveraging GPT-4 to generate new return-predictive formulaic\nalphas, making alpha mining a semi-automated process, and saving time and\nenergy for investors and traders."
                },
                "authors": [
                    {
                        "name": "Yining Wang"
                    },
                    {
                        "name": "Jinman Zhao"
                    },
                    {
                        "name": "Yuri Lawryshyn"
                    }
                ],
                "author_detail": {
                    "name": "Yuri Lawryshyn"
                },
                "author": "Yuri Lawryshyn",
                "arxiv_comment": "13 pages, 16 figures, 1 table, accepted by FINNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]