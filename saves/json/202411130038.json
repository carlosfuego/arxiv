[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06681v1",
                "updated": "2024-11-11T02:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance."
                },
                "authors": [
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Liang Qian"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06680v1",
                "updated": "2024-11-11T02:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Harald C. Gall"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06659v1",
                "updated": "2024-11-11T01:53:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:53:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning"
                },
                "summary": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git ."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi",
                "arxiv_comment": "16 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v4",
                "updated": "2024-11-10T15:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    58,
                    7,
                    6,
                    315,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04873v2",
                "updated": "2024-11-10T10:08:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    10,
                    8,
                    37,
                    6,
                    315,
                    0
                ],
                "published": "2024-06-07T12:12:25Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    12,
                    12,
                    25,
                    4,
                    159,
                    0
                ],
                "title": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior"
                },
                "summary": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main"
                },
                "authors": [
                    {
                        "name": "Tanvir Mahmud"
                    },
                    {
                        "name": "Mustafa Munir"
                    },
                    {
                        "name": "Radu Marculescu"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu",
                "arxiv_comment": "Accepted in WACV 2025. Project page:\n  https://tanvir-utexas.github.io/AdaVE_Demo/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v1",
                "updated": "2024-11-10T08:31:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v1",
                "updated": "2024-11-10T05:12:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v1",
                "updated": "2024-11-08T02:21:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02542v2",
                "updated": "2024-11-07T18:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    58,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    58,
                    3,
                    1,
                    156,
                    0
                ],
                "title": "Loki: Low-rank Keys for Efficient Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loki: Low-rank Keys for Efficient Sparse Attention"
                },
                "summary": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods."
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "Proceedings of the Thirty-Eighth Annual Conference on Neural\n  Information Processing Systems (Main Conference Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v1",
                "updated": "2024-11-07T14:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16591v2",
                "updated": "2024-11-07T09:33:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    33,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-26T14:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    50,
                    40,
                    6,
                    147,
                    0
                ],
                "title": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification"
                },
                "summary": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter."
                },
                "authors": [
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Guandu Liu"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_doi": "10.1145/3664647.3681566",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681566",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Multimedia 2024 Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v2",
                "updated": "2024-11-07T06:40:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    40,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v1",
                "updated": "2024-11-05T15:22:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v1",
                "updated": "2024-11-04T04:15:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01269v1",
                "updated": "2024-11-02T14:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T14:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "Disaggregated Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Database Management Systems"
                },
                "summary": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Philip A. Bernstein"
                    },
                    {
                        "name": "Dhruba Borthakur"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Jai Menon"
                    },
                    {
                        "name": "Sumit Puri"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Puri"
                },
                "author": "Sumit Puri",
                "arxiv_comment": "This paper appeared in the {\\em Performance Evaluation and\n  Benchmarking} - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW,\n  Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in\n  Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01246v1",
                "updated": "2024-11-02T13:52:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T13:52:49Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "title": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores"
                },
                "summary": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    },
                    {
                        "name": "Jason Yap"
                    }
                ],
                "author_detail": {
                    "name": "Jason Yap"
                },
                "author": "Jason Yap",
                "arxiv_doi": "10.1145/2663165.2663317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2663165.2663317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version of CAMP appeared in the Proceedings of the\n  ACM/IFIP/USENIX Middleware Conference, Bordeaux, France, December 2014. See\n  https://github.com/scdblab/CAMP for an implementation",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01142v1",
                "updated": "2024-11-02T05:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T05:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference"
                },
                "summary": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU."
                },
                "authors": [
                    {
                        "name": "Xuanlin Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "Joo Monteiro"
                    },
                    {
                        "name": "tienne Marcotte"
                    },
                    {
                        "name": "Pierre-Andr Nol"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vzquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00131v1",
                "updated": "2024-10-31T18:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T18:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence"
                },
                "summary": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared."
                },
                "authors": [
                    {
                        "name": "John Whitington"
                    }
                ],
                "author_detail": {
                    "name": "John Whitington"
                },
                "author": "John Whitington",
                "arxiv_doi": "10.1145/2788539.27885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2788539.27885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v1",
                "updated": "2024-10-30T02:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "The code is coming soon! For sure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v3",
                "updated": "2024-10-29T02:52:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    2,
                    52,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v2",
                "updated": "2024-10-28T19:32:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    32,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark."
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v1",
                "updated": "2024-10-28T19:08:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v1",
                "updated": "2024-10-28T07:13:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01847v3",
                "updated": "2024-10-27T14:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    14,
                    40,
                    8,
                    6,
                    301,
                    0
                ],
                "published": "2024-04-02T11:12:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    11,
                    12,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Transformer Pre-training with 2:4 Sparsity"
                },
                "summary": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20337v1",
                "updated": "2024-10-27T04:31:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "published": "2024-10-27T04:31:35Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "title": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms"
                },
                "summary": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided."
                },
                "authors": [
                    {
                        "name": "Lorenzo De Stefani"
                    },
                    {
                        "name": "Vedant Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vedant Gupta"
                },
                "author": "Vedant Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04216v3",
                "updated": "2024-10-26T22:19:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    22,
                    19,
                    4,
                    5,
                    300,
                    0
                ],
                "published": "2024-02-06T18:17:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    18,
                    17,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks"
                },
                "summary": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Md Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in IEEE Transactions on\n  Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20149v1",
                "updated": "2024-10-26T11:20:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "published": "2024-10-26T11:20:02Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models"
                },
                "summary": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}."
                },
                "authors": [
                    {
                        "name": "Yabin Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "NIPS 2024 Camera Ready, Codes are available at\n  \\url{https://github.com/YBZh/OpenOOD-VLM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20004v1",
                "updated": "2024-10-25T23:17:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T23:17:56Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "title": "Lightweight, Secure and Stateful Serverless Computing with PSL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight, Secure and Stateful Serverless Computing with PSL"
                },
                "summary": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks."
                },
                "authors": [
                    {
                        "name": "Alexander Thomas"
                    },
                    {
                        "name": "Shubham Mishra"
                    },
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "John Kubiatowicz"
                    }
                ],
                "author_detail": {
                    "name": "John Kubiatowicz"
                },
                "author": "John Kubiatowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05317v2",
                "updated": "2024-10-25T21:09:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    21,
                    9,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-08T01:35:11Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    35,
                    11,
                    5,
                    160,
                    0
                ],
                "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoCoCo: Dropping In Convolutions for Long Context Compression"
                },
                "summary": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v2",
                "updated": "2024-10-25T19:45:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    45,
                    33,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19937v1",
                "updated": "2024-10-25T19:18:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T19:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction"
                },
                "summary": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Changjiang Li"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18248v2",
                "updated": "2024-10-25T19:18:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T19:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Fast Inference for Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference for Augmented Large Language Models"
                },
                "summary": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM."
                },
                "authors": [
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Cong Liang"
                    },
                    {
                        "name": "Shiji Xin"
                    },
                    {
                        "name": "Qianru Lao"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v5",
                "updated": "2024-10-25T18:29:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    29,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v1",
                "updated": "2024-10-25T07:24:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19123v1",
                "updated": "2024-10-24T19:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T19:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design"
                },
                "summary": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18441v1",
                "updated": "2024-10-24T05:29:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:29:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI"
                },
                "summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings."
                },
                "authors": [
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v1",
                "updated": "2024-10-23T14:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v3",
                "updated": "2024-10-23T10:39:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    39,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v5",
                "updated": "2024-10-23T05:55:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    55,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14740v2",
                "updated": "2024-10-23T01:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    1,
                    8,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    33,
                    39,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD."
                },
                "authors": [
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhang Cao"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Zhichao Cao"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11724v2",
                "updated": "2024-10-22T19:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    22,
                    19,
                    7,
                    8,
                    1,
                    296,
                    0
                ],
                "published": "2024-05-20T01:57:34Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    1,
                    57,
                    34,
                    0,
                    141,
                    0
                ],
                "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-wise Influential Training Data Retrieval for Large Language Models"
                },
                "summary": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Jikai Long"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16218v1",
                "updated": "2024-10-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T17:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire"
                },
                "summary": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Swarnav Mukhopadhyay"
                    },
                    {
                        "name": "Md Mobinul Haque"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v2",
                "updated": "2024-10-21T15:59:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    59,
                    18,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v1",
                "updated": "2024-10-21T11:29:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14142v2",
                "updated": "2024-10-21T07:24:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    24,
                    53,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-18T03:30:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels"
                },
                "summary": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed."
                },
                "authors": [
                    {
                        "name": "Tianqing Zhou"
                    },
                    {
                        "name": "Bobo Wang"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Xuefang Nie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15704v1",
                "updated": "2024-10-21T07:20:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T07:20:41Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "title": "Residual vector quantization for KV cache compression in large language\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual vector quantization for KV cache compression in large language\n  model"
                },
                "summary": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision."
                },
                "authors": [
                    {
                        "name": "Ankur Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Kumar"
                },
                "author": "Ankur Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v2",
                "updated": "2024-10-21T05:06:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    5,
                    6,
                    1,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v2",
                "updated": "2024-10-21T02:35:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    35,
                    8,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04053v2",
                "updated": "2024-10-20T13:37:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    13,
                    37,
                    46,
                    6,
                    294,
                    0
                ],
                "published": "2024-07-04T16:51:17Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    16,
                    51,
                    17,
                    3,
                    186,
                    0
                ],
                "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI: A Taxonomy, Systematic Review and Future Directions"
                },
                "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions."
                },
                "authors": [
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Junhui Du"
                    },
                    {
                        "name": "Huaming Wu"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Subramaniam Subramanian Murugesan"
                    },
                    {
                        "name": "Babar Ali"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Prabal Verma"
                    },
                    {
                        "name": "Surendra Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1007/s10586-024-04686-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-024-04686-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in Springer Cluster\n  Computing, 2024",
                "arxiv_journal_ref": "Springer Cluster Computing, Volume 28, article number 18, pages\n  11953 - 11981, (2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15344v1",
                "updated": "2024-10-20T09:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T09:37:07Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "title": "LLC Intra-set Write Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLC Intra-set Write Balancing"
                },
                "summary": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance."
                },
                "authors": [
                    {
                        "name": "Keshav Krishna"
                    },
                    {
                        "name": "Ayush Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Verma"
                },
                "author": "Ayush Verma",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v1",
                "updated": "2024-10-20T08:42:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15252v1",
                "updated": "2024-10-20T02:17:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T02:17:35Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "title": "Lossless KV Cache Compression to 2%",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless KV Cache Compression to 2%"
                },
                "summary": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "J. N. Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.07240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07240v1",
                "updated": "2024-11-11T18:59:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    59,
                    2,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T18:59:02Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    59,
                    2,
                    0,
                    316,
                    0
                ],
                "title": "UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts"
                },
                "summary": "The evaluation of mathematical reasoning capabilities is essential for\nadvancing Artificial General Intelligence (AGI). While Large Language Models\n(LLMs) have shown impressive performance in solving mathematical problems,\nexisting benchmarks such as GSM8K and MATH present limitations, including\nnarrow problem definitions with specific numbers and reliance on predetermined\nrules that hinder accurate assessments of reasoning and adaptability. This\npaper introduces the UTMath Benchmark, which robustly evaluates the models\nthrough extensive unit tests. It consists of 1,053 problems across 9\nmathematical domains, with over 68 test cases per problem.We propose an\ninnovative evaluation framework inspired by unit testing in software\ndevelopment, focusing on both accuracy and reliability of results. Furthermore,\nwe introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which\nencourages LLMs to perform explicit reasoning before generating code, leading\nto generating more advanced solution and improved performance. Furthermore, we\nare releasing not only the UTMath benchmark but also the UTMath-Train training\ndataset (more than 70k samples), to support the community in further exploring\nmathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of mathematical reasoning capabilities is essential for\nadvancing Artificial General Intelligence (AGI). While Large Language Models\n(LLMs) have shown impressive performance in solving mathematical problems,\nexisting benchmarks such as GSM8K and MATH present limitations, including\nnarrow problem definitions with specific numbers and reliance on predetermined\nrules that hinder accurate assessments of reasoning and adaptability. This\npaper introduces the UTMath Benchmark, which robustly evaluates the models\nthrough extensive unit tests. It consists of 1,053 problems across 9\nmathematical domains, with over 68 test cases per problem.We propose an\ninnovative evaluation framework inspired by unit testing in software\ndevelopment, focusing on both accuracy and reliability of results. Furthermore,\nwe introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which\nencourages LLMs to perform explicit reasoning before generating code, leading\nto generating more advanced solution and improved performance. Furthermore, we\nare releasing not only the UTMath benchmark but also the UTMath-Train training\ndataset (more than 70k samples), to support the community in further exploring\nmathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Qingping Yang"
                    },
                    {
                        "name": "Runtao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Runtao Liu"
                },
                "author": "Runtao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07228v1",
                "updated": "2024-11-11T18:46:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    46,
                    37,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T18:46:37Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    46,
                    37,
                    0,
                    316,
                    0
                ],
                "title": "Tooling or Not Tooling? The Impact of Tools on Language Agents for\n  Chemistry Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tooling or Not Tooling? The Impact of Tools on Language Agents for\n  Chemistry Problem Solving"
                },
                "summary": "To enhance large language models (LLMs) for chemistry problem solving,\nseveral LLM-based agents augmented with tools have been proposed, such as\nChemCrow and Coscientist. However, their evaluations are narrow in scope,\nleaving a large gap in understanding the benefits of tools across diverse\nchemistry tasks. To bridge this gap, we develop ChemAgent, an enhanced\nchemistry agent over ChemCrow, and conduct a comprehensive evaluation of its\nperformance on both specialized chemistry tasks and general chemistry\nquestions. Surprisingly, ChemAgent does not consistently outperform its base\nLLMs without tools. Our error analysis with a chemistry expert suggests that:\nFor specialized chemistry tasks, such as synthesis prediction, we should\naugment agents with specialized tools; however, for general chemistry questions\nlike those in exams, agents' ability to reason correctly with chemistry\nknowledge matters more, and tool augmentation does not always help.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enhance large language models (LLMs) for chemistry problem solving,\nseveral LLM-based agents augmented with tools have been proposed, such as\nChemCrow and Coscientist. However, their evaluations are narrow in scope,\nleaving a large gap in understanding the benefits of tools across diverse\nchemistry tasks. To bridge this gap, we develop ChemAgent, an enhanced\nchemistry agent over ChemCrow, and conduct a comprehensive evaluation of its\nperformance on both specialized chemistry tasks and general chemistry\nquestions. Surprisingly, ChemAgent does not consistently outperform its base\nLLMs without tools. Our error analysis with a chemistry expert suggests that:\nFor specialized chemistry tasks, such as synthesis prediction, we should\naugment agents with specialized tools; however, for general chemistry questions\nlike those in exams, agents' ability to reason correctly with chemistry\nknowledge matters more, and tool augmentation does not always help."
                },
                "authors": [
                    {
                        "name": "Botao Yu"
                    },
                    {
                        "name": "Frazier N. Baker"
                    },
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Garrett Herb"
                    },
                    {
                        "name": "Boyu Gou"
                    },
                    {
                        "name": "Daniel Adu-Ampratwum"
                    },
                    {
                        "name": "Xia Ning"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02150v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02150v2",
                "updated": "2024-11-11T18:38:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    38,
                    32,
                    0,
                    316,
                    0
                ],
                "published": "2023-06-03T16:36:43Z",
                "published_parsed": [
                    2023,
                    6,
                    3,
                    16,
                    36,
                    43,
                    5,
                    154,
                    0
                ],
                "title": "An information field theory approach to Bayesian state and parameter\n  estimation in dynamical systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An information field theory approach to Bayesian state and parameter\n  estimation in dynamical systems"
                },
                "summary": "Dynamical system state estimation and parameter calibration problems are\nubiquitous across science and engineering. Bayesian approaches to the problem\nare the gold standard as they allow for the quantification of uncertainties and\nenable the seamless fusion of different experimental modalities. When the\ndynamics are discrete and stochastic, one may employ powerful techniques such\nas Kalman, particle, or variational filters. Practitioners commonly apply these\nmethods to continuous-time, deterministic dynamical systems after discretizing\nthe dynamics and introducing fictitious transition probabilities. However,\napproaches based on time-discretization suffer from the curse of dimensionality\nsince the number of random variables grows linearly with the number of\ntime-steps. Furthermore, the introduction of fictitious transition\nprobabilities is an unsatisfactory solution because it increases the number of\nmodel parameters and may lead to inference bias. To address these drawbacks,\nthe objective of this paper is to develop a scalable Bayesian approach to state\nand parameter estimation suitable for continuous-time, deterministic dynamical\nsystems. Our methodology builds upon information field theory. Specifically, we\nconstruct a physics-informed prior probability measure on the function space of\nsystem responses so that functions that satisfy the physics are more likely.\nThis prior allows us to quantify model form errors. We connect the system's\nresponse to observations through a probabilistic model of the measurement\nprocess. The joint posterior over the system responses and all parameters is\ngiven by Bayes' rule. To approximate the intractable posterior, we develop a\nstochastic variational inference algorithm. In summary, the developed\nmethodology offers a powerful framework for Bayesian estimation in dynamical\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamical system state estimation and parameter calibration problems are\nubiquitous across science and engineering. Bayesian approaches to the problem\nare the gold standard as they allow for the quantification of uncertainties and\nenable the seamless fusion of different experimental modalities. When the\ndynamics are discrete and stochastic, one may employ powerful techniques such\nas Kalman, particle, or variational filters. Practitioners commonly apply these\nmethods to continuous-time, deterministic dynamical systems after discretizing\nthe dynamics and introducing fictitious transition probabilities. However,\napproaches based on time-discretization suffer from the curse of dimensionality\nsince the number of random variables grows linearly with the number of\ntime-steps. Furthermore, the introduction of fictitious transition\nprobabilities is an unsatisfactory solution because it increases the number of\nmodel parameters and may lead to inference bias. To address these drawbacks,\nthe objective of this paper is to develop a scalable Bayesian approach to state\nand parameter estimation suitable for continuous-time, deterministic dynamical\nsystems. Our methodology builds upon information field theory. Specifically, we\nconstruct a physics-informed prior probability measure on the function space of\nsystem responses so that functions that satisfy the physics are more likely.\nThis prior allows us to quantify model form errors. We connect the system's\nresponse to observations through a probabilistic model of the measurement\nprocess. The joint posterior over the system responses and all parameters is\ngiven by Bayes' rule. To approximate the intractable posterior, we develop a\nstochastic variational inference algorithm. In summary, the developed\nmethodology offers a powerful framework for Bayesian estimation in dynamical\nsystems."
                },
                "authors": [
                    {
                        "name": "Kairui Hao"
                    },
                    {
                        "name": "Ilias Bilionis"
                    }
                ],
                "author_detail": {
                    "name": "Ilias Bilionis"
                },
                "author": "Ilias Bilionis",
                "arxiv_doi": "10.48550/arXiv.2306.02150",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.48550/arXiv.2306.02150",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2306.02150v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02150v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07213v1",
                "updated": "2024-11-11T18:36:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    36,
                    17,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T18:36:17Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    36,
                    17,
                    0,
                    316,
                    0
                ],
                "title": "Comparing Bottom-Up and Top-Down Steering Approaches on In-Context\n  Learning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Bottom-Up and Top-Down Steering Approaches on In-Context\n  Learning Tasks"
                },
                "summary": "A key objective of interpretability research on large language models (LLMs)\nis to develop methods for robustly steering models toward desired behaviors. To\nthis end, two distinct approaches to interpretability -- ``bottom-up\" and\n``top-down\" -- have been presented, but there has been little quantitative\ncomparison between them. We present a case study comparing the effectiveness of\nrepresentative vector steering methods from each branch: function vectors (FV;\narXiv:2310.15213), as a bottom-up method, and in-context vectors (ICV;\narXiv:2311.06668) as a top-down method. While both aim to capture compact\nrepresentations of broad in-context learning tasks, we find they are effective\nonly on specific types of tasks: ICVs outperform FVs in behavioral shifting,\nwhereas FVs excel in tasks requiring more precision. We discuss the\nimplications for future evaluations of steering methods and for further\nresearch into top-down and bottom-up steering given these findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key objective of interpretability research on large language models (LLMs)\nis to develop methods for robustly steering models toward desired behaviors. To\nthis end, two distinct approaches to interpretability -- ``bottom-up\" and\n``top-down\" -- have been presented, but there has been little quantitative\ncomparison between them. We present a case study comparing the effectiveness of\nrepresentative vector steering methods from each branch: function vectors (FV;\narXiv:2310.15213), as a bottom-up method, and in-context vectors (ICV;\narXiv:2311.06668) as a top-down method. While both aim to capture compact\nrepresentations of broad in-context learning tasks, we find they are effective\nonly on specific types of tasks: ICVs outperform FVs in behavioral shifting,\nwhereas FVs excel in tasks requiring more precision. We discuss the\nimplications for future evaluations of steering methods and for further\nresearch into top-down and bottom-up steering given these findings."
                },
                "authors": [
                    {
                        "name": "Madeline Brumley"
                    },
                    {
                        "name": "Joe Kwon"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Dmitrii Krasheninnikov"
                    },
                    {
                        "name": "Usman Anwar"
                    }
                ],
                "author_detail": {
                    "name": "Usman Anwar"
                },
                "author": "Usman Anwar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07207v1",
                "updated": "2024-11-11T18:32:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    32,
                    44,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T18:32:44Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    32,
                    44,
                    0,
                    316,
                    0
                ],
                "title": "General Geospatial Inference with a Population Dynamics Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General Geospatial Inference with a Population Dynamics Foundation Model"
                },
                "summary": "Supporting the health and well-being of dynamic populations around the world\nrequires governmental agencies, organizations and researchers to understand and\nreason over complex relationships between human behavior and local contexts in\norder to identify high-risk groups and strategically allocate limited\nresources. Traditional approaches to these classes of problems often entail\ndeveloping manually curated, task-specific features and models to represent\nhuman behavior and the natural and built environment, which can be challenging\nto adapt to new, or even, related tasks. To address this, we introduce a\nPopulation Dynamics Foundation Model (PDFM) that aims to capture the\nrelationships between diverse data modalities and is applicable to a broad\nrange of geospatial tasks. We first construct a geo-indexed dataset for postal\ncodes and counties across the United States, capturing rich aggregated\ninformation on human behavior from maps, busyness, and aggregated search\ntrends, and environmental factors such as weather and air quality. We then\nmodel this data and the complex relationships between locations using a graph\nneural network, producing embeddings that can be adapted to a wide range of\ndownstream tasks using relatively simple models. We evaluate the effectiveness\nof our approach by benchmarking it on 27 downstream tasks spanning three\ndistinct domains: health indicators, socioeconomic factors, and environmental\nmeasurements. The approach achieves state-of-the-art performance on all 27\ngeospatial interpolation tasks, and on 25 out of the 27 extrapolation and\nsuper-resolution tasks. We combined the PDFM with a state-of-the-art\nforecasting foundation model, TimesFM, to predict unemployment and poverty,\nachieving performance that surpasses fully supervised forecasting. The full set\nof embeddings and sample code are publicly available for researchers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supporting the health and well-being of dynamic populations around the world\nrequires governmental agencies, organizations and researchers to understand and\nreason over complex relationships between human behavior and local contexts in\norder to identify high-risk groups and strategically allocate limited\nresources. Traditional approaches to these classes of problems often entail\ndeveloping manually curated, task-specific features and models to represent\nhuman behavior and the natural and built environment, which can be challenging\nto adapt to new, or even, related tasks. To address this, we introduce a\nPopulation Dynamics Foundation Model (PDFM) that aims to capture the\nrelationships between diverse data modalities and is applicable to a broad\nrange of geospatial tasks. We first construct a geo-indexed dataset for postal\ncodes and counties across the United States, capturing rich aggregated\ninformation on human behavior from maps, busyness, and aggregated search\ntrends, and environmental factors such as weather and air quality. We then\nmodel this data and the complex relationships between locations using a graph\nneural network, producing embeddings that can be adapted to a wide range of\ndownstream tasks using relatively simple models. We evaluate the effectiveness\nof our approach by benchmarking it on 27 downstream tasks spanning three\ndistinct domains: health indicators, socioeconomic factors, and environmental\nmeasurements. The approach achieves state-of-the-art performance on all 27\ngeospatial interpolation tasks, and on 25 out of the 27 extrapolation and\nsuper-resolution tasks. We combined the PDFM with a state-of-the-art\nforecasting foundation model, TimesFM, to predict unemployment and poverty,\nachieving performance that surpasses fully supervised forecasting. The full set\nof embeddings and sample code are publicly available for researchers."
                },
                "authors": [
                    {
                        "name": "Mohit Agarwal"
                    },
                    {
                        "name": "Mimi Sun"
                    },
                    {
                        "name": "Chaitanya Kamath"
                    },
                    {
                        "name": "Arbaaz Muslim"
                    },
                    {
                        "name": "Prithul Sarker"
                    },
                    {
                        "name": "Joydeep Paul"
                    },
                    {
                        "name": "Hector Yee"
                    },
                    {
                        "name": "Marcin Sieniek"
                    },
                    {
                        "name": "Kim Jablonski"
                    },
                    {
                        "name": "Yael Mayer"
                    },
                    {
                        "name": "David Fork"
                    },
                    {
                        "name": "Sheila de Guia"
                    },
                    {
                        "name": "Jamie McPike"
                    },
                    {
                        "name": "Adam Boulanger"
                    },
                    {
                        "name": "Tomer Shekel"
                    },
                    {
                        "name": "David Schottlander"
                    },
                    {
                        "name": "Yao Xiao"
                    },
                    {
                        "name": "Manjit Chakravarthy Manukonda"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Neslihan Bulut"
                    },
                    {
                        "name": "Sami Abu-el-haija"
                    },
                    {
                        "name": "Arno Eigenwillig"
                    },
                    {
                        "name": "Parth Kothari"
                    },
                    {
                        "name": "Bryan Perozzi"
                    },
                    {
                        "name": "Monica Bharel"
                    },
                    {
                        "name": "Von Nguyen"
                    },
                    {
                        "name": "Luke Barrington"
                    },
                    {
                        "name": "Niv Efron"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "Greg Corrado"
                    },
                    {
                        "name": "Krish Eswaran"
                    },
                    {
                        "name": "Shruthi Prabhakara"
                    },
                    {
                        "name": "Shravya Shetty"
                    },
                    {
                        "name": "Gautam Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Gautam Prasad"
                },
                "author": "Gautam Prasad",
                "arxiv_comment": "28 pages, 16 figures, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07205v1",
                "updated": "2024-11-11T18:28:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    28,
                    33,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T18:28:33Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    28,
                    33,
                    0,
                    316,
                    0
                ],
                "title": "DLCR: A Generative Data Expansion Framework via Diffusion for\n  Clothes-Changing Person Re-ID",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DLCR: A Generative Data Expansion Framework via Diffusion for\n  Clothes-Changing Person Re-ID"
                },
                "summary": "With the recent exhibited strength of generative diffusion models, an open\nresearch question is \\textit{if images generated by these models can be used to\nlearn better visual representations}. While this generative data expansion may\nsuffice for easier visual tasks, we explore its efficacy on a more difficult\ndiscriminative task: clothes-changing person re-identification (CC-ReID).\nCC-ReID aims to match people appearing in non-overlapping cameras, even when\nthey change their clothes across cameras. Not only are current CC-ReID models\nconstrained by the limited diversity of clothing in current CC-ReID datasets,\nbut generating additional data that retains important personal features for\naccurate identification is a current challenge. To address this issue we\npropose DLCR, a novel data expansion framework that leverages pre-trained\ndiffusion and large language models (LLMs) to accurately generate diverse\nimages of individuals in varied attire. We generate additional data for five\nbenchmark CC-ReID datasets (PRCC, CCVID, LaST, VC-Clothes, and LTCC) and\n\\textbf{increase their clothing diversity by \\boldmath{$10$}x, totaling over\n\\boldmath{$2.1$}M images generated}. DLCR employs diffusion-based text-guided\ninpainting, conditioned on clothing prompts constructed using LLMs, to generate\nsynthetic data that only modifies a subject's clothes while preserving their\npersonally identifiable features. With this massive increase in data, we\nintroduce two novel strategies - progressive learning and test-time prediction\nrefinement - that respectively reduce training time and further boosts CC-ReID\nperformance. On the PRCC dataset, we obtain a large top-1 accuracy improvement\nof $11.3\\%$ by training CAL, a previous state of the art (SOTA) method, with\nDLCR-generated data. We publicly release our code and generated data for each\ndataset here: \\url{https://github.com/CroitoruAlin/dlcr}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent exhibited strength of generative diffusion models, an open\nresearch question is \\textit{if images generated by these models can be used to\nlearn better visual representations}. While this generative data expansion may\nsuffice for easier visual tasks, we explore its efficacy on a more difficult\ndiscriminative task: clothes-changing person re-identification (CC-ReID).\nCC-ReID aims to match people appearing in non-overlapping cameras, even when\nthey change their clothes across cameras. Not only are current CC-ReID models\nconstrained by the limited diversity of clothing in current CC-ReID datasets,\nbut generating additional data that retains important personal features for\naccurate identification is a current challenge. To address this issue we\npropose DLCR, a novel data expansion framework that leverages pre-trained\ndiffusion and large language models (LLMs) to accurately generate diverse\nimages of individuals in varied attire. We generate additional data for five\nbenchmark CC-ReID datasets (PRCC, CCVID, LaST, VC-Clothes, and LTCC) and\n\\textbf{increase their clothing diversity by \\boldmath{$10$}x, totaling over\n\\boldmath{$2.1$}M images generated}. DLCR employs diffusion-based text-guided\ninpainting, conditioned on clothing prompts constructed using LLMs, to generate\nsynthetic data that only modifies a subject's clothes while preserving their\npersonally identifiable features. With this massive increase in data, we\nintroduce two novel strategies - progressive learning and test-time prediction\nrefinement - that respectively reduce training time and further boosts CC-ReID\nperformance. On the PRCC dataset, we obtain a large top-1 accuracy improvement\nof $11.3\\%$ by training CAL, a previous state of the art (SOTA) method, with\nDLCR-generated data. We publicly release our code and generated data for each\ndataset here: \\url{https://github.com/CroitoruAlin/dlcr}."
                },
                "authors": [
                    {
                        "name": "Nyle Siddiqui"
                    },
                    {
                        "name": "Florinel Alin Croitoru"
                    },
                    {
                        "name": "Gaurav Kumar Nayak"
                    },
                    {
                        "name": "Radu Tudor Ionescu"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "arxiv_comment": "Published in WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03354v2",
                "updated": "2024-11-11T18:19:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    19,
                    22,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-04T18:12:14Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    12,
                    14,
                    0,
                    309,
                    0
                ],
                "title": "LLM-based Continuous Intrusion Detection Framework for Next-Gen Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Continuous Intrusion Detection Framework for Next-Gen Networks"
                },
                "summary": "In this paper, we present an adaptive framework designed for the continuous\ndetection, identification and classification of emerging attacks in network\ntraffic. The framework employs a transformer encoder architecture, which\ncaptures hidden patterns in a bidirectional manner to differentiate between\nmalicious and legitimate traffic. Initially, the framework focuses on the\naccurate detection of malicious activities, achieving a perfect recall of 100\\%\nin distinguishing between attack and benign traffic. Subsequently, the system\nincrementally identifies unknown attack types by leveraging a Gaussian Mixture\nModel (GMM) to cluster features derived from high-dimensional BERT embeddings.\nThis approach allows the framework to dynamically adjust its identification\ncapabilities as new attack clusters are discovered, maintaining high detection\naccuracy. Even after integrating additional unknown attack clusters, the\nframework continues to perform at a high level, achieving 95.6\\% in both\nclassification accuracy and recall.The results demonstrate the effectiveness of\nthe proposed framework in adapting to evolving threats while maintaining high\naccuracy in both detection and identification tasks. Our ultimate goal is to\ndevelop a scalable, real-time intrusion detection system that can continuously\nevolve with the ever-changing network threat landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present an adaptive framework designed for the continuous\ndetection, identification and classification of emerging attacks in network\ntraffic. The framework employs a transformer encoder architecture, which\ncaptures hidden patterns in a bidirectional manner to differentiate between\nmalicious and legitimate traffic. Initially, the framework focuses on the\naccurate detection of malicious activities, achieving a perfect recall of 100\\%\nin distinguishing between attack and benign traffic. Subsequently, the system\nincrementally identifies unknown attack types by leveraging a Gaussian Mixture\nModel (GMM) to cluster features derived from high-dimensional BERT embeddings.\nThis approach allows the framework to dynamically adjust its identification\ncapabilities as new attack clusters are discovered, maintaining high detection\naccuracy. Even after integrating additional unknown attack clusters, the\nframework continues to perform at a high level, achieving 95.6\\% in both\nclassification accuracy and recall.The results demonstrate the effectiveness of\nthe proposed framework in adapting to evolving threats while maintaining high\naccuracy in both detection and identification tasks. Our ultimate goal is to\ndevelop a scalable, real-time intrusion detection system that can continuously\nevolve with the ever-changing network threat landscape."
                },
                "authors": [
                    {
                        "name": "Frederic Adjewa"
                    },
                    {
                        "name": "Moez Esseghir"
                    },
                    {
                        "name": "Leila Merghem-Boulahia"
                    }
                ],
                "author_detail": {
                    "name": "Leila Merghem-Boulahia"
                },
                "author": "Leila Merghem-Boulahia",
                "arxiv_comment": "8 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07192v1",
                "updated": "2024-11-11T18:08:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    8,
                    17,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T18:08:17Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    8,
                    17,
                    0,
                    316,
                    0
                ],
                "title": "Data-Driven Predictive Control of Nonholonomic Robots Based on a\n  Bilinear Koopman Realization: Data Does Not Replace Geometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Predictive Control of Nonholonomic Robots Based on a\n  Bilinear Koopman Realization: Data Does Not Replace Geometry"
                },
                "summary": "Advances in machine learning and the growing trend towards effortless data\ngeneration in real-world systems has led to an increasing interest for\ndata-inferred models and data-based control in robotics. It seems appealing to\ngovern robots solely based on data, bypassing the traditional, more elaborate\npipeline of system modeling through first-principles and subsequent controller\ndesign. One promising data-driven approach is the Extended Dynamic Mode\nDecomposition (EDMD) for control-affine systems, a system class which contains\nmany vehicles and machines of immense practical importance including, e.g.,\ntypical wheeled mobile robots. EDMD can be highly data-efficient,\ncomputationally inexpensive, can deal with nonlinear dynamics as prevalent in\nrobotics and mechanics, and has a sound theoretical foundation rooted in\nKoopman theory. On this background, this present paper examines how EDMD models\ncan be integrated into predictive controllers for nonholonomic mobile robots.\nIn addition to the conventional kinematic mobile robot, we also cover the\ncomplete data-driven control pipeline - from data acquisition to control design\n- when the robot is not treated in terms of first-order kinematics but in a\nsecond-order manner, allowing to account for actuator dynamics. Using only\nreal-world measurement data, it is shown in both simulations and hardware\nexperiments that the surrogate models enable high-precision predictive\ncontrollers in the studied cases. However, the findings raise significant\nconcerns about purely data-centric approaches that overlook the underlying\ngeometry of nonholonomic systems, showing that, for nonholonomic systems, some\ngeometric insight seems necessary and cannot be easily compensated for with\nlarge amounts of data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in machine learning and the growing trend towards effortless data\ngeneration in real-world systems has led to an increasing interest for\ndata-inferred models and data-based control in robotics. It seems appealing to\ngovern robots solely based on data, bypassing the traditional, more elaborate\npipeline of system modeling through first-principles and subsequent controller\ndesign. One promising data-driven approach is the Extended Dynamic Mode\nDecomposition (EDMD) for control-affine systems, a system class which contains\nmany vehicles and machines of immense practical importance including, e.g.,\ntypical wheeled mobile robots. EDMD can be highly data-efficient,\ncomputationally inexpensive, can deal with nonlinear dynamics as prevalent in\nrobotics and mechanics, and has a sound theoretical foundation rooted in\nKoopman theory. On this background, this present paper examines how EDMD models\ncan be integrated into predictive controllers for nonholonomic mobile robots.\nIn addition to the conventional kinematic mobile robot, we also cover the\ncomplete data-driven control pipeline - from data acquisition to control design\n- when the robot is not treated in terms of first-order kinematics but in a\nsecond-order manner, allowing to account for actuator dynamics. Using only\nreal-world measurement data, it is shown in both simulations and hardware\nexperiments that the surrogate models enable high-precision predictive\ncontrollers in the studied cases. However, the findings raise significant\nconcerns about purely data-centric approaches that overlook the underlying\ngeometry of nonholonomic systems, showing that, for nonholonomic systems, some\ngeometric insight seems necessary and cannot be easily compensated for with\nlarge amounts of data."
                },
                "authors": [
                    {
                        "name": "Mario Rosenfelder"
                    },
                    {
                        "name": "Lea Bold"
                    },
                    {
                        "name": "Hannes Eschmann"
                    },
                    {
                        "name": "Peter Eberhard"
                    },
                    {
                        "name": "Karl Worthmann"
                    },
                    {
                        "name": "Henrik Ebel"
                    }
                ],
                "author_detail": {
                    "name": "Henrik Ebel"
                },
                "author": "Henrik Ebel",
                "arxiv_comment": "23 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07191v1",
                "updated": "2024-11-11T18:05:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    5,
                    48,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T18:05:48Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    5,
                    48,
                    0,
                    316,
                    0
                ],
                "title": "The Super Weight in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Super Weight in Large Language Models"
                },
                "summary": "Recent works have shown a surprising result: a small fraction of Large\nLanguage Model (LLM) parameter outliers are disproportionately important to the\nquality of the model. LLMs contain billions of parameters, so these small\nfractions, such as 0.01%, translate to hundreds of thousands of parameters. In\nthis work, we present an even more surprising finding: Pruning as few as a\nsingle parameter can destroy an LLM's ability to generate text -- increasing\nperplexity by 3 orders of magnitude and reducing zero-shot accuracy to\nguessing. We propose a data-free method for identifying such parameters, termed\nsuper weights, using a single forward pass through the model. We additionally\nfind that these super weights induce correspondingly rare and large activation\noutliers, termed super activations. When preserved with high precision, super\nactivations can improve simple round-to-nearest quantization to become\ncompetitive with state-of-the-art methods. For weight quantization, we\nsimilarly find that by preserving the super weight and clipping other weight\noutliers, round-to-nearest quantization can scale to much larger block sizes\nthan previously considered. To facilitate further research into super weights,\nwe provide an index of super weight coordinates for common, openly available\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have shown a surprising result: a small fraction of Large\nLanguage Model (LLM) parameter outliers are disproportionately important to the\nquality of the model. LLMs contain billions of parameters, so these small\nfractions, such as 0.01%, translate to hundreds of thousands of parameters. In\nthis work, we present an even more surprising finding: Pruning as few as a\nsingle parameter can destroy an LLM's ability to generate text -- increasing\nperplexity by 3 orders of magnitude and reducing zero-shot accuracy to\nguessing. We propose a data-free method for identifying such parameters, termed\nsuper weights, using a single forward pass through the model. We additionally\nfind that these super weights induce correspondingly rare and large activation\noutliers, termed super activations. When preserved with high precision, super\nactivations can improve simple round-to-nearest quantization to become\ncompetitive with state-of-the-art methods. For weight quantization, we\nsimilarly find that by preserving the super weight and clipping other weight\noutliers, round-to-nearest quantization can scale to much larger block sizes\nthan previously considered. To facilitate further research into super weights,\nwe provide an index of super weight coordinates for common, openly available\nLLMs."
                },
                "authors": [
                    {
                        "name": "Mengxia Yu"
                    },
                    {
                        "name": "De Wang"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Alvin Wan"
                    }
                ],
                "author_detail": {
                    "name": "Alvin Wan"
                },
                "author": "Alvin Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07186v1",
                "updated": "2024-11-11T18:01:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    1,
                    45,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T18:01:45Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    1,
                    45,
                    0,
                    316,
                    0
                ],
                "title": "NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics"
                },
                "summary": "Large language models (LLMs) prompted with text and audio represent the state\nof the art in various auditory tasks, including speech, music, and general\naudio, showing emergent abilities on unseen tasks. However, these capabilities\nhave yet to be fully demonstrated in bioacoustics tasks, such as detecting\nanimal vocalizations in large recordings, classifying rare and endangered\nspecies, and labeling context and behavior - tasks that are crucial for\nconservation, biodiversity monitoring, and the study of animal behavior. In\nthis work, we present NatureLM-audio, the first audio-language foundation model\nspecifically designed for bioacoustics. Our carefully curated training dataset\ncomprises text-audio pairs spanning a diverse range of bioacoustics, speech,\nand music data, designed to address the challenges posed by limited annotated\ndatasets in the field. We demonstrate successful transfer of learned\nrepresentations from music and speech to bioacoustics, and our model shows\npromising generalization to unseen taxa and tasks. Importantly, we test\nNatureLM-audio on a novel benchmark (BEANS-Zero) and it sets the new state of\nthe art (SotA) on several bioacoustics tasks, including zero-shot\nclassification of unseen species. To advance bioacoustics research, we also\nopen-source the code for generating training and benchmark data, as well as for\ntraining the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) prompted with text and audio represent the state\nof the art in various auditory tasks, including speech, music, and general\naudio, showing emergent abilities on unseen tasks. However, these capabilities\nhave yet to be fully demonstrated in bioacoustics tasks, such as detecting\nanimal vocalizations in large recordings, classifying rare and endangered\nspecies, and labeling context and behavior - tasks that are crucial for\nconservation, biodiversity monitoring, and the study of animal behavior. In\nthis work, we present NatureLM-audio, the first audio-language foundation model\nspecifically designed for bioacoustics. Our carefully curated training dataset\ncomprises text-audio pairs spanning a diverse range of bioacoustics, speech,\nand music data, designed to address the challenges posed by limited annotated\ndatasets in the field. We demonstrate successful transfer of learned\nrepresentations from music and speech to bioacoustics, and our model shows\npromising generalization to unseen taxa and tasks. Importantly, we test\nNatureLM-audio on a novel benchmark (BEANS-Zero) and it sets the new state of\nthe art (SotA) on several bioacoustics tasks, including zero-shot\nclassification of unseen species. To advance bioacoustics research, we also\nopen-source the code for generating training and benchmark data, as well as for\ntraining the model."
                },
                "authors": [
                    {
                        "name": "David Robinson"
                    },
                    {
                        "name": "Marius Miron"
                    },
                    {
                        "name": "Masato Hagiwara"
                    },
                    {
                        "name": "Olivier Pietquin"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Pietquin"
                },
                "author": "Olivier Pietquin",
                "arxiv_comment": "Demo page: https://earthspecies.github.io/naturelm-audio-demo/ The\n  code will be open-sourced and available shortly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07185v1",
                "updated": "2024-11-11T17:59:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    59,
                    21,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T17:59:21Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    59,
                    21,
                    0,
                    316,
                    0
                ],
                "title": "Gradual Fine-Tuning with Graph Routing for Multi-Source Unsupervised\n  Domain Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Fine-Tuning with Graph Routing for Multi-Source Unsupervised\n  Domain Adaptation"
                },
                "summary": "Multi-source unsupervised domain adaptation aims to leverage labeled data\nfrom multiple source domains for training a machine learning model to\ngeneralize well on a target domain without labels. Source domain selection\nplays a crucial role in determining the model's performance. It relies on the\nsimilarities amongst source and target domains. Nonetheless, existing work for\nsource domain selection often involves heavyweight computational procedures,\nespecially when dealing with numerous source domains and the need to identify\nthe best ones from them. In this paper, we introduce a framework for gradual\nfine tuning (GFT) of machine learning models on multiple source domains. We\nrepresent multiple source domains as an undirected weighted graph. We then give\na new generalization error bound for GFT along any path within the graph, which\nis used to determine the optimal path corresponding to the optimal training\norder. With this formulation, we introduce three lightweight graph-routing\nstrategies which tend to minimize the error bound. Our best strategy improves\n$2.3\\%$ of accuracy over the state-of-the-art on Natural Language Inference\n(NLI) task and achieves competitive performance on Sentiment Analysis (SA)\ntask, especially a $3.9\\%$ improvement on a more diverse subset of data we use\nfor SA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-source unsupervised domain adaptation aims to leverage labeled data\nfrom multiple source domains for training a machine learning model to\ngeneralize well on a target domain without labels. Source domain selection\nplays a crucial role in determining the model's performance. It relies on the\nsimilarities amongst source and target domains. Nonetheless, existing work for\nsource domain selection often involves heavyweight computational procedures,\nespecially when dealing with numerous source domains and the need to identify\nthe best ones from them. In this paper, we introduce a framework for gradual\nfine tuning (GFT) of machine learning models on multiple source domains. We\nrepresent multiple source domains as an undirected weighted graph. We then give\na new generalization error bound for GFT along any path within the graph, which\nis used to determine the optimal path corresponding to the optimal training\norder. With this formulation, we introduce three lightweight graph-routing\nstrategies which tend to minimize the error bound. Our best strategy improves\n$2.3\\%$ of accuracy over the state-of-the-art on Natural Language Inference\n(NLI) task and achieves competitive performance on Sentiment Analysis (SA)\ntask, especially a $3.9\\%$ improvement on a more diverse subset of data we use\nfor SA."
                },
                "authors": [
                    {
                        "name": "Yao Ma"
                    },
                    {
                        "name": "Samuel Louvan"
                    },
                    {
                        "name": "Zhunxuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhunxuan Wang"
                },
                "author": "Zhunxuan Wang",
                "arxiv_comment": "In Proceedings of the 3rd Conference on Lifelong Learning Agents\n  (CoLLAs 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07180v1",
                "updated": "2024-11-11T17:57:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    57,
                    30,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T17:57:30Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    57,
                    30,
                    0,
                    316,
                    0
                ],
                "title": "Counterfactual Generation from Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Generation from Language Models"
                },
                "summary": "Understanding and manipulating the causal generation mechanisms in language\nmodels is essential for controlling their behavior. Previous work has primarily\nrelied on techniques such as representation surgery -- e.g., model ablations or\nmanipulation of linear subspaces tied to specific concepts -- to intervene on\nthese models. To understand the impact of interventions precisely, it is useful\nto examine counterfactuals -- e.g., how a given sentence would have appeared\nhad it been generated by the model following a specific intervention. We\nhighlight that counterfactual reasoning is conceptually distinct from\ninterventions, as articulated in Pearl's causal hierarchy. Based on this\nobservation, we propose a framework for generating true string counterfactuals\nby reformulating language models as Generalized Structural-equation. Models\nusing the Gumbel-max trick. This allows us to model the joint distribution over\noriginal strings and their counterfactuals resulting from the same\ninstantiation of the sampling noise. We develop an algorithm based on hindsight\nGumbel sampling that allows us to infer the latent noise variables and generate\ncounterfactuals of observed strings. Our experiments demonstrate that the\napproach produces meaningful counterfactuals while at the same time showing\nthat commonly used intervention techniques have considerable undesired side\neffects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and manipulating the causal generation mechanisms in language\nmodels is essential for controlling their behavior. Previous work has primarily\nrelied on techniques such as representation surgery -- e.g., model ablations or\nmanipulation of linear subspaces tied to specific concepts -- to intervene on\nthese models. To understand the impact of interventions precisely, it is useful\nto examine counterfactuals -- e.g., how a given sentence would have appeared\nhad it been generated by the model following a specific intervention. We\nhighlight that counterfactual reasoning is conceptually distinct from\ninterventions, as articulated in Pearl's causal hierarchy. Based on this\nobservation, we propose a framework for generating true string counterfactuals\nby reformulating language models as Generalized Structural-equation. Models\nusing the Gumbel-max trick. This allows us to model the joint distribution over\noriginal strings and their counterfactuals resulting from the same\ninstantiation of the sampling noise. We develop an algorithm based on hindsight\nGumbel sampling that allows us to infer the latent noise variables and generate\ncounterfactuals of observed strings. Our experiments demonstrate that the\napproach produces meaningful counterfactuals while at the same time showing\nthat commonly used intervention techniques have considerable undesired side\neffects."
                },
                "authors": [
                    {
                        "name": "Shauli Ravfogel"
                    },
                    {
                        "name": "Anej Svete"
                    },
                    {
                        "name": "Vsteinn Snbjarnarson"
                    },
                    {
                        "name": "Ryan Cotterell"
                    }
                ],
                "author_detail": {
                    "name": "Ryan Cotterell"
                },
                "author": "Ryan Cotterell",
                "arxiv_comment": "A preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16998v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16998v2",
                "updated": "2024-11-11T17:56:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    56,
                    29,
                    0,
                    316,
                    0
                ],
                "published": "2024-03-25T17:59:09Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    17,
                    59,
                    9,
                    0,
                    85,
                    0
                ],
                "title": "Understanding Long Videos with Multimodal Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Long Videos with Multimodal Language Models"
                },
                "summary": "Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we exploring injecting video-specific\ninformation into an LLM-based framework. We utilize off-the-shelf vision tools\nto extract three object-centric information modalities from videos and then\nleverage natural language as a medium for fusing this information. Our\nresulting Multimodal Video Understanding (MVU) framework demonstrates\nstate-of-the-art performance across multiple video understanding benchmarks.\nStrong performance also on robotics domain tasks establish its strong\ngenerality. Our code will be released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we exploring injecting video-specific\ninformation into an LLM-based framework. We utilize off-the-shelf vision tools\nto extract three object-centric information modalities from videos and then\nleverage natural language as a medium for fusing this information. Our\nresulting Multimodal Video Understanding (MVU) framework demonstrates\nstate-of-the-art performance across multiple video understanding benchmarks.\nStrong performance also on robotics domain tasks establish its strong\ngenerality. Our code will be released publicly."
                },
                "authors": [
                    {
                        "name": "Kanchana Ranasinghe"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    }
                ],
                "author_detail": {
                    "name": "Michael S. Ryoo"
                },
                "author": "Michael S. Ryoo",
                "arxiv_comment": "Code available at https://github.com/kahnchana/mvu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16998v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16998v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07175v1",
                "updated": "2024-11-11T17:56:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    56,
                    15,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T17:56:15Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    56,
                    15,
                    0,
                    316,
                    0
                ],
                "title": "Continual Memorization of Factoids in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Memorization of Factoids in Large Language Models"
                },
                "summary": "Large language models can absorb a massive amount of knowledge through\npretraining, but pretraining is inefficient for acquiring long-tailed or\nspecialized facts. Therefore, fine-tuning on specialized or new knowledge that\nreflects changes in the world has become popular, though it risks disrupting\nthe model's original capabilities. We study this fragility in the context of\ncontinual memorization, where the model is trained on a small set of long-tail\nfactoids (factual associations) and must retain these factoids after multiple\nstages of subsequent training on other datasets. Through extensive experiments,\nwe show that LLMs suffer from forgetting across a wide range of subsequent\ntasks, and simple replay techniques do not fully prevent forgetting, especially\nwhen the factoid datasets are trained in the later stages. We posit that there\nare two ways to alleviate forgetting: 1) protect the memorization process as\nthe model learns the factoids, or 2) reduce interference from training in later\nstages. With this insight, we develop an effective mitigation strategy: REMIX\n(Random and Generic Data Mixing). REMIX prevents forgetting by mixing generic\ndata sampled from pretraining corpora or even randomly generated word sequences\nduring each stage, despite being unrelated to the memorized factoids in the\nfirst stage. REMIX can recover performance from severe forgetting, often\noutperforming replay-based methods that have access to the factoids from the\nfirst stage. We then analyze how REMIX alters the learning process and find\nthat successful forgetting prevention is associated with a pattern: the model\nstores factoids in earlier layers than usual and diversifies the set of layers\nthat store these factoids. The efficacy of REMIX invites further investigation\ninto the underlying dynamics of memorization and forgetting, opening exciting\npossibilities for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can absorb a massive amount of knowledge through\npretraining, but pretraining is inefficient for acquiring long-tailed or\nspecialized facts. Therefore, fine-tuning on specialized or new knowledge that\nreflects changes in the world has become popular, though it risks disrupting\nthe model's original capabilities. We study this fragility in the context of\ncontinual memorization, where the model is trained on a small set of long-tail\nfactoids (factual associations) and must retain these factoids after multiple\nstages of subsequent training on other datasets. Through extensive experiments,\nwe show that LLMs suffer from forgetting across a wide range of subsequent\ntasks, and simple replay techniques do not fully prevent forgetting, especially\nwhen the factoid datasets are trained in the later stages. We posit that there\nare two ways to alleviate forgetting: 1) protect the memorization process as\nthe model learns the factoids, or 2) reduce interference from training in later\nstages. With this insight, we develop an effective mitigation strategy: REMIX\n(Random and Generic Data Mixing). REMIX prevents forgetting by mixing generic\ndata sampled from pretraining corpora or even randomly generated word sequences\nduring each stage, despite being unrelated to the memorized factoids in the\nfirst stage. REMIX can recover performance from severe forgetting, often\noutperforming replay-based methods that have access to the factoids from the\nfirst stage. We then analyze how REMIX alters the learning process and find\nthat successful forgetting prevention is associated with a pattern: the model\nstores factoids in earlier layers than usual and diversifies the set of layers\nthat store these factoids. The efficacy of REMIX invites further investigation\ninto the underlying dynamics of memorization and forgetting, opening exciting\npossibilities for future research."
                },
                "authors": [
                    {
                        "name": "Howard Chen"
                    },
                    {
                        "name": "Jiayi Geng"
                    },
                    {
                        "name": "Adithya Bhaskar"
                    },
                    {
                        "name": "Dan Friedman"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07168v1",
                "updated": "2024-11-11T17:48:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    48,
                    4,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T17:48:04Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    48,
                    4,
                    0,
                    316,
                    0
                ],
                "title": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a\n  TinyML-enabled Hierarchical Inference Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a\n  TinyML-enabled Hierarchical Inference Network"
                },
                "summary": "Mining machinery operating in variable environments faces high wear and\nunpredictable stress, challenging Predictive Maintenance (PdM). This paper\nintroduces the Edge Sensor Network for Predictive Maintenance (ESN-PdM), a\nhierarchical inference framework across edge devices, gateways, and cloud\nservices for real-time condition monitoring. The system dynamically adjusts\ninference locations--on-device, on-gateway, or on-cloud--based on trade-offs\namong accuracy, latency, and battery life, leveraging Tiny Machine Learning\n(TinyML) techniques for model optimization on resource-constrained devices.\nPerformance evaluations showed that on-sensor and on-gateway inference modes\nachieved over 90\\% classification accuracy, while cloud-based inference reached\n99\\%. On-sensor inference reduced power consumption by approximately 44\\%,\nenabling up to 104 hours of operation. Latency was lowest for on-device\ninference (3.33 ms), increasing when offloading to the gateway (146.67 ms) or\ncloud (641.71 ms). The ESN-PdM framework provides a scalable, adaptive solution\nfor reliable anomaly detection and PdM, crucial for maintaining machinery\nuptime in remote environments. By balancing accuracy, latency, and energy\nconsumption, this approach advances PdM frameworks for industrial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mining machinery operating in variable environments faces high wear and\nunpredictable stress, challenging Predictive Maintenance (PdM). This paper\nintroduces the Edge Sensor Network for Predictive Maintenance (ESN-PdM), a\nhierarchical inference framework across edge devices, gateways, and cloud\nservices for real-time condition monitoring. The system dynamically adjusts\ninference locations--on-device, on-gateway, or on-cloud--based on trade-offs\namong accuracy, latency, and battery life, leveraging Tiny Machine Learning\n(TinyML) techniques for model optimization on resource-constrained devices.\nPerformance evaluations showed that on-sensor and on-gateway inference modes\nachieved over 90\\% classification accuracy, while cloud-based inference reached\n99\\%. On-sensor inference reduced power consumption by approximately 44\\%,\nenabling up to 104 hours of operation. Latency was lowest for on-device\ninference (3.33 ms), increasing when offloading to the gateway (146.67 ms) or\ncloud (641.71 ms). The ESN-PdM framework provides a scalable, adaptive solution\nfor reliable anomaly detection and PdM, crucial for maintaining machinery\nuptime in remote environments. By balancing accuracy, latency, and energy\nconsumption, this approach advances PdM frameworks for industrial applications."
                },
                "authors": [
                    {
                        "name": "Ral de la Fuente"
                    },
                    {
                        "name": "Luciano Radrigan"
                    },
                    {
                        "name": "Anibal S Morales"
                    }
                ],
                "author_detail": {
                    "name": "Anibal S Morales"
                },
                "author": "Anibal S Morales",
                "arxiv_comment": "This work has been submitted to the IEEE Access for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10733v2",
                "updated": "2024-11-11T17:42:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    42,
                    37,
                    0,
                    316,
                    0
                ],
                "published": "2024-10-14T17:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    15,
                    7,
                    0,
                    288,
                    0
                ],
                "title": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models"
                },
                "summary": "We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit."
                },
                "authors": [
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Preprint. First two authors contributed equally to this work. Update:\n  add diffusion model scaling results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07163v1",
                "updated": "2024-11-11T17:41:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    41,
                    54,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T17:41:54Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    41,
                    54,
                    0,
                    316,
                    0
                ],
                "title": "A Domain-Agnostic Neurosymbolic Approach for Big Social Data Analysis:\n  Evaluating Mental Health Sentiment on Social Media during COVID-19",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Domain-Agnostic Neurosymbolic Approach for Big Social Data Analysis:\n  Evaluating Mental Health Sentiment on Social Media during COVID-19"
                },
                "summary": "Monitoring public sentiment via social media is potentially helpful during\nhealth crises such as the COVID-19 pandemic. However, traditional\nfrequency-based, data-driven neural network-based approaches can miss newly\nrelevant content due to the evolving nature of language in a dynamically\nevolving environment. Human-curated symbolic knowledge sources, such as\nlexicons for standard language and slang terms, can potentially elevate social\nmedia signals in evolving language. We introduce a neurosymbolic method that\nintegrates neural networks with symbolic knowledge sources, enhancing the\ndetection and interpretation of mental health-related tweets relevant to\nCOVID-19. Our method was evaluated using a corpus of large datasets\n(approximately 12 billion tweets, 2.5 million subreddit data, and 700k news\narticles) and multiple knowledge graphs. This method dynamically adapts to\nevolving language, outperforming purely data-driven models with an F1 score\nexceeding 92\\%. This approach also showed faster adaptation to new data and\nlower computational demands than fine-tuning pre-trained large language models\n(LLMs). This study demonstrates the benefit of neurosymbolic methods in\ninterpreting text in a dynamic environment for tasks such as health\nsurveillance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring public sentiment via social media is potentially helpful during\nhealth crises such as the COVID-19 pandemic. However, traditional\nfrequency-based, data-driven neural network-based approaches can miss newly\nrelevant content due to the evolving nature of language in a dynamically\nevolving environment. Human-curated symbolic knowledge sources, such as\nlexicons for standard language and slang terms, can potentially elevate social\nmedia signals in evolving language. We introduce a neurosymbolic method that\nintegrates neural networks with symbolic knowledge sources, enhancing the\ndetection and interpretation of mental health-related tweets relevant to\nCOVID-19. Our method was evaluated using a corpus of large datasets\n(approximately 12 billion tweets, 2.5 million subreddit data, and 700k news\narticles) and multiple knowledge graphs. This method dynamically adapts to\nevolving language, outperforming purely data-driven models with an F1 score\nexceeding 92\\%. This approach also showed faster adaptation to new data and\nlower computational demands than fine-tuning pre-trained large language models\n(LLMs). This study demonstrates the benefit of neurosymbolic methods in\ninterpreting text in a dynamic environment for tasks such as health\nsurveillance."
                },
                "authors": [
                    {
                        "name": "Vedant Khandelwal"
                    },
                    {
                        "name": "Manas Gaur"
                    },
                    {
                        "name": "Ugur Kursuncu"
                    },
                    {
                        "name": "Valerie Shalin"
                    },
                    {
                        "name": "Amit Sheth"
                    }
                ],
                "author_detail": {
                    "name": "Amit Sheth"
                },
                "author": "Amit Sheth",
                "arxiv_comment": "13 Pages, 5 Figures, 5 Tables, 2024 IEEE International Conference on\n  Big Data, Regular Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4; I.2.6; I.2.7; I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02481v2",
                "updated": "2024-11-11T17:34:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    34,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-04T18:54:39Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    54,
                    39,
                    0,
                    309,
                    0
                ],
                "title": "CDR: Customizable Density Ratios of Strong-over-weak LLMs for Preference\n  Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDR: Customizable Density Ratios of Strong-over-weak LLMs for Preference\n  Annotation"
                },
                "summary": "Preference tuning of large language models (LLMs) relies on high-quality\nhuman preference data, which is often expensive and time-consuming to gather.\nWhile existing methods can use trained reward models or proprietary model as\njudges for preference annotation, they have notable drawbacks: training reward\nmodels remain dependent on initial human data, and using proprietary model\nimposes license restrictions that inhibits commercial usage. In this paper, we\nintroduce customized density ratio (CDR), a training-free and highly effective\nmethod that leverages off-the-shelf LLMs for preference data annotation. Our\napproach uses the log-density ratio between a better-aligned LLM and a less\naligned LLM as a reward signal. We explores 221 different LLMs pairs and\nempirically demonstrate that increasing the performance gap between paired LLMs\ncorrelates with better reward generalization. Furthermore, we show that\ntailoring the density ratio reward function with specific criteria and\npreference exemplars enhances performance across domains and within target\nareas.\n  In our experiment using density ratio from a pair of Mistral-7B models, CDR\nachieves a RewardBench score of 82.6, outperforming the best trained reward\nfunctions from same model class and demonstrating competitive performance\nagainst SoTA models in Safety (91.0) and Reasoning (88.0) domains. We use CDR\nto annotate an on-policy preference dataset with which we preference tune\nLlama-3-8B-Instruct with SimPO. Using reward signals from two relatively weak\nmodels, our approach pushes Llama-3-8B to achieve a 37.4% (+15.1%) win rate on\nArenaHard and a 40.7% (+17.8%) win rate on Length-Controlled AlpacaEval 2.0,\nalong with a score of 8.0 on MT-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference tuning of large language models (LLMs) relies on high-quality\nhuman preference data, which is often expensive and time-consuming to gather.\nWhile existing methods can use trained reward models or proprietary model as\njudges for preference annotation, they have notable drawbacks: training reward\nmodels remain dependent on initial human data, and using proprietary model\nimposes license restrictions that inhibits commercial usage. In this paper, we\nintroduce customized density ratio (CDR), a training-free and highly effective\nmethod that leverages off-the-shelf LLMs for preference data annotation. Our\napproach uses the log-density ratio between a better-aligned LLM and a less\naligned LLM as a reward signal. We explores 221 different LLMs pairs and\nempirically demonstrate that increasing the performance gap between paired LLMs\ncorrelates with better reward generalization. Furthermore, we show that\ntailoring the density ratio reward function with specific criteria and\npreference exemplars enhances performance across domains and within target\nareas.\n  In our experiment using density ratio from a pair of Mistral-7B models, CDR\nachieves a RewardBench score of 82.6, outperforming the best trained reward\nfunctions from same model class and demonstrating competitive performance\nagainst SoTA models in Safety (91.0) and Reasoning (88.0) domains. We use CDR\nto annotate an on-policy preference dataset with which we preference tune\nLlama-3-8B-Instruct with SimPO. Using reward signals from two relatively weak\nmodels, our approach pushes Llama-3-8B to achieve a 37.4% (+15.1%) win rate on\nArenaHard and a 40.7% (+17.8%) win rate on Length-Controlled AlpacaEval 2.0,\nalong with a score of 8.0 on MT-Bench."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xu"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Shivchander Sudalairaj"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07154v1",
                "updated": "2024-11-11T17:32:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    32,
                    47,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T17:32:47Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    32,
                    47,
                    0,
                    316,
                    0
                ],
                "title": "Conditional simulation via entropic optimal transport: Toward\n  non-parametric estimation of conditional Brenier maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional simulation via entropic optimal transport: Toward\n  non-parametric estimation of conditional Brenier maps"
                },
                "summary": "Conditional simulation is a fundamental task in statistical modeling:\nGenerate samples from the conditionals given finitely many data points from a\njoint distribution. One promising approach is to construct conditional Brenier\nmaps, where the components of the map pushforward a reference distribution to\nconditionals of the target. While many estimators exist, few, if any, come with\nstatistical or algorithmic guarantees. To this end, we propose a non-parametric\nestimator for conditional Brenier maps based on the computational scalability\nof \\emph{entropic} optimal transport. Our estimator leverages a result of\nCarlier et al. (2010), which shows that optimal transport maps under a rescaled\nquadratic cost asymptotically converge to conditional Brenier maps; our\nestimator is precisely the entropic analogues of these converging maps. We\nprovide heuristic justifications for choosing the scaling parameter in the cost\nas a function of the number of samples by fully characterizing the Gaussian\nsetting. We conclude by comparing the performance of the estimator to other\nmachine learning and non-parametric approaches on benchmark datasets and\nBayesian inference problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional simulation is a fundamental task in statistical modeling:\nGenerate samples from the conditionals given finitely many data points from a\njoint distribution. One promising approach is to construct conditional Brenier\nmaps, where the components of the map pushforward a reference distribution to\nconditionals of the target. While many estimators exist, few, if any, come with\nstatistical or algorithmic guarantees. To this end, we propose a non-parametric\nestimator for conditional Brenier maps based on the computational scalability\nof \\emph{entropic} optimal transport. Our estimator leverages a result of\nCarlier et al. (2010), which shows that optimal transport maps under a rescaled\nquadratic cost asymptotically converge to conditional Brenier maps; our\nestimator is precisely the entropic analogues of these converging maps. We\nprovide heuristic justifications for choosing the scaling parameter in the cost\nas a function of the number of samples by fully characterizing the Gaussian\nsetting. We conclude by comparing the performance of the estimator to other\nmachine learning and non-parametric approaches on benchmark datasets and\nBayesian inference problems."
                },
                "authors": [
                    {
                        "name": "Ricardo Baptista"
                    },
                    {
                        "name": "Aram-Alexandre Pooladian"
                    },
                    {
                        "name": "Michael Brennan"
                    },
                    {
                        "name": "Youssef Marzouk"
                    },
                    {
                        "name": "Jonathan Niles-Weed"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Niles-Weed"
                },
                "author": "Jonathan Niles-Weed",
                "arxiv_comment": "26 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04528v2",
                "updated": "2024-11-11T17:30:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    30,
                    55,
                    0,
                    316,
                    0
                ],
                "published": "2023-12-07T18:46:50Z",
                "published_parsed": [
                    2023,
                    12,
                    7,
                    18,
                    46,
                    50,
                    3,
                    341,
                    0
                ],
                "title": "Using Large Language Models for Hyperparameter Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for Hyperparameter Optimization"
                },
                "summary": "This paper explores the use of foundational large language models (LLMs) in\nhyperparameter optimization (HPO). Hyperparameters are critical in determining\nthe effectiveness of machine learning models, yet their optimization often\nrelies on manual approaches in limited-budget settings. By prompting LLMs with\ndataset and model descriptions, we develop a methodology where LLMs suggest\nhyperparameter configurations, which are iteratively refined based on model\nperformance. Our empirical evaluations on standard benchmarks reveal that\nwithin constrained search budgets, LLMs can match or outperform traditional HPO\nmethods like Bayesian optimization across different models on standard\nbenchmarks. Furthermore, we propose to treat the code specifying our model as a\nhyperparameter, which the LLM outputs and affords greater flexibility than\nexisting HPO approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the use of foundational large language models (LLMs) in\nhyperparameter optimization (HPO). Hyperparameters are critical in determining\nthe effectiveness of machine learning models, yet their optimization often\nrelies on manual approaches in limited-budget settings. By prompting LLMs with\ndataset and model descriptions, we develop a methodology where LLMs suggest\nhyperparameter configurations, which are iteratively refined based on model\nperformance. Our empirical evaluations on standard benchmarks reveal that\nwithin constrained search budgets, LLMs can match or outperform traditional HPO\nmethods like Bayesian optimization across different models on standard\nbenchmarks. Furthermore, we propose to treat the code specifying our model as a\nhyperparameter, which the LLM outputs and affords greater flexibility than\nexisting HPO approaches."
                },
                "authors": [
                    {
                        "name": "Michael R. Zhang"
                    },
                    {
                        "name": "Nishkrit Desai"
                    },
                    {
                        "name": "Juhan Bae"
                    },
                    {
                        "name": "Jonathan Lorraine"
                    },
                    {
                        "name": "Jimmy Ba"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Ba"
                },
                "author": "Jimmy Ba",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07149v1",
                "updated": "2024-11-11T17:19:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    19,
                    20,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T17:19:20Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    19,
                    20,
                    0,
                    316,
                    0
                ],
                "title": "Transformers for Charged Particle Track Reconstruction in High Energy\n  Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers for Charged Particle Track Reconstruction in High Energy\n  Physics"
                },
                "summary": "Reconstructing charged particle tracks is a fundamental task in modern\ncollider experiments. The unprecedented particle multiplicities expected at the\nHigh-Luminosity Large Hadron Collider (HL-LHC) pose significant challenges for\ntrack reconstruction, where traditional algorithms become computationally\ninfeasible. To address this challenge, we present a novel learned approach to\ntrack reconstruction that adapts recent advances in computer vision and object\ndetection. Our architecture combines a Transformer hit filtering network with a\nMaskFormer reconstruction model that jointly optimises hit assignments and the\nestimation of the charged particles' properties. Evaluated on the TrackML\ndataset, our best performing model achieves state-of-the-art tracking\nperformance with 97% efficiency for a fake rate of 0.6%, and inference times of\n100ms. Our tunable approach enables specialisation for specific applications\nlike triggering systems, while its underlying principles can be extended to\nother reconstruction challenges in high energy physics. This work demonstrates\nthe potential of modern deep learning architectures to address emerging\ncomputational challenges in particle physics while maintaining the precision\nrequired for groundbreaking physics analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing charged particle tracks is a fundamental task in modern\ncollider experiments. The unprecedented particle multiplicities expected at the\nHigh-Luminosity Large Hadron Collider (HL-LHC) pose significant challenges for\ntrack reconstruction, where traditional algorithms become computationally\ninfeasible. To address this challenge, we present a novel learned approach to\ntrack reconstruction that adapts recent advances in computer vision and object\ndetection. Our architecture combines a Transformer hit filtering network with a\nMaskFormer reconstruction model that jointly optimises hit assignments and the\nestimation of the charged particles' properties. Evaluated on the TrackML\ndataset, our best performing model achieves state-of-the-art tracking\nperformance with 97% efficiency for a fake rate of 0.6%, and inference times of\n100ms. Our tunable approach enables specialisation for specific applications\nlike triggering systems, while its underlying principles can be extended to\nother reconstruction challenges in high energy physics. This work demonstrates\nthe potential of modern deep learning architectures to address emerging\ncomputational challenges in particle physics while maintaining the precision\nrequired for groundbreaking physics analysis."
                },
                "authors": [
                    {
                        "name": "Samuel Van Stroud"
                    },
                    {
                        "name": "Philippa Duckett"
                    },
                    {
                        "name": "Max Hart"
                    },
                    {
                        "name": "Nikita Pond"
                    },
                    {
                        "name": "Sbastien Rettie"
                    },
                    {
                        "name": "Gabriel Facini"
                    },
                    {
                        "name": "Tim Scanlon"
                    }
                ],
                "author_detail": {
                    "name": "Tim Scanlon"
                },
                "author": "Tim Scanlon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07140v1",
                "updated": "2024-11-11T17:10:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    10,
                    56,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T17:10:56Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    10,
                    56,
                    0,
                    316,
                    0
                ],
                "title": "Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language\n  Models"
                },
                "summary": "New LLM evaluation benchmarks are important to align with the rapid\ndevelopment of Large Language Models (LLMs). In this work, we present Chinese\nSimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality\nability of language models to answer short questions, and Chinese SimpleQA\nmainly has five properties (i.e., Chinese, Diverse, High-quality, Static,\nEasy-to-evaluate). Specifically, first, we focus on the Chinese language over 6\nmajor topics with 99 diverse subtopics. Second, we conduct a comprehensive\nquality control process to achieve high-quality questions and answers, where\nthe reference answers are static and cannot be changed over time. Third,\nfollowing SimpleQA, the questions and answers are very short, and the grading\nprocess is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we\nperform a comprehensive evaluation on the factuality abilities of existing\nLLMs. Finally, we hope that Chinese SimpleQA could guide the developers to\nbetter understand the Chinese factuality abilities of their models and\nfacilitate the growth of foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New LLM evaluation benchmarks are important to align with the rapid\ndevelopment of Large Language Models (LLMs). In this work, we present Chinese\nSimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality\nability of language models to answer short questions, and Chinese SimpleQA\nmainly has five properties (i.e., Chinese, Diverse, High-quality, Static,\nEasy-to-evaluate). Specifically, first, we focus on the Chinese language over 6\nmajor topics with 99 diverse subtopics. Second, we conduct a comprehensive\nquality control process to achieve high-quality questions and answers, where\nthe reference answers are static and cannot be changed over time. Third,\nfollowing SimpleQA, the questions and answers are very short, and the grading\nprocess is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we\nperform a comprehensive evaluation on the factuality abilities of existing\nLLMs. Finally, we hope that Chinese SimpleQA could guide the developers to\nbetter understand the Chinese factuality abilities of their models and\nfacilitate the growth of foundation models."
                },
                "authors": [
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Chengwei Hu"
                    },
                    {
                        "name": "Boren Zheng"
                    },
                    {
                        "name": "Xuepeng Liu"
                    },
                    {
                        "name": "Dekai Sun"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07133v1",
                "updated": "2024-11-11T17:06:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    6,
                    48,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T17:06:48Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    6,
                    48,
                    0,
                    316,
                    0
                ],
                "title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stronger Models are NOT Stronger Teachers for Instruction Tuning"
                },
                "summary": "Instruction tuning has been widely adopted to ensure large language models\n(LLMs) follow user instructions effectively. The resulting\ninstruction-following capabilities of LLMs heavily rely on the instruction\ndatasets used for tuning. Recently, synthetic instruction datasets have emerged\nas an economically viable solution to provide LLMs diverse and high-quality\ninstructions. However, existing approaches typically assume that larger or\nstronger models are stronger teachers for instruction tuning, and hence simply\nadopt these models as response generators to the synthetic instructions. In\nthis paper, we challenge this commonly-adopted assumption. Our extensive\nexperiments across five base models and twenty response generators reveal that\nlarger and stronger models are not necessarily stronger teachers of smaller\nmodels. We refer to this phenomenon as the Larger Models' Paradox. We observe\nthat existing metrics cannot precisely predict the effectiveness of response\ngenerators since they ignore the compatibility between teachers and base models\nbeing fine-tuned. We thus develop a novel metric, named as\nCompatibility-Adjusted Reward (CAR) to measure the effectiveness of response\ngenerators. Our experiments across five base models demonstrate that CAR\noutperforms almost all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has been widely adopted to ensure large language models\n(LLMs) follow user instructions effectively. The resulting\ninstruction-following capabilities of LLMs heavily rely on the instruction\ndatasets used for tuning. Recently, synthetic instruction datasets have emerged\nas an economically viable solution to provide LLMs diverse and high-quality\ninstructions. However, existing approaches typically assume that larger or\nstronger models are stronger teachers for instruction tuning, and hence simply\nadopt these models as response generators to the synthetic instructions. In\nthis paper, we challenge this commonly-adopted assumption. Our extensive\nexperiments across five base models and twenty response generators reveal that\nlarger and stronger models are not necessarily stronger teachers of smaller\nmodels. We refer to this phenomenon as the Larger Models' Paradox. We observe\nthat existing metrics cannot precisely predict the effectiveness of response\ngenerators since they ignore the compatibility between teachers and base models\nbeing fine-tuned. We thus develop a novel metric, named as\nCompatibility-Adjusted Reward (CAR) to measure the effectiveness of response\ngenerators. Our experiments across five base models demonstrate that CAR\noutperforms almost all baselines."
                },
                "authors": [
                    {
                        "name": "Zhangchen Xu"
                    },
                    {
                        "name": "Fengqing Jiang"
                    },
                    {
                        "name": "Luyao Niu"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Radha Poovendran"
                    }
                ],
                "author_detail": {
                    "name": "Radha Poovendran"
                },
                "author": "Radha Poovendran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07128v1",
                "updated": "2024-11-11T16:59:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    59,
                    22,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T16:59:22Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    59,
                    22,
                    0,
                    316,
                    0
                ],
                "title": "ZT-RIC:A Zero Trust RIC Framework for ensuring data Privacy and\n  Confidentiality in Open RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZT-RIC:A Zero Trust RIC Framework for ensuring data Privacy and\n  Confidentiality in Open RAN"
                },
                "summary": "The advancement of 5G and NextG networks through Open Radio Access Network\n(O-RAN) architecture enables a shift toward virtualized, modular, and\ndisaggregated configurations. A core component of O-RAN is the RAN Intelligent\nController (RIC), which manages RAN using machine learning-driven xApps that\naccess sensitive data from RAN and User Equipment (UE), stored in the near\nReal-Time RIC (Near-RT RIC) database. This shared, open environment increases\nthe risk of unauthorized data exposure. To address these concerns, this paper\nproposes a zero-trust RIC (ZT-RIC) framework that preserves data privacy across\nthe RIC platform, including the RIC database, xApps, and E2 interface. ZT-RIC\nemploys Inner Product Functional Encryption (IPFE) to encrypt RAN/UE data at\nthe base station, preventing leaks through the E2 interface and shared\ndatabase. Additionally, ZT-RIC enables xApps to perform inference on encrypted\ndata without exposing sensitive information. For evaluation, a state-of-the-art\nInterClass xApp, which detects jamming signals using RAN key performance\nmetrics (KPMs), is implemented. Testing on an LTE/5G O-RAN testbed shows that\nZT-RIC preserves data confidentiality while achieving 97.9% accuracy in jamming\ndetection and meeting sub-second latency requirements, with a round-trip time\n(RTT) of 0.527 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of 5G and NextG networks through Open Radio Access Network\n(O-RAN) architecture enables a shift toward virtualized, modular, and\ndisaggregated configurations. A core component of O-RAN is the RAN Intelligent\nController (RIC), which manages RAN using machine learning-driven xApps that\naccess sensitive data from RAN and User Equipment (UE), stored in the near\nReal-Time RIC (Near-RT RIC) database. This shared, open environment increases\nthe risk of unauthorized data exposure. To address these concerns, this paper\nproposes a zero-trust RIC (ZT-RIC) framework that preserves data privacy across\nthe RIC platform, including the RIC database, xApps, and E2 interface. ZT-RIC\nemploys Inner Product Functional Encryption (IPFE) to encrypt RAN/UE data at\nthe base station, preventing leaks through the E2 interface and shared\ndatabase. Additionally, ZT-RIC enables xApps to perform inference on encrypted\ndata without exposing sensitive information. For evaluation, a state-of-the-art\nInterClass xApp, which detects jamming signals using RAN key performance\nmetrics (KPMs), is implemented. Testing on an LTE/5G O-RAN testbed shows that\nZT-RIC preserves data confidentiality while achieving 97.9% accuracy in jamming\ndetection and meeting sub-second latency requirements, with a round-trip time\n(RTT) of 0.527 seconds."
                },
                "authors": [
                    {
                        "name": "Diana Lin"
                    },
                    {
                        "name": "Samarth Bhargav"
                    },
                    {
                        "name": "Azuka Chiejina"
                    },
                    {
                        "name": "Mohamed I. Ibrahem"
                    },
                    {
                        "name": "Vijay K. Shah"
                    }
                ],
                "author_detail": {
                    "name": "Vijay K. Shah"
                },
                "author": "Vijay K. Shah",
                "arxiv_comment": "This paper has been accepted to CCNC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07127v1",
                "updated": "2024-11-11T16:58:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    58,
                    36,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T16:58:36Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    58,
                    36,
                    0,
                    316,
                    0
                ],
                "title": "Benchmarking LLMs' Judgments with No Gold Standard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs' Judgments with No Gold Standard"
                },
                "summary": "We introduce the GEM (Generative Estimator for Mutual Information), an\nevaluation metric for assessing language generation by Large Language Models\n(LLMs), particularly in generating informative judgments, without the need for\na gold standard reference. GEM broadens the scenarios where we can benchmark\nLLM generation performance-from traditional ones, like machine translation and\nsummarization, where gold standard references are readily available, to\nsubjective tasks without clear gold standards, such as academic peer review.\n  GEM uses a generative model to estimate mutual information between candidate\nand reference responses, without requiring the reference to be a gold standard.\nIn experiments on a human-annotated dataset, GEM demonstrates competitive\ncorrelations with human scores compared to the state-of-the-art GPT-4o\nExaminer, and outperforms all other baselines. Additionally, GEM is more robust\nagainst strategic manipulations, such as rephrasing or elongation, which can\nartificially inflate scores under a GPT-4o Examiner.\n  We also present GRE-bench (Generating Review Evaluation Benchmark) which\nevaluates LLMs based on how well they can generate high-quality peer reviews\nfor academic research papers. Because GRE-bench is based upon GEM, it inherits\nits robustness properties. Additionally, GRE-bench circumvents data\ncontamination problems (or data leakage) by using the continuous influx of new\nopen-access research papers and peer reviews each year. We show GRE-bench\nresults of various popular LLMs on their peer review capabilities using the\nICLR2023 dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the GEM (Generative Estimator for Mutual Information), an\nevaluation metric for assessing language generation by Large Language Models\n(LLMs), particularly in generating informative judgments, without the need for\na gold standard reference. GEM broadens the scenarios where we can benchmark\nLLM generation performance-from traditional ones, like machine translation and\nsummarization, where gold standard references are readily available, to\nsubjective tasks without clear gold standards, such as academic peer review.\n  GEM uses a generative model to estimate mutual information between candidate\nand reference responses, without requiring the reference to be a gold standard.\nIn experiments on a human-annotated dataset, GEM demonstrates competitive\ncorrelations with human scores compared to the state-of-the-art GPT-4o\nExaminer, and outperforms all other baselines. Additionally, GEM is more robust\nagainst strategic manipulations, such as rephrasing or elongation, which can\nartificially inflate scores under a GPT-4o Examiner.\n  We also present GRE-bench (Generating Review Evaluation Benchmark) which\nevaluates LLMs based on how well they can generate high-quality peer reviews\nfor academic research papers. Because GRE-bench is based upon GEM, it inherits\nits robustness properties. Additionally, GRE-bench circumvents data\ncontamination problems (or data leakage) by using the continuous influx of new\nopen-access research papers and peer reviews each year. We show GRE-bench\nresults of various popular LLMs on their peer review capabilities using the\nICLR2023 dataset."
                },
                "authors": [
                    {
                        "name": "Shengwei Xu"
                    },
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Grant Schoenebeck"
                    },
                    {
                        "name": "Yuqing Kong"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Kong"
                },
                "author": "Yuqing Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06687v2",
                "updated": "2024-11-11T16:53:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    53,
                    58,
                    0,
                    316,
                    0
                ],
                "published": "2024-05-06T18:09:32Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    18,
                    9,
                    32,
                    0,
                    127,
                    0
                ],
                "title": "Hire Me or Not? Examining Language Model's Behavior with Occupation\n  Attributes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hire Me or Not? Examining Language Model's Behavior with Occupation\n  Attributes"
                },
                "summary": "With the impressive performance in various downstream tasks, large language\nmodels (LLMs) have been widely integrated into production pipelines, like\nrecruitment and recommendation systems. A known issue of models trained on\nnatural language data is the presence of human biases, which can impact the\nfairness of the system. This paper investigates LLMs' behavior with respect to\ngender stereotypes, in the context of occupation decision making. Our framework\nis designed to investigate and quantify the presence of gender stereotypes in\nLLMs' behavior via multi-round question answering. Inspired by prior works, we\nconstruct a dataset by leveraging a standard occupation classification\nknowledge base released by authoritative agencies. We tested three LLMs\n(RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models\nexhibit gender stereotypes analogous to human biases, but with different\npreferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may\nimply the current alignment methods are insufficient for debiasing and could\nintroduce new biases contradicting the traditional gender stereotypes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the impressive performance in various downstream tasks, large language\nmodels (LLMs) have been widely integrated into production pipelines, like\nrecruitment and recommendation systems. A known issue of models trained on\nnatural language data is the presence of human biases, which can impact the\nfairness of the system. This paper investigates LLMs' behavior with respect to\ngender stereotypes, in the context of occupation decision making. Our framework\nis designed to investigate and quantify the presence of gender stereotypes in\nLLMs' behavior via multi-round question answering. Inspired by prior works, we\nconstruct a dataset by leveraging a standard occupation classification\nknowledge base released by authoritative agencies. We tested three LLMs\n(RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models\nexhibit gender stereotypes analogous to human biases, but with different\npreferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may\nimply the current alignment methods are insufficient for debiasing and could\nintroduce new biases contradicting the traditional gender stereotypes."
                },
                "authors": [
                    {
                        "name": "Damin Zhang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Geetanjali Bihani"
                    },
                    {
                        "name": "Julia Rayz"
                    }
                ],
                "author_detail": {
                    "name": "Julia Rayz"
                },
                "author": "Julia Rayz",
                "arxiv_comment": "WIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07122v1",
                "updated": "2024-11-11T16:51:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    51,
                    39,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T16:51:39Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    51,
                    39,
                    0,
                    316,
                    0
                ],
                "title": "SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering\n  in LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating human-like text, but their output may not be aligned with the user\nor even produce harmful content. This paper presents a novel approach to detect\nand steer concepts such as toxicity before generation. We introduce the Sparse\nConditioned Autoencoder (SCAR), a single trained module that extends the\notherwise untouched LLM. SCAR ensures full steerability, towards and away from\nconcepts (e.g., toxic content), without compromising the quality of the model's\ntext generation on standard evaluation benchmarks. We demonstrate the effective\napplication of our approach through a variety of concepts, including toxicity,\nsafety, and writing style alignment. As such, this work establishes a robust\nframework for controlling LLM generations, ensuring their ethical and safe\ndeployment in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating human-like text, but their output may not be aligned with the user\nor even produce harmful content. This paper presents a novel approach to detect\nand steer concepts such as toxicity before generation. We introduce the Sparse\nConditioned Autoencoder (SCAR), a single trained module that extends the\notherwise untouched LLM. SCAR ensures full steerability, towards and away from\nconcepts (e.g., toxic content), without compromising the quality of the model's\ntext generation on standard evaluation benchmarks. We demonstrate the effective\napplication of our approach through a variety of concepts, including toxicity,\nsafety, and writing style alignment. As such, this work establishes a robust\nframework for controlling LLM generations, ensuring their ethical and safe\ndeployment in real-world applications."
                },
                "authors": [
                    {
                        "name": "Ruben Hrle"
                    },
                    {
                        "name": "Felix Friedrich"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Bjrn Deiseroth"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02926v2",
                "updated": "2024-11-11T16:47:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    47,
                    58,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-05T09:13:53Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    13,
                    53,
                    1,
                    310,
                    0
                ],
                "title": "Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic\n  Encryption for Collaborative Anti-Money Laundering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Graph-Based Machine Learning with Fully Homomorphic\n  Encryption for Collaborative Anti-Money Laundering"
                },
                "summary": "Combating money laundering has become increasingly complex with the rise of\ncybercrime and digitalization of financial transactions. Graph-based machine\nlearning techniques have emerged as promising tools for Anti-Money Laundering\n(AML) detection, capturing intricate relationships within money laundering\nnetworks. However, the effectiveness of AML solutions is hindered by data silos\nwithin financial institutions, limiting collaboration and overall efficacy.\nThis research presents a novel privacy-preserving approach for collaborative\nAML machine learning, facilitating secure data sharing across institutions and\nborders while preserving privacy and regulatory compliance. Leveraging Fully\nHomomorphic Encryption (FHE), computations are directly performed on encrypted\ndata, ensuring the confidentiality of financial data. Notably, FHE over the\nTorus (TFHE) was integrated with graph-based machine learning using Zama\nConcrete ML. The research contributes two key privacy-preserving pipelines.\nFirst, the development of a privacy-preserving Graph Neural Network (GNN)\npipeline was explored. Optimization techniques like quantization and pruning\nwere used to render the GNN FHE-compatible. Second, a privacy-preserving\ngraph-based XGBoost pipeline leveraging Graph Feature Preprocessor (GFP) was\nsuccessfully developed. Experiments demonstrated strong predictive performance,\nwith the XGBoost model consistently achieving over 99% accuracy, F1-score,\nprecision, and recall on the balanced AML dataset in both unencrypted and\nFHE-encrypted inference settings. On the imbalanced dataset, the incorporation\nof graph-based features improved the F1-score by 8%. The research highlights\nthe need to balance the trade-off between privacy and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combating money laundering has become increasingly complex with the rise of\ncybercrime and digitalization of financial transactions. Graph-based machine\nlearning techniques have emerged as promising tools for Anti-Money Laundering\n(AML) detection, capturing intricate relationships within money laundering\nnetworks. However, the effectiveness of AML solutions is hindered by data silos\nwithin financial institutions, limiting collaboration and overall efficacy.\nThis research presents a novel privacy-preserving approach for collaborative\nAML machine learning, facilitating secure data sharing across institutions and\nborders while preserving privacy and regulatory compliance. Leveraging Fully\nHomomorphic Encryption (FHE), computations are directly performed on encrypted\ndata, ensuring the confidentiality of financial data. Notably, FHE over the\nTorus (TFHE) was integrated with graph-based machine learning using Zama\nConcrete ML. The research contributes two key privacy-preserving pipelines.\nFirst, the development of a privacy-preserving Graph Neural Network (GNN)\npipeline was explored. Optimization techniques like quantization and pruning\nwere used to render the GNN FHE-compatible. Second, a privacy-preserving\ngraph-based XGBoost pipeline leveraging Graph Feature Preprocessor (GFP) was\nsuccessfully developed. Experiments demonstrated strong predictive performance,\nwith the XGBoost model consistently achieving over 99% accuracy, F1-score,\nprecision, and recall on the balanced AML dataset in both unencrypted and\nFHE-encrypted inference settings. On the imbalanced dataset, the incorporation\nof graph-based features improved the F1-score by 8%. The research highlights\nthe need to balance the trade-off between privacy and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Fabrianne Effendi"
                    },
                    {
                        "name": "Anupam Chattopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Anupam Chattopadhyay"
                },
                "author": "Anupam Chattopadhyay",
                "arxiv_comment": "14th International Conference on Security, Privacy, and Applied\n  Cryptographic Engineering (SPACE) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12935v2",
                "updated": "2024-11-11T16:42:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    42,
                    45,
                    0,
                    316,
                    0
                ],
                "published": "2024-09-19T17:47:02Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    17,
                    47,
                    2,
                    3,
                    263,
                    0
                ],
                "title": "Stochastic gradient descent in continuous time for drift identification\n  in multiscale diffusions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic gradient descent in continuous time for drift identification\n  in multiscale diffusions"
                },
                "summary": "We consider the setting of multiscale overdamped Langevin stochastic\ndifferential equations, and study the problem of learning the drift function of\nthe homogenized dynamics from continuous-time observations of the multiscale\nsystem. We decompose the drift term in a truncated series of basis functions,\nand employ the stochastic gradient descent in continuous time to infer the\ncoefficients of the expansion. Due to the incompatibility between the\nmultiscale data and the homogenized model, the estimator alone is not able to\nreconstruct the exact drift. We therefore propose to filter the original\ntrajectory through appropriate kernels and include filtered data in the\nstochastic differential equation for the estimator, which indeed solves the\nmisspecification issue. Several numerical experiments highlight the accuracy of\nour approach. Moreover, we show theoretically in a simplified framework the\nasymptotic unbiasedness of our estimator in the limit of infinite data and when\nthe multiscale parameter describing the fastest scale vanishes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the setting of multiscale overdamped Langevin stochastic\ndifferential equations, and study the problem of learning the drift function of\nthe homogenized dynamics from continuous-time observations of the multiscale\nsystem. We decompose the drift term in a truncated series of basis functions,\nand employ the stochastic gradient descent in continuous time to infer the\ncoefficients of the expansion. Due to the incompatibility between the\nmultiscale data and the homogenized model, the estimator alone is not able to\nreconstruct the exact drift. We therefore propose to filter the original\ntrajectory through appropriate kernels and include filtered data in the\nstochastic differential equation for the estimator, which indeed solves the\nmisspecification issue. Several numerical experiments highlight the accuracy of\nour approach. Moreover, we show theoretically in a simplified framework the\nasymptotic unbiasedness of our estimator in the limit of infinite data and when\nthe multiscale parameter describing the fastest scale vanishes."
                },
                "authors": [
                    {
                        "name": "Max Hirsch"
                    },
                    {
                        "name": "Andrea Zanoni"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zanoni"
                },
                "author": "Andrea Zanoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07114v1",
                "updated": "2024-11-11T16:41:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    41,
                    22,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T16:41:22Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    41,
                    22,
                    0,
                    316,
                    0
                ],
                "title": "TinyML Security: Exploring Vulnerabilities in Resource-Constrained\n  Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyML Security: Exploring Vulnerabilities in Resource-Constrained\n  Machine Learning Systems"
                },
                "summary": "Tiny Machine Learning (TinyML) systems, which enable machine learning\ninference on highly resource-constrained devices, are transforming edge\ncomputing but encounter unique security challenges. These devices, restricted\nby RAM and CPU capabilities two to three orders of magnitude smaller than\nconventional systems, make traditional software and hardware security solutions\nimpractical. The physical accessibility of these devices exacerbates their\nsusceptibility to side-channel attacks and information leakage. Additionally,\nTinyML models pose security risks, with weights potentially encoding sensitive\ndata and query interfaces that can be exploited. This paper offers the first\nthorough survey of TinyML security threats. We present a device taxonomy that\ndifferentiates between IoT, EdgeML, and TinyML, highlighting vulnerabilities\nunique to TinyML. We list various attack vectors, assess their threat levels\nusing the Common Vulnerability Scoring System, and evaluate both existing and\npossible defenses. Our analysis identifies where traditional security measures\nare adequate and where solutions tailored to TinyML are essential. Our results\nunderscore the pressing need for specialized security solutions in TinyML to\nensure robust and secure edge computing applications. We aim to inform the\nresearch community and inspire innovative approaches to protecting this rapidly\nevolving and critical field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tiny Machine Learning (TinyML) systems, which enable machine learning\ninference on highly resource-constrained devices, are transforming edge\ncomputing but encounter unique security challenges. These devices, restricted\nby RAM and CPU capabilities two to three orders of magnitude smaller than\nconventional systems, make traditional software and hardware security solutions\nimpractical. The physical accessibility of these devices exacerbates their\nsusceptibility to side-channel attacks and information leakage. Additionally,\nTinyML models pose security risks, with weights potentially encoding sensitive\ndata and query interfaces that can be exploited. This paper offers the first\nthorough survey of TinyML security threats. We present a device taxonomy that\ndifferentiates between IoT, EdgeML, and TinyML, highlighting vulnerabilities\nunique to TinyML. We list various attack vectors, assess their threat levels\nusing the Common Vulnerability Scoring System, and evaluate both existing and\npossible defenses. Our analysis identifies where traditional security measures\nare adequate and where solutions tailored to TinyML are essential. Our results\nunderscore the pressing need for specialized security solutions in TinyML to\nensure robust and secure edge computing applications. We aim to inform the\nresearch community and inspire innovative approaches to protecting this rapidly\nevolving and critical field."
                },
                "authors": [
                    {
                        "name": "Jacob Huckelberry"
                    },
                    {
                        "name": "Yuke Zhang"
                    },
                    {
                        "name": "Allison Sansone"
                    },
                    {
                        "name": "James Mickens"
                    },
                    {
                        "name": "Peter A. Beerel"
                    },
                    {
                        "name": "Vijay Janapa Reddi"
                    }
                ],
                "author_detail": {
                    "name": "Vijay Janapa Reddi"
                },
                "author": "Vijay Janapa Reddi",
                "arxiv_comment": "Submitted to Proceedings of the IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.12196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.12196v3",
                "updated": "2024-11-11T16:40:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    40,
                    18,
                    0,
                    316,
                    0
                ],
                "published": "2024-01-22T18:36:29Z",
                "published_parsed": [
                    2024,
                    1,
                    22,
                    18,
                    36,
                    29,
                    0,
                    22,
                    0
                ],
                "title": "Learning Dynamics from Multicellular Graphs with Deep Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Dynamics from Multicellular Graphs with Deep Neural Networks"
                },
                "summary": "Multicellular self-assembly into functional structures is a dynamic process\nthat is critical in the development and diseases, including embryo development,\norgan formation, tumor invasion, and others. Being able to infer collective\ncell migratory dynamics from their static configuration is valuable for both\nunderstanding and predicting these complex processes. However, the\nidentification of structural features that can indicate multicellular motion\nhas been difficult, and existing metrics largely rely on physical instincts.\nHere we show that using a graph neural network (GNN), the motion of\nmulticellular collectives can be inferred from a static snapshot of cell\npositions, in both experimental and synthetic datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multicellular self-assembly into functional structures is a dynamic process\nthat is critical in the development and diseases, including embryo development,\norgan formation, tumor invasion, and others. Being able to infer collective\ncell migratory dynamics from their static configuration is valuable for both\nunderstanding and predicting these complex processes. However, the\nidentification of structural features that can indicate multicellular motion\nhas been difficult, and existing metrics largely rely on physical instincts.\nHere we show that using a graph neural network (GNN), the motion of\nmulticellular collectives can be inferred from a static snapshot of cell\npositions, in both experimental and synthetic datasets."
                },
                "authors": [
                    {
                        "name": "Haiqian Yang"
                    },
                    {
                        "name": "Florian Meyer"
                    },
                    {
                        "name": "Shaoxun Huang"
                    },
                    {
                        "name": "Liu Yang"
                    },
                    {
                        "name": "Cristiana Lungu"
                    },
                    {
                        "name": "Monilola A. Olayioye"
                    },
                    {
                        "name": "Markus J. Buehler"
                    },
                    {
                        "name": "Ming Guo"
                    }
                ],
                "author_detail": {
                    "name": "Ming Guo"
                },
                "author": "Ming Guo",
                "arxiv_doi": "10.1103/PRXLife.2.043010",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PRXLife.2.043010",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.12196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.12196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication at PRX Life",
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07112v1",
                "updated": "2024-11-11T16:39:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    39,
                    13,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T16:39:13Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    39,
                    13,
                    0,
                    316,
                    0
                ],
                "title": "ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large\n  Language Models for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large\n  Language Models for Code Generation"
                },
                "summary": "Large language models (LLMs) have achieved impressive performance in code\ngeneration recently, offering programmers revolutionary assistance in software\ndevelopment. However, due to the auto-regressive nature of LLMs, they are\nsusceptible to error accumulation during code generation. Once an error is\nproduced, LLMs can merely continue to generate the subsequent code conditioned\non it, given their inability to adjust previous outputs. Existing LLM-based\napproaches typically consider post-revising after code generation, leading to\nthe challenging resolution of accumulated errors and the significant wastage of\nresources. Ideally, LLMs should rollback and resolve the occurred error in time\nduring code generation, rather than proceed on the basis of the error and wait\nfor post-revising after generation. In this paper, we propose ROCODE, which\nintegrates the backtracking mechanism and program analysis into LLMs for code\ngeneration. Specifically, we employ program analysis to perform incremental\nerror detection during the generation process. When an error is detected, the\nbacktracking mechanism is triggered to priming rollback strategies and\nconstraint regeneration, thereby eliminating the error early and ensuring\ncontinued generation on the correct basis. Experiments on multiple code\ngeneration benchmarks show that ROCODE can significantly reduce the errors\ngenerated by LLMs, with a compilation pass rate of 99.1%. The test pass rate is\nimproved by up to 23.8% compared to the best baseline approach. Compared to the\npost-revising baseline, the token cost is reduced by 19.3%. Moreover, our\napproach is model-agnostic and achieves consistent improvements across nine\nrepresentative LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive performance in code\ngeneration recently, offering programmers revolutionary assistance in software\ndevelopment. However, due to the auto-regressive nature of LLMs, they are\nsusceptible to error accumulation during code generation. Once an error is\nproduced, LLMs can merely continue to generate the subsequent code conditioned\non it, given their inability to adjust previous outputs. Existing LLM-based\napproaches typically consider post-revising after code generation, leading to\nthe challenging resolution of accumulated errors and the significant wastage of\nresources. Ideally, LLMs should rollback and resolve the occurred error in time\nduring code generation, rather than proceed on the basis of the error and wait\nfor post-revising after generation. In this paper, we propose ROCODE, which\nintegrates the backtracking mechanism and program analysis into LLMs for code\ngeneration. Specifically, we employ program analysis to perform incremental\nerror detection during the generation process. When an error is detected, the\nbacktracking mechanism is triggered to priming rollback strategies and\nconstraint regeneration, thereby eliminating the error early and ensuring\ncontinued generation on the correct basis. Experiments on multiple code\ngeneration benchmarks show that ROCODE can significantly reduce the errors\ngenerated by LLMs, with a compilation pass rate of 99.1%. The test pass rate is\nimproved by up to 23.8% compared to the best baseline approach. Compared to the\npost-revising baseline, the token cost is reduced by 19.3%. Moreover, our\napproach is model-agnostic and achieves consistent improvements across nine\nrepresentative LLMs."
                },
                "authors": [
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Yongding Tao"
                    },
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Wenpin Jiao"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "arxiv_comment": "ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07111v1",
                "updated": "2024-11-11T16:37:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    37,
                    40,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T16:37:40Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    37,
                    40,
                    0,
                    316,
                    0
                ],
                "title": "Building a Taiwanese Mandarin Spoken Language Model: A First Attempt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a Taiwanese Mandarin Spoken Language Model: A First Attempt"
                },
                "summary": "This technical report presents our initial attempt to build a spoken large\nlanguage model (LLM) for Taiwanese Mandarin, specifically tailored to enable\nreal-time, speech-to-speech interaction in multi-turn conversations. Our\nend-to-end model incorporates a decoder-only transformer architecture and aims\nto achieve seamless interaction while preserving the conversational flow,\nincluding full-duplex capabilities allowing simultaneous speaking and\nlistening. The paper also details the training process, including data\npreparation with synthesized dialogues and adjustments for real-time\ninteraction. We also developed a platform to evaluate conversational fluency\nand response coherence in multi-turn dialogues. We hope the release of the\nreport can contribute to the future development of spoken LLMs in Taiwanese\nMandarin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents our initial attempt to build a spoken large\nlanguage model (LLM) for Taiwanese Mandarin, specifically tailored to enable\nreal-time, speech-to-speech interaction in multi-turn conversations. Our\nend-to-end model incorporates a decoder-only transformer architecture and aims\nto achieve seamless interaction while preserving the conversational flow,\nincluding full-duplex capabilities allowing simultaneous speaking and\nlistening. The paper also details the training process, including data\npreparation with synthesized dialogues and adjustments for real-time\ninteraction. We also developed a platform to evaluate conversational fluency\nand response coherence in multi-turn dialogues. We hope the release of the\nreport can contribute to the future development of spoken LLMs in Taiwanese\nMandarin."
                },
                "authors": [
                    {
                        "name": "Chih-Kai Yang"
                    },
                    {
                        "name": "Yu-Kuan Fu"
                    },
                    {
                        "name": "Chen-An Li"
                    },
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Yu-Xiang Lin"
                    },
                    {
                        "name": "Wei-Chih Chen"
                    },
                    {
                        "name": "Ho Lam Chung"
                    },
                    {
                        "name": "Chun-Yi Kuan"
                    },
                    {
                        "name": "Wei-Ping Huang"
                    },
                    {
                        "name": "Ke-Han Lu"
                    },
                    {
                        "name": "Tzu-Quan Lin"
                    },
                    {
                        "name": "Hsiu-Hsuan Wang"
                    },
                    {
                        "name": "En-Pei Hu"
                    },
                    {
                        "name": "Chan-Jan Hsu"
                    },
                    {
                        "name": "Liang-Hsuan Tseng"
                    },
                    {
                        "name": "I-Hsiang Chiu"
                    },
                    {
                        "name": "Ulin Sanga"
                    },
                    {
                        "name": "Xuanjun Chen"
                    },
                    {
                        "name": "Po-chun Hsu"
                    },
                    {
                        "name": "Shu-wen Yang"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12060v2",
                "updated": "2024-11-11T16:33:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    33,
                    25,
                    0,
                    316,
                    0
                ],
                "published": "2024-06-17T20:00:04Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    20,
                    0,
                    4,
                    0,
                    169,
                    0
                ],
                "title": "Not Eliminate but Aggregate: Post-Hoc Control over Mixture-of-Experts to\n  Address Shortcut Shifts in Natural Language Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not Eliminate but Aggregate: Post-Hoc Control over Mixture-of-Experts to\n  Address Shortcut Shifts in Natural Language Understanding"
                },
                "summary": "Recent models for natural language understanding are inclined to exploit\nsimple patterns in datasets, commonly known as shortcuts. These shortcuts hinge\non spurious correlations between labels and latent features existing in the\ntraining data. At inference time, shortcut-dependent models are likely to\ngenerate erroneous predictions under distribution shifts, particularly when\nsome latent features are no longer correlated with the labels. To avoid this,\nprevious studies have trained models to eliminate the reliance on shortcuts. In\nthis study, we explore a different direction: pessimistically aggregating the\npredictions of a mixture-of-experts, assuming each expert captures relatively\ndifferent latent features. The experimental results demonstrate that our\npost-hoc control over the experts significantly enhances the model's robustness\nto the distribution shift in shortcuts. Besides, we show that our approach has\nsome practical advantages. We also analyze our model and provide results to\nsupport the assumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent models for natural language understanding are inclined to exploit\nsimple patterns in datasets, commonly known as shortcuts. These shortcuts hinge\non spurious correlations between labels and latent features existing in the\ntraining data. At inference time, shortcut-dependent models are likely to\ngenerate erroneous predictions under distribution shifts, particularly when\nsome latent features are no longer correlated with the labels. To avoid this,\nprevious studies have trained models to eliminate the reliance on shortcuts. In\nthis study, we explore a different direction: pessimistically aggregating the\npredictions of a mixture-of-experts, assuming each expert captures relatively\ndifferent latent features. The experimental results demonstrate that our\npost-hoc control over the experts significantly enhances the model's robustness\nto the distribution shift in shortcuts. Besides, we show that our approach has\nsome practical advantages. We also analyze our model and provide results to\nsupport the assumption."
                },
                "authors": [
                    {
                        "name": "Ukyo Honda"
                    },
                    {
                        "name": "Tatsushi Oka"
                    },
                    {
                        "name": "Peinan Zhang"
                    },
                    {
                        "name": "Masato Mita"
                    }
                ],
                "author_detail": {
                    "name": "Masato Mita"
                },
                "author": "Masato Mita",
                "arxiv_doi": "10.1162/tacl_a_00701",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1162/tacl_a_00701",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.12060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 5 figures (the layout differs from the MIT Press\n  publication version)",
                "arxiv_journal_ref": "Transactions of the Association for Computational Linguistics\n  (TACL), Vol 12 (2024), pages 1268-1289",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09971v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09971v2",
                "updated": "2024-11-11T16:20:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    20,
                    28,
                    0,
                    316,
                    0
                ],
                "published": "2024-03-15T02:28:26Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    2,
                    28,
                    26,
                    4,
                    75,
                    0
                ],
                "title": "Advancing Object Goal Navigation Through LLM-enhanced Object Affinities\n  Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Object Goal Navigation Through LLM-enhanced Object Affinities\n  Transfer"
                },
                "summary": "In object goal navigation, agents navigate towards objects identified by\ncategory labels using visual and spatial information. Previously, solely\nnetwork-based methods typically rely on historical data for object affinities\nestimation, lacking adaptability to new environments and unseen targets.\nSimultaneously, employing Large Language Models (LLMs) for navigation as either\nplanners or agents, though offering a broad knowledge base, is cost-inefficient\nand lacks targeted historical experience. Addressing these challenges, we\npresent the LLM-enhanced Object Affinities Transfer (LOAT) framework,\nintegrating LLM-derived object semantics with network-based approaches to\nleverage experiential object affinities, thus improving adaptability in\nunfamiliar settings. LOAT employs a dual-module strategy: a generalized\naffinities module for accessing LLMs' vast knowledge and an experiential\naffinities module for applying learned object semantic relationships,\ncomplemented by a dynamic fusion module harmonizing these information sources\nbased on temporal context. The resulting scores activate semantic maps before\nfeeding into downstream policies, enhancing navigation systems with\ncontext-aware inputs. Our evaluations conducted in the AI2-THOR and Habitat\nsimulators indicate significant improvements in both navigation success rates\nand overall efficiency. Furthermore, the system performs effectively when\ndeployed on a real robot without requiring additional training, thereby\nvalidating the efficacy of LOAT in integrating LLM insights for enhanced\nobject-goal navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In object goal navigation, agents navigate towards objects identified by\ncategory labels using visual and spatial information. Previously, solely\nnetwork-based methods typically rely on historical data for object affinities\nestimation, lacking adaptability to new environments and unseen targets.\nSimultaneously, employing Large Language Models (LLMs) for navigation as either\nplanners or agents, though offering a broad knowledge base, is cost-inefficient\nand lacks targeted historical experience. Addressing these challenges, we\npresent the LLM-enhanced Object Affinities Transfer (LOAT) framework,\nintegrating LLM-derived object semantics with network-based approaches to\nleverage experiential object affinities, thus improving adaptability in\nunfamiliar settings. LOAT employs a dual-module strategy: a generalized\naffinities module for accessing LLMs' vast knowledge and an experiential\naffinities module for applying learned object semantic relationships,\ncomplemented by a dynamic fusion module harmonizing these information sources\nbased on temporal context. The resulting scores activate semantic maps before\nfeeding into downstream policies, enhancing navigation systems with\ncontext-aware inputs. Our evaluations conducted in the AI2-THOR and Habitat\nsimulators indicate significant improvements in both navigation success rates\nand overall efficiency. Furthermore, the system performs effectively when\ndeployed on a real robot without requiring additional training, thereby\nvalidating the efficacy of LOAT in integrating LLM insights for enhanced\nobject-goal navigation."
                },
                "authors": [
                    {
                        "name": "Mengying Lin"
                    },
                    {
                        "name": "Shugao Liu"
                    },
                    {
                        "name": "Dingxi Zhang"
                    },
                    {
                        "name": "Yaran Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Dongbin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongbin Zhao"
                },
                "author": "Dongbin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09971v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09971v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07098v1",
                "updated": "2024-11-11T16:20:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    20,
                    27,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T16:20:27Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    20,
                    27,
                    0,
                    316,
                    0
                ],
                "title": "A Multi-Agent Approach for REST API Testing with Semantic Graphs and\n  LLM-Driven Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Agent Approach for REST API Testing with Semantic Graphs and\n  LLM-Driven Inputs"
                },
                "summary": "As modern web services increasingly rely on REST APIs, their thorough testing\nhas become crucial. Furthermore, the advent of REST API specifications such as\nthe OpenAPI Specification has led to the emergence of many black-box REST API\ntesting tools. However, these tools often focus on individual test elements in\nisolation (e.g., APIs, parameters, values), resulting in lower coverage and\nless effectiveness in detecting faults (i.e., 500 response codes). To address\nthese limitations, we present AutoRestTest, the first black-box framework to\nadopt a dependency-embedded multi-agent approach for REST API testing,\nintegrating Multi-Agent Reinforcement Learning (MARL) with a Semantic Property\nDependency Graph (SPDG) and Large Language Models (LLMs). Our approach treats\nREST API testing as a separable problem, where four agents -- API, dependency,\nparameter, and value -- collaborate to optimize API exploration. LLMs handle\ndomain-specific value restrictions, the SPDG model simplifies the search space\nfor dependencies using a similarity score between API operations, and MARL\ndynamically optimizes the agents' behavior. Evaluated on 12 real-world REST\nservices, AutoRestTest outperforms the four leading black-box REST API testing\ntools, including those assisted by RESTGPT (which augments realistic test\ninputs using LLMs), in terms of code coverage, operation coverage, and fault\ndetection. Notably, AutoRestTest is the only tool able to identify an internal\nserver error in Spotify. Our ablation study underscores the significant\ncontributions of the agent learning, SPDG, and LLM components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As modern web services increasingly rely on REST APIs, their thorough testing\nhas become crucial. Furthermore, the advent of REST API specifications such as\nthe OpenAPI Specification has led to the emergence of many black-box REST API\ntesting tools. However, these tools often focus on individual test elements in\nisolation (e.g., APIs, parameters, values), resulting in lower coverage and\nless effectiveness in detecting faults (i.e., 500 response codes). To address\nthese limitations, we present AutoRestTest, the first black-box framework to\nadopt a dependency-embedded multi-agent approach for REST API testing,\nintegrating Multi-Agent Reinforcement Learning (MARL) with a Semantic Property\nDependency Graph (SPDG) and Large Language Models (LLMs). Our approach treats\nREST API testing as a separable problem, where four agents -- API, dependency,\nparameter, and value -- collaborate to optimize API exploration. LLMs handle\ndomain-specific value restrictions, the SPDG model simplifies the search space\nfor dependencies using a similarity score between API operations, and MARL\ndynamically optimizes the agents' behavior. Evaluated on 12 real-world REST\nservices, AutoRestTest outperforms the four leading black-box REST API testing\ntools, including those assisted by RESTGPT (which augments realistic test\ninputs using LLMs), in terms of code coverage, operation coverage, and fault\ndetection. Notably, AutoRestTest is the only tool able to identify an internal\nserver error in Spotify. Our ablation study underscores the significant\ncontributions of the agent learning, SPDG, and LLM components."
                },
                "authors": [
                    {
                        "name": "Myeongsoo Kim"
                    },
                    {
                        "name": "Tyler Stennett"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Alessandro Orso"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Orso"
                },
                "author": "Alessandro Orso",
                "arxiv_comment": "To be published in the 47th IEEE/ACM International Conference on\n  Software Engineering (ICSE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07091v1",
                "updated": "2024-11-11T16:12:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    12,
                    11,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T16:12:11Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    12,
                    11,
                    0,
                    316,
                    0
                ],
                "title": "Impact of LLM-based Review Comment Generation in Practice: A Mixed\n  Open-/Closed-source User Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of LLM-based Review Comment Generation in Practice: A Mixed\n  Open-/Closed-source User Study"
                },
                "summary": "We conduct a large-scale empirical user study in a live setup to evaluate the\nacceptance of LLM-generated comments and their impact on the review process.\nThis user study was performed in two organizations, Mozilla (which has its\ncodebase available as open source) and Ubisoft (fully closed-source). Inside\ntheir usual review environment, participants were given access to RevMate, an\nLLM-based assistive tool suggesting generated review comments using an\noff-the-shelf LLM with Retrieval Augmented Generation to provide extra code and\nreview context, combined with LLM-as-a-Judge, to auto-evaluate the generated\ncomments and discard irrelevant cases. Based on more than 587 patch reviews\nprovided by RevMate, we observed that 8.1% and 7.2%, respectively, of\nLLM-generated comments were accepted by reviewers in each organization, while\n14.6% and 20.5% other comments were still marked as valuable as review or\ndevelopment tips. Refactoring-related comments are more likely to be accepted\nthan Functional comments (18.2% and 18.6% compared to 4.8% and 5.2%). The extra\ntime spent by reviewers to inspect generated comments or edit accepted ones\n(36/119), yielding an overall median of 43s per patch, is reasonable. The\naccepted generated comments are as likely to yield future revisions of the\nrevised patch as human-written comments (74% vs 73% at chunk-level).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We conduct a large-scale empirical user study in a live setup to evaluate the\nacceptance of LLM-generated comments and their impact on the review process.\nThis user study was performed in two organizations, Mozilla (which has its\ncodebase available as open source) and Ubisoft (fully closed-source). Inside\ntheir usual review environment, participants were given access to RevMate, an\nLLM-based assistive tool suggesting generated review comments using an\noff-the-shelf LLM with Retrieval Augmented Generation to provide extra code and\nreview context, combined with LLM-as-a-Judge, to auto-evaluate the generated\ncomments and discard irrelevant cases. Based on more than 587 patch reviews\nprovided by RevMate, we observed that 8.1% and 7.2%, respectively, of\nLLM-generated comments were accepted by reviewers in each organization, while\n14.6% and 20.5% other comments were still marked as valuable as review or\ndevelopment tips. Refactoring-related comments are more likely to be accepted\nthan Functional comments (18.2% and 18.6% compared to 4.8% and 5.2%). The extra\ntime spent by reviewers to inspect generated comments or edit accepted ones\n(36/119), yielding an overall median of 43s per patch, is reasonable. The\naccepted generated comments are as likely to yield future revisions of the\nrevised patch as human-written comments (74% vs 73% at chunk-level)."
                },
                "authors": [
                    {
                        "name": "Doriane Olewicki"
                    },
                    {
                        "name": "Leuson Da Silva"
                    },
                    {
                        "name": "Suhaib Mujahid"
                    },
                    {
                        "name": "Arezou Amini"
                    },
                    {
                        "name": "Benjamin Mah"
                    },
                    {
                        "name": "Marco Castelluccio"
                    },
                    {
                        "name": "Sarra Habchi"
                    },
                    {
                        "name": "Foutse Khomh"
                    },
                    {
                        "name": "Bram Adams"
                    }
                ],
                "author_detail": {
                    "name": "Bram Adams"
                },
                "author": "Bram Adams",
                "arxiv_comment": "12pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07072v1",
                "updated": "2024-11-11T15:47:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    47,
                    25,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T15:47:25Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    47,
                    25,
                    0,
                    316,
                    0
                ],
                "title": "An Interpretable X-ray Style Transfer via Trainable Local Laplacian\n  Filter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Interpretable X-ray Style Transfer via Trainable Local Laplacian\n  Filter"
                },
                "summary": "Radiologists have preferred visual impressions or 'styles' of X-ray images\nthat are manually adjusted to their needs to support their diagnostic\nperformance. In this work, we propose an automatic and interpretable X-ray\nstyle transfer by introducing a trainable version of the Local Laplacian Filter\n(LLF). From the shape of the LLF's optimized remap function, the\ncharacteristics of the style transfer can be inferred and reliability of the\nalgorithm can be ensured. Moreover, we enable the LLF to capture complex X-ray\nstyle features by replacing the remap function with a Multi-Layer Perceptron\n(MLP) and adding a trainable normalization layer. We demonstrate the\neffectiveness of the proposed method by transforming unprocessed mammographic\nX-ray images into images that match the style of target mammograms and achieve\na Structural Similarity Index (SSIM) of 0.94 compared to 0.82 of the baseline\nLLF style transfer method from Aubry et al.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiologists have preferred visual impressions or 'styles' of X-ray images\nthat are manually adjusted to their needs to support their diagnostic\nperformance. In this work, we propose an automatic and interpretable X-ray\nstyle transfer by introducing a trainable version of the Local Laplacian Filter\n(LLF). From the shape of the LLF's optimized remap function, the\ncharacteristics of the style transfer can be inferred and reliability of the\nalgorithm can be ensured. Moreover, we enable the LLF to capture complex X-ray\nstyle features by replacing the remap function with a Multi-Layer Perceptron\n(MLP) and adding a trainable normalization layer. We demonstrate the\neffectiveness of the proposed method by transforming unprocessed mammographic\nX-ray images into images that match the style of target mammograms and achieve\na Structural Similarity Index (SSIM) of 0.94 compared to 0.82 of the baseline\nLLF style transfer method from Aubry et al."
                },
                "authors": [
                    {
                        "name": "Dominik Eckert"
                    },
                    {
                        "name": "Ludwig Ritschl"
                    },
                    {
                        "name": "Christopher Syben"
                    },
                    {
                        "name": "Christian Hmmer"
                    },
                    {
                        "name": "Julia Wicklein"
                    },
                    {
                        "name": "Marcel Beister"
                    },
                    {
                        "name": "Steffen Kappler"
                    },
                    {
                        "name": "Sebastian Stober"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Stober"
                },
                "author": "Sebastian Stober",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07071v1",
                "updated": "2024-11-11T15:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    47,
                    15,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T15:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    47,
                    15,
                    0,
                    316,
                    0
                ],
                "title": "Universal Response and Emergence of Induction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Response and Emergence of Induction in LLMs"
                },
                "summary": "While induction is considered a key mechanism for in-context learning in\nLLMs, understanding its precise circuit decomposition beyond toy models remains\nelusive. Here, we study the emergence of induction behavior within LLMs by\nprobing their response to weak single-token perturbations of the residual\nstream. We find that LLMs exhibit a robust, universal regime in which their\nresponse remains scale-invariant under changes in perturbation strength,\nthereby allowing us to quantify the build-up of token correlations throughout\nthe model. By applying our method, we observe signatures of induction behavior\nwithin the residual stream of Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL. Across\nall models, we find that these induction signatures gradually emerge within\nintermediate layers and identify the relevant model sections composing this\nbehavior. Our results provide insights into the collective interplay of\ncomponents within LLMs and serve as a benchmark for large-scale circuit\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While induction is considered a key mechanism for in-context learning in\nLLMs, understanding its precise circuit decomposition beyond toy models remains\nelusive. Here, we study the emergence of induction behavior within LLMs by\nprobing their response to weak single-token perturbations of the residual\nstream. We find that LLMs exhibit a robust, universal regime in which their\nresponse remains scale-invariant under changes in perturbation strength,\nthereby allowing us to quantify the build-up of token correlations throughout\nthe model. By applying our method, we observe signatures of induction behavior\nwithin the residual stream of Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL. Across\nall models, we find that these induction signatures gradually emerge within\nintermediate layers and identify the relevant model sections composing this\nbehavior. Our results provide insights into the collective interplay of\ncomponents within LLMs and serve as a benchmark for large-scale circuit\nanalysis."
                },
                "authors": [
                    {
                        "name": "Niclas Luick"
                    }
                ],
                "author_detail": {
                    "name": "Niclas Luick"
                },
                "author": "Niclas Luick",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07070v1",
                "updated": "2024-11-11T15:46:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    46,
                    7,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T15:46:07Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    46,
                    7,
                    0,
                    316,
                    0
                ],
                "title": "On Active Privacy Auditing in Supervised Fine-tuning for White-Box\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Active Privacy Auditing in Supervised Fine-tuning for White-Box\n  Language Models"
                },
                "summary": "The pretraining and fine-tuning approach has become the leading technique for\nvarious NLP applications. However, recent studies reveal that fine-tuning data,\ndue to their sensitive nature, domain-specific characteristics, and\nidentifiability, pose significant privacy concerns. To help develop more\nprivacy-resilient fine-tuning models, we introduce a novel active privacy\nauditing framework, dubbed Parsing, designed to identify and quantify privacy\nleakage risks during the supervised fine-tuning (SFT) of language models (LMs).\nThe framework leverages improved white-box membership inference attacks (MIAs)\nas the core technology, utilizing novel learning objectives and a two-stage\npipeline to monitor the privacy of the LMs' fine-tuning process, maximizing the\nexposure of privacy risks. Additionally, we have improved the effectiveness of\nMIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our\nresearch aims to provide the SFT community of LMs with a reliable, ready-to-use\nprivacy auditing tool, and to offer valuable insights into safeguarding privacy\nduring the fine-tuning process. Experimental results confirm the framework's\nefficiency across various models and tasks, emphasizing notable privacy\nconcerns in the fine-tuning process. Project code available for\nhttps://github.com/mapleleavesss/PARSING.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pretraining and fine-tuning approach has become the leading technique for\nvarious NLP applications. However, recent studies reveal that fine-tuning data,\ndue to their sensitive nature, domain-specific characteristics, and\nidentifiability, pose significant privacy concerns. To help develop more\nprivacy-resilient fine-tuning models, we introduce a novel active privacy\nauditing framework, dubbed Parsing, designed to identify and quantify privacy\nleakage risks during the supervised fine-tuning (SFT) of language models (LMs).\nThe framework leverages improved white-box membership inference attacks (MIAs)\nas the core technology, utilizing novel learning objectives and a two-stage\npipeline to monitor the privacy of the LMs' fine-tuning process, maximizing the\nexposure of privacy risks. Additionally, we have improved the effectiveness of\nMIAs on large LMs including GPT-2, Llama2, and certain variants of them. Our\nresearch aims to provide the SFT community of LMs with a reliable, ready-to-use\nprivacy auditing tool, and to offer valuable insights into safeguarding privacy\nduring the fine-tuning process. Experimental results confirm the framework's\nefficiency across various models and tasks, emphasizing notable privacy\nconcerns in the fine-tuning process. Project code available for\nhttps://github.com/mapleleavesss/PARSING."
                },
                "authors": [
                    {
                        "name": "Qian Sun"
                    },
                    {
                        "name": "Hanpeng Wu"
                    },
                    {
                        "name": "Xi Sheryl Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xi Sheryl Zhang"
                },
                "author": "Xi Sheryl Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10825v2",
                "updated": "2024-11-11T15:45:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    45,
                    2,
                    0,
                    316,
                    0
                ],
                "published": "2024-01-19T17:21:05Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    17,
                    21,
                    5,
                    4,
                    19,
                    0
                ],
                "title": "Recent Advances in Named Entity Recognition: A Comprehensive Survey and\n  Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Named Entity Recognition: A Comprehensive Survey and\n  Comparative Study"
                },
                "summary": "Named Entity Recognition seeks to extract substrings within a text that name\nreal-world objects and to determine their type (for example, whether they refer\nto persons or organizations). In this survey, we first present an overview of\nrecent popular approaches, including advancements in Transformer-based methods\nand Large Language Models (LLMs) that have not had much coverage in other\nsurveys. In addition, we discuss reinforcement learning and graph-based\napproaches, highlighting their role in enhancing NER performance. Second, we\nfocus on methods designed for datasets with scarce annotations. Third, we\nevaluate the performance of the main NER implementations on a variety of\ndatasets with differing characteristics (as regards their domain, their size,\nand their number of classes). We thus provide a deep comparison of algorithms\nthat have never been considered together. Our experiments shed some light on\nhow the characteristics of datasets affect the behavior of the methods we\ncompare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition seeks to extract substrings within a text that name\nreal-world objects and to determine their type (for example, whether they refer\nto persons or organizations). In this survey, we first present an overview of\nrecent popular approaches, including advancements in Transformer-based methods\nand Large Language Models (LLMs) that have not had much coverage in other\nsurveys. In addition, we discuss reinforcement learning and graph-based\napproaches, highlighting their role in enhancing NER performance. Second, we\nfocus on methods designed for datasets with scarce annotations. Third, we\nevaluate the performance of the main NER implementations on a variety of\ndatasets with differing characteristics (as regards their domain, their size,\nand their number of classes). We thus provide a deep comparison of algorithms\nthat have never been considered together. Our experiments shed some light on\nhow the characteristics of datasets affect the behavior of the methods we\ncompare."
                },
                "authors": [
                    {
                        "name": "Imed Keraghel"
                    },
                    {
                        "name": "Stanislas Morbieu"
                    },
                    {
                        "name": "Mohamed Nadif"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Nadif"
                },
                "author": "Mohamed Nadif",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68Q32",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07066v1",
                "updated": "2024-11-11T15:30:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    30,
                    16,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T15:30:16Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    30,
                    16,
                    0,
                    316,
                    0
                ],
                "title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training"
                },
                "summary": "Network pruning is a set of computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has focused on pruning and re-training, which\nnowadays is inconvenient due to the vast amount of pre-trained models, which\nare in any case too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAl}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs, that\nmodifies the block-wise and row-wise sparsity ratios to maximize the\n\\emph{neuron alignment} among activations. Moreover, differently from existing\nmethods, our approach adaptively selects the best parameters for the block-wise\nand row-wise sparsity ratios w.r.t. to the model and the desired sparsity\n(given as input), and requires \\emph{no re-training}. We test our method on 4\ndifferent LLM families and 3 different sparsity ratios, showing how it\nconsistently outperforms the latest state-of-the-art techniques. The code is\navailable at https://github.com/eliacunegatti/NeuroAL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network pruning is a set of computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has focused on pruning and re-training, which\nnowadays is inconvenient due to the vast amount of pre-trained models, which\nare in any case too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAl}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs, that\nmodifies the block-wise and row-wise sparsity ratios to maximize the\n\\emph{neuron alignment} among activations. Moreover, differently from existing\nmethods, our approach adaptively selects the best parameters for the block-wise\nand row-wise sparsity ratios w.r.t. to the model and the desired sparsity\n(given as input), and requires \\emph{no re-training}. We test our method on 4\ndifferent LLM families and 3 different sparsity ratios, showing how it\nconsistently outperforms the latest state-of-the-art techniques. The code is\navailable at https://github.com/eliacunegatti/NeuroAL."
                },
                "authors": [
                    {
                        "name": "Elia Cunegatti"
                    },
                    {
                        "name": "Leonardo Lucio Custode"
                    },
                    {
                        "name": "Giovanni Iacca"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Iacca"
                },
                "author": "Giovanni Iacca",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15225v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15225v3",
                "updated": "2024-11-11T15:27:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    27,
                    46,
                    0,
                    316,
                    0
                ],
                "published": "2023-12-23T11:14:33Z",
                "published_parsed": [
                    2023,
                    12,
                    23,
                    11,
                    14,
                    33,
                    5,
                    357,
                    0
                ],
                "title": "Statistical Inference with Limited Memory: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical Inference with Limited Memory: A Survey"
                },
                "summary": "The problem of statistical inference in its various forms has been the\nsubject of decades-long extensive research. Most of the effort has been focused\non characterizing the behavior as a function of the number of available\nsamples, with far less attention given to the effect of memory limitations on\nperformance. Recently, this latter topic has drawn much interest in the\nengineering and computer science literature. In this survey paper, we attempt\nto review the state-of-the-art of statistical inference under memory\nconstraints in several canonical problems, including hypothesis testing,\nparameter estimation, and distribution property testing/estimation. We discuss\nthe main results in this developing field, and by identifying recurrent themes,\nwe extract some fundamental building blocks for algorithmic construction, as\nwell as useful techniques for lower bound derivations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The problem of statistical inference in its various forms has been the\nsubject of decades-long extensive research. Most of the effort has been focused\non characterizing the behavior as a function of the number of available\nsamples, with far less attention given to the effect of memory limitations on\nperformance. Recently, this latter topic has drawn much interest in the\nengineering and computer science literature. In this survey paper, we attempt\nto review the state-of-the-art of statistical inference under memory\nconstraints in several canonical problems, including hypothesis testing,\nparameter estimation, and distribution property testing/estimation. We discuss\nthe main results in this developing field, and by identifying recurrent themes,\nwe extract some fundamental building blocks for algorithmic construction, as\nwell as useful techniques for lower bound derivations."
                },
                "authors": [
                    {
                        "name": "Tomer Berg"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Ofer Shayevitz"
                    }
                ],
                "author_detail": {
                    "name": "Ofer Shayevitz"
                },
                "author": "Ofer Shayevitz",
                "arxiv_comment": "Published in JSAIT Special Issue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15225v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15225v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07037v1",
                "updated": "2024-11-11T14:43:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    43,
                    51,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T14:43:51Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    43,
                    51,
                    0,
                    316,
                    0
                ],
                "title": "LIFBench: Evaluating the Instruction Following Performance and Stability\n  of Large Language Models in Long-Context Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIFBench: Evaluating the Instruction Following Performance and Stability\n  of Large Language Models in Long-Context Scenarios"
                },
                "summary": "As Large Language Models (LLMs) continue to advance in natural language\nprocessing (NLP), their ability to stably follow instructions in long-context\ninputs has become crucial for real-world applications. While existing\nbenchmarks assess various LLM capabilities, they rarely focus on\ninstruction-following in long-context scenarios or stability on different\ninputs. In response, we introduce the Long-context Instruction-Following\nBenchmark (LIFBench), a scalable dataset designed to evaluate LLMs'\ninstruction-following capabilities and stability across long contexts. LIFBench\ncomprises three long-context scenarios and eleven diverse tasks, supported by\n2,766 instructions generated through an automated expansion method across three\ndimensions: length, expression, and variables. For evaluation, we propose\nLIFEval, a rubric-based assessment framework that provides precise, automated\nscoring of complex LLM responses without relying on LLM-assisted evaluations or\nhuman judgments. This approach facilitates a comprehensive analysis of model\nperformance and stability across various perspectives. We conduct extensive\nexperiments on 20 notable LLMs across six length intervals, analyzing their\ninstruction-following capabilities and stability. Our work contributes LIFBench\nand LIFEval as robust tools for assessing LLM performance in complex,\nlong-context settings, providing insights that can inform future LLM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to advance in natural language\nprocessing (NLP), their ability to stably follow instructions in long-context\ninputs has become crucial for real-world applications. While existing\nbenchmarks assess various LLM capabilities, they rarely focus on\ninstruction-following in long-context scenarios or stability on different\ninputs. In response, we introduce the Long-context Instruction-Following\nBenchmark (LIFBench), a scalable dataset designed to evaluate LLMs'\ninstruction-following capabilities and stability across long contexts. LIFBench\ncomprises three long-context scenarios and eleven diverse tasks, supported by\n2,766 instructions generated through an automated expansion method across three\ndimensions: length, expression, and variables. For evaluation, we propose\nLIFEval, a rubric-based assessment framework that provides precise, automated\nscoring of complex LLM responses without relying on LLM-assisted evaluations or\nhuman judgments. This approach facilitates a comprehensive analysis of model\nperformance and stability across various perspectives. We conduct extensive\nexperiments on 20 notable LLMs across six length intervals, analyzing their\ninstruction-following capabilities and stability. Our work contributes LIFBench\nand LIFEval as robust tools for assessing LLM performance in complex,\nlong-context settings, providing insights that can inform future LLM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Xiaodong Wu"
                    },
                    {
                        "name": "Minhao Wang"
                    },
                    {
                        "name": "Yichen Liu"
                    },
                    {
                        "name": "Xiaoming Shi"
                    },
                    {
                        "name": "He Yan"
                    },
                    {
                        "name": "Xiangju Lu"
                    },
                    {
                        "name": "Junmin Zhu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "17 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07035v1",
                "updated": "2024-11-11T14:43:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    43,
                    25,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T14:43:25Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    43,
                    25,
                    0,
                    316,
                    0
                ],
                "title": "Inferring jet physics from neutron star - black hole mergers with\n  gravitational waves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring jet physics from neutron star - black hole mergers with\n  gravitational waves"
                },
                "summary": "Neutron star - black hole (NSBH) mergers that undergo tidal disruption may\nlaunch jets that could power a gamma-ray burst. We use a population of\nsimulated NSBH systems to measure jet parameters from the gravitational waves\nemitted by these systems. The conditions during the tidal disruption and merger\nphase required to power a gamma-ray burst are uncertain. It is likely that the\nsystem must achieve some minimum remnant baryonic mass after the merger before\na jet can be launched to power a gamma-ray burst. Assuming a fiducial neutron\nstar equation of state, we show how Bayesian hierarchical inference can be used\nto infer the minimum remnant mass required to launch a gamma-ray burst jet as\nwell as the maximum gamma-ray burst viewing angle to detect a gamma-ray burst.\nWe find that with 200 NSBH observations, we can measure the minimum disk mass\nto within 0.01 solar masses at 90% credibility. We simultaneously infer the\nmaximum gamma-ray burst viewing angle to within 13 degrees at 90% credibility.\nWe conclude that upcoming upgrades to the LIGO observatories may provide\nimportant new insights into the physics of NSBH jets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron star - black hole (NSBH) mergers that undergo tidal disruption may\nlaunch jets that could power a gamma-ray burst. We use a population of\nsimulated NSBH systems to measure jet parameters from the gravitational waves\nemitted by these systems. The conditions during the tidal disruption and merger\nphase required to power a gamma-ray burst are uncertain. It is likely that the\nsystem must achieve some minimum remnant baryonic mass after the merger before\na jet can be launched to power a gamma-ray burst. Assuming a fiducial neutron\nstar equation of state, we show how Bayesian hierarchical inference can be used\nto infer the minimum remnant mass required to launch a gamma-ray burst jet as\nwell as the maximum gamma-ray burst viewing angle to detect a gamma-ray burst.\nWe find that with 200 NSBH observations, we can measure the minimum disk mass\nto within 0.01 solar masses at 90% credibility. We simultaneously infer the\nmaximum gamma-ray burst viewing angle to within 13 degrees at 90% credibility.\nWe conclude that upcoming upgrades to the LIGO observatories may provide\nimportant new insights into the physics of NSBH jets."
                },
                "authors": [
                    {
                        "name": "Teagan A. Clarke"
                    },
                    {
                        "name": "Paul D. Lasky"
                    },
                    {
                        "name": "Eric Thrane"
                    }
                ],
                "author_detail": {
                    "name": "Eric Thrane"
                },
                "author": "Eric Thrane",
                "arxiv_comment": "9 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09824v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09824v4",
                "updated": "2024-11-11T14:41:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    41,
                    53,
                    0,
                    316,
                    0
                ],
                "published": "2024-10-13T12:57:08Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    12,
                    57,
                    8,
                    6,
                    287,
                    0
                ],
                "title": "Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent\n  Simulation"
                },
                "summary": "Graph generation is a fundamental task that has been extensively studied in\nsocial, technological, and scientific analysis. For modeling the dynamic graph\nevolution process, traditional rule-based methods struggle to capture community\nstructures within graphs, while deep learning methods only focus on fitting\ntraining graphs. This limits existing graph generators to producing graphs that\nadhere to predefined rules or closely resemble training datasets, achieving\npoor performance in dynamic graph generation. Given that graphs are abstract\nrepresentations arising from pairwise interactions in human activities, a\nrealistic simulation of human-wise interaction could provide deeper insights\ninto the graph evolution mechanism. With the increasing recognition of large\nlanguage models (LLMs) in simulating human behavior, we introduce\nGraphAgent-Generator (GAG), a novel simulation-based framework for dynamic\ngraph generation. Without training or fine-tuning process of LLM, our framework\neffectively replicates seven macro-level structural characteristics in\nestablished network science theories while surpassing existing baselines in\ngraph expansion tasks by 31\\% on specific evaluation metrics. Through node\nclassification task, we validate GAG effectively preserves characteristics of\nreal-world network for node-wise textual features in generated text-rich graph.\nFurthermore, by incorporating parallel acceleration, GAG supports generating\ngraphs with up to nearly 100,000 nodes or 10 million edges through large-scale\nLLM-based agent simulation, with a minimum speed-up of 90.4\\%. The source code\nis available at https://anonymous.4open.science/r/GraphAgent-2206.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph generation is a fundamental task that has been extensively studied in\nsocial, technological, and scientific analysis. For modeling the dynamic graph\nevolution process, traditional rule-based methods struggle to capture community\nstructures within graphs, while deep learning methods only focus on fitting\ntraining graphs. This limits existing graph generators to producing graphs that\nadhere to predefined rules or closely resemble training datasets, achieving\npoor performance in dynamic graph generation. Given that graphs are abstract\nrepresentations arising from pairwise interactions in human activities, a\nrealistic simulation of human-wise interaction could provide deeper insights\ninto the graph evolution mechanism. With the increasing recognition of large\nlanguage models (LLMs) in simulating human behavior, we introduce\nGraphAgent-Generator (GAG), a novel simulation-based framework for dynamic\ngraph generation. Without training or fine-tuning process of LLM, our framework\neffectively replicates seven macro-level structural characteristics in\nestablished network science theories while surpassing existing baselines in\ngraph expansion tasks by 31\\% on specific evaluation metrics. Through node\nclassification task, we validate GAG effectively preserves characteristics of\nreal-world network for node-wise textual features in generated text-rich graph.\nFurthermore, by incorporating parallel acceleration, GAG supports generating\ngraphs with up to nearly 100,000 nodes or 10 million edges through large-scale\nLLM-based agent simulation, with a minimum speed-up of 90.4\\%. The source code\nis available at https://anonymous.4open.science/r/GraphAgent-2206."
                },
                "authors": [
                    {
                        "name": "Jiarui Ji"
                    },
                    {
                        "name": "Runlin Lei"
                    },
                    {
                        "name": "Jialing Bi"
                    },
                    {
                        "name": "Zhewei Wei"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Ding"
                },
                "author": "Bolin Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09824v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09824v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07032v1",
                "updated": "2024-11-11T14:38:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    38,
                    40,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T14:38:40Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    38,
                    40,
                    0,
                    316,
                    0
                ],
                "title": "Scaling Long-Horizon Online POMDP Planning via Rapid State Space\n  Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Long-Horizon Online POMDP Planning via Rapid State Space\n  Sampling"
                },
                "summary": "Partially Observable Markov Decision Processes (POMDPs) are a general and\nprincipled framework for motion planning under uncertainty. Despite tremendous\nimprovement in the scalability of POMDP solvers, long-horizon POMDPs (e.g.,\n$\\geq15$ steps) remain difficult to solve. This paper proposes a new\napproximate online POMDP solver, called Reference-Based Online POMDP Planning\nvia Rapid State Space Sampling (ROP-RaS3). ROP-RaS3 uses novel extremely fast\nsampling-based motion planning techniques to sample the state space and\ngenerate a diverse set of macro actions online which are then used to bias\nbelief-space sampling and infer high-quality policies without requiring\nexhaustive enumeration of the action space -- a fundamental constraint for\nmodern online POMDP solvers. ROP-RaS3 is evaluated on various long-horizon\nPOMDPs, including on a problem with a planning horizon of more than 100 steps\nand a problem with a 15-dimensional state space that requires more than 20 look\nahead steps. In all of these problems, ROP-RaS3 substantially outperforms other\nstate-of-the-art methods by up to multiple folds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partially Observable Markov Decision Processes (POMDPs) are a general and\nprincipled framework for motion planning under uncertainty. Despite tremendous\nimprovement in the scalability of POMDP solvers, long-horizon POMDPs (e.g.,\n$\\geq15$ steps) remain difficult to solve. This paper proposes a new\napproximate online POMDP solver, called Reference-Based Online POMDP Planning\nvia Rapid State Space Sampling (ROP-RaS3). ROP-RaS3 uses novel extremely fast\nsampling-based motion planning techniques to sample the state space and\ngenerate a diverse set of macro actions online which are then used to bias\nbelief-space sampling and infer high-quality policies without requiring\nexhaustive enumeration of the action space -- a fundamental constraint for\nmodern online POMDP solvers. ROP-RaS3 is evaluated on various long-horizon\nPOMDPs, including on a problem with a planning horizon of more than 100 steps\nand a problem with a 15-dimensional state space that requires more than 20 look\nahead steps. In all of these problems, ROP-RaS3 substantially outperforms other\nstate-of-the-art methods by up to multiple folds."
                },
                "authors": [
                    {
                        "name": "Yuanchu Liang"
                    },
                    {
                        "name": "Edward Kim"
                    },
                    {
                        "name": "Wil Thomason"
                    },
                    {
                        "name": "Zachary Kingston"
                    },
                    {
                        "name": "Hanna Kurniawati"
                    },
                    {
                        "name": "Lydia E. Kavraki"
                    }
                ],
                "author_detail": {
                    "name": "Lydia E. Kavraki"
                },
                "author": "Lydia E. Kavraki",
                "arxiv_comment": "16 pages, 4 tables, 1 figure. To be presented at the International\n  Symposium of Robotics Research 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19266v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19266v4",
                "updated": "2024-11-11T14:36:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    36,
                    35,
                    0,
                    316,
                    0
                ],
                "published": "2024-05-29T16:59:38Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    16,
                    59,
                    38,
                    2,
                    150,
                    0
                ],
                "title": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for\n  Pediatric Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for\n  Pediatric Applications"
                },
                "summary": "Developing intelligent pediatric consultation systems offers promising\nprospects for improving diagnostic efficiency, especially in China, where\nhealthcare resources are scarce. Despite recent advances in Large Language\nModels (LLMs) for Chinese medicine, their performance is sub-optimal in\npediatric applications due to inadequate instruction data and vulnerable\ntraining procedures. To address the above issues, this paper builds PedCorpus,\na high-quality dataset of over 300,000 multi-task instructions from pediatric\ntextbooks, guidelines, and knowledge graph resources to fulfil diverse\ndiagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the\nfirst Chinese pediatric LLM assistant built on a systematic and robust training\npipeline. In the continuous pre-training phase, we introduce a hybrid\ninstruction pre-training mechanism to mitigate the internal-injected knowledge\ninconsistency of LLMs for medical domain adaptation. Immediately, the\nfull-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the\ngeneral medical knowledge schema into the models. After that, we devise a\ndirect following preference optimization to enhance the generation of\npediatrician-like humanistic responses. In the parameter-efficient secondary\nSFT phase, a mixture of universal-specific experts strategy is presented to\nresolve the competency conflict between medical generalist and pediatric\nexpertise mastery. Extensive results based on the metrics, GPT-4, and doctor\nevaluations on distinct doctor downstream tasks show that PediatricsGPT\nconsistently outperforms previous Chinese medical LLMs. Our model and dataset\nwill be open-source for community development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing intelligent pediatric consultation systems offers promising\nprospects for improving diagnostic efficiency, especially in China, where\nhealthcare resources are scarce. Despite recent advances in Large Language\nModels (LLMs) for Chinese medicine, their performance is sub-optimal in\npediatric applications due to inadequate instruction data and vulnerable\ntraining procedures. To address the above issues, this paper builds PedCorpus,\na high-quality dataset of over 300,000 multi-task instructions from pediatric\ntextbooks, guidelines, and knowledge graph resources to fulfil diverse\ndiagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the\nfirst Chinese pediatric LLM assistant built on a systematic and robust training\npipeline. In the continuous pre-training phase, we introduce a hybrid\ninstruction pre-training mechanism to mitigate the internal-injected knowledge\ninconsistency of LLMs for medical domain adaptation. Immediately, the\nfull-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the\ngeneral medical knowledge schema into the models. After that, we devise a\ndirect following preference optimization to enhance the generation of\npediatrician-like humanistic responses. In the parameter-efficient secondary\nSFT phase, a mixture of universal-specific experts strategy is presented to\nresolve the competency conflict between medical generalist and pediatric\nexpertise mastery. Extensive results based on the metrics, GPT-4, and doctor\nevaluations on distinct doctor downstream tasks show that PediatricsGPT\nconsistently outperforms previous Chinese medical LLMs. Our model and dataset\nwill be open-source for community development."
                },
                "authors": [
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Jinjie Wei"
                    },
                    {
                        "name": "Dongling Xiao"
                    },
                    {
                        "name": "Shunli Wang"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Shuaibing Wang"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Yue Jiang"
                    },
                    {
                        "name": "Qingyao Xu"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Peng Zhai"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2024. A Technical Report on a Chinese Medical\n  Large Language Model",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19266v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19266v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08877v2",
                "updated": "2024-11-11T14:35:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    35,
                    45,
                    0,
                    316,
                    0
                ],
                "published": "2024-04-13T02:36:40Z",
                "published_parsed": [
                    2024,
                    4,
                    13,
                    2,
                    36,
                    40,
                    5,
                    104,
                    0
                ],
                "title": "Aligning LLMs for FL-free Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning LLMs for FL-free Program Repair"
                },
                "summary": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR."
                },
                "authors": [
                    {
                        "name": "Junjielong Xu"
                    },
                    {
                        "name": "Ying Fu"
                    },
                    {
                        "name": "Shin Hwei Tan"
                    },
                    {
                        "name": "Pinjia He"
                    }
                ],
                "author_detail": {
                    "name": "Pinjia He"
                },
                "author": "Pinjia He",
                "arxiv_comment": "Accepted by ICSE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07023v1",
                "updated": "2024-11-11T14:29:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    29,
                    59,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T14:29:59Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    29,
                    59,
                    0,
                    316,
                    0
                ],
                "title": "The Inherent Adversarial Robustness of Analog In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Inherent Adversarial Robustness of Analog In-Memory Computing"
                },
                "summary": "A key challenge for Deep Neural Network (DNN) algorithms is their\nvulnerability to adversarial attacks. Inherently non-deterministic compute\nsubstrates, such as those based on Analog In-Memory Computing (AIMC), have been\nspeculated to provide significant adversarial robustness when performing DNN\ninference. In this paper, we experimentally validate this conjecture for the\nfirst time on an AIMC chip based on Phase Change Memory (PCM) devices. We\ndemonstrate higher adversarial robustness against different types of\nadversarial attacks when implementing an image classification network.\nAdditional robustness is also observed when performing hardware-in-the-loop\nattacks, for which the attacker is assumed to have full access to the hardware.\nA careful study of the various noise sources indicate that a combination of\nstochastic noise sources (both recurrent and non-recurrent) are responsible for\nthe adversarial robustness and that their type and magnitude disproportionately\neffects this property. Finally, it is demonstrated, via simulations, that when\na much larger transformer network is used to implement a Natural Language\nProcessing (NLP) task, additional robustness is still observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge for Deep Neural Network (DNN) algorithms is their\nvulnerability to adversarial attacks. Inherently non-deterministic compute\nsubstrates, such as those based on Analog In-Memory Computing (AIMC), have been\nspeculated to provide significant adversarial robustness when performing DNN\ninference. In this paper, we experimentally validate this conjecture for the\nfirst time on an AIMC chip based on Phase Change Memory (PCM) devices. We\ndemonstrate higher adversarial robustness against different types of\nadversarial attacks when implementing an image classification network.\nAdditional robustness is also observed when performing hardware-in-the-loop\nattacks, for which the attacker is assumed to have full access to the hardware.\nA careful study of the various noise sources indicate that a combination of\nstochastic noise sources (both recurrent and non-recurrent) are responsible for\nthe adversarial robustness and that their type and magnitude disproportionately\neffects this property. Finally, it is demonstrated, via simulations, that when\na much larger transformer network is used to implement a Natural Language\nProcessing (NLP) task, additional robustness is still observed."
                },
                "authors": [
                    {
                        "name": "Corey Lammie"
                    },
                    {
                        "name": "Julian Bchel"
                    },
                    {
                        "name": "Athanasios Vasilopoulos"
                    },
                    {
                        "name": "Manuel Le Gallo"
                    },
                    {
                        "name": "Abu Sebastian"
                    }
                ],
                "author_detail": {
                    "name": "Abu Sebastian"
                },
                "author": "Abu Sebastian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07021v1",
                "updated": "2024-11-11T14:25:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    25,
                    37,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T14:25:37Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    25,
                    37,
                    0,
                    316,
                    0
                ],
                "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) has shown impressive capability in\nproviding reliable answer predictions and addressing hallucination problems. A\ntypical RAG implementation uses powerful retrieval models to extract external\ninformation and large language models (LLMs) to generate answers. In contrast,\nrecent LLM-based retrieval has gained attention for its substantial\nimprovements in information retrieval (IR) due to the LLMs' semantic\nunderstanding capability. However, directly applying LLM to RAG systems\npresents challenges. This may cause feature locality problems as massive\nparametric knowledge can hinder effective usage of global information across\nthe corpus; for example, an LLM-based retriever often inputs document summaries\ninstead of full documents. Moreover, various pre-trained tasks in LLMs\nintroduce variance, further weakening performance as a retriever.\n  To address these issues, we propose a novel two-stage fine-tuning\narchitecture called Invar-RAG. In the retrieval stage, an LLM-based retriever\nis constructed by integrating LoRA-based representation learning to tackle\nfeature locality issues. To enhance retrieval performance, we develop two\npatterns (invariant and variant patterns) and an invariance loss to reduce LLM\nvariance. In the generation stage, a refined fine-tuning method is employed to\nimprove LLM accuracy in generating answers based on retrieved information.\nExperimental results show that Invar-RAG significantly outperforms existing\nbaselines across three open-domain question answering (ODQA) datasets. Code is\navailable in the Supplementary Material for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has shown impressive capability in\nproviding reliable answer predictions and addressing hallucination problems. A\ntypical RAG implementation uses powerful retrieval models to extract external\ninformation and large language models (LLMs) to generate answers. In contrast,\nrecent LLM-based retrieval has gained attention for its substantial\nimprovements in information retrieval (IR) due to the LLMs' semantic\nunderstanding capability. However, directly applying LLM to RAG systems\npresents challenges. This may cause feature locality problems as massive\nparametric knowledge can hinder effective usage of global information across\nthe corpus; for example, an LLM-based retriever often inputs document summaries\ninstead of full documents. Moreover, various pre-trained tasks in LLMs\nintroduce variance, further weakening performance as a retriever.\n  To address these issues, we propose a novel two-stage fine-tuning\narchitecture called Invar-RAG. In the retrieval stage, an LLM-based retriever\nis constructed by integrating LoRA-based representation learning to tackle\nfeature locality issues. To enhance retrieval performance, we develop two\npatterns (invariant and variant patterns) and an invariance loss to reduce LLM\nvariance. In the generation stage, a refined fine-tuning method is employed to\nimprove LLM accuracy in generating answers based on retrieved information.\nExperimental results show that Invar-RAG significantly outperforms existing\nbaselines across three open-domain question answering (ODQA) datasets. Code is\navailable in the Supplementary Material for reproducibility."
                },
                "authors": [
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Qian Li"
                    },
                    {
                        "name": "Jianghua Wu"
                    },
                    {
                        "name": "Guangxu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Guangxu Zhu"
                },
                "author": "Guangxu Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07006v1",
                "updated": "2024-11-11T14:05:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    5,
                    39,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T14:05:39Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    5,
                    39,
                    0,
                    316,
                    0
                ],
                "title": "Estimating Causal Effects in Partially Directed Parametric Causal Factor\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Causal Effects in Partially Directed Parametric Causal Factor\n  Graphs"
                },
                "summary": "Lifting uses a representative of indistinguishable individuals to exploit\nsymmetries in probabilistic relational models, denoted as parametric factor\ngraphs, to speed up inference while maintaining exact answers. In this paper,\nwe show how lifting can be applied to causal inference in partially directed\ngraphs, i.e., graphs that contain both directed and undirected edges to\nrepresent causal relationships between random variables. We present partially\ndirected parametric causal factor graphs (PPCFGs) as a generalisation of\npreviously introduced parametric causal factor graphs, which require a fully\ndirected graph. We further show how causal inference can be performed on a\nlifted level in PPCFGs, thereby extending the applicability of lifted causal\ninference to a broader range of models requiring less prior knowledge about\ncausal relationships.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifting uses a representative of indistinguishable individuals to exploit\nsymmetries in probabilistic relational models, denoted as parametric factor\ngraphs, to speed up inference while maintaining exact answers. In this paper,\nwe show how lifting can be applied to causal inference in partially directed\ngraphs, i.e., graphs that contain both directed and undirected edges to\nrepresent causal relationships between random variables. We present partially\ndirected parametric causal factor graphs (PPCFGs) as a generalisation of\npreviously introduced parametric causal factor graphs, which require a fully\ndirected graph. We further show how causal inference can be performed on a\nlifted level in PPCFGs, thereby extending the applicability of lifted causal\ninference to a broader range of models requiring less prior knowledge about\ncausal relationships."
                },
                "authors": [
                    {
                        "name": "Malte Luttermann"
                    },
                    {
                        "name": "Tanya Braun"
                    },
                    {
                        "name": "Ralf Mller"
                    },
                    {
                        "name": "Marcel Gehrke"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Gehrke"
                },
                "author": "Marcel Gehrke",
                "arxiv_comment": "Accepted to the Proceedings of the 16th International Conference on\n  Scalable Uncertainty Management (SUM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07003v1",
                "updated": "2024-11-11T14:01:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    1,
                    15,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T14:01:15Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    1,
                    15,
                    0,
                    316,
                    0
                ],
                "title": "Enhancing Robot Assistive Behaviour with Reinforcement Learning and\n  Theory of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Robot Assistive Behaviour with Reinforcement Learning and\n  Theory of Mind"
                },
                "summary": "The adaptation to users' preferences and the ability to infer and interpret\nhumans' beliefs and intents, which is known as the Theory of Mind (ToM), are\ntwo crucial aspects for achieving effective human-robot collaboration. Despite\nits importance, very few studies have investigated the impact of adaptive\nrobots with ToM abilities. In this work, we present an exploratory comparative\nstudy to investigate how social robots equipped with ToM abilities impact\nusers' performance and perception. We design a two-layer architecture. The\nQ-learning agent on the first layer learns the robot's higher-level behaviour.\nOn the second layer, a heuristic-based ToM infers the user's intended strategy\nand is responsible for implementing the robot's assistance, as well as\nproviding the motivation behind its choice. We conducted a user study in a\nreal-world setting, involving 56 participants who interacted with either an\nadaptive robot capable of ToM, or with a robot lacking such abilities. Our\nfindings suggest that participants in the ToM condition performed better,\naccepted the robot's assistance more often, and perceived its ability to adapt,\npredict and recognise their intents to a higher degree. Our preliminary\ninsights could inform future research and pave the way for designing more\ncomplex computation architectures for adaptive behaviour with ToM capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adaptation to users' preferences and the ability to infer and interpret\nhumans' beliefs and intents, which is known as the Theory of Mind (ToM), are\ntwo crucial aspects for achieving effective human-robot collaboration. Despite\nits importance, very few studies have investigated the impact of adaptive\nrobots with ToM abilities. In this work, we present an exploratory comparative\nstudy to investigate how social robots equipped with ToM abilities impact\nusers' performance and perception. We design a two-layer architecture. The\nQ-learning agent on the first layer learns the robot's higher-level behaviour.\nOn the second layer, a heuristic-based ToM infers the user's intended strategy\nand is responsible for implementing the robot's assistance, as well as\nproviding the motivation behind its choice. We conducted a user study in a\nreal-world setting, involving 56 participants who interacted with either an\nadaptive robot capable of ToM, or with a robot lacking such abilities. Our\nfindings suggest that participants in the ToM condition performed better,\naccepted the robot's assistance more often, and perceived its ability to adapt,\npredict and recognise their intents to a higher degree. Our preliminary\ninsights could inform future research and pave the way for designing more\ncomplex computation architectures for adaptive behaviour with ToM capabilities."
                },
                "authors": [
                    {
                        "name": "Antonio Andriella"
                    },
                    {
                        "name": "Giovanni Falcone"
                    },
                    {
                        "name": "Silvia Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Silvia Rossi"
                },
                "author": "Silvia Rossi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16209v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16209v2",
                "updated": "2024-11-11T13:56:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    13,
                    56,
                    30,
                    0,
                    316,
                    0
                ],
                "published": "2024-09-24T16:09:29Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    9,
                    29,
                    1,
                    268,
                    0
                ],
                "title": "LLMCount: Enhancing Stationary mmWave Detection with Multimodal-LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMCount: Enhancing Stationary mmWave Detection with Multimodal-LLM"
                },
                "summary": "Millimeter wave sensing provides people with the capability of sensing the\nsurrounding crowds in a non-invasive and privacy-preserving manner, which holds\nhuge application potential. However, detecting stationary crowds remains\nchallenging due to several factors such as minimal movements (like breathing or\ncasual fidgets), which can be easily treated as noise clusters during data\ncollection and consequently filtered in the following processing procedures.\nAdditionally, the uneven distribution of signal power due to signal power\nattenuation and interferences resulting from external reflectors or absorbers\nfurther complicates accurate detection. To address these challenges and enable\nstationary crowd detection across various application scenarios requiring\nspecialized domain adaption, we introduce LLMCount, the first system to harness\nthe capabilities of large-language models (LLMs) to enhance crowd detection\nperformance. By exploiting the decision-making capability of LLM, we can\nsuccessfully compensate the signal power to acquire a uniform distribution and\nthereby achieve a detection with higher accuracy. To assess the system's\nperformance, comprehensive evaluations are conducted under diversified\nscenarios like hall, meeting room, and cinema. The evaluation results show that\nour proposed approach reaches high detection accuracy with lower overall\nlatency compared with previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Millimeter wave sensing provides people with the capability of sensing the\nsurrounding crowds in a non-invasive and privacy-preserving manner, which holds\nhuge application potential. However, detecting stationary crowds remains\nchallenging due to several factors such as minimal movements (like breathing or\ncasual fidgets), which can be easily treated as noise clusters during data\ncollection and consequently filtered in the following processing procedures.\nAdditionally, the uneven distribution of signal power due to signal power\nattenuation and interferences resulting from external reflectors or absorbers\nfurther complicates accurate detection. To address these challenges and enable\nstationary crowd detection across various application scenarios requiring\nspecialized domain adaption, we introduce LLMCount, the first system to harness\nthe capabilities of large-language models (LLMs) to enhance crowd detection\nperformance. By exploiting the decision-making capability of LLM, we can\nsuccessfully compensate the signal power to acquire a uniform distribution and\nthereby achieve a detection with higher accuracy. To assess the system's\nperformance, comprehensive evaluations are conducted under diversified\nscenarios like hall, meeting room, and cinema. The evaluation results show that\nour proposed approach reaches high detection accuracy with lower overall\nlatency compared with previous methods."
                },
                "authors": [
                    {
                        "name": "Boyan Li"
                    },
                    {
                        "name": "Shengyi Ding"
                    },
                    {
                        "name": "Deen Ma"
                    },
                    {
                        "name": "Yixuan Wu"
                    },
                    {
                        "name": "Hongjie Liao"
                    },
                    {
                        "name": "Kaiyuan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyuan Hu"
                },
                "author": "Kaiyuan Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16209v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16209v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06959v1",
                "updated": "2024-11-11T13:05:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    13,
                    5,
                    39,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T13:05:39Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    13,
                    5,
                    39,
                    0,
                    316,
                    0
                ],
                "title": "ENAT: Rethinking Spatial-temporal Interactions in Token-based Image\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ENAT: Rethinking Spatial-temporal Interactions in Token-based Image\n  Synthesis"
                },
                "summary": "Recently, token-based generation have demonstrated their effectiveness in\nimage synthesis. As a representative example, non-autoregressive Transformers\n(NATs) can generate decent-quality images in a few steps. NATs perform\ngeneration in a progressive manner, where the latent tokens of a resulting\nimage are incrementally revealed. At each step, the unrevealed image regions\nare padded with mask tokens and inferred by NAT. In this paper, we delve into\nthe mechanisms behind the effectiveness of NATs and uncover two important\npatterns that naturally emerge from NATs: Spatially (within a step), although\nmask and visible tokens are processed uniformly by NATs, the interactions\nbetween them are highly asymmetric. In specific, mask tokens mainly gather\ninformation for decoding, while visible tokens tend to primarily provide\ninformation, and their deep representations can be built only upon themselves.\nTemporally (across steps), the interactions between adjacent generation steps\nmostly concentrate on updating the representations of a few critical tokens,\nwhile the computation for the majority of tokens is generally repetitive.\nDriven by these findings, we propose EfficientNAT (ENAT), a NAT model that\nexplicitly encourages these critical interactions inherent in NATs. At the\nspatial level, we disentangle the computations of visible and mask tokens by\nencoding visible tokens independently, while decoding mask tokens conditioned\non the fully encoded visible tokens. At the temporal level, we prioritize the\ncomputation of the critical tokens at each step, while maximally reusing\npreviously computed token representations to supplement necessary information.\nENAT improves the performance of NATs notably with significantly reduced\ncomputational cost. Experiments on ImageNet-256, ImageNet-512 and MS-COCO\nvalidate the effectiveness of ENAT. Code is available at\nhttps://github.com/LeapLabTHU/ENAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, token-based generation have demonstrated their effectiveness in\nimage synthesis. As a representative example, non-autoregressive Transformers\n(NATs) can generate decent-quality images in a few steps. NATs perform\ngeneration in a progressive manner, where the latent tokens of a resulting\nimage are incrementally revealed. At each step, the unrevealed image regions\nare padded with mask tokens and inferred by NAT. In this paper, we delve into\nthe mechanisms behind the effectiveness of NATs and uncover two important\npatterns that naturally emerge from NATs: Spatially (within a step), although\nmask and visible tokens are processed uniformly by NATs, the interactions\nbetween them are highly asymmetric. In specific, mask tokens mainly gather\ninformation for decoding, while visible tokens tend to primarily provide\ninformation, and their deep representations can be built only upon themselves.\nTemporally (across steps), the interactions between adjacent generation steps\nmostly concentrate on updating the representations of a few critical tokens,\nwhile the computation for the majority of tokens is generally repetitive.\nDriven by these findings, we propose EfficientNAT (ENAT), a NAT model that\nexplicitly encourages these critical interactions inherent in NATs. At the\nspatial level, we disentangle the computations of visible and mask tokens by\nencoding visible tokens independently, while decoding mask tokens conditioned\non the fully encoded visible tokens. At the temporal level, we prioritize the\ncomputation of the critical tokens at each step, while maximally reusing\npreviously computed token representations to supplement necessary information.\nENAT improves the performance of NATs notably with significantly reduced\ncomputational cost. Experiments on ImageNet-256, ImageNet-512 and MS-COCO\nvalidate the effectiveness of ENAT. Code is available at\nhttps://github.com/LeapLabTHU/ENAT."
                },
                "authors": [
                    {
                        "name": "Zanlin Ni"
                    },
                    {
                        "name": "Yulin Wang"
                    },
                    {
                        "name": "Renping Zhou"
                    },
                    {
                        "name": "Yizeng Han"
                    },
                    {
                        "name": "Jiayi Guo"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Gao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Gao Huang"
                },
                "author": "Gao Huang",
                "arxiv_comment": "Accepted by NeurIPS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06950v1",
                "updated": "2024-11-11T12:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    56,
                    52,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T12:56:52Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    56,
                    52,
                    0,
                    316,
                    0
                ],
                "title": "Sniff AI: Is My 'Spicy' Your 'Spicy'? Exploring LLM's Perceptual\n  Alignment with Human Smell Experiences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sniff AI: Is My 'Spicy' Your 'Spicy'? Exploring LLM's Perceptual\n  Alignment with Human Smell Experiences"
                },
                "summary": "Aligning AI with human intent is important, yet perceptual alignment-how AI\ninterprets what we see, hear, or smell-remains underexplored. This work focuses\non olfaction, human smell experiences. We conducted a user study with 40\nparticipants to investigate how well AI can interpret human descriptions of\nscents. Participants performed \"sniff and describe\" interactive tasks, with our\ndesigned AI system attempting to guess what scent the participants were\nexperiencing based on their descriptions. These tasks evaluated the Large\nLanguage Model's (LLMs) contextual understanding and representation of scent\nrelationships within its internal states - high-dimensional embedding space.\nBoth quantitative and qualitative methods were used to evaluate the AI system's\nperformance. Results indicated limited perceptual alignment, with biases\ntowards certain scents, like lemon and peppermint, and continued failing to\nidentify others, like rosemary. We discuss these findings in light of human-AI\nalignment advancements, highlighting the limitations and opportunities for\nenhancing HCI systems with multisensory experience integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning AI with human intent is important, yet perceptual alignment-how AI\ninterprets what we see, hear, or smell-remains underexplored. This work focuses\non olfaction, human smell experiences. We conducted a user study with 40\nparticipants to investigate how well AI can interpret human descriptions of\nscents. Participants performed \"sniff and describe\" interactive tasks, with our\ndesigned AI system attempting to guess what scent the participants were\nexperiencing based on their descriptions. These tasks evaluated the Large\nLanguage Model's (LLMs) contextual understanding and representation of scent\nrelationships within its internal states - high-dimensional embedding space.\nBoth quantitative and qualitative methods were used to evaluate the AI system's\nperformance. Results indicated limited perceptual alignment, with biases\ntowards certain scents, like lemon and peppermint, and continued failing to\nidentify others, like rosemary. We discuss these findings in light of human-AI\nalignment advancements, highlighting the limitations and opportunities for\nenhancing HCI systems with multisensory experience integration."
                },
                "authors": [
                    {
                        "name": "Shu Zhong"
                    },
                    {
                        "name": "Zetao Zhou"
                    },
                    {
                        "name": "Christopher Dawes"
                    },
                    {
                        "name": "Giada Brianz"
                    },
                    {
                        "name": "Marianna Obrist"
                    }
                ],
                "author_detail": {
                    "name": "Marianna Obrist"
                },
                "author": "Marianna Obrist",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06946v1",
                "updated": "2024-11-11T12:54:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    54,
                    22,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T12:54:22Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    54,
                    22,
                    0,
                    316,
                    0
                ],
                "title": "Cancer-Answer: Empowering Cancer Care with Advanced Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer-Answer: Empowering Cancer Care with Advanced Large Language\n  Models"
                },
                "summary": "Gastrointestinal (GI) tract cancers account for a substantial portion of the\nglobal cancer burden, where early diagnosis is critical for improved management\nand patient outcomes. The complex aetiologies and overlapping symptoms across\nGI cancers often delay diagnosis, leading to suboptimal treatment strategies.\nCancer-related queries are crucial for timely diagnosis, treatment, and patient\neducation, as access to accurate, comprehensive information can significantly\ninfluence outcomes. However, the complexity of cancer as a disease, combined\nwith the vast amount of available data, makes it difficult for clinicians and\npatients to quickly find precise answers. To address these challenges, we\nleverage large language models (LLMs) such as GPT-3.5 Turbo to generate\naccurate, contextually relevant responses to cancer-related queries.\nPre-trained with medical data, these models provide timely, actionable insights\nthat support informed decision-making in cancer diagnosis and care, ultimately\nimproving patient outcomes. We calculate two metrics: A1 (which represents the\nfraction of entities present in the model-generated answer compared to the gold\nstandard) and A2 (which represents the linguistic correctness and\nmeaningfulness of the model-generated answer with respect to the gold\nstandard), achieving maximum values of 0.546 and 0.881, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gastrointestinal (GI) tract cancers account for a substantial portion of the\nglobal cancer burden, where early diagnosis is critical for improved management\nand patient outcomes. The complex aetiologies and overlapping symptoms across\nGI cancers often delay diagnosis, leading to suboptimal treatment strategies.\nCancer-related queries are crucial for timely diagnosis, treatment, and patient\neducation, as access to accurate, comprehensive information can significantly\ninfluence outcomes. However, the complexity of cancer as a disease, combined\nwith the vast amount of available data, makes it difficult for clinicians and\npatients to quickly find precise answers. To address these challenges, we\nleverage large language models (LLMs) such as GPT-3.5 Turbo to generate\naccurate, contextually relevant responses to cancer-related queries.\nPre-trained with medical data, these models provide timely, actionable insights\nthat support informed decision-making in cancer diagnosis and care, ultimately\nimproving patient outcomes. We calculate two metrics: A1 (which represents the\nfraction of entities present in the model-generated answer compared to the gold\nstandard) and A2 (which represents the linguistic correctness and\nmeaningfulness of the model-generated answer with respect to the gold\nstandard), achieving maximum values of 0.546 and 0.881, respectively."
                },
                "authors": [
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Subhankar Maity"
                    }
                ],
                "author_detail": {
                    "name": "Subhankar Maity"
                },
                "author": "Subhankar Maity",
                "arxiv_comment": "Accepted at FIRE 2024 (Track: Conversational System for Differential\n  Diagnosis of GI Cancer)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.01265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.01265v2",
                "updated": "2024-11-11T12:51:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    51,
                    41,
                    0,
                    316,
                    0
                ],
                "published": "2023-07-03T18:00:06Z",
                "published_parsed": [
                    2023,
                    7,
                    3,
                    18,
                    0,
                    6,
                    0,
                    184,
                    0
                ],
                "title": "Inferring reionization and galaxy properties from the patchy kinetic\n  Sunyaev-Zel'dovich signal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring reionization and galaxy properties from the patchy kinetic\n  Sunyaev-Zel'dovich signal"
                },
                "summary": "The patchy kinetic Sunyaev-Zel'dovich (kSZ) signal is an integral probe of\nthe timing and morphology of the epoch of reionization (EoR). Recent\nobservations have claimed a low signal-to-noise (S/N) measurement, with a\ndramatic increase in S/N expected in the near future. In this work, we quantify\nwhat we can learn about the EoR from the kSZ signal. We perform Bayesian\ninference by sampling galaxy properties and using forward-models of the kSZ as\nwell as other EoR and galaxy observations in the likelihood. Including the\nrecent kSZ measurement obtained by the South Pole Telescope\n($\\mathcal{D}_{3000}^{\\rm{pkSZ}} = 1.1_{-0.7}^{+1.1} \\mu$K$^2$) shifts the\nposterior distribution in favor of faster and later reionization models,\nresulting in lower values of the optical depth to the CMB: $\\tau_e =\n0.052_{-0.008}^{+0.009}$ with a 68$\\%$ confidence interval (C.I.). The combined\nEoR and UV luminosity function observations also imply a typical ionizing\nescape fraction of $0.04_{-0.03}^{+0.05}$ (95$\\%$ C.I.), without a strong\ndependence on halo mass. We show how the patchy kSZ power from our posterior\ndepends on the commonly-used parameters of reionization. For a given midpoint\nand duration, the EoR morphology only has a few percent impact on the patchy\nkSZ power in our posterior. However, a physical model is needed to obtain tight\nconstraints from the current low S/N patchy kSZ measurement, as it allows us to\ntake advantage of complimentary high-$z$ observations. Future high S/N\ndetections of the patchy kSZ should decrease the current uncertainties on the\ntiming of the EoR by factors of $\\sim$2 - 3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The patchy kinetic Sunyaev-Zel'dovich (kSZ) signal is an integral probe of\nthe timing and morphology of the epoch of reionization (EoR). Recent\nobservations have claimed a low signal-to-noise (S/N) measurement, with a\ndramatic increase in S/N expected in the near future. In this work, we quantify\nwhat we can learn about the EoR from the kSZ signal. We perform Bayesian\ninference by sampling galaxy properties and using forward-models of the kSZ as\nwell as other EoR and galaxy observations in the likelihood. Including the\nrecent kSZ measurement obtained by the South Pole Telescope\n($\\mathcal{D}_{3000}^{\\rm{pkSZ}} = 1.1_{-0.7}^{+1.1} \\mu$K$^2$) shifts the\nposterior distribution in favor of faster and later reionization models,\nresulting in lower values of the optical depth to the CMB: $\\tau_e =\n0.052_{-0.008}^{+0.009}$ with a 68$\\%$ confidence interval (C.I.). The combined\nEoR and UV luminosity function observations also imply a typical ionizing\nescape fraction of $0.04_{-0.03}^{+0.05}$ (95$\\%$ C.I.), without a strong\ndependence on halo mass. We show how the patchy kSZ power from our posterior\ndepends on the commonly-used parameters of reionization. For a given midpoint\nand duration, the EoR morphology only has a few percent impact on the patchy\nkSZ power in our posterior. However, a physical model is needed to obtain tight\nconstraints from the current low S/N patchy kSZ measurement, as it allows us to\ntake advantage of complimentary high-$z$ observations. Future high S/N\ndetections of the patchy kSZ should decrease the current uncertainties on the\ntiming of the EoR by factors of $\\sim$2 - 3."
                },
                "authors": [
                    {
                        "name": "Ivan Nikoli"
                    },
                    {
                        "name": "Andrei Mesinger"
                    },
                    {
                        "name": "Yuxiang Qin"
                    },
                    {
                        "name": "Adlie Gorce"
                    }
                ],
                "author_detail": {
                    "name": "Adlie Gorce"
                },
                "author": "Adlie Gorce",
                "arxiv_comment": "14 pages, 6 figures, accepted to MNRAS (MNRAS, 526, 3170)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.01265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.01265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11032v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11032v3",
                "updated": "2024-11-11T12:50:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    50,
                    44,
                    0,
                    316,
                    0
                ],
                "published": "2024-09-17T09:56:12Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    56,
                    12,
                    1,
                    261,
                    0
                ],
                "title": "Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI"
                },
                "summary": "Written texts reflect an author's perspective, making the thorough analysis\nof literature a key research method in fields such as the humanities and social\nsciences. However, conventional text mining techniques like sentiment analysis\nand topic modeling are limited in their ability to capture the hierarchical\nnarrative structures that reveal deeper argumentative patterns. To address this\ngap, we propose a method that leverages large language models (LLMs) to extract\nand organize these structures into a hierarchical framework. We validate this\napproach by analyzing public opinions on generative AI collected by Japan's\nAgency for Cultural Affairs, comparing the narratives of supporters and\ncritics. Our analysis provides clearer visualization of the factors influencing\ndivergent opinions on generative AI, offering deeper insights into the\nstructures of agreement and disagreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Written texts reflect an author's perspective, making the thorough analysis\nof literature a key research method in fields such as the humanities and social\nsciences. However, conventional text mining techniques like sentiment analysis\nand topic modeling are limited in their ability to capture the hierarchical\nnarrative structures that reveal deeper argumentative patterns. To address this\ngap, we propose a method that leverages large language models (LLMs) to extract\nand organize these structures into a hierarchical framework. We validate this\napproach by analyzing public opinions on generative AI collected by Japan's\nAgency for Cultural Affairs, comparing the narratives of supporters and\ncritics. Our analysis provides clearer visualization of the factors influencing\ndivergent opinions on generative AI, offering deeper insights into the\nstructures of agreement and disagreement."
                },
                "authors": [
                    {
                        "name": "Riona Matsuoka"
                    },
                    {
                        "name": "Hiroki Matsumoto"
                    },
                    {
                        "name": "Takahiro Yoshida"
                    },
                    {
                        "name": "Tomohiro Watanabe"
                    },
                    {
                        "name": "Ryoma Kondo"
                    },
                    {
                        "name": "Ryohei Hisano"
                    }
                ],
                "author_detail": {
                    "name": "Ryohei Hisano"
                },
                "author": "Ryohei Hisano",
                "arxiv_journal_ref": "Proceedings of Jinmoncon 2024, IPSJ SIG Computers and the\n  Humanities",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11032v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11032v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20361v2",
                "updated": "2024-11-11T12:45:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    45,
                    51,
                    0,
                    316,
                    0
                ],
                "published": "2024-09-30T14:59:22Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    59,
                    22,
                    0,
                    274,
                    0
                ],
                "title": "Rotated Runtime Smooth: Training-Free Activation Smoother for accurate\n  INT4 inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotated Runtime Smooth: Training-Free Activation Smoother for accurate\n  INT4 inference"
                },
                "summary": "Large language models have demonstrated promising capabilities upon scaling\nup parameters. However, serving large language models incurs substantial\ncomputation and memory movement costs due to their large scale. Quantization\nmethods have been employed to reduce service costs and latency. Nevertheless,\noutliers in activations hinder the development of INT4 weight-activation\nquantization. Existing approaches separate outliers and normal values into two\nmatrices or migrate outliers from activations to weights, suffering from high\nlatency or accuracy degradation. Based on observing activations from large\nlanguage models, outliers can be classified into channel-wise and spike\noutliers. In this work, we propose Rotated Runtime Smooth (RRS), a\nplug-and-play activation smoother for quantization, consisting of Runtime\nSmooth and the Rotation operation. Runtime Smooth (RS) is introduced to\neliminate channel-wise outliers by smoothing activations with channel-wise\nmaximums during runtime. The rotation operation can narrow the gap between\nspike outliers and normal values, alleviating the effect of victims caused by\nchannel-wise smoothing. The proposed method outperforms the state-of-the-art\nmethod in the LLaMA and Qwen families and improves WikiText-2 perplexity from\n57.33 to 6.66 for INT4 inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated promising capabilities upon scaling\nup parameters. However, serving large language models incurs substantial\ncomputation and memory movement costs due to their large scale. Quantization\nmethods have been employed to reduce service costs and latency. Nevertheless,\noutliers in activations hinder the development of INT4 weight-activation\nquantization. Existing approaches separate outliers and normal values into two\nmatrices or migrate outliers from activations to weights, suffering from high\nlatency or accuracy degradation. Based on observing activations from large\nlanguage models, outliers can be classified into channel-wise and spike\noutliers. In this work, we propose Rotated Runtime Smooth (RRS), a\nplug-and-play activation smoother for quantization, consisting of Runtime\nSmooth and the Rotation operation. Runtime Smooth (RS) is introduced to\neliminate channel-wise outliers by smoothing activations with channel-wise\nmaximums during runtime. The rotation operation can narrow the gap between\nspike outliers and normal values, alleviating the effect of victims caused by\nchannel-wise smoothing. The proposed method outperforms the state-of-the-art\nmethod in the LLaMA and Qwen families and improves WikiText-2 perplexity from\n57.33 to 6.66 for INT4 inference."
                },
                "authors": [
                    {
                        "name": "Ke Yi"
                    },
                    {
                        "name": "Zengke Liu"
                    },
                    {
                        "name": "Jianwei Zhang"
                    },
                    {
                        "name": "Chengyuan Li"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06927v1",
                "updated": "2024-11-11T12:32:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    32,
                    18,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T12:32:18Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    32,
                    18,
                    0,
                    316,
                    0
                ],
                "title": "Multi-modal Iterative and Deep Fusion Frameworks for Enhanced Passive\n  DOA Sensing via a Green Massive H2AD MIMO Receiver",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Iterative and Deep Fusion Frameworks for Enhanced Passive\n  DOA Sensing via a Green Massive H2AD MIMO Receiver"
                },
                "summary": "Most existing DOA estimation methods assume ideal source incident angles with\nminimal noise. Moreover, directly using pre-estimated angles to calculate\nweighted coefficients can lead to performance loss. Thus, a green multi-modal\n(MM) fusion DOA framework is proposed to realize a more practical, low-cost and\nhigh time-efficiency DOA estimation for a H$^2$AD array. Firstly, two more\nefficient clustering methods, global maximum cos\\_similarity clustering\n(GMaxCS) and global minimum distance clustering (GMinD), are presented to infer\nmore precise true solutions from the candidate solution sets. Based on this, an\niteration weighted fusion (IWF)-based method is introduced to iteratively\nupdate weighted fusion coefficients and the clustering center of the true\nsolution classes by using the estimated values. Particularly, the coarse DOA\ncalculated by fully digital (FD) subarray, serves as the initial cluster\ncenter. The above process yields two methods called MM-IWF-GMaxCS and\nMM-IWF-GMinD. To further provide a higher-accuracy DOA estimation, a fusion\nnetwork (fusionNet) is proposed to aggregate the inferred two-part true angles\nand thus generates two effective approaches called MM-fusionNet-GMaxCS and\nMM-fusionNet-GMinD. The simulation outcomes show the proposed four approaches\ncan achieve the ideal DOA performance and the CRLB. Meanwhile, proposed\nMM-fusionNet-GMaxCS and MM-fusionNet-GMinD exhibit superior DOA performance\ncompared to MM-IWF-GMaxCS and MM-IWF-GMinD, especially in extremely-low SNR\nrange.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most existing DOA estimation methods assume ideal source incident angles with\nminimal noise. Moreover, directly using pre-estimated angles to calculate\nweighted coefficients can lead to performance loss. Thus, a green multi-modal\n(MM) fusion DOA framework is proposed to realize a more practical, low-cost and\nhigh time-efficiency DOA estimation for a H$^2$AD array. Firstly, two more\nefficient clustering methods, global maximum cos\\_similarity clustering\n(GMaxCS) and global minimum distance clustering (GMinD), are presented to infer\nmore precise true solutions from the candidate solution sets. Based on this, an\niteration weighted fusion (IWF)-based method is introduced to iteratively\nupdate weighted fusion coefficients and the clustering center of the true\nsolution classes by using the estimated values. Particularly, the coarse DOA\ncalculated by fully digital (FD) subarray, serves as the initial cluster\ncenter. The above process yields two methods called MM-IWF-GMaxCS and\nMM-IWF-GMinD. To further provide a higher-accuracy DOA estimation, a fusion\nnetwork (fusionNet) is proposed to aggregate the inferred two-part true angles\nand thus generates two effective approaches called MM-fusionNet-GMaxCS and\nMM-fusionNet-GMinD. The simulation outcomes show the proposed four approaches\ncan achieve the ideal DOA performance and the CRLB. Meanwhile, proposed\nMM-fusionNet-GMaxCS and MM-fusionNet-GMinD exhibit superior DOA performance\ncompared to MM-IWF-GMaxCS and MM-IWF-GMinD, especially in extremely-low SNR\nrange."
                },
                "authors": [
                    {
                        "name": "Jiatong Bai"
                    },
                    {
                        "name": "Minghao Chen"
                    },
                    {
                        "name": "Wankai Tang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Cunhua Pan"
                    },
                    {
                        "name": "Yongpeng Wu"
                    },
                    {
                        "name": "Feng Shu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Shu"
                },
                "author": "Feng Shu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06923v1",
                "updated": "2024-11-11T12:28:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    28,
                    25,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T12:28:25Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    28,
                    25,
                    0,
                    316,
                    0
                ],
                "title": "Detecting Filamentarity in Climate and Galactic Spatial Point Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Filamentarity in Climate and Galactic Spatial Point Processes"
                },
                "summary": "Evidence of excess filamentarity is considered for two spatial point process\napplications: local minima in whole earth precipitation modelling and locations\nof cold clumps in the Milky Way. A diagnostic test using the number of aligned\ntriads and tetrads is developed. A Poisson filament process is proposed based\non a parent Poisson process with correlated random walk offspring locations.\nFilaments are initially identified using an arc search method, with ABC for\nsubsequent inference. Simulations indicate good performance. In both\napplications there is strong evidence of filamentarity. The method successfully\nidentifies two outlying precipitation data sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence of excess filamentarity is considered for two spatial point process\napplications: local minima in whole earth precipitation modelling and locations\nof cold clumps in the Milky Way. A diagnostic test using the number of aligned\ntriads and tetrads is developed. A Poisson filament process is proposed based\non a parent Poisson process with correlated random walk offspring locations.\nFilaments are initially identified using an arc search method, with ABC for\nsubsequent inference. Simulations indicate good performance. In both\napplications there is strong evidence of filamentarity. The method successfully\nidentifies two outlying precipitation data sets."
                },
                "authors": [
                    {
                        "name": "Aida Gjoka"
                    },
                    {
                        "name": "Robin Henderson"
                    },
                    {
                        "name": "Paul Oman"
                    }
                ],
                "author_detail": {
                    "name": "Paul Oman"
                },
                "author": "Paul Oman",
                "arxiv_comment": "23 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06911v1",
                "updated": "2024-11-11T12:13:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    13,
                    58,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T12:13:58Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    13,
                    58,
                    0,
                    316,
                    0
                ],
                "title": "Gaussian Process Emulators for Few-Shot Segmentation in Cardiac MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Process Emulators for Few-Shot Segmentation in Cardiac MRI"
                },
                "summary": "Segmentation of cardiac magnetic resonance images (MRI) is crucial for the\nanalysis and assessment of cardiac function, helping to diagnose and treat\nvarious cardiovascular diseases. Most recent techniques rely on deep learning\nand usually require an extensive amount of labeled data. To overcome this\nproblem, few-shot learning has the capability of reducing data dependency on\nlabeled data. In this work, we introduce a new method that merges few-shot\nlearning with a U-Net architecture and Gaussian Process Emulators (GPEs),\nenhancing data integration from a support set for improved performance. GPEs\nare trained to learn the relation between the support images and the\ncorresponding masks in latent space, facilitating the segmentation of unseen\nquery images given only a small labeled support set at inference. We test our\nmodel with the M&Ms-2 public dataset to assess its ability to segment the heart\nin cardiac magnetic resonance imaging from different orientations, and compare\nit with state-of-the-art unsupervised and few-shot methods. Our architecture\nshows higher DICE coefficients compared to these methods, especially in the\nmore challenging setups where the size of the support set is considerably\nsmall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segmentation of cardiac magnetic resonance images (MRI) is crucial for the\nanalysis and assessment of cardiac function, helping to diagnose and treat\nvarious cardiovascular diseases. Most recent techniques rely on deep learning\nand usually require an extensive amount of labeled data. To overcome this\nproblem, few-shot learning has the capability of reducing data dependency on\nlabeled data. In this work, we introduce a new method that merges few-shot\nlearning with a U-Net architecture and Gaussian Process Emulators (GPEs),\nenhancing data integration from a support set for improved performance. GPEs\nare trained to learn the relation between the support images and the\ncorresponding masks in latent space, facilitating the segmentation of unseen\nquery images given only a small labeled support set at inference. We test our\nmodel with the M&Ms-2 public dataset to assess its ability to segment the heart\nin cardiac magnetic resonance imaging from different orientations, and compare\nit with state-of-the-art unsupervised and few-shot methods. Our architecture\nshows higher DICE coefficients compared to these methods, especially in the\nmore challenging setups where the size of the support set is considerably\nsmall."
                },
                "authors": [
                    {
                        "name": "Bruno Viti"
                    },
                    {
                        "name": "Franz Thaler"
                    },
                    {
                        "name": "Kathrin Lisa Kapper"
                    },
                    {
                        "name": "Martin Urschler"
                    },
                    {
                        "name": "Martin Holler"
                    },
                    {
                        "name": "Elias Karabelas"
                    }
                ],
                "author_detail": {
                    "name": "Elias Karabelas"
                },
                "author": "Elias Karabelas",
                "arxiv_comment": "Submitted to Statistical Atlases and Computational Modeling of the\n  Heart (STACOM) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10426v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10426v7",
                "updated": "2024-11-11T12:05:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    5,
                    10,
                    0,
                    316,
                    0
                ],
                "published": "2022-12-20T17:04:50Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    17,
                    4,
                    50,
                    1,
                    354,
                    0
                ],
                "title": "Deep Riemannian Networks for End-to-End EEG Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Riemannian Networks for End-to-End EEG Decoding"
                },
                "summary": "State-of-the-art performance in electroencephalography (EEG) decoding tasks\nis currently often achieved with either Deep-Learning (DL) or\nRiemannian-Geometry-based decoders (RBDs). Recently, there is growing interest\nin Deep Riemannian Networks (DRNs) possibly combining the advantages of both\nprevious classes of methods. However, there are still a range of topics where\nadditional insight is needed to pave the way for a more widespread application\nof DRNs in EEG. These include architecture design questions such as network\nsize and end-to-end ability. How these factors affect model performance has not\nbeen explored. Additionally, it is not clear how the data within these networks\nis transformed, and whether this would correlate with traditional EEG decoding.\nOur study aims to lay the groundwork in the area of these topics through the\nanalysis of DRNs for EEG with a wide range of hyperparameters. Networks were\ntested on five public EEG datasets and compared with state-of-the-art ConvNets.\n  Here we propose EE(G)-SPDNet, and we show that this wide, end-to-end DRN can\noutperform the ConvNets, and in doing so use physiologically plausible\nfrequency regions. We also show that the end-to-end approach learns more\ncomplex filters than traditional band-pass filters targeting the classical\nalpha, beta, and gamma frequency bands of the EEG, and that performance can\nbenefit from channel specific filtering approaches. Additionally, architectural\nanalysis revealed areas for further improvement due to the possible under\nutilisation of Riemannian specific information throughout the network. Our\nstudy thus shows how to design and train DRNs to infer task-related information\nfrom the raw EEG without the need of handcrafted filterbanks and highlights the\npotential of end-to-end DRNs such as EE(G)-SPDNet for high-performance EEG\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art performance in electroencephalography (EEG) decoding tasks\nis currently often achieved with either Deep-Learning (DL) or\nRiemannian-Geometry-based decoders (RBDs). Recently, there is growing interest\nin Deep Riemannian Networks (DRNs) possibly combining the advantages of both\nprevious classes of methods. However, there are still a range of topics where\nadditional insight is needed to pave the way for a more widespread application\nof DRNs in EEG. These include architecture design questions such as network\nsize and end-to-end ability. How these factors affect model performance has not\nbeen explored. Additionally, it is not clear how the data within these networks\nis transformed, and whether this would correlate with traditional EEG decoding.\nOur study aims to lay the groundwork in the area of these topics through the\nanalysis of DRNs for EEG with a wide range of hyperparameters. Networks were\ntested on five public EEG datasets and compared with state-of-the-art ConvNets.\n  Here we propose EE(G)-SPDNet, and we show that this wide, end-to-end DRN can\noutperform the ConvNets, and in doing so use physiologically plausible\nfrequency regions. We also show that the end-to-end approach learns more\ncomplex filters than traditional band-pass filters targeting the classical\nalpha, beta, and gamma frequency bands of the EEG, and that performance can\nbenefit from channel specific filtering approaches. Additionally, architectural\nanalysis revealed areas for further improvement due to the possible under\nutilisation of Riemannian specific information throughout the network. Our\nstudy thus shows how to design and train DRNs to infer task-related information\nfrom the raw EEG without the need of handcrafted filterbanks and highlights the\npotential of end-to-end DRNs such as EE(G)-SPDNet for high-performance EEG\ndecoding."
                },
                "authors": [
                    {
                        "name": "Daniel Wilson"
                    },
                    {
                        "name": "Robin Tibor Schirrmeister"
                    },
                    {
                        "name": "Lukas Alexander Wilhelm Gemein"
                    },
                    {
                        "name": "Tonio Ball"
                    }
                ],
                "author_detail": {
                    "name": "Tonio Ball"
                },
                "author": "Tonio Ball",
                "arxiv_comment": "32 pages, 16 Figures, + Supplementary Material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10426v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10426v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07599v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07599v3",
                "updated": "2024-11-11T12:00:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    0,
                    35,
                    0,
                    316,
                    0
                ],
                "published": "2024-06-11T16:42:02Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    16,
                    42,
                    2,
                    1,
                    163,
                    0
                ],
                "title": "CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence"
                },
                "summary": "Cyber threat intelligence (CTI) is crucial in today's cybersecurity\nlandscape, providing essential insights to understand and mitigate the\never-evolving cyber threats. The recent rise of Large Language Models (LLMs)\nhave shown potential in this domain, but concerns about their reliability,\naccuracy, and hallucinations persist. While existing benchmarks provide general\nevaluations of LLMs, there are no benchmarks that address the practical and\napplied aspects of CTI-specific tasks. To bridge this gap, we introduce\nCTIBench, a benchmark designed to assess LLMs' performance in CTI applications.\nCTIBench includes multiple datasets focused on evaluating knowledge acquired by\nLLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art\nmodels on these tasks provides insights into their strengths and weaknesses in\nCTI contexts, contributing to a better understanding of LLM capabilities in\nCTI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber threat intelligence (CTI) is crucial in today's cybersecurity\nlandscape, providing essential insights to understand and mitigate the\never-evolving cyber threats. The recent rise of Large Language Models (LLMs)\nhave shown potential in this domain, but concerns about their reliability,\naccuracy, and hallucinations persist. While existing benchmarks provide general\nevaluations of LLMs, there are no benchmarks that address the practical and\napplied aspects of CTI-specific tasks. To bridge this gap, we introduce\nCTIBench, a benchmark designed to assess LLMs' performance in CTI applications.\nCTIBench includes multiple datasets focused on evaluating knowledge acquired by\nLLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art\nmodels on these tasks provides insights into their strengths and weaknesses in\nCTI contexts, contributing to a better understanding of LLM capabilities in\nCTI."
                },
                "authors": [
                    {
                        "name": "Md Tanvirul Alam"
                    },
                    {
                        "name": "Dipkamal Bhusal"
                    },
                    {
                        "name": "Le Nguyen"
                    },
                    {
                        "name": "Nidhi Rastogi"
                    }
                ],
                "author_detail": {
                    "name": "Nidhi Rastogi"
                },
                "author": "Nidhi Rastogi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07599v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07599v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06899v1",
                "updated": "2024-11-11T11:57:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    57,
                    37,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T11:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    57,
                    37,
                    0,
                    316,
                    0
                ],
                "title": "LongSafetyBench: Long-Context LLMs Struggle with Safety Issues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSafetyBench: Long-Context LLMs Struggle with Safety Issues"
                },
                "summary": "With the development of large language models (LLMs), the sequence length of\nthese models continues to increase, drawing significant attention to\nlong-context language models. However, the evaluation of these models has been\nprimarily limited to their capabilities, with a lack of research focusing on\ntheir safety. Existing work, such as ManyShotJailbreak, has to some extent\ndemonstrated that long-context language models can exhibit safety concerns.\nHowever, the methods used are limited and lack comprehensiveness. In response,\nwe introduce \\textbf{LongSafetyBench}, the first benchmark designed to\nobjectively and comprehensively evaluate the safety of long-context models.\nLongSafetyBench consists of 10 task categories, with an average length of\n41,889 words. After testing eight long-context language models on\nLongSafetyBench, we found that existing models generally exhibit insufficient\nsafety capabilities. The proportion of safe responses from most mainstream\nlong-context LLMs is below 50\\%. Moreover, models' safety performance in\nlong-context scenarios does not always align with that in short-context\nscenarios. Further investigation revealed that long-context models tend to\noverlook harmful content within lengthy texts. We also proposed a simple yet\neffective solution, allowing open-source models to achieve performance\ncomparable to that of top-tier closed-source models. We believe that\nLongSafetyBench can serve as a valuable benchmark for evaluating the safety\ncapabilities of long-context language models. We hope that our work will\nencourage the broader community to pay attention to the safety of long-context\nmodels and contribute to the development of solutions to improve the safety of\nlong-context LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the sequence length of\nthese models continues to increase, drawing significant attention to\nlong-context language models. However, the evaluation of these models has been\nprimarily limited to their capabilities, with a lack of research focusing on\ntheir safety. Existing work, such as ManyShotJailbreak, has to some extent\ndemonstrated that long-context language models can exhibit safety concerns.\nHowever, the methods used are limited and lack comprehensiveness. In response,\nwe introduce \\textbf{LongSafetyBench}, the first benchmark designed to\nobjectively and comprehensively evaluate the safety of long-context models.\nLongSafetyBench consists of 10 task categories, with an average length of\n41,889 words. After testing eight long-context language models on\nLongSafetyBench, we found that existing models generally exhibit insufficient\nsafety capabilities. The proportion of safe responses from most mainstream\nlong-context LLMs is below 50\\%. Moreover, models' safety performance in\nlong-context scenarios does not always align with that in short-context\nscenarios. Further investigation revealed that long-context models tend to\noverlook harmful content within lengthy texts. We also proposed a simple yet\neffective solution, allowing open-source models to achieve performance\ncomparable to that of top-tier closed-source models. We believe that\nLongSafetyBench can serve as a valuable benchmark for evaluating the safety\ncapabilities of long-context language models. We hope that our work will\nencourage the broader community to pay attention to the safety of long-context\nmodels and contribute to the development of solutions to improve the safety of\nlong-context LLMs."
                },
                "authors": [
                    {
                        "name": "Mianqiu Huang"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Shaojun Zhou"
                    },
                    {
                        "name": "Mozhi Zhang"
                    },
                    {
                        "name": "Chenkun Tan"
                    },
                    {
                        "name": "Pengyu Wang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Linyang Li"
                    },
                    {
                        "name": "Zhikai Lei"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Yaqian Zhou"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09085v2",
                "updated": "2024-11-11T11:35:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    35,
                    28,
                    0,
                    316,
                    0
                ],
                "published": "2024-03-14T04:06:13Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    4,
                    6,
                    13,
                    3,
                    74,
                    0
                ],
                "title": "Meaningful Learning: Enhancing Abstract Reasoning in Large Language\n  Models via Generic Fact Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meaningful Learning: Enhancing Abstract Reasoning in Large Language\n  Models via Generic Fact Guidance"
                },
                "summary": "Large language models (LLMs) have developed impressive performance and strong\nexplainability across various reasoning scenarios, marking a significant stride\ntowards mimicking human-like intelligence. Despite this, when tasked with\nseveral simple questions supported by a generic fact, LLMs often struggle to\nabstract and apply the generic fact to provide consistent and precise answers,\nrevealing a deficiency in abstract reasoning abilities. This has sparked a\nvigorous debate about whether LLMs are genuinely reasoning or merely\nmemorizing. In light of this, we design a preliminary study to quantify and\ndelve into the abstract reasoning abilities of existing LLMs. Our findings\nreveal a substantial discrepancy between their general reasoning and abstract\nreasoning performances. To relieve this problem, we tailor an abstract\nreasoning dataset (AbsR) together with a meaningful learning paradigm to teach\nLLMs how to leverage generic facts for reasoning purposes. The results show\nthat our approach not only boosts the general reasoning performance of LLMs but\nalso makes considerable strides towards their capacity for abstract reasoning,\nmoving beyond simple memorization or imitation to a more nuanced understanding\nand application of generic facts. The code is available at\nhttps://github.com/Waste-Wood/MeanLearn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have developed impressive performance and strong\nexplainability across various reasoning scenarios, marking a significant stride\ntowards mimicking human-like intelligence. Despite this, when tasked with\nseveral simple questions supported by a generic fact, LLMs often struggle to\nabstract and apply the generic fact to provide consistent and precise answers,\nrevealing a deficiency in abstract reasoning abilities. This has sparked a\nvigorous debate about whether LLMs are genuinely reasoning or merely\nmemorizing. In light of this, we design a preliminary study to quantify and\ndelve into the abstract reasoning abilities of existing LLMs. Our findings\nreveal a substantial discrepancy between their general reasoning and abstract\nreasoning performances. To relieve this problem, we tailor an abstract\nreasoning dataset (AbsR) together with a meaningful learning paradigm to teach\nLLMs how to leverage generic facts for reasoning purposes. The results show\nthat our approach not only boosts the general reasoning performance of LLMs but\nalso makes considerable strides towards their capacity for abstract reasoning,\nmoving beyond simple memorization or imitation to a more nuanced understanding\nand application of generic facts. The code is available at\nhttps://github.com/Waste-Wood/MeanLearn."
                },
                "authors": [
                    {
                        "name": "Kai Xiong"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Dongliang Xu"
                    },
                    {
                        "name": "Qing Yang"
                    },
                    {
                        "name": "Hongtao Liu"
                    },
                    {
                        "name": "Yixin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Cao"
                },
                "author": "Yixin Cao",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17632v2",
                "updated": "2024-11-11T11:32:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    32,
                    21,
                    0,
                    316,
                    0
                ],
                "published": "2024-10-23T07:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    48,
                    51,
                    2,
                    297,
                    0
                ],
                "title": "LMLPA: Language Model Linguistic Personality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMLPA: Language Model Linguistic Personality Assessment"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in everyday life and\nresearch. One of the most common use cases is conversational interactions,\nenabled by the language generation capabilities of LLMs. Just as between two\nhumans, a conversation between an LLM-powered entity and a human depends on the\npersonality of the conversants. However, measuring the personality of a given\nLLM is currently a challenge. This paper introduces the Language Model\nLinguistic Personality Assessment (LMLPA), a system designed to evaluate the\nlinguistic personalities of LLMs. Our system helps to understand LLMs' language\ngeneration capabilities by quantitatively assessing the distinct personality\ntraits reflected in their linguistic outputs. Unlike traditional human-centric\npsychometrics, the LMLPA adapts a personality assessment questionnaire,\nspecifically the Big Five Inventory, to align with the operational capabilities\nof LLMs, and also incorporates the findings from previous language-based\npersonality measurement literature. To mitigate sensitivity to the order of\noptions, our questionnaire is designed to be open-ended, resulting in textual\nanswers. Thus, the AI rater is needed to transform ambiguous personality\ninformation from text responses into clear numerical indicators of personality\ntraits. Utilising Principal Component Analysis and reliability validations, our\nfindings demonstrate that LLMs possess distinct personality traits that can be\neffectively quantified by the LMLPA. This research contributes to\nHuman-Computer Interaction and Human-Centered AI, providing a robust framework\nfor future studies to refine AI personality assessments and expand their\napplications in multiple areas, including education and manufacturing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in everyday life and\nresearch. One of the most common use cases is conversational interactions,\nenabled by the language generation capabilities of LLMs. Just as between two\nhumans, a conversation between an LLM-powered entity and a human depends on the\npersonality of the conversants. However, measuring the personality of a given\nLLM is currently a challenge. This paper introduces the Language Model\nLinguistic Personality Assessment (LMLPA), a system designed to evaluate the\nlinguistic personalities of LLMs. Our system helps to understand LLMs' language\ngeneration capabilities by quantitatively assessing the distinct personality\ntraits reflected in their linguistic outputs. Unlike traditional human-centric\npsychometrics, the LMLPA adapts a personality assessment questionnaire,\nspecifically the Big Five Inventory, to align with the operational capabilities\nof LLMs, and also incorporates the findings from previous language-based\npersonality measurement literature. To mitigate sensitivity to the order of\noptions, our questionnaire is designed to be open-ended, resulting in textual\nanswers. Thus, the AI rater is needed to transform ambiguous personality\ninformation from text responses into clear numerical indicators of personality\ntraits. Utilising Principal Component Analysis and reliability validations, our\nfindings demonstrate that LLMs possess distinct personality traits that can be\neffectively quantified by the LMLPA. This research contributes to\nHuman-Computer Interaction and Human-Centered AI, providing a robust framework\nfor future studies to refine AI personality assessments and expand their\napplications in multiple areas, including education and manufacturing."
                },
                "authors": [
                    {
                        "name": "Jingyao Zheng"
                    },
                    {
                        "name": "Xian Wang"
                    },
                    {
                        "name": "Simo Hosio"
                    },
                    {
                        "name": "Xiaoxian Xu"
                    },
                    {
                        "name": "Lik-Hang Lee"
                    }
                ],
                "author_detail": {
                    "name": "Lik-Hang Lee"
                },
                "author": "Lik-Hang Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06877v1",
                "updated": "2024-11-11T11:17:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    17,
                    35,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T11:17:35Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    17,
                    35,
                    0,
                    316,
                    0
                ],
                "title": "LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help?"
                },
                "summary": "Test collections are information retrieval tools that allow researchers to\nquickly and easily evaluate ranking algorithms. While test collections have\nbecome an integral part of IR research, the process of data creation involves\nsignificant efforts in manual annotations, which often makes it very expensive\nand time-consuming. Thus, the test collections could become small when the\nbudget is limited, which may lead to unstable evaluations. As an alternative,\nrecent studies have proposed the use of large language models (LLMs) to\ncompletely replace human assessors. However, while LLMs seem to somewhat\ncorrelate with human judgments, they are not perfect and often show bias.\nMoreover, even if a well-performing LLM or prompt is found on one dataset,\nthere is no guarantee that it will perform similarly in practice, due to\ndifference in tasks and data. Thus a complete replacement with LLMs is argued\nto be too risky and not fully trustable.\n  Thus, in this paper, we propose \\textbf{L}LM-\\textbf{A}ssisted\n\\textbf{R}elevance \\textbf{A}ssessments (\\textbf{LARA}), an effective method to\nbalance manual annotations with LLM annotations, which helps to make a rich and\nreliable test collection. We use the LLM's predicted relevance probabilities in\norder to select the most profitable documents to manually annotate under a\nbudget constraint. While solely relying on LLM's predicted probabilities to\nmanually annotate performs fairly well, with theoretical reasoning, LARA guides\nthe human annotation process even more effectively via online calibration\nlearning. Then, using the calibration model learned from the limited manual\nannotations, LARA debiases the LLM predictions to annotate the remaining\nnon-assessed data. Empirical evaluations on TREC-COVID and TREC-8 Ad Hoc\ndatasets show that LARA outperforms the alternative solutions under almost any\nbudget constraint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test collections are information retrieval tools that allow researchers to\nquickly and easily evaluate ranking algorithms. While test collections have\nbecome an integral part of IR research, the process of data creation involves\nsignificant efforts in manual annotations, which often makes it very expensive\nand time-consuming. Thus, the test collections could become small when the\nbudget is limited, which may lead to unstable evaluations. As an alternative,\nrecent studies have proposed the use of large language models (LLMs) to\ncompletely replace human assessors. However, while LLMs seem to somewhat\ncorrelate with human judgments, they are not perfect and often show bias.\nMoreover, even if a well-performing LLM or prompt is found on one dataset,\nthere is no guarantee that it will perform similarly in practice, due to\ndifference in tasks and data. Thus a complete replacement with LLMs is argued\nto be too risky and not fully trustable.\n  Thus, in this paper, we propose \\textbf{L}LM-\\textbf{A}ssisted\n\\textbf{R}elevance \\textbf{A}ssessments (\\textbf{LARA}), an effective method to\nbalance manual annotations with LLM annotations, which helps to make a rich and\nreliable test collection. We use the LLM's predicted relevance probabilities in\norder to select the most profitable documents to manually annotate under a\nbudget constraint. While solely relying on LLM's predicted probabilities to\nmanually annotate performs fairly well, with theoretical reasoning, LARA guides\nthe human annotation process even more effectively via online calibration\nlearning. Then, using the calibration model learned from the limited manual\nannotations, LARA debiases the LLM predictions to annotate the remaining\nnon-assessed data. Empirical evaluations on TREC-COVID and TREC-8 Ad Hoc\ndatasets show that LARA outperforms the alternative solutions under almost any\nbudget constraint."
                },
                "authors": [
                    {
                        "name": "Rikiya Takehi"
                    },
                    {
                        "name": "Ellen M. Voorhees"
                    },
                    {
                        "name": "Tetsuya Sakai"
                    }
                ],
                "author_detail": {
                    "name": "Tetsuya Sakai"
                },
                "author": "Tetsuya Sakai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06869v1",
                "updated": "2024-11-11T11:08:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    8,
                    26,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T11:08:26Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    8,
                    26,
                    0,
                    316,
                    0
                ],
                "title": "CapeLLM: Support-Free Category-Agnostic Pose Estimation with Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapeLLM: Support-Free Category-Agnostic Pose Estimation with Multimodal\n  Large Language Models"
                },
                "summary": "Category-agnostic pose estimation (CAPE) has traditionally relied on support\nimages with annotated keypoints, a process that is often cumbersome and may\nfail to fully capture the necessary correspondences across diverse object\ncategories. Recent efforts have begun exploring the use of text-based queries,\nwhere the need for support keypoints is eliminated. However, the optimal use of\ntextual descriptions for keypoints remains an underexplored area. In this work,\nwe introduce CapeLLM, a novel approach that leverages a text-based multimodal\nlarge language model (MLLM) for CAPE. Our method only employs query image and\ndetailed text descriptions as an input to estimate category-agnostic keypoints.\nWe conduct extensive experiments to systematically explore the design space of\nLLM-based CAPE, investigating factors such as choosing the optimal description\nfor keypoints, neural network architectures, and training strategies. Thanks to\nthe advanced reasoning capabilities of the pre-trained MLLM, CapeLLM\ndemonstrates superior generalization and robust performance. Our approach sets\na new state-of-the-art on the MP-100 benchmark in the challenging 1-shot\nsetting, marking a significant advancement in the field of category-agnostic\npose estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Category-agnostic pose estimation (CAPE) has traditionally relied on support\nimages with annotated keypoints, a process that is often cumbersome and may\nfail to fully capture the necessary correspondences across diverse object\ncategories. Recent efforts have begun exploring the use of text-based queries,\nwhere the need for support keypoints is eliminated. However, the optimal use of\ntextual descriptions for keypoints remains an underexplored area. In this work,\nwe introduce CapeLLM, a novel approach that leverages a text-based multimodal\nlarge language model (MLLM) for CAPE. Our method only employs query image and\ndetailed text descriptions as an input to estimate category-agnostic keypoints.\nWe conduct extensive experiments to systematically explore the design space of\nLLM-based CAPE, investigating factors such as choosing the optimal description\nfor keypoints, neural network architectures, and training strategies. Thanks to\nthe advanced reasoning capabilities of the pre-trained MLLM, CapeLLM\ndemonstrates superior generalization and robust performance. Our approach sets\na new state-of-the-art on the MP-100 benchmark in the challenging 1-shot\nsetting, marking a significant advancement in the field of category-agnostic\npose estimation."
                },
                "authors": [
                    {
                        "name": "Junho Kim"
                    },
                    {
                        "name": "Hyungjin Chung"
                    },
                    {
                        "name": "Byung-Hoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Byung-Hoon Kim"
                },
                "author": "Byung-Hoon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02943v2",
                "updated": "2024-11-11T10:51:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    51,
                    31,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-05T09:37:23Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    37,
                    23,
                    1,
                    310,
                    0
                ],
                "title": "Capturing research literature attitude towards Sustainable Development\n  Goals: an LLM-based topic modeling approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing research literature attitude towards Sustainable Development\n  Goals: an LLM-based topic modeling approach"
                },
                "summary": "The world is facing a multitude of challenges that hinder the development of\nhuman civilization and the well-being of humanity on the planet. The\nSustainable Development Goals (SDGs) were formulated by the United Nations in\n2015 to address these global challenges by 2030. Natural language processing\ntechniques can help uncover discussions on SDGs within research literature. We\npropose a completely automated pipeline to 1) fetch content from the Scopus\ndatabase and prepare datasets dedicated to five groups of SDGs; 2) perform\ntopic modeling, a statistical technique used to identify topics in large\ncollections of textual data; and 3) enable topic exploration through\nkeywords-based search and topic frequency time series extraction. For topic\nmodeling, we leverage the stack of BERTopic scaled up to be applied on large\ncorpora of textual documents (we find hundreds of topics on hundreds of\nthousands of documents), introducing i) a novel LLM-based embeddings\ncomputation for representing scientific abstracts in the continuous space and\nii) a hyperparameter optimizer to efficiently find the best configuration for\nany new big datasets. We additionally produce the visualization of results on\ninteractive dashboards reporting topics' temporal evolution. Results are made\ninspectable and explorable, contributing to the interpretability of the topic\nmodeling process. Our proposed LLM-based topic modeling pipeline for big-text\ndatasets allows users to capture insights on the evolution of the attitude\ntoward SDGs within scientific abstracts in the 2006-2023 time span. All the\nresults are reproducible by using our system; the workflow can be generalized\nto be applied at any point in time to any big corpus of textual documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The world is facing a multitude of challenges that hinder the development of\nhuman civilization and the well-being of humanity on the planet. The\nSustainable Development Goals (SDGs) were formulated by the United Nations in\n2015 to address these global challenges by 2030. Natural language processing\ntechniques can help uncover discussions on SDGs within research literature. We\npropose a completely automated pipeline to 1) fetch content from the Scopus\ndatabase and prepare datasets dedicated to five groups of SDGs; 2) perform\ntopic modeling, a statistical technique used to identify topics in large\ncollections of textual data; and 3) enable topic exploration through\nkeywords-based search and topic frequency time series extraction. For topic\nmodeling, we leverage the stack of BERTopic scaled up to be applied on large\ncorpora of textual documents (we find hundreds of topics on hundreds of\nthousands of documents), introducing i) a novel LLM-based embeddings\ncomputation for representing scientific abstracts in the continuous space and\nii) a hyperparameter optimizer to efficiently find the best configuration for\nany new big datasets. We additionally produce the visualization of results on\ninteractive dashboards reporting topics' temporal evolution. Results are made\ninspectable and explorable, contributing to the interpretability of the topic\nmodeling process. Our proposed LLM-based topic modeling pipeline for big-text\ndatasets allows users to capture insights on the evolution of the attitude\ntoward SDGs within scientific abstracts in the 2006-2023 time span. All the\nresults are reproducible by using our system; the workflow can be generalized\nto be applied at any point in time to any big corpus of textual documents."
                },
                "authors": [
                    {
                        "name": "Francesco Invernici"
                    },
                    {
                        "name": "Francesca Curati"
                    },
                    {
                        "name": "Jelena Jakimov"
                    },
                    {
                        "name": "Amirhossein Samavi"
                    },
                    {
                        "name": "Anna Bernasconi"
                    }
                ],
                "author_detail": {
                    "name": "Anna Bernasconi"
                },
                "author": "Anna Bernasconi",
                "arxiv_comment": "27 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16040v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16040v5",
                "updated": "2024-11-11T10:40:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    40,
                    50,
                    0,
                    316,
                    0
                ],
                "published": "2024-02-25T09:41:50Z",
                "published_parsed": [
                    2024,
                    2,
                    25,
                    9,
                    41,
                    50,
                    6,
                    56,
                    0
                ],
                "title": "EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using\n  Discharge Summaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using\n  Discharge Summaries"
                },
                "summary": "Discharge summaries in Electronic Health Records (EHRs) are crucial for\nclinical decision-making, but their length and complexity make information\nextraction challenging, especially when dealing with accumulated summaries\nacross multiple patient admissions. Large Language Models (LLMs) show promise\nin addressing this challenge by efficiently analyzing vast and complex data.\nExisting benchmarks, however, fall short in properly evaluating LLMs'\ncapabilities in this context, as they typically focus on single-note\ninformation or limited topics, failing to reflect the real-world inquiries\nrequired by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel\nbenchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each\nlinked to distinct patients' discharge summaries. Every QA pair is initially\ngenerated using GPT-4 and then manually reviewed and refined by three\nclinicians to ensure clinical relevance. EHRNoteQA includes questions that\nrequire information across multiple discharge summaries and covers eight\ndiverse topics, mirroring the complexity and diversity of real clinical\ninquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice\nquestion answering, and propose a reliable evaluation method for each. We\nevaluate 27 LLMs using EHRNoteQA and examine various factors affecting the\nmodel performance (e.g., the length and number of discharge summaries).\nFurthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations\nin clinical practice, we measure the correlation between the LLM performance on\nEHRNoteQA, and the LLM performance manually evaluated by clinicians. Results\nshow that LLM performance on EHRNoteQA have higher correlation with\nclinician-evaluated performance (Spearman: 0.78, Kendall: 0.62) compared to\nother benchmarks, demonstrating its practical relevance in evaluating LLMs in\nclinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discharge summaries in Electronic Health Records (EHRs) are crucial for\nclinical decision-making, but their length and complexity make information\nextraction challenging, especially when dealing with accumulated summaries\nacross multiple patient admissions. Large Language Models (LLMs) show promise\nin addressing this challenge by efficiently analyzing vast and complex data.\nExisting benchmarks, however, fall short in properly evaluating LLMs'\ncapabilities in this context, as they typically focus on single-note\ninformation or limited topics, failing to reflect the real-world inquiries\nrequired by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel\nbenchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each\nlinked to distinct patients' discharge summaries. Every QA pair is initially\ngenerated using GPT-4 and then manually reviewed and refined by three\nclinicians to ensure clinical relevance. EHRNoteQA includes questions that\nrequire information across multiple discharge summaries and covers eight\ndiverse topics, mirroring the complexity and diversity of real clinical\ninquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice\nquestion answering, and propose a reliable evaluation method for each. We\nevaluate 27 LLMs using EHRNoteQA and examine various factors affecting the\nmodel performance (e.g., the length and number of discharge summaries).\nFurthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations\nin clinical practice, we measure the correlation between the LLM performance on\nEHRNoteQA, and the LLM performance manually evaluated by clinicians. Results\nshow that LLM performance on EHRNoteQA have higher correlation with\nclinician-evaluated performance (Spearman: 0.78, Kendall: 0.62) compared to\nother benchmarks, demonstrating its practical relevance in evaluating LLMs in\nclinical settings."
                },
                "authors": [
                    {
                        "name": "Sunjun Kweon"
                    },
                    {
                        "name": "Jiyoun Kim"
                    },
                    {
                        "name": "Heeyoung Kwak"
                    },
                    {
                        "name": "Dongchul Cha"
                    },
                    {
                        "name": "Hangyul Yoon"
                    },
                    {
                        "name": "Kwanghyun Kim"
                    },
                    {
                        "name": "Jeewon Yang"
                    },
                    {
                        "name": "Seunghyun Won"
                    },
                    {
                        "name": "Edward Choi"
                    }
                ],
                "author_detail": {
                    "name": "Edward Choi"
                },
                "author": "Edward Choi",
                "arxiv_comment": "NeurIPS 2024 (Datasets and Benchmarks)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16040v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16040v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06852v1",
                "updated": "2024-11-11T10:36:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    36,
                    4,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T10:36:04Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    36,
                    4,
                    0,
                    316,
                    0
                ],
                "title": "Evaluating Large Language Models on Financial Report Summarization: An\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models on Financial Report Summarization: An\n  Empirical Study"
                },
                "summary": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\nversatility across various applications, including natural language\nunderstanding, domain-specific knowledge tasks, etc. However, applying LLMs to\ncomplex, high-stakes domains like finance requires rigorous evaluation to\nensure reliability, accuracy, and compliance with industry standards. To\naddress this need, we conduct a comprehensive and comparative study on three\nstate-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their\neffectiveness in generating automated financial reports. Our primary motivation\nis to explore how these models can be harnessed within finance, a field\ndemanding precision, contextual relevance, and robustness against erroneous or\nmisleading information. By examining each model's capabilities, we aim to\nprovide an insightful assessment of their strengths and limitations. Our paper\noffers benchmarks for financial report analysis, encompassing proposed metrics\nsuch as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative\nevaluation framework that integrates both quantitative metrics (e.g.,\nprecision, recall) and qualitative analyses (e.g., contextual fit, consistency)\nto provide a holistic view of each model's output quality. Additionally, we\nmake our financial dataset publicly available, inviting researchers and\npractitioners to leverage, scrutinize, and enhance our findings through broader\ncommunity engagement and collaborative improvement. Our dataset is available on\nhuggingface.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\nversatility across various applications, including natural language\nunderstanding, domain-specific knowledge tasks, etc. However, applying LLMs to\ncomplex, high-stakes domains like finance requires rigorous evaluation to\nensure reliability, accuracy, and compliance with industry standards. To\naddress this need, we conduct a comprehensive and comparative study on three\nstate-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their\neffectiveness in generating automated financial reports. Our primary motivation\nis to explore how these models can be harnessed within finance, a field\ndemanding precision, contextual relevance, and robustness against erroneous or\nmisleading information. By examining each model's capabilities, we aim to\nprovide an insightful assessment of their strengths and limitations. Our paper\noffers benchmarks for financial report analysis, encompassing proposed metrics\nsuch as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative\nevaluation framework that integrates both quantitative metrics (e.g.,\nprecision, recall) and qualitative analyses (e.g., contextual fit, consistency)\nto provide a holistic view of each model's output quality. Additionally, we\nmake our financial dataset publicly available, inviting researchers and\npractitioners to leverage, scrutinize, and enhance our findings through broader\ncommunity engagement and collaborative improvement. Our dataset is available on\nhuggingface."
                },
                "authors": [
                    {
                        "name": "Xinqi Yang"
                    },
                    {
                        "name": "Scott Zang"
                    },
                    {
                        "name": "Yong Ren"
                    },
                    {
                        "name": "Dingjie Peng"
                    },
                    {
                        "name": "Zheng Wen"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wen"
                },
                "author": "Zheng Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06851v1",
                "updated": "2024-11-11T10:35:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    35,
                    23,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T10:35:23Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    35,
                    23,
                    0,
                    316,
                    0
                ],
                "title": "Fast and Efficient Transformer-based Method for Bird's Eye View Instance\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Efficient Transformer-based Method for Bird's Eye View Instance\n  Prediction"
                },
                "summary": "Accurate object detection and prediction are critical to ensure the safety\nand efficiency of self-driving architectures. Predicting object trajectories\nand occupancy enables autonomous vehicles to anticipate movements and make\ndecisions with future information, increasing their adaptability and reducing\nthe risk of accidents. Current State-Of-The-Art (SOTA) approaches often isolate\nthe detection, tracking, and prediction stages, which can lead to significant\nprediction errors due to accumulated inaccuracies between stages. Recent\nadvances have improved the feature representation of multi-camera perception\nsystems through Bird's-Eye View (BEV) transformations, boosting the development\nof end-to-end systems capable of predicting environmental elements directly\nfrom vehicle sensor data. These systems, however, often suffer from high\nprocessing times and number of parameters, creating challenges for real-world\ndeployment. To address these issues, this paper introduces a novel BEV instance\nprediction architecture based on a simplified paradigm that relies only on\ninstance segmentation and flow prediction. The proposed system prioritizes\nspeed, aiming at reduced parameter counts and inference times compared to\nexisting SOTA architectures, thanks to the incorporation of an efficient\ntransformer-based architecture. Furthermore, the implementation of the proposed\narchitecture is optimized for performance improvements in PyTorch version 2.1.\nCode and trained models are available at\nhttps://github.com/miguelag99/Efficient-Instance-Prediction",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate object detection and prediction are critical to ensure the safety\nand efficiency of self-driving architectures. Predicting object trajectories\nand occupancy enables autonomous vehicles to anticipate movements and make\ndecisions with future information, increasing their adaptability and reducing\nthe risk of accidents. Current State-Of-The-Art (SOTA) approaches often isolate\nthe detection, tracking, and prediction stages, which can lead to significant\nprediction errors due to accumulated inaccuracies between stages. Recent\nadvances have improved the feature representation of multi-camera perception\nsystems through Bird's-Eye View (BEV) transformations, boosting the development\nof end-to-end systems capable of predicting environmental elements directly\nfrom vehicle sensor data. These systems, however, often suffer from high\nprocessing times and number of parameters, creating challenges for real-world\ndeployment. To address these issues, this paper introduces a novel BEV instance\nprediction architecture based on a simplified paradigm that relies only on\ninstance segmentation and flow prediction. The proposed system prioritizes\nspeed, aiming at reduced parameter counts and inference times compared to\nexisting SOTA architectures, thanks to the incorporation of an efficient\ntransformer-based architecture. Furthermore, the implementation of the proposed\narchitecture is optimized for performance improvements in PyTorch version 2.1.\nCode and trained models are available at\nhttps://github.com/miguelag99/Efficient-Instance-Prediction"
                },
                "authors": [
                    {
                        "name": "Miguel Antunes-Garca"
                    },
                    {
                        "name": "Luis M. Bergasa"
                    },
                    {
                        "name": "Santiago Montiel-Marn"
                    },
                    {
                        "name": "Rafael Barea"
                    },
                    {
                        "name": "Fabio Snchez-Garca"
                    },
                    {
                        "name": "ngel Llamazares"
                    }
                ],
                "author_detail": {
                    "name": "ngel Llamazares"
                },
                "author": "ngel Llamazares",
                "arxiv_comment": "The article has been presented in the 27th IEEE International\n  Conference on Intelligent Transportation Systems (IEEE ITSC 2024) on\n  September, 2024. Number of pages: 6, Number of figures: 4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06850v1",
                "updated": "2024-11-11T10:34:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    34,
                    36,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T10:34:36Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    34,
                    36,
                    0,
                    316,
                    0
                ],
                "title": "1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of\n  Language, Hate Speech, and Targets using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of\n  Language, Hate Speech, and Targets using LLMs"
                },
                "summary": "This paper presents a detailed system description of our entry for the\nCHiPSAL 2025 shared task, focusing on language detection, hate speech\nidentification, and target detection in Devanagari script languages. We\nexperimented with a combination of large language models and their ensembles,\nincluding MuRIL, IndicBERT, and Gemma-2, and leveraged unique techniques like\nfocal loss to address challenges in the natural understanding of Devanagari\nlanguages, such as multilingual processing and class imbalance. Our approach\nachieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804\nfor Sub-tasks A, B, and C respectively. This work provides insights into the\neffectiveness of transformer models in tasks with domain-specific and\nlinguistic challenges, as well as areas for potential improvement in future\niterations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a detailed system description of our entry for the\nCHiPSAL 2025 shared task, focusing on language detection, hate speech\nidentification, and target detection in Devanagari script languages. We\nexperimented with a combination of large language models and their ensembles,\nincluding MuRIL, IndicBERT, and Gemma-2, and leveraged unique techniques like\nfocal loss to address challenges in the natural understanding of Devanagari\nlanguages, such as multilingual processing and class imbalance. Our approach\nachieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804\nfor Sub-tasks A, B, and C respectively. This work provides insights into the\neffectiveness of transformer models in tasks with domain-specific and\nlinguistic challenges, as well as areas for potential improvement in future\niterations."
                },
                "authors": [
                    {
                        "name": "Jebish Purbey"
                    },
                    {
                        "name": "Siddartha Pullakhandam"
                    },
                    {
                        "name": "Kanwal Mehreen"
                    },
                    {
                        "name": "Muhammad Arham"
                    },
                    {
                        "name": "Drishti Sharma"
                    },
                    {
                        "name": "Ashay Srivastava"
                    },
                    {
                        "name": "Ram Mohan Rao Kadiyala"
                    }
                ],
                "author_detail": {
                    "name": "Ram Mohan Rao Kadiyala"
                },
                "author": "Ram Mohan Rao Kadiyala",
                "arxiv_comment": "13 pages, Submitted to CHIPSAL workshop @ COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06839v1",
                "updated": "2024-11-11T10:07:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    7,
                    51,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T10:07:51Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    7,
                    51,
                    0,
                    316,
                    0
                ],
                "title": "LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language\n  Models"
                },
                "summary": "In this paper, we propose a novel LLM-Neo framework that efficiently\ntransfers knowledge from a large language model (LLM) teacher to a compact\nstudent. Initially, we revisit the knowledge distillation (KD) and low-rank\nadaption (LoRA), and argue that they share the same paradigm. Inspired by this\nobservation, we explore the strategy that combines LoRA and KD to enhance the\nefficiency of knowledge transfer. We first summarize some guidelines for this\ndesign and further develop the LLM-Neo. Experimental results on compressing\nLlama 2 and Llama 3 show that LLM-Neo outperforms various baselines. Further\nanalysis demonstrates the robustness of the proposed LLM-Neo on variants of\nLoRA. The trained models have been available at\n\\href{https://huggingface.co/collections/yang31210999/llm-neo-66e3c882f5579b829ff57eba}{this\nrepository}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel LLM-Neo framework that efficiently\ntransfers knowledge from a large language model (LLM) teacher to a compact\nstudent. Initially, we revisit the knowledge distillation (KD) and low-rank\nadaption (LoRA), and argue that they share the same paradigm. Inspired by this\nobservation, we explore the strategy that combines LoRA and KD to enhance the\nefficiency of knowledge transfer. We first summarize some guidelines for this\ndesign and further develop the LLM-Neo. Experimental results on compressing\nLlama 2 and Llama 3 show that LLM-Neo outperforms various baselines. Further\nanalysis demonstrates the robustness of the proposed LLM-Neo on variants of\nLoRA. The trained models have been available at\n\\href{https://huggingface.co/collections/yang31210999/llm-neo-66e3c882f5579b829ff57eba}{this\nrepository}."
                },
                "authors": [
                    {
                        "name": "Runming Yang"
                    },
                    {
                        "name": "Taiqiang Wu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Pengfei Hu"
                    },
                    {
                        "name": "Ngai Wong"
                    },
                    {
                        "name": "Yujiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yujiu Yang"
                },
                "author": "Yujiu Yang",
                "arxiv_comment": "ICASSP 25' under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06837v1",
                "updated": "2024-11-11T10:05:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    5,
                    52,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T10:05:52Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    5,
                    52,
                    0,
                    316,
                    0
                ],
                "title": "Persuasion with Large Language Models: a Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasion with Large Language Models: a Survey"
                },
                "summary": "The rapid rise of Large Language Models (LLMs) has created new disruptive\npossibilities for persuasive communication, by enabling fully-automated\npersonalized and interactive content generation at an unprecedented scale. In\nthis paper, we survey the research field of LLM-based persuasion that has\nemerged as a result. We begin by exploring the different modes in which LLM\nSystems are used to influence human attitudes and behaviors. In areas such as\npolitics, marketing, public health, e-commerce, and charitable giving, such LLM\nSystems have already achieved human-level or even super-human persuasiveness.\nWe identify key factors influencing their effectiveness, such as the manner of\npersonalization and whether the content is labelled as AI-generated. We also\nsummarize the experimental designs that have been used to evaluate progress.\nOur survey suggests that the current and future potential of LLM-based\npersuasion poses profound ethical and societal risks, including the spread of\nmisinformation, the magnification of biases, and the invasion of privacy. These\nrisks underscore the urgent need for ethical guidelines and updated regulatory\nframeworks to avoid the widespread deployment of irresponsible and harmful LLM\nSystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of Large Language Models (LLMs) has created new disruptive\npossibilities for persuasive communication, by enabling fully-automated\npersonalized and interactive content generation at an unprecedented scale. In\nthis paper, we survey the research field of LLM-based persuasion that has\nemerged as a result. We begin by exploring the different modes in which LLM\nSystems are used to influence human attitudes and behaviors. In areas such as\npolitics, marketing, public health, e-commerce, and charitable giving, such LLM\nSystems have already achieved human-level or even super-human persuasiveness.\nWe identify key factors influencing their effectiveness, such as the manner of\npersonalization and whether the content is labelled as AI-generated. We also\nsummarize the experimental designs that have been used to evaluate progress.\nOur survey suggests that the current and future potential of LLM-based\npersuasion poses profound ethical and societal risks, including the spread of\nmisinformation, the magnification of biases, and the invasion of privacy. These\nrisks underscore the urgent need for ethical guidelines and updated regulatory\nframeworks to avoid the widespread deployment of irresponsible and harmful LLM\nSystems."
                },
                "authors": [
                    {
                        "name": "Alexander Rogiers"
                    },
                    {
                        "name": "Sander Noels"
                    },
                    {
                        "name": "Maarten Buyl"
                    },
                    {
                        "name": "Tijl De Bie"
                    }
                ],
                "author_detail": {
                    "name": "Tijl De Bie"
                },
                "author": "Tijl De Bie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04242v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04242v3",
                "updated": "2024-11-11T10:03:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    3,
                    47,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-06T20:11:19Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    20,
                    11,
                    19,
                    2,
                    311,
                    0
                ],
                "title": "Multimodal Structure-Aware Quantum Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Structure-Aware Quantum Data Processing"
                },
                "summary": "While large language models (LLMs) have advanced the field of natural\nlanguage processing (NLP), their \"black box\" nature obscures their\ndecision-making processes. To address this, researchers developed structured\napproaches using higher order tensors. These are able to model linguistic\nrelations, but stall when training on classical computers due to their\nexcessive size. Tensors are natural inhabitants of quantum systems and training\non quantum computers provides a solution by translating text to variational\nquantum circuits. In this paper, we develop MultiQ-NLP: a framework for\nstructure-aware data processing with multimodal text+image data. Here,\n\"structure\" refers to syntactic and grammatical relationships in language, as\nwell as the hierarchical organization of visual elements in images. We enrich\nthe translation with new types and type homomorphisms and develop novel\narchitectures to represent structure. When tested on a main stream image\nclassification task (SVO Probes), our best model showed a par performance with\nthe state of the art classical models; moreover the best model was fully\nstructured.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have advanced the field of natural\nlanguage processing (NLP), their \"black box\" nature obscures their\ndecision-making processes. To address this, researchers developed structured\napproaches using higher order tensors. These are able to model linguistic\nrelations, but stall when training on classical computers due to their\nexcessive size. Tensors are natural inhabitants of quantum systems and training\non quantum computers provides a solution by translating text to variational\nquantum circuits. In this paper, we develop MultiQ-NLP: a framework for\nstructure-aware data processing with multimodal text+image data. Here,\n\"structure\" refers to syntactic and grammatical relationships in language, as\nwell as the hierarchical organization of visual elements in images. We enrich\nthe translation with new types and type homomorphisms and develop novel\narchitectures to represent structure. When tested on a main stream image\nclassification task (SVO Probes), our best model showed a par performance with\nthe state of the art classical models; moreover the best model was fully\nstructured."
                },
                "authors": [
                    {
                        "name": "Hala Hawashin"
                    },
                    {
                        "name": "Mehrnoosh Sadrzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehrnoosh Sadrzadeh"
                },
                "author": "Mehrnoosh Sadrzadeh",
                "arxiv_comment": "10 Pages, 16 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04242v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04242v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68T50, 68Q12, 68U15, 68U10, 81P45, 81P68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.10; H.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06835v1",
                "updated": "2024-11-11T10:02:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    2,
                    49,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T10:02:49Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    2,
                    49,
                    0,
                    316,
                    0
                ],
                "title": "HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of\n  Quantization on Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of\n  Quantization on Model Alignment"
                },
                "summary": "With the introduction of the transformers architecture, LLMs have\nrevolutionized the NLP field with ever more powerful models. Nevertheless,\ntheir development came up with several challenges. The exponential growth in\ncomputational power and reasoning capabilities of language models has\nheightened concerns about their security. As models become more powerful,\nensuring their safety has become a crucial focus in research. This paper aims\nto address gaps in the current literature on jailbreaking techniques and the\nevaluation of LLM vulnerabilities. Our contributions include the creation of a\nnovel dataset designed to assess the harmfulness of model outputs across\nmultiple harm levels, as well as a focus on fine-grained harm-level analysis.\nUsing this framework, we provide a comprehensive benchmark of state-of-the-art\njailbreaking attacks, specifically targeting the Vicuna 13B v1.5 model.\nAdditionally, we examine how quantization techniques, such as AWQ and GPTQ,\ninfluence the alignment and robustness of models, revealing trade-offs between\nenhanced robustness with regards to transfer attacks and potential increases in\nvulnerability on direct ones. This study aims to demonstrate the influence of\nharmful input queries on the complexity of jailbreaking techniques, as well as\nto deepen our understanding of LLM vulnerabilities and improve methods for\nassessing model robustness when confronted with harmful content, particularly\nin the context of compression strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the introduction of the transformers architecture, LLMs have\nrevolutionized the NLP field with ever more powerful models. Nevertheless,\ntheir development came up with several challenges. The exponential growth in\ncomputational power and reasoning capabilities of language models has\nheightened concerns about their security. As models become more powerful,\nensuring their safety has become a crucial focus in research. This paper aims\nto address gaps in the current literature on jailbreaking techniques and the\nevaluation of LLM vulnerabilities. Our contributions include the creation of a\nnovel dataset designed to assess the harmfulness of model outputs across\nmultiple harm levels, as well as a focus on fine-grained harm-level analysis.\nUsing this framework, we provide a comprehensive benchmark of state-of-the-art\njailbreaking attacks, specifically targeting the Vicuna 13B v1.5 model.\nAdditionally, we examine how quantization techniques, such as AWQ and GPTQ,\ninfluence the alignment and robustness of models, revealing trade-offs between\nenhanced robustness with regards to transfer attacks and potential increases in\nvulnerability on direct ones. This study aims to demonstrate the influence of\nharmful input queries on the complexity of jailbreaking techniques, as well as\nto deepen our understanding of LLM vulnerabilities and improve methods for\nassessing model robustness when confronted with harmful content, particularly\nin the context of compression strategies."
                },
                "authors": [
                    {
                        "name": "Yannis Belkhiter"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Sergio Maffeis"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Maffeis"
                },
                "author": "Sergio Maffeis",
                "arxiv_comment": "NeurIPS 2024 Workshop on Safe Generative Artificial Intelligence\n  (SafeGenAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13704v2",
                "updated": "2024-11-11T10:02:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    2,
                    24,
                    0,
                    316,
                    0
                ],
                "published": "2024-09-05T10:27:32Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    10,
                    27,
                    32,
                    3,
                    249,
                    0
                ],
                "title": "Entity Extraction from High-Level Corruption Schemes via Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity Extraction from High-Level Corruption Schemes via Large Language\n  Models"
                },
                "summary": "The rise of financial crime that has been observed in recent years has\ncreated an increasing concern around the topic and many people, organizations\nand governments are more and more frequently trying to combat it. Despite the\nincrease of interest in this area, there is a lack of specialized datasets that\ncan be used to train and evaluate works that try to tackle those problems. This\narticle proposes a new micro-benchmark dataset for algorithms and models that\nidentify individuals and organizations, and their multiple writings, in news\narticles, and presents an approach that assists in its creation. Experimental\nefforts are also reported, using this dataset, to identify individuals and\norganizations in financial-crime-related articles using various low-billion\nparameter Large Language Models (LLMs). For these experiments, standard metrics\n(Accuracy, Precision, Recall, F1 Score) are reported and various prompt\nvariants comprising the best practices of prompt engineering are tested. In\naddition, to address the problem of ambiguous entity mentions, a simple, yet\neffective LLM-based disambiguation method is proposed, ensuring that the\nevaluation aligns with reality. Finally, the proposed approach is compared\nagainst a widely used state-of-the-art open-source baseline, showing the\nsuperiority of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of financial crime that has been observed in recent years has\ncreated an increasing concern around the topic and many people, organizations\nand governments are more and more frequently trying to combat it. Despite the\nincrease of interest in this area, there is a lack of specialized datasets that\ncan be used to train and evaluate works that try to tackle those problems. This\narticle proposes a new micro-benchmark dataset for algorithms and models that\nidentify individuals and organizations, and their multiple writings, in news\narticles, and presents an approach that assists in its creation. Experimental\nefforts are also reported, using this dataset, to identify individuals and\norganizations in financial-crime-related articles using various low-billion\nparameter Large Language Models (LLMs). For these experiments, standard metrics\n(Accuracy, Precision, Recall, F1 Score) are reported and various prompt\nvariants comprising the best practices of prompt engineering are tested. In\naddition, to address the problem of ambiguous entity mentions, a simple, yet\neffective LLM-based disambiguation method is proposed, ensuring that the\nevaluation aligns with reality. Finally, the proposed approach is compared\nagainst a widely used state-of-the-art open-source baseline, showing the\nsuperiority of the proposed method."
                },
                "authors": [
                    {
                        "name": "Panagiotis Koletsis"
                    },
                    {
                        "name": "Panagiotis-Konstantinos Gemos"
                    },
                    {
                        "name": "Christos Chronis"
                    },
                    {
                        "name": "Iraklis Varlamis"
                    },
                    {
                        "name": "Vasilis Efthymiou"
                    },
                    {
                        "name": "Georgios Th. Papadopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Th. Papadopoulos"
                },
                "author": "Georgios Th. Papadopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08928v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08928v4",
                "updated": "2024-11-11T09:56:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    56,
                    59,
                    0,
                    316,
                    0
                ],
                "published": "2024-09-13T15:45:40Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    15,
                    45,
                    40,
                    4,
                    257,
                    0
                ],
                "title": "Self-Organizing State-Space Models with Artificial Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Organizing State-Space Models with Artificial Dynamics"
                },
                "summary": "We consider a state-space model (SSM) parametrized by some parameter $\\theta$\nand aim at performing joint parameter and state inference. A popular idea to\ncarry out this task is to replace $\\theta$ by a Markov chain $(\\theta_t)_{t\\geq\n0}$ and then to apply a filtering algorithm to the extended, or self-organizing\nSSM (SO-SSM). However, the practical implementation of this idea in a\ntheoretically justified way has remained an open problem. In this paper we fill\nthis gap by introducing constructions of $(\\theta_t)_{t\\geq 0}$ that ensure the\nvalidity of the SO-SSM for joint parameter and state inference. Notably, we\nshow that such SO-SSMs can be defined even if\n$\\|\\mathrm{Var}(\\theta_{t}|\\theta_{t-1})\\|\\rightarrow 0$ slowly as\n$t\\rightarrow\\infty$. This result is important since these models can be\nefficiently approximated using a particle filter. While SO-SSMs have been\nintroduced for online inference, the development of iterated filtering (IF) has\nshown that they can also serve for computing the maximum likelihood estimator\nof an SSM. We also derive constructions of $(\\theta_t)_{t\\geq 0}$ and\ntheoretical guarantees tailored to these specific applications of SO-SSMs and\nintroduce new IF algorithms. From a practical point of view, the algorithms we\ndevelop are simple to implement and only require minimal tuning to perform\nwell.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a state-space model (SSM) parametrized by some parameter $\\theta$\nand aim at performing joint parameter and state inference. A popular idea to\ncarry out this task is to replace $\\theta$ by a Markov chain $(\\theta_t)_{t\\geq\n0}$ and then to apply a filtering algorithm to the extended, or self-organizing\nSSM (SO-SSM). However, the practical implementation of this idea in a\ntheoretically justified way has remained an open problem. In this paper we fill\nthis gap by introducing constructions of $(\\theta_t)_{t\\geq 0}$ that ensure the\nvalidity of the SO-SSM for joint parameter and state inference. Notably, we\nshow that such SO-SSMs can be defined even if\n$\\|\\mathrm{Var}(\\theta_{t}|\\theta_{t-1})\\|\\rightarrow 0$ slowly as\n$t\\rightarrow\\infty$. This result is important since these models can be\nefficiently approximated using a particle filter. While SO-SSMs have been\nintroduced for online inference, the development of iterated filtering (IF) has\nshown that they can also serve for computing the maximum likelihood estimator\nof an SSM. We also derive constructions of $(\\theta_t)_{t\\geq 0}$ and\ntheoretical guarantees tailored to these specific applications of SO-SSMs and\nintroduce new IF algorithms. From a practical point of view, the algorithms we\ndevelop are simple to implement and only require minimal tuning to perform\nwell."
                },
                "authors": [
                    {
                        "name": "Yuan Chen"
                    },
                    {
                        "name": "Mathieu Gerber"
                    },
                    {
                        "name": "Christophe Andrieu"
                    },
                    {
                        "name": "Randal Douc"
                    }
                ],
                "author_detail": {
                    "name": "Randal Douc"
                },
                "author": "Randal Douc",
                "arxiv_comment": "102 pages (28 pages for the paper, 6 for the appendix and 68 for the\n  supplementary material), 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08928v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08928v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06833v1",
                "updated": "2024-11-11T09:51:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    51,
                    22,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T09:51:22Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    51,
                    22,
                    0,
                    316,
                    0
                ],
                "title": "Learning Interpretable Network Dynamics via Universal Neural Symbolic\n  Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Interpretable Network Dynamics via Universal Neural Symbolic\n  Regression"
                },
                "summary": "Discovering governing equations of complex network dynamics is a fundamental\nchallenge in contemporary science with rich data, which can uncover the\nmysterious patterns and mechanisms of the formation and evolution of complex\nphenomena in various fields and assist in decision-making. In this work, we\ndevelop a universal computational tool that can automatically, efficiently, and\naccurately learn the symbolic changing patterns of complex system states by\ncombining the excellent fitting ability from deep learning and the equation\ninference ability from pre-trained symbolic regression. We conduct intensive\nexperimental verifications on more than ten representative scenarios from\nphysics, biochemistry, ecology, epidemiology, etc. Results demonstrate the\noutstanding effectiveness and efficiency of our tool by comparing with the\nstate-of-the-art symbolic regression techniques for network dynamics. The\napplication to real-world systems including global epidemic transmission and\npedestrian movements has verified its practical applicability. We believe that\nour tool can serve as a universal solution to dispel the fog of hidden\nmechanisms of changes in complex phenomena, advance toward interpretability,\nand inspire more scientific discoveries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering governing equations of complex network dynamics is a fundamental\nchallenge in contemporary science with rich data, which can uncover the\nmysterious patterns and mechanisms of the formation and evolution of complex\nphenomena in various fields and assist in decision-making. In this work, we\ndevelop a universal computational tool that can automatically, efficiently, and\naccurately learn the symbolic changing patterns of complex system states by\ncombining the excellent fitting ability from deep learning and the equation\ninference ability from pre-trained symbolic regression. We conduct intensive\nexperimental verifications on more than ten representative scenarios from\nphysics, biochemistry, ecology, epidemiology, etc. Results demonstrate the\noutstanding effectiveness and efficiency of our tool by comparing with the\nstate-of-the-art symbolic regression techniques for network dynamics. The\napplication to real-world systems including global epidemic transmission and\npedestrian movements has verified its practical applicability. We believe that\nour tool can serve as a universal solution to dispel the fog of hidden\nmechanisms of changes in complex phenomena, advance toward interpretability,\nand inspire more scientific discoveries."
                },
                "authors": [
                    {
                        "name": "Jiao Hu"
                    },
                    {
                        "name": "Jiaxu Cui"
                    },
                    {
                        "name": "Bo Yang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yang"
                },
                "author": "Bo Yang",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06828v1",
                "updated": "2024-11-11T09:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    40,
                    36,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T09:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    40,
                    36,
                    0,
                    316,
                    0
                ],
                "title": "Test-Time Training with Quantum Auto-Encoder: From Distribution Shift to\n  Noisy Quantum Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Training with Quantum Auto-Encoder: From Distribution Shift to\n  Noisy Quantum Circuits"
                },
                "summary": "In this paper, we propose test-time training with the quantum auto-encoder\n(QTTT). QTTT adapts to (1) data distribution shifts between training and\ntesting data and (2) quantum circuit error by minimizing the self-supervised\nloss of the quantum auto-encoder. Empirically, we show that QTTT is robust\nagainst data distribution shifts and effective in mitigating random unitary\nnoise in the quantum circuits during the inference. Additionally, we establish\nthe theoretical performance guarantee of the QTTT architecture. Our novel\nframework presents a significant advancement in developing quantum neural\nnetworks for future real-world applications and functions as a plug-and-play\nextension for quantum machine learning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose test-time training with the quantum auto-encoder\n(QTTT). QTTT adapts to (1) data distribution shifts between training and\ntesting data and (2) quantum circuit error by minimizing the self-supervised\nloss of the quantum auto-encoder. Empirically, we show that QTTT is robust\nagainst data distribution shifts and effective in mitigating random unitary\nnoise in the quantum circuits during the inference. Additionally, we establish\nthe theoretical performance guarantee of the QTTT architecture. Our novel\nframework presents a significant advancement in developing quantum neural\nnetworks for future real-world applications and functions as a plug-and-play\nextension for quantum machine learning models."
                },
                "authors": [
                    {
                        "name": "Damien Jian"
                    },
                    {
                        "name": "Yu-Chao Huang"
                    },
                    {
                        "name": "Hsi-Sheng Goan"
                    }
                ],
                "author_detail": {
                    "name": "Hsi-Sheng Goan"
                },
                "author": "Hsi-Sheng Goan",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06824v1",
                "updated": "2024-11-11T09:32:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    32,
                    20,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T09:32:20Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    32,
                    20,
                    0,
                    316,
                    0
                ],
                "title": "Combining Domain and Alignment Vectors to Achieve Better\n  Knowledge-Safety Trade-offs in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Domain and Alignment Vectors to Achieve Better\n  Knowledge-Safety Trade-offs in LLMs"
                },
                "summary": "There is a growing interest in training domain-expert LLMs that excel in\nspecific technical fields compared to their general-purpose instruction-tuned\ncounterparts. However, these expert models often experience a loss in their\nsafety abilities in the process, making them capable of generating harmful\ncontent. As a solution, we introduce an efficient and effective merging-based\nalignment method called \\textsc{MergeAlign} that interpolates the domain and\nalignment vectors, creating safer domain-specific models while preserving their\nutility. We apply \\textsc{MergeAlign} on Llama3 variants that are experts in\nmedicine and finance, obtaining substantial alignment improvements with minimal\nto no degradation on domain-specific benchmarks. We study the impact of model\nmerging through model similarity metrics and contributions of individual models\nbeing merged. We hope our findings open new research avenues and inspire more\nefficient development of safe expert LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing interest in training domain-expert LLMs that excel in\nspecific technical fields compared to their general-purpose instruction-tuned\ncounterparts. However, these expert models often experience a loss in their\nsafety abilities in the process, making them capable of generating harmful\ncontent. As a solution, we introduce an efficient and effective merging-based\nalignment method called \\textsc{MergeAlign} that interpolates the domain and\nalignment vectors, creating safer domain-specific models while preserving their\nutility. We apply \\textsc{MergeAlign} on Llama3 variants that are experts in\nmedicine and finance, obtaining substantial alignment improvements with minimal\nto no degradation on domain-specific benchmarks. We study the impact of model\nmerging through model similarity metrics and contributions of individual models\nbeing merged. We hope our findings open new research avenues and inspire more\nefficient development of safe expert LLMs."
                },
                "authors": [
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Yash More"
                    },
                    {
                        "name": "Quentin Fournier"
                    },
                    {
                        "name": "Matthew Riemer"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Amal Zouaq"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Sarath Chandar"
                    }
                ],
                "author_detail": {
                    "name": "Sarath Chandar"
                },
                "author": "Sarath Chandar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06823v1",
                "updated": "2024-11-11T09:31:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    31,
                    46,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T09:31:46Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    31,
                    46,
                    0,
                    316,
                    0
                ],
                "title": "Large Language Model in Medical Informatics: Direct Classification and\n  Enhanced Text Representations for Automatic ICD Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model in Medical Informatics: Direct Classification and\n  Enhanced Text Representations for Automatic ICD Coding"
                },
                "summary": "Addressing the complexity of accurately classifying International\nClassification of Diseases (ICD) codes from medical discharge summaries is\nchallenging due to the intricate nature of medical documentation. This paper\nexplores the use of Large Language Models (LLM), specifically the LLAMA\narchitecture, to enhance ICD code classification through two methodologies:\ndirect application as a classifier and as a generator of enriched text\nrepresentations within a Multi-Filter Residual Convolutional Neural Network\n(MultiResCNN) framework. We evaluate these methods by comparing them against\nstate-of-the-art approaches, revealing LLAMA's potential to significantly\nimprove classification outcomes by providing deep contextual insights into\nmedical texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the complexity of accurately classifying International\nClassification of Diseases (ICD) codes from medical discharge summaries is\nchallenging due to the intricate nature of medical documentation. This paper\nexplores the use of Large Language Models (LLM), specifically the LLAMA\narchitecture, to enhance ICD code classification through two methodologies:\ndirect application as a classifier and as a generator of enriched text\nrepresentations within a Multi-Filter Residual Convolutional Neural Network\n(MultiResCNN) framework. We evaluate these methods by comparing them against\nstate-of-the-art approaches, revealing LLAMA's potential to significantly\nimprove classification outcomes by providing deep contextual insights into\nmedical texts."
                },
                "authors": [
                    {
                        "name": "Zeyd Boukhers"
                    },
                    {
                        "name": "AmeerAli Khan"
                    },
                    {
                        "name": "Qusai Ramadan"
                    },
                    {
                        "name": "Cong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Cong Yang"
                },
                "author": "Cong Yang",
                "arxiv_comment": "accepted at the 2024 IEEE International Conference on Bioinformatics\n  and Biomedicine (BIBM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.10792v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.10792v7",
                "updated": "2024-11-11T09:25:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    25,
                    48,
                    0,
                    316,
                    0
                ],
                "published": "2023-08-21T15:35:16Z",
                "published_parsed": [
                    2023,
                    8,
                    21,
                    15,
                    35,
                    16,
                    0,
                    233,
                    0
                ],
                "title": "Instruction Tuning for Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Tuning for Large Language Models: A Survey"
                },
                "summary": "This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), which can also be referred to as supervised\nfine-tuning (SFT)\\footnote{In this paper, unless specified otherwise,\ninstruction tuning (IT) will be equivalent to supervised fine-tuning (SFT).}, a\ncrucial technique to enhance the capabilities and controllability of large\nlanguage models (LLMs). Instruction tuning refers to the process of further\ntraining LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs\nin a supervised fashion, which bridges the gap between the next-word prediction\nobjective of LLMs and the users' objective of having LLMs adhere to human\ninstructions. In this work, we make a systematic review of the literature,\nincluding the general methodology of IT, the construction of IT datasets, the\ntraining of IT models, and applications to different modalities, domains and\napplication, along with analysis on aspects that influence the outcome of IT\n(e.g., generation of instruction outputs, size of the instruction dataset,\netc). We also review the potential pitfalls of IT along with criticism against\nit, along with efforts pointing out current deficiencies of existing strategies\nand suggest some avenues for fruitful research.Project page:\ngithub.com/xiaoya-li/Instruction-Tuning-Survey",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), which can also be referred to as supervised\nfine-tuning (SFT)\\footnote{In this paper, unless specified otherwise,\ninstruction tuning (IT) will be equivalent to supervised fine-tuning (SFT).}, a\ncrucial technique to enhance the capabilities and controllability of large\nlanguage models (LLMs). Instruction tuning refers to the process of further\ntraining LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs\nin a supervised fashion, which bridges the gap between the next-word prediction\nobjective of LLMs and the users' objective of having LLMs adhere to human\ninstructions. In this work, we make a systematic review of the literature,\nincluding the general methodology of IT, the construction of IT datasets, the\ntraining of IT models, and applications to different modalities, domains and\napplication, along with analysis on aspects that influence the outcome of IT\n(e.g., generation of instruction outputs, size of the instruction dataset,\netc). We also review the potential pitfalls of IT along with criticism against\nit, along with efforts pointing out current deficiencies of existing strategies\nand suggest some avenues for fruitful research.Project page:\ngithub.com/xiaoya-li/Instruction-Tuning-Survey"
                },
                "authors": [
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Linfeng Dong"
                    },
                    {
                        "name": "Xiaoya Li"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Xiaofei Sun"
                    },
                    {
                        "name": "Shuhe Wang"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Runyi Hu"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Guoyin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoyin Wang"
                },
                "author": "Guoyin Wang",
                "arxiv_comment": "V4; Last update: Nov 11, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.10792v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.10792v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03911v2",
                "updated": "2024-11-11T09:23:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    23,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-05-07T00:08:15Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    0,
                    8,
                    15,
                    1,
                    128,
                    0
                ],
                "title": "Federated Graph Condensation with Information Bottleneck Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Graph Condensation with Information Bottleneck Principles"
                },
                "summary": "Graph condensation, which reduces the size of a large-scale graph by\nsynthesizing a small-scale condensed graph as its substitution, has immediately\nbenefited various graph learning tasks. However, existing graph condensation\nmethods rely on centralized data storage, which is unfeasible for real-world\ndecentralized data distribution, and overlook data holders' privacy-preserving\nrequirements. To bridge the gap, we propose and study the novel problem of\nfederated graph condensation for graph neural networks (GNNs). Specifically, we\nfirst propose a general framework for federated graph condensation, in which we\ndecouple the typical gradient matching process for graph condensation into\nclient-side gradient calculation and server-side gradient matching. In this\nway, the burdensome computation cost in client-side is largely alleviated.\nBesides, our empirical studies show that under the federated setting, the\ncondensed graph will consistently leak data membership privacy, i.e., the\ncondensed graph during the federated training can be utilized to steal the\ntraining data under the membership inference attacks (MIA). To tackle this\nissue, we innovatively incorporate information bottleneck principles into the\nfederated graph condensation, which only needs to extract partial node features\nin one local pre-training step and utilize the features during federated\ntraining. Extensive experiments on real-world datasets demonstrate that our\nframework can consistently protect membership privacy during training.\nMeanwhile, it also achieves comparable and even superior performance against\nexisting centralized graph condensation and federated graph learning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph condensation, which reduces the size of a large-scale graph by\nsynthesizing a small-scale condensed graph as its substitution, has immediately\nbenefited various graph learning tasks. However, existing graph condensation\nmethods rely on centralized data storage, which is unfeasible for real-world\ndecentralized data distribution, and overlook data holders' privacy-preserving\nrequirements. To bridge the gap, we propose and study the novel problem of\nfederated graph condensation for graph neural networks (GNNs). Specifically, we\nfirst propose a general framework for federated graph condensation, in which we\ndecouple the typical gradient matching process for graph condensation into\nclient-side gradient calculation and server-side gradient matching. In this\nway, the burdensome computation cost in client-side is largely alleviated.\nBesides, our empirical studies show that under the federated setting, the\ncondensed graph will consistently leak data membership privacy, i.e., the\ncondensed graph during the federated training can be utilized to steal the\ntraining data under the membership inference attacks (MIA). To tackle this\nissue, we innovatively incorporate information bottleneck principles into the\nfederated graph condensation, which only needs to extract partial node features\nin one local pre-training step and utilize the features during federated\ntraining. Extensive experiments on real-world datasets demonstrate that our\nframework can consistently protect membership privacy during training.\nMeanwhile, it also achieves comparable and even superior performance against\nexisting centralized graph condensation and federated graph learning methods."
                },
                "authors": [
                    {
                        "name": "Bo Yan"
                    },
                    {
                        "name": "Sihao He"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Yang Cao"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09056v2",
                "updated": "2024-11-11T09:19:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    19,
                    46,
                    0,
                    316,
                    0
                ],
                "published": "2024-06-13T12:43:40Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    12,
                    43,
                    40,
                    3,
                    165,
                    0
                ],
                "title": "CUDRT: Benchmarking the Detection Models of Human vs. Large Language\n  Models Generated Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUDRT: Benchmarking the Detection Models of Human vs. Large Language\n  Models Generated Texts"
                },
                "summary": "While large language models (LLMs) have greatly enhanced text generation\nacross industries, their human-like outputs make distinguishing between human\nand AI authorship challenging. Although many LLM-generated text detectors\nexist, current benchmarks mainly rely on static datasets, limiting their\neffectiveness in assessing model-based detectors requiring prior training.\nFurthermore, these benchmarks focus on specific scenarios like question\nanswering and text refinement and are primarily limited to English, overlooking\nbroader linguistic applications and LLM subtleties. To address these gaps, we\nconstruct a comprehensive bilingual benchmark in Chinese and English to\nrigorously evaluate mainstream LLM-generated text detection methods. We\ncategorize LLM text generation into five key operations-Create, Update, Delete,\nRewrite, and Translate (CUDRT)-covering the full range of LLM activities. For\neach CUDRT category, we developed extensive datasets enabling thorough\nassessment of detection performance, incorporating the latest mainstream LLMs\nfor each language. We also establish a robust evaluation framework to support\nscalable, reproducible experiments, facilitating an in-depth analysis of how\nLLM operations, different LLMs, datasets, and multilingual training sets impact\ndetector performance, particularly for model-based methods. Our extensive\nexperiments provide critical insights for optimizing LLM-generated text\ndetectors and suggest future directions to improve detection accuracy and\ngeneralization across diverse scenarios.Source code and dataset are available\nat GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have greatly enhanced text generation\nacross industries, their human-like outputs make distinguishing between human\nand AI authorship challenging. Although many LLM-generated text detectors\nexist, current benchmarks mainly rely on static datasets, limiting their\neffectiveness in assessing model-based detectors requiring prior training.\nFurthermore, these benchmarks focus on specific scenarios like question\nanswering and text refinement and are primarily limited to English, overlooking\nbroader linguistic applications and LLM subtleties. To address these gaps, we\nconstruct a comprehensive bilingual benchmark in Chinese and English to\nrigorously evaluate mainstream LLM-generated text detection methods. We\ncategorize LLM text generation into five key operations-Create, Update, Delete,\nRewrite, and Translate (CUDRT)-covering the full range of LLM activities. For\neach CUDRT category, we developed extensive datasets enabling thorough\nassessment of detection performance, incorporating the latest mainstream LLMs\nfor each language. We also establish a robust evaluation framework to support\nscalable, reproducible experiments, facilitating an in-depth analysis of how\nLLM operations, different LLMs, datasets, and multilingual training sets impact\ndetector performance, particularly for model-based methods. Our extensive\nexperiments provide critical insights for optimizing LLM-generated text\ndetectors and suggest future directions to improve detection accuracy and\ngeneralization across diverse scenarios.Source code and dataset are available\nat GitHub."
                },
                "authors": [
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Yanfang Chen"
                    },
                    {
                        "name": "Dinghao Xi"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.09220v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.09220v3",
                "updated": "2024-11-11T09:16:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    16,
                    56,
                    0,
                    316,
                    0
                ],
                "published": "2024-05-15T09:59:37Z",
                "published_parsed": [
                    2024,
                    5,
                    15,
                    9,
                    59,
                    37,
                    2,
                    136,
                    0
                ],
                "title": "ALPINE: Unveiling the Planning Capability of Autoregressive Learning in\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALPINE: Unveiling the Planning Capability of Autoregressive Learning in\n  Language Models"
                },
                "summary": "Planning is a crucial element of both human intelligence and contemporary\nlarge language models (LLMs). In this paper, we initiate a theoretical\ninvestigation into the emergence of planning capabilities in Transformer-based\nLLMs via their next-word prediction mechanisms. We model planning as a network\npath-finding task, where the objective is to generate a valid path from a\nspecified source node to a designated target node. Our mathematical\ncharacterization shows that Transformer architectures can execute path-finding\nby embedding the adjacency and reachability matrices within their weights.\nFurthermore, our theoretical analysis of gradient-based learning dynamics\nreveals that LLMs can learn both the adjacency and a limited form of the\nreachability matrices. These theoretical insights are then validated through\nexperiments, which demonstrate that Transformer architectures indeed learn the\nadjacency and an incomplete reachability matrices, consistent with our\ntheoretical predictions. When applying our methodology to the real-world\nplanning benchmark Blocksworld, our observations remain consistent.\nAdditionally, our analyses uncover a fundamental limitation of current\nTransformer architectures in path-finding: these architectures cannot identify\nreachability relationships through transitivity, which leads to failures in\ngenerating paths when concatenation is required. These findings provide new\ninsights into how the internal mechanisms of autoregressive learning facilitate\nintelligent planning and deepen our understanding of how future LLMs might\nachieve more advanced and general planning-and-reasoning capabilities across\ndiverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning is a crucial element of both human intelligence and contemporary\nlarge language models (LLMs). In this paper, we initiate a theoretical\ninvestigation into the emergence of planning capabilities in Transformer-based\nLLMs via their next-word prediction mechanisms. We model planning as a network\npath-finding task, where the objective is to generate a valid path from a\nspecified source node to a designated target node. Our mathematical\ncharacterization shows that Transformer architectures can execute path-finding\nby embedding the adjacency and reachability matrices within their weights.\nFurthermore, our theoretical analysis of gradient-based learning dynamics\nreveals that LLMs can learn both the adjacency and a limited form of the\nreachability matrices. These theoretical insights are then validated through\nexperiments, which demonstrate that Transformer architectures indeed learn the\nadjacency and an incomplete reachability matrices, consistent with our\ntheoretical predictions. When applying our methodology to the real-world\nplanning benchmark Blocksworld, our observations remain consistent.\nAdditionally, our analyses uncover a fundamental limitation of current\nTransformer architectures in path-finding: these architectures cannot identify\nreachability relationships through transitivity, which leads to failures in\ngenerating paths when concatenation is required. These findings provide new\ninsights into how the internal mechanisms of autoregressive learning facilitate\nintelligent planning and deepen our understanding of how future LLMs might\nachieve more advanced and general planning-and-reasoning capabilities across\ndiverse applications."
                },
                "authors": [
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Yifei Shen"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Shang-Hua Teng"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.09220v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.09220v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06807v1",
                "updated": "2024-11-11T09:03:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    3,
                    58,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T09:03:58Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    3,
                    58,
                    0,
                    316,
                    0
                ],
                "title": "Wavehax: Aliasing-Free Neural Waveform Synthesis Based on 2D Convolution\n  and Harmonic Prior for Reliable Complex Spectrogram Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wavehax: Aliasing-Free Neural Waveform Synthesis Based on 2D Convolution\n  and Harmonic Prior for Reliable Complex Spectrogram Estimation"
                },
                "summary": "Neural vocoders often struggle with aliasing in latent feature spaces, caused\nby time-domain nonlinear operations and resampling layers. Aliasing folds\nhigh-frequency components into the low-frequency range, making aliased and\noriginal frequency components indistinguishable and introducing two practical\nissues. First, aliasing complicates the waveform generation process, as the\nsubsequent layers must address these aliasing effects, increasing the\ncomputational complexity. Second, it limits extrapolation performance,\nparticularly in handling high fundamental frequencies, which degrades the\nperceptual quality of generated speech waveforms. This paper demonstrates that\n1) time-domain nonlinear operations inevitably introduce aliasing but provide a\nstrong inductive bias for harmonic generation, and 2) time-frequency-domain\nprocessing can achieve aliasing-free waveform synthesis but lacks the inductive\nbias for effective harmonic generation. Building on this insight, we propose\nWavehax, an aliasing-free neural WAVEform generator that integrates 2D\nconvolution and a HArmonic prior for reliable Complex Spectrogram estimation.\nExperimental results show that Wavehax achieves speech quality comparable to\nexisting high-fidelity neural vocoders and exhibits exceptional robustness in\nscenarios requiring high fundamental frequency extrapolation, where aliasing\neffects become typically severe. Moreover, Wavehax requires less than 5% of the\nmultiply-accumulate operations and model parameters compared to HiFi-GAN V1,\nwhile achieving over four times faster CPU inference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural vocoders often struggle with aliasing in latent feature spaces, caused\nby time-domain nonlinear operations and resampling layers. Aliasing folds\nhigh-frequency components into the low-frequency range, making aliased and\noriginal frequency components indistinguishable and introducing two practical\nissues. First, aliasing complicates the waveform generation process, as the\nsubsequent layers must address these aliasing effects, increasing the\ncomputational complexity. Second, it limits extrapolation performance,\nparticularly in handling high fundamental frequencies, which degrades the\nperceptual quality of generated speech waveforms. This paper demonstrates that\n1) time-domain nonlinear operations inevitably introduce aliasing but provide a\nstrong inductive bias for harmonic generation, and 2) time-frequency-domain\nprocessing can achieve aliasing-free waveform synthesis but lacks the inductive\nbias for effective harmonic generation. Building on this insight, we propose\nWavehax, an aliasing-free neural WAVEform generator that integrates 2D\nconvolution and a HArmonic prior for reliable Complex Spectrogram estimation.\nExperimental results show that Wavehax achieves speech quality comparable to\nexisting high-fidelity neural vocoders and exhibits exceptional robustness in\nscenarios requiring high fundamental frequency extrapolation, where aliasing\neffects become typically severe. Moreover, Wavehax requires less than 5% of the\nmultiply-accumulate operations and model parameters compared to HiFi-GAN V1,\nwhile achieving over four times faster CPU inference speed."
                },
                "authors": [
                    {
                        "name": "Reo Yoneyama"
                    },
                    {
                        "name": "Atsushi Miyashita"
                    },
                    {
                        "name": "Ryuichi Yamamoto"
                    },
                    {
                        "name": "Tomoki Toda"
                    }
                ],
                "author_detail": {
                    "name": "Tomoki Toda"
                },
                "author": "Tomoki Toda",
                "arxiv_comment": "13 pages, 5 figures, Submitted to IEEE/ACM Trans. ASLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06805v1",
                "updated": "2024-11-11T09:03:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    3,
                    52,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T09:03:52Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    3,
                    52,
                    0,
                    316,
                    0
                ],
                "title": "AssistRAG: Boosting the Potential of Large Language Models with an\n  Intelligent Information Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AssistRAG: Boosting the Potential of Large Language Models with an\n  Intelligent Information Assistant"
                },
                "summary": "The emergence of Large Language Models (LLMs) has significantly advanced\nnatural language processing, but these models often generate factually\nincorrect information, known as \"hallucination\". Initial retrieval-augmented\ngeneration (RAG) methods like the \"Retrieve-Read\" framework was inadequate for\ncomplex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised\nFine-Tuning (SFT) methods improved performance but required frequent retraining\nand risked altering foundational LLM capabilities. To cope with these\nchallenges, we propose Assistant-based Retrieval-Augmented Generation\n(AssistRAG), integrating an intelligent information assistant within LLMs. This\nassistant manages memory and knowledge through tool usage, action execution,\nmemory building, and plan specification. Using a two-phase training approach,\nCurriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG\nenhances information retrieval and decision-making. Experiments show AssistRAG\nsignificantly outperforms benchmarks, especially benefiting less advanced LLMs,\nby providing superior reasoning capabilities and accurate responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) has significantly advanced\nnatural language processing, but these models often generate factually\nincorrect information, known as \"hallucination\". Initial retrieval-augmented\ngeneration (RAG) methods like the \"Retrieve-Read\" framework was inadequate for\ncomplex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised\nFine-Tuning (SFT) methods improved performance but required frequent retraining\nand risked altering foundational LLM capabilities. To cope with these\nchallenges, we propose Assistant-based Retrieval-Augmented Generation\n(AssistRAG), integrating an intelligent information assistant within LLMs. This\nassistant manages memory and knowledge through tool usage, action execution,\nmemory building, and plan specification. Using a two-phase training approach,\nCurriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG\nenhances information retrieval and decision-making. Experiments show AssistRAG\nsignificantly outperforms benchmarks, especially benefiting less advanced LLMs,\nby providing superior reasoning capabilities and accurate responses."
                },
                "authors": [
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Accepted by NeurIPS 2024 (poster)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06804v1",
                "updated": "2024-11-11T09:01:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    1,
                    36,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T09:01:36Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    1,
                    36,
                    0,
                    316,
                    0
                ],
                "title": "Predicting ionic conductivity in solids from the machine-learned\n  potential energy landscape",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting ionic conductivity in solids from the machine-learned\n  potential energy landscape"
                },
                "summary": "Discovering new superionic materials is essential for advancing solid-state\nbatteries, which offer improved energy density and safety compared to the\ntraditional lithium-ion batteries with liquid electrolytes. Conventional\ncomputational methods for identifying such materials are resource-intensive and\nnot easily scalable. Recently, universal interatomic potential models have been\ndeveloped using equivariant graph neural networks. These models are trained on\nextensive datasets of first-principles force and energy calculations. One can\nachieve significant computational advantages by leveraging them as the\nfoundation for traditional methods of assessing the ionic conductivity, such as\nmolecular dynamics or nudged elastic band techniques. However, the\ngeneralization error from model inference on diverse atomic structures arising\nin such calculations can compromise the reliability of the results. In this\nwork, we propose an approach for the quick and reliable evaluation of ionic\nconductivity through the analysis of a universal interatomic potential. Our\nmethod incorporates a set of heuristic structure descriptors that effectively\nemploy the rich knowledge of the underlying model while requiring minimal\ngeneralization capabilities. Using our descriptors, we rank lithium-containing\nmaterials in the Materials Project database according to their expected ionic\nconductivity. Eight out of the ten highest-ranked materials are confirmed to be\nsuperionic at room temperature in first-principles calculations. Notably, our\nmethod achieves a speed-up factor of approximately 50 compared to molecular\ndynamics driven by a machine-learning potential, and is at least 3,000 times\nfaster compared to first-principles molecular dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering new superionic materials is essential for advancing solid-state\nbatteries, which offer improved energy density and safety compared to the\ntraditional lithium-ion batteries with liquid electrolytes. Conventional\ncomputational methods for identifying such materials are resource-intensive and\nnot easily scalable. Recently, universal interatomic potential models have been\ndeveloped using equivariant graph neural networks. These models are trained on\nextensive datasets of first-principles force and energy calculations. One can\nachieve significant computational advantages by leveraging them as the\nfoundation for traditional methods of assessing the ionic conductivity, such as\nmolecular dynamics or nudged elastic band techniques. However, the\ngeneralization error from model inference on diverse atomic structures arising\nin such calculations can compromise the reliability of the results. In this\nwork, we propose an approach for the quick and reliable evaluation of ionic\nconductivity through the analysis of a universal interatomic potential. Our\nmethod incorporates a set of heuristic structure descriptors that effectively\nemploy the rich knowledge of the underlying model while requiring minimal\ngeneralization capabilities. Using our descriptors, we rank lithium-containing\nmaterials in the Materials Project database according to their expected ionic\nconductivity. Eight out of the ten highest-ranked materials are confirmed to be\nsuperionic at room temperature in first-principles calculations. Notably, our\nmethod achieves a speed-up factor of approximately 50 compared to molecular\ndynamics driven by a machine-learning potential, and is at least 3,000 times\nfaster compared to first-principles molecular dynamics."
                },
                "authors": [
                    {
                        "name": "Artem Maevskiy"
                    },
                    {
                        "name": "Alexandra Carvalho"
                    },
                    {
                        "name": "Emil Sataev"
                    },
                    {
                        "name": "Volha Turchyna"
                    },
                    {
                        "name": "Keian Noori"
                    },
                    {
                        "name": "Aleksandr Rodin"
                    },
                    {
                        "name": "A. H. Castro Neto"
                    },
                    {
                        "name": "Andrey Ustyuzhanin"
                    }
                ],
                "author_detail": {
                    "name": "Andrey Ustyuzhanin"
                },
                "author": "Andrey Ustyuzhanin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06802v1",
                "updated": "2024-11-11T08:57:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    57,
                    44,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T08:57:44Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    57,
                    44,
                    0,
                    316,
                    0
                ],
                "title": "Identifying the impact of local connectivity patterns on dynamics in\n  excitatory-inhibitory networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying the impact of local connectivity patterns on dynamics in\n  excitatory-inhibitory networks"
                },
                "summary": "Networks of excitatory and inhibitory (EI) neurons form a canonical circuit\nin the brain. Seminal theoretical results on dynamics of such networks are\nbased on the assumption that synaptic strengths depend on the type of neurons\nthey connect, but are otherwise statistically independent. Recent synaptic\nphysiology datasets however highlight the prominence of specific connectivity\npatterns that go well beyond what is expected from independent connections.\nWhile decades of influential research have demonstrated the strong role of the\nbasic EI cell type structure, to which extent additional connectivity features\ninfluence dynamics remains to be fully determined. Here we examine the effects\nof pairwise connectivity motifs on the linear dynamics in EI networks using an\nanalytical framework that approximates the connectivity in terms of low-rank\nstructures. This low-rank approximation is based on a mathematical derivation\nof the dominant eigenvalues of the connectivity matrix and predicts the impact\non responses to external inputs of connectivity motifs and their interactions\nwith cell-type structure. Our results reveal that a particular pattern of\nconnectivity, chain motifs, have a much stronger impact on dominant eigenmodes\nthan other pairwise motifs. An overrepresentation of chain motifs induces a\nstrong positive eigenvalue in inhibition-dominated networks and generates a\npotential instability that requires revisiting the classical\nexcitation-inhibition balance criteria. Examining effects of external inputs,\nwe show that chain motifs can on their own induce paradoxical responses where\nan increased input to inhibitory neurons leads to a decrease in their activity\ndue to the recurrent feedback. These findings have direct implications for the\ninterpretation of experiments in which responses to optogenetic perturbations\nare measured and used to infer the dynamical regime of cortical circuits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Networks of excitatory and inhibitory (EI) neurons form a canonical circuit\nin the brain. Seminal theoretical results on dynamics of such networks are\nbased on the assumption that synaptic strengths depend on the type of neurons\nthey connect, but are otherwise statistically independent. Recent synaptic\nphysiology datasets however highlight the prominence of specific connectivity\npatterns that go well beyond what is expected from independent connections.\nWhile decades of influential research have demonstrated the strong role of the\nbasic EI cell type structure, to which extent additional connectivity features\ninfluence dynamics remains to be fully determined. Here we examine the effects\nof pairwise connectivity motifs on the linear dynamics in EI networks using an\nanalytical framework that approximates the connectivity in terms of low-rank\nstructures. This low-rank approximation is based on a mathematical derivation\nof the dominant eigenvalues of the connectivity matrix and predicts the impact\non responses to external inputs of connectivity motifs and their interactions\nwith cell-type structure. Our results reveal that a particular pattern of\nconnectivity, chain motifs, have a much stronger impact on dominant eigenmodes\nthan other pairwise motifs. An overrepresentation of chain motifs induces a\nstrong positive eigenvalue in inhibition-dominated networks and generates a\npotential instability that requires revisiting the classical\nexcitation-inhibition balance criteria. Examining effects of external inputs,\nwe show that chain motifs can on their own induce paradoxical responses where\nan increased input to inhibitory neurons leads to a decrease in their activity\ndue to the recurrent feedback. These findings have direct implications for the\ninterpretation of experiments in which responses to optogenetic perturbations\nare measured and used to infer the dynamical regime of cortical circuits."
                },
                "authors": [
                    {
                        "name": "Yuxiu Shao"
                    },
                    {
                        "name": "David Dahmen"
                    },
                    {
                        "name": "Stefano Recanatesi"
                    },
                    {
                        "name": "Eric Shea-Brown"
                    },
                    {
                        "name": "Srdjan Ostojic"
                    }
                ],
                "author_detail": {
                    "name": "Srdjan Ostojic"
                },
                "arxiv_affiliation": "Laboratoire de Neurosciences Cognitives et Computationnelles, INSERM U960, Ecole Normale Superieure - PSL Research University, Paris, France",
                "author": "Srdjan Ostojic",
                "arxiv_comment": "25 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06796v1",
                "updated": "2024-11-11T08:50:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    50,
                    24,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T08:50:24Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    50,
                    24,
                    0,
                    316,
                    0
                ],
                "title": "Automatically Write Code Checker: An LLM-based Approach with\n  Logic-guided API Retrieval and Case by Case Iteration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Write Code Checker: An LLM-based Approach with\n  Logic-guided API Retrieval and Case by Case Iteration"
                },
                "summary": "With the rising demand for code quality assurance, developers are not only\nutilizing existing static code checkers but also seeking custom checkers to\nsatisfy their specific needs. Nowadays, various code-checking frameworks\nprovide extensive checker customization interfaces to meet this need. However,\nboth the abstract checking logic as well as the complex API usage of\nlarge-scale frameworks make this task challenging. To this end, automated code\nchecker generation is anticipated to ease the burden of checker development. In\nthis paper, we explore the feasibility of automated checker generation and\npropose AutoChecker, an innovative LLM-powered approach that can write code\ncheckers automatically based on only a rule description and a test suite.\nInstead of generating the checker at once, AutoChecker incrementally updates\nthe checker with the rule and one single test case each time, i.e., it\niteratively generates the checker case by case. During each iteration,\nAutoChecker first decomposes the whole logic into a series of sub-operations\nand then uses the logic-guided API-context retrieval strategy to search related\nAPI-contexts from all the framework APIs. To evaluate the effectiveness of\nAutoChecker, we apply AutoChecker and two LLM-based baseline approaches to\nautomatically generate checkers for 20 built-in PMD rules, including easy rules\nand hard rules. Experimental results demonstrate that AutoChecker significantly\noutperforms baseline approaches across all effectiveness metrics, where its\naverage test pass rate improved over 4.2 times. Moreover, the checkers\ngenerated by AutoChecker are successfully applied to real-world projects,\nmatching the performance of official checkers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for code quality assurance, developers are not only\nutilizing existing static code checkers but also seeking custom checkers to\nsatisfy their specific needs. Nowadays, various code-checking frameworks\nprovide extensive checker customization interfaces to meet this need. However,\nboth the abstract checking logic as well as the complex API usage of\nlarge-scale frameworks make this task challenging. To this end, automated code\nchecker generation is anticipated to ease the burden of checker development. In\nthis paper, we explore the feasibility of automated checker generation and\npropose AutoChecker, an innovative LLM-powered approach that can write code\ncheckers automatically based on only a rule description and a test suite.\nInstead of generating the checker at once, AutoChecker incrementally updates\nthe checker with the rule and one single test case each time, i.e., it\niteratively generates the checker case by case. During each iteration,\nAutoChecker first decomposes the whole logic into a series of sub-operations\nand then uses the logic-guided API-context retrieval strategy to search related\nAPI-contexts from all the framework APIs. To evaluate the effectiveness of\nAutoChecker, we apply AutoChecker and two LLM-based baseline approaches to\nautomatically generate checkers for 20 built-in PMD rules, including easy rules\nand hard rules. Experimental results demonstrate that AutoChecker significantly\noutperforms baseline approaches across all effectiveness metrics, where its\naverage test pass rate improved over 4.2 times. Moreover, the checkers\ngenerated by AutoChecker are successfully applied to real-world projects,\nmatching the performance of official checkers."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Xie"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Jiwei Yan"
                    },
                    {
                        "name": "Jinhao Huang"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11607v2",
                "updated": "2024-11-11T08:44:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    44,
                    33,
                    0,
                    316,
                    0
                ],
                "published": "2024-06-17T14:56:44Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    14,
                    56,
                    44,
                    0,
                    169,
                    0
                ],
                "title": "Multi-spectral Sirens: Gravitational-wave Cosmology with (Multi-)\n  Sub-populations of Binary Black Holes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-spectral Sirens: Gravitational-wave Cosmology with (Multi-)\n  Sub-populations of Binary Black Holes"
                },
                "summary": "The cosmic expansion rate can be directly measured with gravitational-wave\n(GW) data of the compact binary mergers by jointly constraining the mass\nfunction of the population and the cosmological model via the so-called\nspectral sirens. Such a method relies on the features in the mass functions,\nwhich may originate from some individual subpopulations, and hence become\nblurred/indistinct due to the superposition of different subpopulations. In\nthis work we propose a novel approach to constrain the cosmic expansion rate\nwith subpopulations of GW events, named multi-spectral sirens. The advantage of\nthe multi-spectral sirens compared to the traditional spectral sirens is\ndemonstrated by the simulation with the mock data. The application of this\napproach to the GWTC-3 data yields $H_0=73.3^{+29.9}_{-25.6}~{\\rm\nMpc}^{-1}~{\\rm km}~{\\rm s}^{-1}$ (median and symmetric 68.3\\% credible\ninterval), which is about 19\\% tighter than the result inferred with the\ntraditional spectral sirens utilizing a PowerLaw+Peak mass function. The\nincorporation of the bright standard siren GW170817 with a uniform prior in\n[10,200] (log-uniform prior in [20,140]) ${\\rm Mpc}^{-1}~{\\rm km}~{\\rm s}^{-1}$\ngives $H_0=71.1^{+15.0}_{-7.5}~(70.3^{+12.9}_{-7.1})~{\\rm Mpc}^{-1}~{\\rm\nkm}~{\\rm s}^{-1}$ (68.3\\% confidence level), corresponding to an improvement of\n$\\sim26\\%$ (23\\%) with respect to the measurement from sole GW170817.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cosmic expansion rate can be directly measured with gravitational-wave\n(GW) data of the compact binary mergers by jointly constraining the mass\nfunction of the population and the cosmological model via the so-called\nspectral sirens. Such a method relies on the features in the mass functions,\nwhich may originate from some individual subpopulations, and hence become\nblurred/indistinct due to the superposition of different subpopulations. In\nthis work we propose a novel approach to constrain the cosmic expansion rate\nwith subpopulations of GW events, named multi-spectral sirens. The advantage of\nthe multi-spectral sirens compared to the traditional spectral sirens is\ndemonstrated by the simulation with the mock data. The application of this\napproach to the GWTC-3 data yields $H_0=73.3^{+29.9}_{-25.6}~{\\rm\nMpc}^{-1}~{\\rm km}~{\\rm s}^{-1}$ (median and symmetric 68.3\\% credible\ninterval), which is about 19\\% tighter than the result inferred with the\ntraditional spectral sirens utilizing a PowerLaw+Peak mass function. The\nincorporation of the bright standard siren GW170817 with a uniform prior in\n[10,200] (log-uniform prior in [20,140]) ${\\rm Mpc}^{-1}~{\\rm km}~{\\rm s}^{-1}$\ngives $H_0=71.1^{+15.0}_{-7.5}~(70.3^{+12.9}_{-7.1})~{\\rm Mpc}^{-1}~{\\rm\nkm}~{\\rm s}^{-1}$ (68.3\\% confidence level), corresponding to an improvement of\n$\\sim26\\%$ (23\\%) with respect to the measurement from sole GW170817."
                },
                "authors": [
                    {
                        "name": "Yin-Jie Li"
                    },
                    {
                        "name": "Shao-Peng Tang"
                    },
                    {
                        "name": "Yuan-Zhu Wang"
                    },
                    {
                        "name": "Yi-Zhong Fan"
                    }
                ],
                "author_detail": {
                    "name": "Yi-Zhong Fan"
                },
                "author": "Yi-Zhong Fan",
                "arxiv_doi": "10.3847/1538-4357/ad888b",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad888b",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.11607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, 11 figures, 2 tables; Accepted for publication in ApJ",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06790v1",
                "updated": "2024-11-11T08:36:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    36,
                    49,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T08:36:49Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    36,
                    49,
                    0,
                    316,
                    0
                ],
                "title": "Large-scale moral machine experiment on large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale moral machine experiment on large language models"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) and their potential\nintegration into autonomous driving systems necessitates understanding their\nmoral decision-making capabilities. While our previous study examined four\nprominent LLMs using the Moral Machine experimental framework, the dynamic\nlandscape of LLM development demands a more comprehensive analysis. Here, we\nevaluate moral judgments across 51 different LLMs, including multiple versions\nof proprietary models (GPT, Claude, Gemini) and open-source alternatives\n(Llama, Gemma), to assess their alignment with human moral preferences in\nautonomous driving scenarios. Using a conjoint analysis framework, we evaluated\nhow closely LLM responses aligned with human preferences in ethical dilemmas\nand examined the effects of model size, updates, and architecture. Results\nshowed that proprietary models and open-source models exceeding 10 billion\nparameters demonstrated relatively close alignment with human judgments, with a\nsignificant negative correlation between model size and distance from human\njudgments in open-source models. However, model updates did not consistently\nimprove alignment with human preferences, and many LLMs showed excessive\nemphasis on specific ethical principles. These findings suggest that while\nincreasing model size may naturally lead to more human-like moral judgments,\npractical implementation in autonomous driving systems requires careful\nconsideration of the trade-off between judgment quality and computational\nefficiency. Our comprehensive analysis provides crucial insights for the\nethical design of autonomous systems and highlights the importance of\nconsidering cultural contexts in AI moral decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) and their potential\nintegration into autonomous driving systems necessitates understanding their\nmoral decision-making capabilities. While our previous study examined four\nprominent LLMs using the Moral Machine experimental framework, the dynamic\nlandscape of LLM development demands a more comprehensive analysis. Here, we\nevaluate moral judgments across 51 different LLMs, including multiple versions\nof proprietary models (GPT, Claude, Gemini) and open-source alternatives\n(Llama, Gemma), to assess their alignment with human moral preferences in\nautonomous driving scenarios. Using a conjoint analysis framework, we evaluated\nhow closely LLM responses aligned with human preferences in ethical dilemmas\nand examined the effects of model size, updates, and architecture. Results\nshowed that proprietary models and open-source models exceeding 10 billion\nparameters demonstrated relatively close alignment with human judgments, with a\nsignificant negative correlation between model size and distance from human\njudgments in open-source models. However, model updates did not consistently\nimprove alignment with human preferences, and many LLMs showed excessive\nemphasis on specific ethical principles. These findings suggest that while\nincreasing model size may naturally lead to more human-like moral judgments,\npractical implementation in autonomous driving systems requires careful\nconsideration of the trade-off between judgment quality and computational\nefficiency. Our comprehensive analysis provides crucial insights for the\nethical design of autonomous systems and highlights the importance of\nconsidering cultural contexts in AI moral decision-making."
                },
                "authors": [
                    {
                        "name": "Muhammad Shahrul Zaim bin Ahmad"
                    },
                    {
                        "name": "Kazuhiro Takemoto"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhiro Takemoto"
                },
                "author": "Kazuhiro Takemoto",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06774v1",
                "updated": "2024-11-11T08:05:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    5,
                    37,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T08:05:37Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    5,
                    37,
                    0,
                    316,
                    0
                ],
                "title": "The First Prompt Counts the Most! An Evaluation of Large Language Models\n  on Iterative Example-based Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The First Prompt Counts the Most! An Evaluation of Large Language Models\n  on Iterative Example-based Code Generation"
                },
                "summary": "The capabilities of Large Language Models (LLMs) in code generation,\nparticularly for implementing target functionalities from natural language\ndescriptions, have been extensively studied. As an alternative form of natural\nlanguage, input-output examples (I/O examples) provide an accessible,\nunambiguous, and flexible way to describe functionalities, but the diversity,\nsparseness, and incompleteness of I/O examples also place challenges on\nunderstanding and implementing requirements. Therefore, generating code from\ninput-output examples (i.e., example-based code generation) provides a new\nperspective, allowing us to evaluate LLMs' capability to infer target\nfunctionalities from limited information and to process new-form requirements.\nHowever, related research about LLMs in example-based code generation remains\nlargely unexplored. To fill this gap, this paper presents the first\ncomprehensive study on example-based code generation using LLMs. To address the\nincorrectness caused by the incompleteness of I/O examples, we adopt an\niterative evaluation framework and formalize the objective of example-based\ncode generation as two sequential sub-objectives: generating code conforming to\ngiven examples and generating code that successfully implements the target\nfunctionalities from (iteratively) given examples. We assess six\nstate-of-the-art LLMs using a new benchmark of 168 diverse target\nfunctionalities. The results demonstrate that when requirements were described\nusing iterative I/O examples rather than natural language, the LLMs' score\ndecreased by over 60%, indicating that example-based code generation remains\nchallenging for the evaluated LLMs. More interestingly, the vast majority (even\nover 95%) of successfully implemented functionalities are achieved in the first\nround of iterations, suggesting that the LLMs struggle to effectively utilize\nthe iteratively supplemented requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities of Large Language Models (LLMs) in code generation,\nparticularly for implementing target functionalities from natural language\ndescriptions, have been extensively studied. As an alternative form of natural\nlanguage, input-output examples (I/O examples) provide an accessible,\nunambiguous, and flexible way to describe functionalities, but the diversity,\nsparseness, and incompleteness of I/O examples also place challenges on\nunderstanding and implementing requirements. Therefore, generating code from\ninput-output examples (i.e., example-based code generation) provides a new\nperspective, allowing us to evaluate LLMs' capability to infer target\nfunctionalities from limited information and to process new-form requirements.\nHowever, related research about LLMs in example-based code generation remains\nlargely unexplored. To fill this gap, this paper presents the first\ncomprehensive study on example-based code generation using LLMs. To address the\nincorrectness caused by the incompleteness of I/O examples, we adopt an\niterative evaluation framework and formalize the objective of example-based\ncode generation as two sequential sub-objectives: generating code conforming to\ngiven examples and generating code that successfully implements the target\nfunctionalities from (iteratively) given examples. We assess six\nstate-of-the-art LLMs using a new benchmark of 168 diverse target\nfunctionalities. The results demonstrate that when requirements were described\nusing iterative I/O examples rather than natural language, the LLMs' score\ndecreased by over 60%, indicating that example-based code generation remains\nchallenging for the evaluated LLMs. More interestingly, the vast majority (even\nover 95%) of successfully implemented functionalities are achieved in the first\nround of iterations, suggesting that the LLMs struggle to effectively utilize\nthe iteratively supplemented requirements."
                },
                "authors": [
                    {
                        "name": "Yingjie Fu"
                    },
                    {
                        "name": "Bozhou Li"
                    },
                    {
                        "name": "Linyi Li"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06767v1",
                "updated": "2024-11-11T07:47:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    7,
                    47,
                    20,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T07:47:20Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    7,
                    47,
                    20,
                    0,
                    316,
                    0
                ],
                "title": "PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing"
                },
                "summary": "Code Large Language Models (Code LLMs), such as Code llama and\nDeepSeek-Coder, have demonstrated exceptional performance in the code\ngeneration tasks. However, most existing models focus on the abilities of\ngenerating correct code, but often struggle with bug repair. We introduce a\nsuit of methods to enhance LLM's SQL bug-fixing abilities. The methods are\nmainly consisted of two parts: A Progressive Dataset Construction (PDC) from\nscratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data\nexpansion methods from the perspectives of breadth first and depth first\nrespectively. DM-SFT introduces an efficient bug-fixing supervised learning\napproach, which effectively reduce the total training steps and mitigate the\n\"disorientation\" in SQL code bug-fixing training. In our evaluation, the code\nLLM models trained with two methods have exceeds all current best performing\nmodel which size is much larger.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Large Language Models (Code LLMs), such as Code llama and\nDeepSeek-Coder, have demonstrated exceptional performance in the code\ngeneration tasks. However, most existing models focus on the abilities of\ngenerating correct code, but often struggle with bug repair. We introduce a\nsuit of methods to enhance LLM's SQL bug-fixing abilities. The methods are\nmainly consisted of two parts: A Progressive Dataset Construction (PDC) from\nscratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data\nexpansion methods from the perspectives of breadth first and depth first\nrespectively. DM-SFT introduces an efficient bug-fixing supervised learning\napproach, which effectively reduce the total training steps and mitigate the\n\"disorientation\" in SQL code bug-fixing training. In our evaluation, the code\nLLM models trained with two methods have exceeds all current best performing\nmodel which size is much larger."
                },
                "authors": [
                    {
                        "name": "Yiwen Duan"
                    },
                    {
                        "name": "Yonghong Yu"
                    },
                    {
                        "name": "Xiaoming Zhao"
                    },
                    {
                        "name": "Yichang Wu"
                    },
                    {
                        "name": "Wenbo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wenbo Liu"
                },
                "author": "Wenbo Liu",
                "arxiv_comment": "COLING-Industry 2025 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16758v2",
                "updated": "2024-11-11T07:34:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    7,
                    34,
                    25,
                    0,
                    316,
                    0
                ],
                "published": "2024-06-24T16:06:50Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    16,
                    6,
                    50,
                    0,
                    176,
                    0
                ],
                "title": "Towards Fast Multilingual LLM Inference: Speculative Decoding and\n  Specialized Drafters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fast Multilingual LLM Inference: Speculative Decoding and\n  Specialized Drafters"
                },
                "summary": "Large language models (LLMs) have revolutionized natural language processing\nand broadened their applicability across diverse commercial applications.\nHowever, the deployment of these models is constrained by high inference time\nin multilingual settings. To mitigate this challenge, this paper explores a\ntraining recipe of an assistant model in speculative decoding, which is\nleveraged to draft and-then its future tokens are verified by the target LLM.\nWe show that language-specific draft models, optimized through a targeted\npretrain-and-finetune strategy, substantially brings a speedup in inference\ntime compared to the previous methods. We validate these models across various\nlanguages in inference time, out-of-domain speedup, and GPT-4o evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized natural language processing\nand broadened their applicability across diverse commercial applications.\nHowever, the deployment of these models is constrained by high inference time\nin multilingual settings. To mitigate this challenge, this paper explores a\ntraining recipe of an assistant model in speculative decoding, which is\nleveraged to draft and-then its future tokens are verified by the target LLM.\nWe show that language-specific draft models, optimized through a targeted\npretrain-and-finetune strategy, substantially brings a speedup in inference\ntime compared to the previous methods. We validate these models across various\nlanguages in inference time, out-of-domain speedup, and GPT-4o evaluation."
                },
                "authors": [
                    {
                        "name": "Euiin Yi"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hongseok Jeung"
                    },
                    {
                        "name": "Du-Seong Chang"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.15997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.15997v2",
                "updated": "2024-11-11T07:27:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    7,
                    27,
                    3,
                    0,
                    316,
                    0
                ],
                "published": "2023-07-29T14:47:07Z",
                "published_parsed": [
                    2023,
                    7,
                    29,
                    14,
                    47,
                    7,
                    5,
                    210,
                    0
                ],
                "title": "RoCar: A Relationship Network-based Evaluation Method for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoCar: A Relationship Network-based Evaluation Method for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have received increasing attention. However, due\nto the complexity of its capabilities, how to rationally evaluate the\ncapabilities of LLMs is still a task to be solved. We propose the RoCar method,\nwhich utilizes the defined basic schemas to randomly construct a task graph and\ngenerates natural language evaluation tasks based on the task graph to evaluate\nthe reasoning and memory abilities of LLMs respectively. Due to the very large\nrandomness of the task construction process, it is possible to ensure that none\nof the LLMs to be tested has directly learned the evaluation tasks,\nguaranteeing the fairness of the evaluation method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have received increasing attention. However, due\nto the complexity of its capabilities, how to rationally evaluate the\ncapabilities of LLMs is still a task to be solved. We propose the RoCar method,\nwhich utilizes the defined basic schemas to randomly construct a task graph and\ngenerates natural language evaluation tasks based on the task graph to evaluate\nthe reasoning and memory abilities of LLMs respectively. Due to the very large\nrandomness of the task construction process, it is possible to ensure that none\nof the LLMs to be tested has directly learned the evaluation tasks,\nguaranteeing the fairness of the evaluation method."
                },
                "authors": [
                    {
                        "name": "Ming Wang"
                    },
                    {
                        "name": "Wenfang Wu"
                    },
                    {
                        "name": "Chongyun Gao"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Yifei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Zhang"
                },
                "author": "Yifei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.15997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.15997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.07240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07240v1",
                "updated": "2024-11-11T18:59:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    59,
                    2,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T18:59:02Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    59,
                    2,
                    0,
                    316,
                    0
                ],
                "title": "UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UTMath: Math Evaluation with Unit Test via Reasoning-to-Coding Thoughts"
                },
                "summary": "The evaluation of mathematical reasoning capabilities is essential for\nadvancing Artificial General Intelligence (AGI). While Large Language Models\n(LLMs) have shown impressive performance in solving mathematical problems,\nexisting benchmarks such as GSM8K and MATH present limitations, including\nnarrow problem definitions with specific numbers and reliance on predetermined\nrules that hinder accurate assessments of reasoning and adaptability. This\npaper introduces the UTMath Benchmark, which robustly evaluates the models\nthrough extensive unit tests. It consists of 1,053 problems across 9\nmathematical domains, with over 68 test cases per problem.We propose an\ninnovative evaluation framework inspired by unit testing in software\ndevelopment, focusing on both accuracy and reliability of results. Furthermore,\nwe introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which\nencourages LLMs to perform explicit reasoning before generating code, leading\nto generating more advanced solution and improved performance. Furthermore, we\nare releasing not only the UTMath benchmark but also the UTMath-Train training\ndataset (more than 70k samples), to support the community in further exploring\nmathematical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of mathematical reasoning capabilities is essential for\nadvancing Artificial General Intelligence (AGI). While Large Language Models\n(LLMs) have shown impressive performance in solving mathematical problems,\nexisting benchmarks such as GSM8K and MATH present limitations, including\nnarrow problem definitions with specific numbers and reliance on predetermined\nrules that hinder accurate assessments of reasoning and adaptability. This\npaper introduces the UTMath Benchmark, which robustly evaluates the models\nthrough extensive unit tests. It consists of 1,053 problems across 9\nmathematical domains, with over 68 test cases per problem.We propose an\ninnovative evaluation framework inspired by unit testing in software\ndevelopment, focusing on both accuracy and reliability of results. Furthermore,\nwe introduce the Reasoning-to-Coding of Thoughts (RCoT) approach, which\nencourages LLMs to perform explicit reasoning before generating code, leading\nto generating more advanced solution and improved performance. Furthermore, we\nare releasing not only the UTMath benchmark but also the UTMath-Train training\ndataset (more than 70k samples), to support the community in further exploring\nmathematical reasoning."
                },
                "authors": [
                    {
                        "name": "Bo Yang"
                    },
                    {
                        "name": "Qingping Yang"
                    },
                    {
                        "name": "Runtao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Runtao Liu"
                },
                "author": "Runtao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07238v1",
                "updated": "2024-11-11T18:58:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    58,
                    46,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T18:58:46Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    58,
                    46,
                    0,
                    316,
                    0
                ],
                "title": "OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenThaiGPT 1.5: A Thai-Centric Open Source Large Language Model"
                },
                "summary": "OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,\nfinetuned on over 2,000,000 Thai instruction pairs. This report provides an\nengineering perspective on the model's development, capabilities, and\nperformance. We discuss the model's architecture, training process, and key\nfeatures, including multi-turn conversation support, Retrieval Augmented\nGeneration (RAG) compatibility, and tool-calling functionality. Benchmark\nresults demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various\nThai language tasks, outperforming other open-source Thai language models. We\nalso address practical considerations such as GPU memory requirements and\ndeployment strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenThaiGPT 1.5 is an advanced Thai language chat model based on Qwen v2.5,\nfinetuned on over 2,000,000 Thai instruction pairs. This report provides an\nengineering perspective on the model's development, capabilities, and\nperformance. We discuss the model's architecture, training process, and key\nfeatures, including multi-turn conversation support, Retrieval Augmented\nGeneration (RAG) compatibility, and tool-calling functionality. Benchmark\nresults demonstrate OpenThaiGPT 1.5's state-of-the-art performance on various\nThai language tasks, outperforming other open-source Thai language models. We\nalso address practical considerations such as GPU memory requirements and\ndeployment strategies."
                },
                "authors": [
                    {
                        "name": "Sumeth Yuenyong"
                    },
                    {
                        "name": "Kobkrit Viriyayudhakorn"
                    },
                    {
                        "name": "Apivadee Piyatumrong"
                    },
                    {
                        "name": "Jillaphat Jaroenkantasima"
                    }
                ],
                "author_detail": {
                    "name": "Jillaphat Jaroenkantasima"
                },
                "author": "Jillaphat Jaroenkantasima",
                "arxiv_comment": "8 pages, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07228v1",
                "updated": "2024-11-11T18:46:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    46,
                    37,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T18:46:37Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    46,
                    37,
                    0,
                    316,
                    0
                ],
                "title": "Tooling or Not Tooling? The Impact of Tools on Language Agents for\n  Chemistry Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tooling or Not Tooling? The Impact of Tools on Language Agents for\n  Chemistry Problem Solving"
                },
                "summary": "To enhance large language models (LLMs) for chemistry problem solving,\nseveral LLM-based agents augmented with tools have been proposed, such as\nChemCrow and Coscientist. However, their evaluations are narrow in scope,\nleaving a large gap in understanding the benefits of tools across diverse\nchemistry tasks. To bridge this gap, we develop ChemAgent, an enhanced\nchemistry agent over ChemCrow, and conduct a comprehensive evaluation of its\nperformance on both specialized chemistry tasks and general chemistry\nquestions. Surprisingly, ChemAgent does not consistently outperform its base\nLLMs without tools. Our error analysis with a chemistry expert suggests that:\nFor specialized chemistry tasks, such as synthesis prediction, we should\naugment agents with specialized tools; however, for general chemistry questions\nlike those in exams, agents' ability to reason correctly with chemistry\nknowledge matters more, and tool augmentation does not always help.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To enhance large language models (LLMs) for chemistry problem solving,\nseveral LLM-based agents augmented with tools have been proposed, such as\nChemCrow and Coscientist. However, their evaluations are narrow in scope,\nleaving a large gap in understanding the benefits of tools across diverse\nchemistry tasks. To bridge this gap, we develop ChemAgent, an enhanced\nchemistry agent over ChemCrow, and conduct a comprehensive evaluation of its\nperformance on both specialized chemistry tasks and general chemistry\nquestions. Surprisingly, ChemAgent does not consistently outperform its base\nLLMs without tools. Our error analysis with a chemistry expert suggests that:\nFor specialized chemistry tasks, such as synthesis prediction, we should\naugment agents with specialized tools; however, for general chemistry questions\nlike those in exams, agents' ability to reason correctly with chemistry\nknowledge matters more, and tool augmentation does not always help."
                },
                "authors": [
                    {
                        "name": "Botao Yu"
                    },
                    {
                        "name": "Frazier N. Baker"
                    },
                    {
                        "name": "Ziru Chen"
                    },
                    {
                        "name": "Garrett Herb"
                    },
                    {
                        "name": "Boyu Gou"
                    },
                    {
                        "name": "Daniel Adu-Ampratwum"
                    },
                    {
                        "name": "Xia Ning"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07213v1",
                "updated": "2024-11-11T18:36:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    36,
                    17,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T18:36:17Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    36,
                    17,
                    0,
                    316,
                    0
                ],
                "title": "Comparing Bottom-Up and Top-Down Steering Approaches on In-Context\n  Learning Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing Bottom-Up and Top-Down Steering Approaches on In-Context\n  Learning Tasks"
                },
                "summary": "A key objective of interpretability research on large language models (LLMs)\nis to develop methods for robustly steering models toward desired behaviors. To\nthis end, two distinct approaches to interpretability -- ``bottom-up\" and\n``top-down\" -- have been presented, but there has been little quantitative\ncomparison between them. We present a case study comparing the effectiveness of\nrepresentative vector steering methods from each branch: function vectors (FV;\narXiv:2310.15213), as a bottom-up method, and in-context vectors (ICV;\narXiv:2311.06668) as a top-down method. While both aim to capture compact\nrepresentations of broad in-context learning tasks, we find they are effective\nonly on specific types of tasks: ICVs outperform FVs in behavioral shifting,\nwhereas FVs excel in tasks requiring more precision. We discuss the\nimplications for future evaluations of steering methods and for further\nresearch into top-down and bottom-up steering given these findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key objective of interpretability research on large language models (LLMs)\nis to develop methods for robustly steering models toward desired behaviors. To\nthis end, two distinct approaches to interpretability -- ``bottom-up\" and\n``top-down\" -- have been presented, but there has been little quantitative\ncomparison between them. We present a case study comparing the effectiveness of\nrepresentative vector steering methods from each branch: function vectors (FV;\narXiv:2310.15213), as a bottom-up method, and in-context vectors (ICV;\narXiv:2311.06668) as a top-down method. While both aim to capture compact\nrepresentations of broad in-context learning tasks, we find they are effective\nonly on specific types of tasks: ICVs outperform FVs in behavioral shifting,\nwhereas FVs excel in tasks requiring more precision. We discuss the\nimplications for future evaluations of steering methods and for further\nresearch into top-down and bottom-up steering given these findings."
                },
                "authors": [
                    {
                        "name": "Madeline Brumley"
                    },
                    {
                        "name": "Joe Kwon"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Dmitrii Krasheninnikov"
                    },
                    {
                        "name": "Usman Anwar"
                    }
                ],
                "author_detail": {
                    "name": "Usman Anwar"
                },
                "author": "Usman Anwar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06754v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06754v4",
                "updated": "2024-11-11T18:32:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    32,
                    16,
                    0,
                    316,
                    0
                ],
                "published": "2024-09-10T16:05:02Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    16,
                    5,
                    2,
                    1,
                    254,
                    0
                ],
                "title": "Scaling Law Hypothesis for Multimodal Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Law Hypothesis for Multimodal Model"
                },
                "summary": "We propose a scaling law hypothesis for multimodal models processing text,\naudio, images, and video within a shared token and embedding space. Our\nframework predicts model performance based on modality-specific compression and\ntokenization efficiency, extending established scaling laws from text-based\ndecoder models to mixed-modality systems. We explore whether leveraging more\ntraining data in multiple modalities can reduce the size of the multimodal\nmodel, enabling efficient deployment on resource-constrained devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a scaling law hypothesis for multimodal models processing text,\naudio, images, and video within a shared token and embedding space. Our\nframework predicts model performance based on modality-specific compression and\ntokenization efficiency, extending established scaling laws from text-based\ndecoder models to mixed-modality systems. We explore whether leveraging more\ntraining data in multiple modalities can reduce the size of the multimodal\nmodel, enabling efficient deployment on resource-constrained devices."
                },
                "authors": [
                    {
                        "name": "Qingyun Sun"
                    },
                    {
                        "name": "Zhen Guo"
                    },
                    {
                        "name": "PIN AI Team"
                    }
                ],
                "author_detail": {
                    "name": "PIN AI Team"
                },
                "author": "PIN AI Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06754v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06754v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07205v1",
                "updated": "2024-11-11T18:28:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    28,
                    33,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T18:28:33Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    28,
                    33,
                    0,
                    316,
                    0
                ],
                "title": "DLCR: A Generative Data Expansion Framework via Diffusion for\n  Clothes-Changing Person Re-ID",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DLCR: A Generative Data Expansion Framework via Diffusion for\n  Clothes-Changing Person Re-ID"
                },
                "summary": "With the recent exhibited strength of generative diffusion models, an open\nresearch question is \\textit{if images generated by these models can be used to\nlearn better visual representations}. While this generative data expansion may\nsuffice for easier visual tasks, we explore its efficacy on a more difficult\ndiscriminative task: clothes-changing person re-identification (CC-ReID).\nCC-ReID aims to match people appearing in non-overlapping cameras, even when\nthey change their clothes across cameras. Not only are current CC-ReID models\nconstrained by the limited diversity of clothing in current CC-ReID datasets,\nbut generating additional data that retains important personal features for\naccurate identification is a current challenge. To address this issue we\npropose DLCR, a novel data expansion framework that leverages pre-trained\ndiffusion and large language models (LLMs) to accurately generate diverse\nimages of individuals in varied attire. We generate additional data for five\nbenchmark CC-ReID datasets (PRCC, CCVID, LaST, VC-Clothes, and LTCC) and\n\\textbf{increase their clothing diversity by \\boldmath{$10$}x, totaling over\n\\boldmath{$2.1$}M images generated}. DLCR employs diffusion-based text-guided\ninpainting, conditioned on clothing prompts constructed using LLMs, to generate\nsynthetic data that only modifies a subject's clothes while preserving their\npersonally identifiable features. With this massive increase in data, we\nintroduce two novel strategies - progressive learning and test-time prediction\nrefinement - that respectively reduce training time and further boosts CC-ReID\nperformance. On the PRCC dataset, we obtain a large top-1 accuracy improvement\nof $11.3\\%$ by training CAL, a previous state of the art (SOTA) method, with\nDLCR-generated data. We publicly release our code and generated data for each\ndataset here: \\url{https://github.com/CroitoruAlin/dlcr}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the recent exhibited strength of generative diffusion models, an open\nresearch question is \\textit{if images generated by these models can be used to\nlearn better visual representations}. While this generative data expansion may\nsuffice for easier visual tasks, we explore its efficacy on a more difficult\ndiscriminative task: clothes-changing person re-identification (CC-ReID).\nCC-ReID aims to match people appearing in non-overlapping cameras, even when\nthey change their clothes across cameras. Not only are current CC-ReID models\nconstrained by the limited diversity of clothing in current CC-ReID datasets,\nbut generating additional data that retains important personal features for\naccurate identification is a current challenge. To address this issue we\npropose DLCR, a novel data expansion framework that leverages pre-trained\ndiffusion and large language models (LLMs) to accurately generate diverse\nimages of individuals in varied attire. We generate additional data for five\nbenchmark CC-ReID datasets (PRCC, CCVID, LaST, VC-Clothes, and LTCC) and\n\\textbf{increase their clothing diversity by \\boldmath{$10$}x, totaling over\n\\boldmath{$2.1$}M images generated}. DLCR employs diffusion-based text-guided\ninpainting, conditioned on clothing prompts constructed using LLMs, to generate\nsynthetic data that only modifies a subject's clothes while preserving their\npersonally identifiable features. With this massive increase in data, we\nintroduce two novel strategies - progressive learning and test-time prediction\nrefinement - that respectively reduce training time and further boosts CC-ReID\nperformance. On the PRCC dataset, we obtain a large top-1 accuracy improvement\nof $11.3\\%$ by training CAL, a previous state of the art (SOTA) method, with\nDLCR-generated data. We publicly release our code and generated data for each\ndataset here: \\url{https://github.com/CroitoruAlin/dlcr}."
                },
                "authors": [
                    {
                        "name": "Nyle Siddiqui"
                    },
                    {
                        "name": "Florinel Alin Croitoru"
                    },
                    {
                        "name": "Gaurav Kumar Nayak"
                    },
                    {
                        "name": "Radu Tudor Ionescu"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "arxiv_comment": "Published in WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11212v2",
                "updated": "2024-11-11T18:26:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    26,
                    31,
                    0,
                    316,
                    0
                ],
                "published": "2024-01-20T11:37:44Z",
                "published_parsed": [
                    2024,
                    1,
                    20,
                    11,
                    37,
                    44,
                    5,
                    20,
                    0
                ],
                "title": "Programming Distributed Collective Processes in the eXchange Calculus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming Distributed Collective Processes in the eXchange Calculus"
                },
                "summary": "Recent trends like the Internet of Things (IoT) suggest a vision of dense and\nmulti-scale deployments of computing devices in nearly all kinds of\nenvironments. A prominent engineering challenge revolves around programming the\ncollective adaptive behaviour of such computational ecosystems. This requires\nabstractions able to capture concepts like ensembles (dynamic groups of\ncooperating devices) and collective tasks (joint activities carried out by\nensembles). In this work, we consider collections of devices interacting with\nneighbours and that execute in nearly-synchronised sense-compute-interact\nrounds, where the computation is given by a single program mapping sensing\nvalues and incoming messages to output and outcoming messages. To support\nprogramming whole computational collectives, we propose the abstraction of a\ndistributed collective process, which can be used to define at once the\nensemble formation logic and its collective task. We formalise the abstraction\nin the eXchange Calculus (XC), a core functional language based on neighbouring\nvalues (maps from neighbours to values) where state and interaction is handled\nthrough a single primitive, exchange, and provide a corresponding\nimplementation in the FCPP language. Then, we exercise distributed collective\nprocesses using two case studies: multi-hop message propagation and distributed\nmonitoring of spatial properties. Finally, we discuss the features of the\nabstraction and its suitability for different kinds of distributed computing\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent trends like the Internet of Things (IoT) suggest a vision of dense and\nmulti-scale deployments of computing devices in nearly all kinds of\nenvironments. A prominent engineering challenge revolves around programming the\ncollective adaptive behaviour of such computational ecosystems. This requires\nabstractions able to capture concepts like ensembles (dynamic groups of\ncooperating devices) and collective tasks (joint activities carried out by\nensembles). In this work, we consider collections of devices interacting with\nneighbours and that execute in nearly-synchronised sense-compute-interact\nrounds, where the computation is given by a single program mapping sensing\nvalues and incoming messages to output and outcoming messages. To support\nprogramming whole computational collectives, we propose the abstraction of a\ndistributed collective process, which can be used to define at once the\nensemble formation logic and its collective task. We formalise the abstraction\nin the eXchange Calculus (XC), a core functional language based on neighbouring\nvalues (maps from neighbours to values) where state and interaction is handled\nthrough a single primitive, exchange, and provide a corresponding\nimplementation in the FCPP language. Then, we exercise distributed collective\nprocesses using two case studies: multi-hop message propagation and distributed\nmonitoring of spatial properties. Finally, we discuss the features of the\nabstraction and its suitability for different kinds of distributed computing\napplications."
                },
                "authors": [
                    {
                        "name": "Giorgio Audrito"
                    },
                    {
                        "name": "Roberto Casadei"
                    },
                    {
                        "name": "Ferruccio Damiani"
                    },
                    {
                        "name": "Gianluca Torta"
                    },
                    {
                        "name": "Mirko Viroli"
                    }
                ],
                "author_detail": {
                    "name": "Mirko Viroli"
                },
                "author": "Mirko Viroli",
                "arxiv_comment": "41 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.1.3; F.1.1; F.4.3; I.2.11; J.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03354v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03354v2",
                "updated": "2024-11-11T18:19:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    19,
                    22,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-04T18:12:14Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    12,
                    14,
                    0,
                    309,
                    0
                ],
                "title": "LLM-based Continuous Intrusion Detection Framework for Next-Gen Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Continuous Intrusion Detection Framework for Next-Gen Networks"
                },
                "summary": "In this paper, we present an adaptive framework designed for the continuous\ndetection, identification and classification of emerging attacks in network\ntraffic. The framework employs a transformer encoder architecture, which\ncaptures hidden patterns in a bidirectional manner to differentiate between\nmalicious and legitimate traffic. Initially, the framework focuses on the\naccurate detection of malicious activities, achieving a perfect recall of 100\\%\nin distinguishing between attack and benign traffic. Subsequently, the system\nincrementally identifies unknown attack types by leveraging a Gaussian Mixture\nModel (GMM) to cluster features derived from high-dimensional BERT embeddings.\nThis approach allows the framework to dynamically adjust its identification\ncapabilities as new attack clusters are discovered, maintaining high detection\naccuracy. Even after integrating additional unknown attack clusters, the\nframework continues to perform at a high level, achieving 95.6\\% in both\nclassification accuracy and recall.The results demonstrate the effectiveness of\nthe proposed framework in adapting to evolving threats while maintaining high\naccuracy in both detection and identification tasks. Our ultimate goal is to\ndevelop a scalable, real-time intrusion detection system that can continuously\nevolve with the ever-changing network threat landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present an adaptive framework designed for the continuous\ndetection, identification and classification of emerging attacks in network\ntraffic. The framework employs a transformer encoder architecture, which\ncaptures hidden patterns in a bidirectional manner to differentiate between\nmalicious and legitimate traffic. Initially, the framework focuses on the\naccurate detection of malicious activities, achieving a perfect recall of 100\\%\nin distinguishing between attack and benign traffic. Subsequently, the system\nincrementally identifies unknown attack types by leveraging a Gaussian Mixture\nModel (GMM) to cluster features derived from high-dimensional BERT embeddings.\nThis approach allows the framework to dynamically adjust its identification\ncapabilities as new attack clusters are discovered, maintaining high detection\naccuracy. Even after integrating additional unknown attack clusters, the\nframework continues to perform at a high level, achieving 95.6\\% in both\nclassification accuracy and recall.The results demonstrate the effectiveness of\nthe proposed framework in adapting to evolving threats while maintaining high\naccuracy in both detection and identification tasks. Our ultimate goal is to\ndevelop a scalable, real-time intrusion detection system that can continuously\nevolve with the ever-changing network threat landscape."
                },
                "authors": [
                    {
                        "name": "Frederic Adjewa"
                    },
                    {
                        "name": "Moez Esseghir"
                    },
                    {
                        "name": "Leila Merghem-Boulahia"
                    }
                ],
                "author_detail": {
                    "name": "Leila Merghem-Boulahia"
                },
                "author": "Leila Merghem-Boulahia",
                "arxiv_comment": "8 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03354v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03354v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07191v1",
                "updated": "2024-11-11T18:05:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    5,
                    48,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T18:05:48Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    5,
                    48,
                    0,
                    316,
                    0
                ],
                "title": "The Super Weight in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Super Weight in Large Language Models"
                },
                "summary": "Recent works have shown a surprising result: a small fraction of Large\nLanguage Model (LLM) parameter outliers are disproportionately important to the\nquality of the model. LLMs contain billions of parameters, so these small\nfractions, such as 0.01%, translate to hundreds of thousands of parameters. In\nthis work, we present an even more surprising finding: Pruning as few as a\nsingle parameter can destroy an LLM's ability to generate text -- increasing\nperplexity by 3 orders of magnitude and reducing zero-shot accuracy to\nguessing. We propose a data-free method for identifying such parameters, termed\nsuper weights, using a single forward pass through the model. We additionally\nfind that these super weights induce correspondingly rare and large activation\noutliers, termed super activations. When preserved with high precision, super\nactivations can improve simple round-to-nearest quantization to become\ncompetitive with state-of-the-art methods. For weight quantization, we\nsimilarly find that by preserving the super weight and clipping other weight\noutliers, round-to-nearest quantization can scale to much larger block sizes\nthan previously considered. To facilitate further research into super weights,\nwe provide an index of super weight coordinates for common, openly available\nLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have shown a surprising result: a small fraction of Large\nLanguage Model (LLM) parameter outliers are disproportionately important to the\nquality of the model. LLMs contain billions of parameters, so these small\nfractions, such as 0.01%, translate to hundreds of thousands of parameters. In\nthis work, we present an even more surprising finding: Pruning as few as a\nsingle parameter can destroy an LLM's ability to generate text -- increasing\nperplexity by 3 orders of magnitude and reducing zero-shot accuracy to\nguessing. We propose a data-free method for identifying such parameters, termed\nsuper weights, using a single forward pass through the model. We additionally\nfind that these super weights induce correspondingly rare and large activation\noutliers, termed super activations. When preserved with high precision, super\nactivations can improve simple round-to-nearest quantization to become\ncompetitive with state-of-the-art methods. For weight quantization, we\nsimilarly find that by preserving the super weight and clipping other weight\noutliers, round-to-nearest quantization can scale to much larger block sizes\nthan previously considered. To facilitate further research into super weights,\nwe provide an index of super weight coordinates for common, openly available\nLLMs."
                },
                "authors": [
                    {
                        "name": "Mengxia Yu"
                    },
                    {
                        "name": "De Wang"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Alvin Wan"
                    }
                ],
                "author_detail": {
                    "name": "Alvin Wan"
                },
                "author": "Alvin Wan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07186v1",
                "updated": "2024-11-11T18:01:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    1,
                    45,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T18:01:45Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    18,
                    1,
                    45,
                    0,
                    316,
                    0
                ],
                "title": "NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics"
                },
                "summary": "Large language models (LLMs) prompted with text and audio represent the state\nof the art in various auditory tasks, including speech, music, and general\naudio, showing emergent abilities on unseen tasks. However, these capabilities\nhave yet to be fully demonstrated in bioacoustics tasks, such as detecting\nanimal vocalizations in large recordings, classifying rare and endangered\nspecies, and labeling context and behavior - tasks that are crucial for\nconservation, biodiversity monitoring, and the study of animal behavior. In\nthis work, we present NatureLM-audio, the first audio-language foundation model\nspecifically designed for bioacoustics. Our carefully curated training dataset\ncomprises text-audio pairs spanning a diverse range of bioacoustics, speech,\nand music data, designed to address the challenges posed by limited annotated\ndatasets in the field. We demonstrate successful transfer of learned\nrepresentations from music and speech to bioacoustics, and our model shows\npromising generalization to unseen taxa and tasks. Importantly, we test\nNatureLM-audio on a novel benchmark (BEANS-Zero) and it sets the new state of\nthe art (SotA) on several bioacoustics tasks, including zero-shot\nclassification of unseen species. To advance bioacoustics research, we also\nopen-source the code for generating training and benchmark data, as well as for\ntraining the model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) prompted with text and audio represent the state\nof the art in various auditory tasks, including speech, music, and general\naudio, showing emergent abilities on unseen tasks. However, these capabilities\nhave yet to be fully demonstrated in bioacoustics tasks, such as detecting\nanimal vocalizations in large recordings, classifying rare and endangered\nspecies, and labeling context and behavior - tasks that are crucial for\nconservation, biodiversity monitoring, and the study of animal behavior. In\nthis work, we present NatureLM-audio, the first audio-language foundation model\nspecifically designed for bioacoustics. Our carefully curated training dataset\ncomprises text-audio pairs spanning a diverse range of bioacoustics, speech,\nand music data, designed to address the challenges posed by limited annotated\ndatasets in the field. We demonstrate successful transfer of learned\nrepresentations from music and speech to bioacoustics, and our model shows\npromising generalization to unseen taxa and tasks. Importantly, we test\nNatureLM-audio on a novel benchmark (BEANS-Zero) and it sets the new state of\nthe art (SotA) on several bioacoustics tasks, including zero-shot\nclassification of unseen species. To advance bioacoustics research, we also\nopen-source the code for generating training and benchmark data, as well as for\ntraining the model."
                },
                "authors": [
                    {
                        "name": "David Robinson"
                    },
                    {
                        "name": "Marius Miron"
                    },
                    {
                        "name": "Masato Hagiwara"
                    },
                    {
                        "name": "Olivier Pietquin"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Pietquin"
                },
                "author": "Olivier Pietquin",
                "arxiv_comment": "Demo page: https://earthspecies.github.io/naturelm-audio-demo/ The\n  code will be open-sourced and available shortly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16998v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16998v2",
                "updated": "2024-11-11T17:56:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    56,
                    29,
                    0,
                    316,
                    0
                ],
                "published": "2024-03-25T17:59:09Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    17,
                    59,
                    9,
                    0,
                    85,
                    0
                ],
                "title": "Understanding Long Videos with Multimodal Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Long Videos with Multimodal Language Models"
                },
                "summary": "Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we exploring injecting video-specific\ninformation into an LLM-based framework. We utilize off-the-shelf vision tools\nto extract three object-centric information modalities from videos and then\nleverage natural language as a medium for fusing this information. Our\nresulting Multimodal Video Understanding (MVU) framework demonstrates\nstate-of-the-art performance across multiple video understanding benchmarks.\nStrong performance also on robotics domain tasks establish its strong\ngenerality. Our code will be released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have allowed recent LLM-based approaches to\nachieve excellent performance on long-video understanding benchmarks. We\ninvestigate how extensive world knowledge and strong reasoning skills of\nunderlying LLMs influence this strong performance. Surprisingly, we discover\nthat LLM-based approaches can yield surprisingly good accuracy on long-video\ntasks with limited video information, sometimes even with no video specific\ninformation. Building on this, we exploring injecting video-specific\ninformation into an LLM-based framework. We utilize off-the-shelf vision tools\nto extract three object-centric information modalities from videos and then\nleverage natural language as a medium for fusing this information. Our\nresulting Multimodal Video Understanding (MVU) framework demonstrates\nstate-of-the-art performance across multiple video understanding benchmarks.\nStrong performance also on robotics domain tasks establish its strong\ngenerality. Our code will be released publicly."
                },
                "authors": [
                    {
                        "name": "Kanchana Ranasinghe"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    }
                ],
                "author_detail": {
                    "name": "Michael S. Ryoo"
                },
                "author": "Michael S. Ryoo",
                "arxiv_comment": "Code available at https://github.com/kahnchana/mvu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16998v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16998v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07175v1",
                "updated": "2024-11-11T17:56:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    56,
                    15,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T17:56:15Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    56,
                    15,
                    0,
                    316,
                    0
                ],
                "title": "Continual Memorization of Factoids in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Memorization of Factoids in Large Language Models"
                },
                "summary": "Large language models can absorb a massive amount of knowledge through\npretraining, but pretraining is inefficient for acquiring long-tailed or\nspecialized facts. Therefore, fine-tuning on specialized or new knowledge that\nreflects changes in the world has become popular, though it risks disrupting\nthe model's original capabilities. We study this fragility in the context of\ncontinual memorization, where the model is trained on a small set of long-tail\nfactoids (factual associations) and must retain these factoids after multiple\nstages of subsequent training on other datasets. Through extensive experiments,\nwe show that LLMs suffer from forgetting across a wide range of subsequent\ntasks, and simple replay techniques do not fully prevent forgetting, especially\nwhen the factoid datasets are trained in the later stages. We posit that there\nare two ways to alleviate forgetting: 1) protect the memorization process as\nthe model learns the factoids, or 2) reduce interference from training in later\nstages. With this insight, we develop an effective mitigation strategy: REMIX\n(Random and Generic Data Mixing). REMIX prevents forgetting by mixing generic\ndata sampled from pretraining corpora or even randomly generated word sequences\nduring each stage, despite being unrelated to the memorized factoids in the\nfirst stage. REMIX can recover performance from severe forgetting, often\noutperforming replay-based methods that have access to the factoids from the\nfirst stage. We then analyze how REMIX alters the learning process and find\nthat successful forgetting prevention is associated with a pattern: the model\nstores factoids in earlier layers than usual and diversifies the set of layers\nthat store these factoids. The efficacy of REMIX invites further investigation\ninto the underlying dynamics of memorization and forgetting, opening exciting\npossibilities for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can absorb a massive amount of knowledge through\npretraining, but pretraining is inefficient for acquiring long-tailed or\nspecialized facts. Therefore, fine-tuning on specialized or new knowledge that\nreflects changes in the world has become popular, though it risks disrupting\nthe model's original capabilities. We study this fragility in the context of\ncontinual memorization, where the model is trained on a small set of long-tail\nfactoids (factual associations) and must retain these factoids after multiple\nstages of subsequent training on other datasets. Through extensive experiments,\nwe show that LLMs suffer from forgetting across a wide range of subsequent\ntasks, and simple replay techniques do not fully prevent forgetting, especially\nwhen the factoid datasets are trained in the later stages. We posit that there\nare two ways to alleviate forgetting: 1) protect the memorization process as\nthe model learns the factoids, or 2) reduce interference from training in later\nstages. With this insight, we develop an effective mitigation strategy: REMIX\n(Random and Generic Data Mixing). REMIX prevents forgetting by mixing generic\ndata sampled from pretraining corpora or even randomly generated word sequences\nduring each stage, despite being unrelated to the memorized factoids in the\nfirst stage. REMIX can recover performance from severe forgetting, often\noutperforming replay-based methods that have access to the factoids from the\nfirst stage. We then analyze how REMIX alters the learning process and find\nthat successful forgetting prevention is associated with a pattern: the model\nstores factoids in earlier layers than usual and diversifies the set of layers\nthat store these factoids. The efficacy of REMIX invites further investigation\ninto the underlying dynamics of memorization and forgetting, opening exciting\npossibilities for future research."
                },
                "authors": [
                    {
                        "name": "Howard Chen"
                    },
                    {
                        "name": "Jiayi Geng"
                    },
                    {
                        "name": "Adithya Bhaskar"
                    },
                    {
                        "name": "Dan Friedman"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07163v1",
                "updated": "2024-11-11T17:41:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    41,
                    54,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T17:41:54Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    41,
                    54,
                    0,
                    316,
                    0
                ],
                "title": "A Domain-Agnostic Neurosymbolic Approach for Big Social Data Analysis:\n  Evaluating Mental Health Sentiment on Social Media during COVID-19",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Domain-Agnostic Neurosymbolic Approach for Big Social Data Analysis:\n  Evaluating Mental Health Sentiment on Social Media during COVID-19"
                },
                "summary": "Monitoring public sentiment via social media is potentially helpful during\nhealth crises such as the COVID-19 pandemic. However, traditional\nfrequency-based, data-driven neural network-based approaches can miss newly\nrelevant content due to the evolving nature of language in a dynamically\nevolving environment. Human-curated symbolic knowledge sources, such as\nlexicons for standard language and slang terms, can potentially elevate social\nmedia signals in evolving language. We introduce a neurosymbolic method that\nintegrates neural networks with symbolic knowledge sources, enhancing the\ndetection and interpretation of mental health-related tweets relevant to\nCOVID-19. Our method was evaluated using a corpus of large datasets\n(approximately 12 billion tweets, 2.5 million subreddit data, and 700k news\narticles) and multiple knowledge graphs. This method dynamically adapts to\nevolving language, outperforming purely data-driven models with an F1 score\nexceeding 92\\%. This approach also showed faster adaptation to new data and\nlower computational demands than fine-tuning pre-trained large language models\n(LLMs). This study demonstrates the benefit of neurosymbolic methods in\ninterpreting text in a dynamic environment for tasks such as health\nsurveillance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring public sentiment via social media is potentially helpful during\nhealth crises such as the COVID-19 pandemic. However, traditional\nfrequency-based, data-driven neural network-based approaches can miss newly\nrelevant content due to the evolving nature of language in a dynamically\nevolving environment. Human-curated symbolic knowledge sources, such as\nlexicons for standard language and slang terms, can potentially elevate social\nmedia signals in evolving language. We introduce a neurosymbolic method that\nintegrates neural networks with symbolic knowledge sources, enhancing the\ndetection and interpretation of mental health-related tweets relevant to\nCOVID-19. Our method was evaluated using a corpus of large datasets\n(approximately 12 billion tweets, 2.5 million subreddit data, and 700k news\narticles) and multiple knowledge graphs. This method dynamically adapts to\nevolving language, outperforming purely data-driven models with an F1 score\nexceeding 92\\%. This approach also showed faster adaptation to new data and\nlower computational demands than fine-tuning pre-trained large language models\n(LLMs). This study demonstrates the benefit of neurosymbolic methods in\ninterpreting text in a dynamic environment for tasks such as health\nsurveillance."
                },
                "authors": [
                    {
                        "name": "Vedant Khandelwal"
                    },
                    {
                        "name": "Manas Gaur"
                    },
                    {
                        "name": "Ugur Kursuncu"
                    },
                    {
                        "name": "Valerie Shalin"
                    },
                    {
                        "name": "Amit Sheth"
                    }
                ],
                "author_detail": {
                    "name": "Amit Sheth"
                },
                "author": "Amit Sheth",
                "arxiv_comment": "13 Pages, 5 Figures, 5 Tables, 2024 IEEE International Conference on\n  Big Data, Regular Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4; I.2.6; I.2.7; I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02481v2",
                "updated": "2024-11-11T17:34:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    34,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-04T18:54:39Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    54,
                    39,
                    0,
                    309,
                    0
                ],
                "title": "CDR: Customizable Density Ratios of Strong-over-weak LLMs for Preference\n  Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDR: Customizable Density Ratios of Strong-over-weak LLMs for Preference\n  Annotation"
                },
                "summary": "Preference tuning of large language models (LLMs) relies on high-quality\nhuman preference data, which is often expensive and time-consuming to gather.\nWhile existing methods can use trained reward models or proprietary model as\njudges for preference annotation, they have notable drawbacks: training reward\nmodels remain dependent on initial human data, and using proprietary model\nimposes license restrictions that inhibits commercial usage. In this paper, we\nintroduce customized density ratio (CDR), a training-free and highly effective\nmethod that leverages off-the-shelf LLMs for preference data annotation. Our\napproach uses the log-density ratio between a better-aligned LLM and a less\naligned LLM as a reward signal. We explores 221 different LLMs pairs and\nempirically demonstrate that increasing the performance gap between paired LLMs\ncorrelates with better reward generalization. Furthermore, we show that\ntailoring the density ratio reward function with specific criteria and\npreference exemplars enhances performance across domains and within target\nareas.\n  In our experiment using density ratio from a pair of Mistral-7B models, CDR\nachieves a RewardBench score of 82.6, outperforming the best trained reward\nfunctions from same model class and demonstrating competitive performance\nagainst SoTA models in Safety (91.0) and Reasoning (88.0) domains. We use CDR\nto annotate an on-policy preference dataset with which we preference tune\nLlama-3-8B-Instruct with SimPO. Using reward signals from two relatively weak\nmodels, our approach pushes Llama-3-8B to achieve a 37.4% (+15.1%) win rate on\nArenaHard and a 40.7% (+17.8%) win rate on Length-Controlled AlpacaEval 2.0,\nalong with a score of 8.0 on MT-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference tuning of large language models (LLMs) relies on high-quality\nhuman preference data, which is often expensive and time-consuming to gather.\nWhile existing methods can use trained reward models or proprietary model as\njudges for preference annotation, they have notable drawbacks: training reward\nmodels remain dependent on initial human data, and using proprietary model\nimposes license restrictions that inhibits commercial usage. In this paper, we\nintroduce customized density ratio (CDR), a training-free and highly effective\nmethod that leverages off-the-shelf LLMs for preference data annotation. Our\napproach uses the log-density ratio between a better-aligned LLM and a less\naligned LLM as a reward signal. We explores 221 different LLMs pairs and\nempirically demonstrate that increasing the performance gap between paired LLMs\ncorrelates with better reward generalization. Furthermore, we show that\ntailoring the density ratio reward function with specific criteria and\npreference exemplars enhances performance across domains and within target\nareas.\n  In our experiment using density ratio from a pair of Mistral-7B models, CDR\nachieves a RewardBench score of 82.6, outperforming the best trained reward\nfunctions from same model class and demonstrating competitive performance\nagainst SoTA models in Safety (91.0) and Reasoning (88.0) domains. We use CDR\nto annotate an on-policy preference dataset with which we preference tune\nLlama-3-8B-Instruct with SimPO. Using reward signals from two relatively weak\nmodels, our approach pushes Llama-3-8B to achieve a 37.4% (+15.1%) win rate on\nArenaHard and a 40.7% (+17.8%) win rate on Length-Controlled AlpacaEval 2.0,\nalong with a score of 8.0 on MT-Bench."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xu"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Shivchander Sudalairaj"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04528v2",
                "updated": "2024-11-11T17:30:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    30,
                    55,
                    0,
                    316,
                    0
                ],
                "published": "2023-12-07T18:46:50Z",
                "published_parsed": [
                    2023,
                    12,
                    7,
                    18,
                    46,
                    50,
                    3,
                    341,
                    0
                ],
                "title": "Using Large Language Models for Hyperparameter Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for Hyperparameter Optimization"
                },
                "summary": "This paper explores the use of foundational large language models (LLMs) in\nhyperparameter optimization (HPO). Hyperparameters are critical in determining\nthe effectiveness of machine learning models, yet their optimization often\nrelies on manual approaches in limited-budget settings. By prompting LLMs with\ndataset and model descriptions, we develop a methodology where LLMs suggest\nhyperparameter configurations, which are iteratively refined based on model\nperformance. Our empirical evaluations on standard benchmarks reveal that\nwithin constrained search budgets, LLMs can match or outperform traditional HPO\nmethods like Bayesian optimization across different models on standard\nbenchmarks. Furthermore, we propose to treat the code specifying our model as a\nhyperparameter, which the LLM outputs and affords greater flexibility than\nexisting HPO approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the use of foundational large language models (LLMs) in\nhyperparameter optimization (HPO). Hyperparameters are critical in determining\nthe effectiveness of machine learning models, yet their optimization often\nrelies on manual approaches in limited-budget settings. By prompting LLMs with\ndataset and model descriptions, we develop a methodology where LLMs suggest\nhyperparameter configurations, which are iteratively refined based on model\nperformance. Our empirical evaluations on standard benchmarks reveal that\nwithin constrained search budgets, LLMs can match or outperform traditional HPO\nmethods like Bayesian optimization across different models on standard\nbenchmarks. Furthermore, we propose to treat the code specifying our model as a\nhyperparameter, which the LLM outputs and affords greater flexibility than\nexisting HPO approaches."
                },
                "authors": [
                    {
                        "name": "Michael R. Zhang"
                    },
                    {
                        "name": "Nishkrit Desai"
                    },
                    {
                        "name": "Juhan Bae"
                    },
                    {
                        "name": "Jonathan Lorraine"
                    },
                    {
                        "name": "Jimmy Ba"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Ba"
                },
                "author": "Jimmy Ba",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07140v1",
                "updated": "2024-11-11T17:10:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    10,
                    56,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T17:10:56Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    10,
                    56,
                    0,
                    316,
                    0
                ],
                "title": "Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language\n  Models"
                },
                "summary": "New LLM evaluation benchmarks are important to align with the rapid\ndevelopment of Large Language Models (LLMs). In this work, we present Chinese\nSimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality\nability of language models to answer short questions, and Chinese SimpleQA\nmainly has five properties (i.e., Chinese, Diverse, High-quality, Static,\nEasy-to-evaluate). Specifically, first, we focus on the Chinese language over 6\nmajor topics with 99 diverse subtopics. Second, we conduct a comprehensive\nquality control process to achieve high-quality questions and answers, where\nthe reference answers are static and cannot be changed over time. Third,\nfollowing SimpleQA, the questions and answers are very short, and the grading\nprocess is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we\nperform a comprehensive evaluation on the factuality abilities of existing\nLLMs. Finally, we hope that Chinese SimpleQA could guide the developers to\nbetter understand the Chinese factuality abilities of their models and\nfacilitate the growth of foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New LLM evaluation benchmarks are important to align with the rapid\ndevelopment of Large Language Models (LLMs). In this work, we present Chinese\nSimpleQA, the first comprehensive Chinese benchmark to evaluate the factuality\nability of language models to answer short questions, and Chinese SimpleQA\nmainly has five properties (i.e., Chinese, Diverse, High-quality, Static,\nEasy-to-evaluate). Specifically, first, we focus on the Chinese language over 6\nmajor topics with 99 diverse subtopics. Second, we conduct a comprehensive\nquality control process to achieve high-quality questions and answers, where\nthe reference answers are static and cannot be changed over time. Third,\nfollowing SimpleQA, the questions and answers are very short, and the grading\nprocess is easy-to-evaluate based on OpenAI API. Based on Chinese SimpleQA, we\nperform a comprehensive evaluation on the factuality abilities of existing\nLLMs. Finally, we hope that Chinese SimpleQA could guide the developers to\nbetter understand the Chinese factuality abilities of their models and\nfacilitate the growth of foundation models."
                },
                "authors": [
                    {
                        "name": "Yancheng He"
                    },
                    {
                        "name": "Shilong Li"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Xingyuan Bu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Chengwei Hu"
                    },
                    {
                        "name": "Boren Zheng"
                    },
                    {
                        "name": "Xuepeng Liu"
                    },
                    {
                        "name": "Dekai Sun"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07133v1",
                "updated": "2024-11-11T17:06:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    6,
                    48,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T17:06:48Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    17,
                    6,
                    48,
                    0,
                    316,
                    0
                ],
                "title": "Stronger Models are NOT Stronger Teachers for Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stronger Models are NOT Stronger Teachers for Instruction Tuning"
                },
                "summary": "Instruction tuning has been widely adopted to ensure large language models\n(LLMs) follow user instructions effectively. The resulting\ninstruction-following capabilities of LLMs heavily rely on the instruction\ndatasets used for tuning. Recently, synthetic instruction datasets have emerged\nas an economically viable solution to provide LLMs diverse and high-quality\ninstructions. However, existing approaches typically assume that larger or\nstronger models are stronger teachers for instruction tuning, and hence simply\nadopt these models as response generators to the synthetic instructions. In\nthis paper, we challenge this commonly-adopted assumption. Our extensive\nexperiments across five base models and twenty response generators reveal that\nlarger and stronger models are not necessarily stronger teachers of smaller\nmodels. We refer to this phenomenon as the Larger Models' Paradox. We observe\nthat existing metrics cannot precisely predict the effectiveness of response\ngenerators since they ignore the compatibility between teachers and base models\nbeing fine-tuned. We thus develop a novel metric, named as\nCompatibility-Adjusted Reward (CAR) to measure the effectiveness of response\ngenerators. Our experiments across five base models demonstrate that CAR\noutperforms almost all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has been widely adopted to ensure large language models\n(LLMs) follow user instructions effectively. The resulting\ninstruction-following capabilities of LLMs heavily rely on the instruction\ndatasets used for tuning. Recently, synthetic instruction datasets have emerged\nas an economically viable solution to provide LLMs diverse and high-quality\ninstructions. However, existing approaches typically assume that larger or\nstronger models are stronger teachers for instruction tuning, and hence simply\nadopt these models as response generators to the synthetic instructions. In\nthis paper, we challenge this commonly-adopted assumption. Our extensive\nexperiments across five base models and twenty response generators reveal that\nlarger and stronger models are not necessarily stronger teachers of smaller\nmodels. We refer to this phenomenon as the Larger Models' Paradox. We observe\nthat existing metrics cannot precisely predict the effectiveness of response\ngenerators since they ignore the compatibility between teachers and base models\nbeing fine-tuned. We thus develop a novel metric, named as\nCompatibility-Adjusted Reward (CAR) to measure the effectiveness of response\ngenerators. Our experiments across five base models demonstrate that CAR\noutperforms almost all baselines."
                },
                "authors": [
                    {
                        "name": "Zhangchen Xu"
                    },
                    {
                        "name": "Fengqing Jiang"
                    },
                    {
                        "name": "Luyao Niu"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Radha Poovendran"
                    }
                ],
                "author_detail": {
                    "name": "Radha Poovendran"
                },
                "author": "Radha Poovendran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07127v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07127v1",
                "updated": "2024-11-11T16:58:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    58,
                    36,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T16:58:36Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    58,
                    36,
                    0,
                    316,
                    0
                ],
                "title": "Benchmarking LLMs' Judgments with No Gold Standard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs' Judgments with No Gold Standard"
                },
                "summary": "We introduce the GEM (Generative Estimator for Mutual Information), an\nevaluation metric for assessing language generation by Large Language Models\n(LLMs), particularly in generating informative judgments, without the need for\na gold standard reference. GEM broadens the scenarios where we can benchmark\nLLM generation performance-from traditional ones, like machine translation and\nsummarization, where gold standard references are readily available, to\nsubjective tasks without clear gold standards, such as academic peer review.\n  GEM uses a generative model to estimate mutual information between candidate\nand reference responses, without requiring the reference to be a gold standard.\nIn experiments on a human-annotated dataset, GEM demonstrates competitive\ncorrelations with human scores compared to the state-of-the-art GPT-4o\nExaminer, and outperforms all other baselines. Additionally, GEM is more robust\nagainst strategic manipulations, such as rephrasing or elongation, which can\nartificially inflate scores under a GPT-4o Examiner.\n  We also present GRE-bench (Generating Review Evaluation Benchmark) which\nevaluates LLMs based on how well they can generate high-quality peer reviews\nfor academic research papers. Because GRE-bench is based upon GEM, it inherits\nits robustness properties. Additionally, GRE-bench circumvents data\ncontamination problems (or data leakage) by using the continuous influx of new\nopen-access research papers and peer reviews each year. We show GRE-bench\nresults of various popular LLMs on their peer review capabilities using the\nICLR2023 dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the GEM (Generative Estimator for Mutual Information), an\nevaluation metric for assessing language generation by Large Language Models\n(LLMs), particularly in generating informative judgments, without the need for\na gold standard reference. GEM broadens the scenarios where we can benchmark\nLLM generation performance-from traditional ones, like machine translation and\nsummarization, where gold standard references are readily available, to\nsubjective tasks without clear gold standards, such as academic peer review.\n  GEM uses a generative model to estimate mutual information between candidate\nand reference responses, without requiring the reference to be a gold standard.\nIn experiments on a human-annotated dataset, GEM demonstrates competitive\ncorrelations with human scores compared to the state-of-the-art GPT-4o\nExaminer, and outperforms all other baselines. Additionally, GEM is more robust\nagainst strategic manipulations, such as rephrasing or elongation, which can\nartificially inflate scores under a GPT-4o Examiner.\n  We also present GRE-bench (Generating Review Evaluation Benchmark) which\nevaluates LLMs based on how well they can generate high-quality peer reviews\nfor academic research papers. Because GRE-bench is based upon GEM, it inherits\nits robustness properties. Additionally, GRE-bench circumvents data\ncontamination problems (or data leakage) by using the continuous influx of new\nopen-access research papers and peer reviews each year. We show GRE-bench\nresults of various popular LLMs on their peer review capabilities using the\nICLR2023 dataset."
                },
                "authors": [
                    {
                        "name": "Shengwei Xu"
                    },
                    {
                        "name": "Yuxuan Lu"
                    },
                    {
                        "name": "Grant Schoenebeck"
                    },
                    {
                        "name": "Yuqing Kong"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Kong"
                },
                "author": "Yuqing Kong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07127v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07127v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06687v2",
                "updated": "2024-11-11T16:53:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    53,
                    58,
                    0,
                    316,
                    0
                ],
                "published": "2024-05-06T18:09:32Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    18,
                    9,
                    32,
                    0,
                    127,
                    0
                ],
                "title": "Hire Me or Not? Examining Language Model's Behavior with Occupation\n  Attributes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hire Me or Not? Examining Language Model's Behavior with Occupation\n  Attributes"
                },
                "summary": "With the impressive performance in various downstream tasks, large language\nmodels (LLMs) have been widely integrated into production pipelines, like\nrecruitment and recommendation systems. A known issue of models trained on\nnatural language data is the presence of human biases, which can impact the\nfairness of the system. This paper investigates LLMs' behavior with respect to\ngender stereotypes, in the context of occupation decision making. Our framework\nis designed to investigate and quantify the presence of gender stereotypes in\nLLMs' behavior via multi-round question answering. Inspired by prior works, we\nconstruct a dataset by leveraging a standard occupation classification\nknowledge base released by authoritative agencies. We tested three LLMs\n(RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models\nexhibit gender stereotypes analogous to human biases, but with different\npreferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may\nimply the current alignment methods are insufficient for debiasing and could\nintroduce new biases contradicting the traditional gender stereotypes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the impressive performance in various downstream tasks, large language\nmodels (LLMs) have been widely integrated into production pipelines, like\nrecruitment and recommendation systems. A known issue of models trained on\nnatural language data is the presence of human biases, which can impact the\nfairness of the system. This paper investigates LLMs' behavior with respect to\ngender stereotypes, in the context of occupation decision making. Our framework\nis designed to investigate and quantify the presence of gender stereotypes in\nLLMs' behavior via multi-round question answering. Inspired by prior works, we\nconstruct a dataset by leveraging a standard occupation classification\nknowledge base released by authoritative agencies. We tested three LLMs\n(RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models\nexhibit gender stereotypes analogous to human biases, but with different\npreferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may\nimply the current alignment methods are insufficient for debiasing and could\nintroduce new biases contradicting the traditional gender stereotypes."
                },
                "authors": [
                    {
                        "name": "Damin Zhang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Geetanjali Bihani"
                    },
                    {
                        "name": "Julia Rayz"
                    }
                ],
                "author_detail": {
                    "name": "Julia Rayz"
                },
                "author": "Julia Rayz",
                "arxiv_comment": "WIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07122v1",
                "updated": "2024-11-11T16:51:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    51,
                    39,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T16:51:39Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    51,
                    39,
                    0,
                    316,
                    0
                ],
                "title": "SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAR: Sparse Conditioned Autoencoders for Concept Detection and Steering\n  in LLMs"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating human-like text, but their output may not be aligned with the user\nor even produce harmful content. This paper presents a novel approach to detect\nand steer concepts such as toxicity before generation. We introduce the Sparse\nConditioned Autoencoder (SCAR), a single trained module that extends the\notherwise untouched LLM. SCAR ensures full steerability, towards and away from\nconcepts (e.g., toxic content), without compromising the quality of the model's\ntext generation on standard evaluation benchmarks. We demonstrate the effective\napplication of our approach through a variety of concepts, including toxicity,\nsafety, and writing style alignment. As such, this work establishes a robust\nframework for controlling LLM generations, ensuring their ethical and safe\ndeployment in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating human-like text, but their output may not be aligned with the user\nor even produce harmful content. This paper presents a novel approach to detect\nand steer concepts such as toxicity before generation. We introduce the Sparse\nConditioned Autoencoder (SCAR), a single trained module that extends the\notherwise untouched LLM. SCAR ensures full steerability, towards and away from\nconcepts (e.g., toxic content), without compromising the quality of the model's\ntext generation on standard evaluation benchmarks. We demonstrate the effective\napplication of our approach through a variety of concepts, including toxicity,\nsafety, and writing style alignment. As such, this work establishes a robust\nframework for controlling LLM generations, ensuring their ethical and safe\ndeployment in real-world applications."
                },
                "authors": [
                    {
                        "name": "Ruben Hrle"
                    },
                    {
                        "name": "Felix Friedrich"
                    },
                    {
                        "name": "Manuel Brack"
                    },
                    {
                        "name": "Bjrn Deiseroth"
                    },
                    {
                        "name": "Patrick Schramowski"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07112v1",
                "updated": "2024-11-11T16:39:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    39,
                    13,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T16:39:13Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    39,
                    13,
                    0,
                    316,
                    0
                ],
                "title": "ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large\n  Language Models for Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROCODE: Integrating Backtracking Mechanism and Program Analysis in Large\n  Language Models for Code Generation"
                },
                "summary": "Large language models (LLMs) have achieved impressive performance in code\ngeneration recently, offering programmers revolutionary assistance in software\ndevelopment. However, due to the auto-regressive nature of LLMs, they are\nsusceptible to error accumulation during code generation. Once an error is\nproduced, LLMs can merely continue to generate the subsequent code conditioned\non it, given their inability to adjust previous outputs. Existing LLM-based\napproaches typically consider post-revising after code generation, leading to\nthe challenging resolution of accumulated errors and the significant wastage of\nresources. Ideally, LLMs should rollback and resolve the occurred error in time\nduring code generation, rather than proceed on the basis of the error and wait\nfor post-revising after generation. In this paper, we propose ROCODE, which\nintegrates the backtracking mechanism and program analysis into LLMs for code\ngeneration. Specifically, we employ program analysis to perform incremental\nerror detection during the generation process. When an error is detected, the\nbacktracking mechanism is triggered to priming rollback strategies and\nconstraint regeneration, thereby eliminating the error early and ensuring\ncontinued generation on the correct basis. Experiments on multiple code\ngeneration benchmarks show that ROCODE can significantly reduce the errors\ngenerated by LLMs, with a compilation pass rate of 99.1%. The test pass rate is\nimproved by up to 23.8% compared to the best baseline approach. Compared to the\npost-revising baseline, the token cost is reduced by 19.3%. Moreover, our\napproach is model-agnostic and achieves consistent improvements across nine\nrepresentative LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive performance in code\ngeneration recently, offering programmers revolutionary assistance in software\ndevelopment. However, due to the auto-regressive nature of LLMs, they are\nsusceptible to error accumulation during code generation. Once an error is\nproduced, LLMs can merely continue to generate the subsequent code conditioned\non it, given their inability to adjust previous outputs. Existing LLM-based\napproaches typically consider post-revising after code generation, leading to\nthe challenging resolution of accumulated errors and the significant wastage of\nresources. Ideally, LLMs should rollback and resolve the occurred error in time\nduring code generation, rather than proceed on the basis of the error and wait\nfor post-revising after generation. In this paper, we propose ROCODE, which\nintegrates the backtracking mechanism and program analysis into LLMs for code\ngeneration. Specifically, we employ program analysis to perform incremental\nerror detection during the generation process. When an error is detected, the\nbacktracking mechanism is triggered to priming rollback strategies and\nconstraint regeneration, thereby eliminating the error early and ensuring\ncontinued generation on the correct basis. Experiments on multiple code\ngeneration benchmarks show that ROCODE can significantly reduce the errors\ngenerated by LLMs, with a compilation pass rate of 99.1%. The test pass rate is\nimproved by up to 23.8% compared to the best baseline approach. Compared to the\npost-revising baseline, the token cost is reduced by 19.3%. Moreover, our\napproach is model-agnostic and achieves consistent improvements across nine\nrepresentative LLMs."
                },
                "authors": [
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Yongding Tao"
                    },
                    {
                        "name": "Huanyu Liu"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Wenpin Jiao"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "arxiv_comment": "ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07111v1",
                "updated": "2024-11-11T16:37:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    37,
                    40,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T16:37:40Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    37,
                    40,
                    0,
                    316,
                    0
                ],
                "title": "Building a Taiwanese Mandarin Spoken Language Model: A First Attempt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a Taiwanese Mandarin Spoken Language Model: A First Attempt"
                },
                "summary": "This technical report presents our initial attempt to build a spoken large\nlanguage model (LLM) for Taiwanese Mandarin, specifically tailored to enable\nreal-time, speech-to-speech interaction in multi-turn conversations. Our\nend-to-end model incorporates a decoder-only transformer architecture and aims\nto achieve seamless interaction while preserving the conversational flow,\nincluding full-duplex capabilities allowing simultaneous speaking and\nlistening. The paper also details the training process, including data\npreparation with synthesized dialogues and adjustments for real-time\ninteraction. We also developed a platform to evaluate conversational fluency\nand response coherence in multi-turn dialogues. We hope the release of the\nreport can contribute to the future development of spoken LLMs in Taiwanese\nMandarin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents our initial attempt to build a spoken large\nlanguage model (LLM) for Taiwanese Mandarin, specifically tailored to enable\nreal-time, speech-to-speech interaction in multi-turn conversations. Our\nend-to-end model incorporates a decoder-only transformer architecture and aims\nto achieve seamless interaction while preserving the conversational flow,\nincluding full-duplex capabilities allowing simultaneous speaking and\nlistening. The paper also details the training process, including data\npreparation with synthesized dialogues and adjustments for real-time\ninteraction. We also developed a platform to evaluate conversational fluency\nand response coherence in multi-turn dialogues. We hope the release of the\nreport can contribute to the future development of spoken LLMs in Taiwanese\nMandarin."
                },
                "authors": [
                    {
                        "name": "Chih-Kai Yang"
                    },
                    {
                        "name": "Yu-Kuan Fu"
                    },
                    {
                        "name": "Chen-An Li"
                    },
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Yu-Xiang Lin"
                    },
                    {
                        "name": "Wei-Chih Chen"
                    },
                    {
                        "name": "Ho Lam Chung"
                    },
                    {
                        "name": "Chun-Yi Kuan"
                    },
                    {
                        "name": "Wei-Ping Huang"
                    },
                    {
                        "name": "Ke-Han Lu"
                    },
                    {
                        "name": "Tzu-Quan Lin"
                    },
                    {
                        "name": "Hsiu-Hsuan Wang"
                    },
                    {
                        "name": "En-Pei Hu"
                    },
                    {
                        "name": "Chan-Jan Hsu"
                    },
                    {
                        "name": "Liang-Hsuan Tseng"
                    },
                    {
                        "name": "I-Hsiang Chiu"
                    },
                    {
                        "name": "Ulin Sanga"
                    },
                    {
                        "name": "Xuanjun Chen"
                    },
                    {
                        "name": "Po-chun Hsu"
                    },
                    {
                        "name": "Shu-wen Yang"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09971v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09971v2",
                "updated": "2024-11-11T16:20:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    20,
                    28,
                    0,
                    316,
                    0
                ],
                "published": "2024-03-15T02:28:26Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    2,
                    28,
                    26,
                    4,
                    75,
                    0
                ],
                "title": "Advancing Object Goal Navigation Through LLM-enhanced Object Affinities\n  Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Object Goal Navigation Through LLM-enhanced Object Affinities\n  Transfer"
                },
                "summary": "In object goal navigation, agents navigate towards objects identified by\ncategory labels using visual and spatial information. Previously, solely\nnetwork-based methods typically rely on historical data for object affinities\nestimation, lacking adaptability to new environments and unseen targets.\nSimultaneously, employing Large Language Models (LLMs) for navigation as either\nplanners or agents, though offering a broad knowledge base, is cost-inefficient\nand lacks targeted historical experience. Addressing these challenges, we\npresent the LLM-enhanced Object Affinities Transfer (LOAT) framework,\nintegrating LLM-derived object semantics with network-based approaches to\nleverage experiential object affinities, thus improving adaptability in\nunfamiliar settings. LOAT employs a dual-module strategy: a generalized\naffinities module for accessing LLMs' vast knowledge and an experiential\naffinities module for applying learned object semantic relationships,\ncomplemented by a dynamic fusion module harmonizing these information sources\nbased on temporal context. The resulting scores activate semantic maps before\nfeeding into downstream policies, enhancing navigation systems with\ncontext-aware inputs. Our evaluations conducted in the AI2-THOR and Habitat\nsimulators indicate significant improvements in both navigation success rates\nand overall efficiency. Furthermore, the system performs effectively when\ndeployed on a real robot without requiring additional training, thereby\nvalidating the efficacy of LOAT in integrating LLM insights for enhanced\nobject-goal navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In object goal navigation, agents navigate towards objects identified by\ncategory labels using visual and spatial information. Previously, solely\nnetwork-based methods typically rely on historical data for object affinities\nestimation, lacking adaptability to new environments and unseen targets.\nSimultaneously, employing Large Language Models (LLMs) for navigation as either\nplanners or agents, though offering a broad knowledge base, is cost-inefficient\nand lacks targeted historical experience. Addressing these challenges, we\npresent the LLM-enhanced Object Affinities Transfer (LOAT) framework,\nintegrating LLM-derived object semantics with network-based approaches to\nleverage experiential object affinities, thus improving adaptability in\nunfamiliar settings. LOAT employs a dual-module strategy: a generalized\naffinities module for accessing LLMs' vast knowledge and an experiential\naffinities module for applying learned object semantic relationships,\ncomplemented by a dynamic fusion module harmonizing these information sources\nbased on temporal context. The resulting scores activate semantic maps before\nfeeding into downstream policies, enhancing navigation systems with\ncontext-aware inputs. Our evaluations conducted in the AI2-THOR and Habitat\nsimulators indicate significant improvements in both navigation success rates\nand overall efficiency. Furthermore, the system performs effectively when\ndeployed on a real robot without requiring additional training, thereby\nvalidating the efficacy of LOAT in integrating LLM insights for enhanced\nobject-goal navigation."
                },
                "authors": [
                    {
                        "name": "Mengying Lin"
                    },
                    {
                        "name": "Shugao Liu"
                    },
                    {
                        "name": "Dingxi Zhang"
                    },
                    {
                        "name": "Yaran Chen"
                    },
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Dongbin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongbin Zhao"
                },
                "author": "Dongbin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09971v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09971v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07098v1",
                "updated": "2024-11-11T16:20:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    20,
                    27,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T16:20:27Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    20,
                    27,
                    0,
                    316,
                    0
                ],
                "title": "A Multi-Agent Approach for REST API Testing with Semantic Graphs and\n  LLM-Driven Inputs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Agent Approach for REST API Testing with Semantic Graphs and\n  LLM-Driven Inputs"
                },
                "summary": "As modern web services increasingly rely on REST APIs, their thorough testing\nhas become crucial. Furthermore, the advent of REST API specifications such as\nthe OpenAPI Specification has led to the emergence of many black-box REST API\ntesting tools. However, these tools often focus on individual test elements in\nisolation (e.g., APIs, parameters, values), resulting in lower coverage and\nless effectiveness in detecting faults (i.e., 500 response codes). To address\nthese limitations, we present AutoRestTest, the first black-box framework to\nadopt a dependency-embedded multi-agent approach for REST API testing,\nintegrating Multi-Agent Reinforcement Learning (MARL) with a Semantic Property\nDependency Graph (SPDG) and Large Language Models (LLMs). Our approach treats\nREST API testing as a separable problem, where four agents -- API, dependency,\nparameter, and value -- collaborate to optimize API exploration. LLMs handle\ndomain-specific value restrictions, the SPDG model simplifies the search space\nfor dependencies using a similarity score between API operations, and MARL\ndynamically optimizes the agents' behavior. Evaluated on 12 real-world REST\nservices, AutoRestTest outperforms the four leading black-box REST API testing\ntools, including those assisted by RESTGPT (which augments realistic test\ninputs using LLMs), in terms of code coverage, operation coverage, and fault\ndetection. Notably, AutoRestTest is the only tool able to identify an internal\nserver error in Spotify. Our ablation study underscores the significant\ncontributions of the agent learning, SPDG, and LLM components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As modern web services increasingly rely on REST APIs, their thorough testing\nhas become crucial. Furthermore, the advent of REST API specifications such as\nthe OpenAPI Specification has led to the emergence of many black-box REST API\ntesting tools. However, these tools often focus on individual test elements in\nisolation (e.g., APIs, parameters, values), resulting in lower coverage and\nless effectiveness in detecting faults (i.e., 500 response codes). To address\nthese limitations, we present AutoRestTest, the first black-box framework to\nadopt a dependency-embedded multi-agent approach for REST API testing,\nintegrating Multi-Agent Reinforcement Learning (MARL) with a Semantic Property\nDependency Graph (SPDG) and Large Language Models (LLMs). Our approach treats\nREST API testing as a separable problem, where four agents -- API, dependency,\nparameter, and value -- collaborate to optimize API exploration. LLMs handle\ndomain-specific value restrictions, the SPDG model simplifies the search space\nfor dependencies using a similarity score between API operations, and MARL\ndynamically optimizes the agents' behavior. Evaluated on 12 real-world REST\nservices, AutoRestTest outperforms the four leading black-box REST API testing\ntools, including those assisted by RESTGPT (which augments realistic test\ninputs using LLMs), in terms of code coverage, operation coverage, and fault\ndetection. Notably, AutoRestTest is the only tool able to identify an internal\nserver error in Spotify. Our ablation study underscores the significant\ncontributions of the agent learning, SPDG, and LLM components."
                },
                "authors": [
                    {
                        "name": "Myeongsoo Kim"
                    },
                    {
                        "name": "Tyler Stennett"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Alessandro Orso"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Orso"
                },
                "author": "Alessandro Orso",
                "arxiv_comment": "To be published in the 47th IEEE/ACM International Conference on\n  Software Engineering (ICSE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07091v1",
                "updated": "2024-11-11T16:12:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    12,
                    11,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T16:12:11Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    12,
                    11,
                    0,
                    316,
                    0
                ],
                "title": "Impact of LLM-based Review Comment Generation in Practice: A Mixed\n  Open-/Closed-source User Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of LLM-based Review Comment Generation in Practice: A Mixed\n  Open-/Closed-source User Study"
                },
                "summary": "We conduct a large-scale empirical user study in a live setup to evaluate the\nacceptance of LLM-generated comments and their impact on the review process.\nThis user study was performed in two organizations, Mozilla (which has its\ncodebase available as open source) and Ubisoft (fully closed-source). Inside\ntheir usual review environment, participants were given access to RevMate, an\nLLM-based assistive tool suggesting generated review comments using an\noff-the-shelf LLM with Retrieval Augmented Generation to provide extra code and\nreview context, combined with LLM-as-a-Judge, to auto-evaluate the generated\ncomments and discard irrelevant cases. Based on more than 587 patch reviews\nprovided by RevMate, we observed that 8.1% and 7.2%, respectively, of\nLLM-generated comments were accepted by reviewers in each organization, while\n14.6% and 20.5% other comments were still marked as valuable as review or\ndevelopment tips. Refactoring-related comments are more likely to be accepted\nthan Functional comments (18.2% and 18.6% compared to 4.8% and 5.2%). The extra\ntime spent by reviewers to inspect generated comments or edit accepted ones\n(36/119), yielding an overall median of 43s per patch, is reasonable. The\naccepted generated comments are as likely to yield future revisions of the\nrevised patch as human-written comments (74% vs 73% at chunk-level).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We conduct a large-scale empirical user study in a live setup to evaluate the\nacceptance of LLM-generated comments and their impact on the review process.\nThis user study was performed in two organizations, Mozilla (which has its\ncodebase available as open source) and Ubisoft (fully closed-source). Inside\ntheir usual review environment, participants were given access to RevMate, an\nLLM-based assistive tool suggesting generated review comments using an\noff-the-shelf LLM with Retrieval Augmented Generation to provide extra code and\nreview context, combined with LLM-as-a-Judge, to auto-evaluate the generated\ncomments and discard irrelevant cases. Based on more than 587 patch reviews\nprovided by RevMate, we observed that 8.1% and 7.2%, respectively, of\nLLM-generated comments were accepted by reviewers in each organization, while\n14.6% and 20.5% other comments were still marked as valuable as review or\ndevelopment tips. Refactoring-related comments are more likely to be accepted\nthan Functional comments (18.2% and 18.6% compared to 4.8% and 5.2%). The extra\ntime spent by reviewers to inspect generated comments or edit accepted ones\n(36/119), yielding an overall median of 43s per patch, is reasonable. The\naccepted generated comments are as likely to yield future revisions of the\nrevised patch as human-written comments (74% vs 73% at chunk-level)."
                },
                "authors": [
                    {
                        "name": "Doriane Olewicki"
                    },
                    {
                        "name": "Leuson Da Silva"
                    },
                    {
                        "name": "Suhaib Mujahid"
                    },
                    {
                        "name": "Arezou Amini"
                    },
                    {
                        "name": "Benjamin Mah"
                    },
                    {
                        "name": "Marco Castelluccio"
                    },
                    {
                        "name": "Sarra Habchi"
                    },
                    {
                        "name": "Foutse Khomh"
                    },
                    {
                        "name": "Bram Adams"
                    }
                ],
                "author_detail": {
                    "name": "Bram Adams"
                },
                "author": "Bram Adams",
                "arxiv_comment": "12pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10858v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10858v4",
                "updated": "2024-11-11T15:48:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    48,
                    28,
                    0,
                    316,
                    0
                ],
                "published": "2023-04-21T10:01:56Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    10,
                    1,
                    56,
                    4,
                    111,
                    0
                ],
                "title": "Orchestration Framework for Open System Models with Autonomous RISs and\n  Oblivious Base Stations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orchestration Framework for Open System Models with Autonomous RISs and\n  Oblivious Base Stations"
                },
                "summary": "Autonomous reconfigurable intelligent surface (RIS) offers the potential to\nsimplify deployment by reducing the need for real-time remote control between a\nbase station (BS) and an RIS. However, we highlight two major challenges posed\nby autonomy. The first is implementation complexity, as autonomy requires\nhybrid RISs (HRISs) equipped with additional on-board hardware to monitor the\npropagation environment and conduct local channel estimation (CHEST), a process\nknown as probing. The second challenge, termed probe distortion, reflects a\nform of the observer effect: during probing, an HRIS can inadvertently alter\nthe propagation environment, potentially disrupting the operations of other\ncommunicating devices. While implementation complexity has been extensively\nstudied, probe distortion remains largely unexplored. To further assess the\npotential of autonomous RISs, this paper comprehensively and pragmatically\nstudies fundamental trade-offs posed by these challenges. We examine the\nrobustness of an HRIS-assisted massive multiple-input multiple-output (mMIMO)\nsystem under minimal design choices that reflect the essential elements and\nstringent conditions, including (a) two extremes of implementation complexity\nrealized through minimalist operational designs of two HRIS hardware\narchitectures, and (b) an oblivious BS that fully embraces probe distortion. To\nmake our analysis possible, we propose a physical-layer orchestration framework\nthat aligns HRIS and mMIMO operations. We provide empirical evidence showing\nthat autonomous RIS holds promise even under these strict conditions and\npropose new research directions, particularly for advancing the understanding\nof probe distortion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous reconfigurable intelligent surface (RIS) offers the potential to\nsimplify deployment by reducing the need for real-time remote control between a\nbase station (BS) and an RIS. However, we highlight two major challenges posed\nby autonomy. The first is implementation complexity, as autonomy requires\nhybrid RISs (HRISs) equipped with additional on-board hardware to monitor the\npropagation environment and conduct local channel estimation (CHEST), a process\nknown as probing. The second challenge, termed probe distortion, reflects a\nform of the observer effect: during probing, an HRIS can inadvertently alter\nthe propagation environment, potentially disrupting the operations of other\ncommunicating devices. While implementation complexity has been extensively\nstudied, probe distortion remains largely unexplored. To further assess the\npotential of autonomous RISs, this paper comprehensively and pragmatically\nstudies fundamental trade-offs posed by these challenges. We examine the\nrobustness of an HRIS-assisted massive multiple-input multiple-output (mMIMO)\nsystem under minimal design choices that reflect the essential elements and\nstringent conditions, including (a) two extremes of implementation complexity\nrealized through minimalist operational designs of two HRIS hardware\narchitectures, and (b) an oblivious BS that fully embraces probe distortion. To\nmake our analysis possible, we propose a physical-layer orchestration framework\nthat aligns HRIS and mMIMO operations. We provide empirical evidence showing\nthat autonomous RIS holds promise even under these strict conditions and\npropose new research directions, particularly for advancing the understanding\nof probe distortion."
                },
                "authors": [
                    {
                        "name": "Victor Croisfelt"
                    },
                    {
                        "name": "Francesco Devoti"
                    },
                    {
                        "name": "Fabio Saggese"
                    },
                    {
                        "name": "Vincenzo Sciancalepore"
                    },
                    {
                        "name": "Xavier Costa-Prez"
                    },
                    {
                        "name": "Petar Popovski"
                    }
                ],
                "author_detail": {
                    "name": "Petar Popovski"
                },
                "author": "Petar Popovski",
                "arxiv_comment": "16 pages, 9 figures, submitted to IEEE Transactions on Wireless\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10858v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10858v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07071v1",
                "updated": "2024-11-11T15:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    47,
                    15,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T15:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    47,
                    15,
                    0,
                    316,
                    0
                ],
                "title": "Universal Response and Emergence of Induction in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Response and Emergence of Induction in LLMs"
                },
                "summary": "While induction is considered a key mechanism for in-context learning in\nLLMs, understanding its precise circuit decomposition beyond toy models remains\nelusive. Here, we study the emergence of induction behavior within LLMs by\nprobing their response to weak single-token perturbations of the residual\nstream. We find that LLMs exhibit a robust, universal regime in which their\nresponse remains scale-invariant under changes in perturbation strength,\nthereby allowing us to quantify the build-up of token correlations throughout\nthe model. By applying our method, we observe signatures of induction behavior\nwithin the residual stream of Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL. Across\nall models, we find that these induction signatures gradually emerge within\nintermediate layers and identify the relevant model sections composing this\nbehavior. Our results provide insights into the collective interplay of\ncomponents within LLMs and serve as a benchmark for large-scale circuit\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While induction is considered a key mechanism for in-context learning in\nLLMs, understanding its precise circuit decomposition beyond toy models remains\nelusive. Here, we study the emergence of induction behavior within LLMs by\nprobing their response to weak single-token perturbations of the residual\nstream. We find that LLMs exhibit a robust, universal regime in which their\nresponse remains scale-invariant under changes in perturbation strength,\nthereby allowing us to quantify the build-up of token correlations throughout\nthe model. By applying our method, we observe signatures of induction behavior\nwithin the residual stream of Gemma-2-2B, Llama-3.2-3B, and GPT-2-XL. Across\nall models, we find that these induction signatures gradually emerge within\nintermediate layers and identify the relevant model sections composing this\nbehavior. Our results provide insights into the collective interplay of\ncomponents within LLMs and serve as a benchmark for large-scale circuit\nanalysis."
                },
                "authors": [
                    {
                        "name": "Niclas Luick"
                    }
                ],
                "author_detail": {
                    "name": "Niclas Luick"
                },
                "author": "Niclas Luick",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10825v2",
                "updated": "2024-11-11T15:45:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    45,
                    2,
                    0,
                    316,
                    0
                ],
                "published": "2024-01-19T17:21:05Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    17,
                    21,
                    5,
                    4,
                    19,
                    0
                ],
                "title": "Recent Advances in Named Entity Recognition: A Comprehensive Survey and\n  Comparative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Named Entity Recognition: A Comprehensive Survey and\n  Comparative Study"
                },
                "summary": "Named Entity Recognition seeks to extract substrings within a text that name\nreal-world objects and to determine their type (for example, whether they refer\nto persons or organizations). In this survey, we first present an overview of\nrecent popular approaches, including advancements in Transformer-based methods\nand Large Language Models (LLMs) that have not had much coverage in other\nsurveys. In addition, we discuss reinforcement learning and graph-based\napproaches, highlighting their role in enhancing NER performance. Second, we\nfocus on methods designed for datasets with scarce annotations. Third, we\nevaluate the performance of the main NER implementations on a variety of\ndatasets with differing characteristics (as regards their domain, their size,\nand their number of classes). We thus provide a deep comparison of algorithms\nthat have never been considered together. Our experiments shed some light on\nhow the characteristics of datasets affect the behavior of the methods we\ncompare.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition seeks to extract substrings within a text that name\nreal-world objects and to determine their type (for example, whether they refer\nto persons or organizations). In this survey, we first present an overview of\nrecent popular approaches, including advancements in Transformer-based methods\nand Large Language Models (LLMs) that have not had much coverage in other\nsurveys. In addition, we discuss reinforcement learning and graph-based\napproaches, highlighting their role in enhancing NER performance. Second, we\nfocus on methods designed for datasets with scarce annotations. Third, we\nevaluate the performance of the main NER implementations on a variety of\ndatasets with differing characteristics (as regards their domain, their size,\nand their number of classes). We thus provide a deep comparison of algorithms\nthat have never been considered together. Our experiments shed some light on\nhow the characteristics of datasets affect the behavior of the methods we\ncompare."
                },
                "authors": [
                    {
                        "name": "Imed Keraghel"
                    },
                    {
                        "name": "Stanislas Morbieu"
                    },
                    {
                        "name": "Mohamed Nadif"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Nadif"
                },
                "author": "Mohamed Nadif",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68Q32",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07066v1",
                "updated": "2024-11-11T15:30:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    30,
                    16,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T15:30:16Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    30,
                    16,
                    0,
                    316,
                    0
                ],
                "title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training"
                },
                "summary": "Network pruning is a set of computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has focused on pruning and re-training, which\nnowadays is inconvenient due to the vast amount of pre-trained models, which\nare in any case too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAl}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs, that\nmodifies the block-wise and row-wise sparsity ratios to maximize the\n\\emph{neuron alignment} among activations. Moreover, differently from existing\nmethods, our approach adaptively selects the best parameters for the block-wise\nand row-wise sparsity ratios w.r.t. to the model and the desired sparsity\n(given as input), and requires \\emph{no re-training}. We test our method on 4\ndifferent LLM families and 3 different sparsity ratios, showing how it\nconsistently outperforms the latest state-of-the-art techniques. The code is\navailable at https://github.com/eliacunegatti/NeuroAL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network pruning is a set of computational techniques that aim to reduce a\ngiven model's computational cost by removing a subset of its parameters while\nhaving minimal impact on performance. Throughout the last decade, the most\nwidely used pruning paradigm has focused on pruning and re-training, which\nnowadays is inconvenient due to the vast amount of pre-trained models, which\nare in any case too expensive to re-train. In this paper, we exploit functional\ninformation from dense pre-trained models, i.e., their activations, to obtain\nsparse models that maximize the activations' alignment w.r.t. their\ncorresponding dense models. Hence, we propose \\textsc{NeuroAl}, a \\emph{top-up}\nalgorithm that can be used on top of any given pruning algorithm for LLMs, that\nmodifies the block-wise and row-wise sparsity ratios to maximize the\n\\emph{neuron alignment} among activations. Moreover, differently from existing\nmethods, our approach adaptively selects the best parameters for the block-wise\nand row-wise sparsity ratios w.r.t. to the model and the desired sparsity\n(given as input), and requires \\emph{no re-training}. We test our method on 4\ndifferent LLM families and 3 different sparsity ratios, showing how it\nconsistently outperforms the latest state-of-the-art techniques. The code is\navailable at https://github.com/eliacunegatti/NeuroAL."
                },
                "authors": [
                    {
                        "name": "Elia Cunegatti"
                    },
                    {
                        "name": "Leonardo Lucio Custode"
                    },
                    {
                        "name": "Giovanni Iacca"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Iacca"
                },
                "author": "Giovanni Iacca",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.10593v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.10593v5",
                "updated": "2024-11-11T15:16:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    15,
                    16,
                    27,
                    0,
                    316,
                    0
                ],
                "published": "2023-12-17T02:55:13Z",
                "published_parsed": [
                    2023,
                    12,
                    17,
                    2,
                    55,
                    13,
                    6,
                    351,
                    0
                ],
                "title": "A Novel RFID Authentication Protocol Based on A Block-Order-Modulus\n  Variable Matrix Encryption Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel RFID Authentication Protocol Based on A Block-Order-Modulus\n  Variable Matrix Encryption Algorithm"
                },
                "summary": "In this paper, authentication for mobile radio frequency identification\n(RFID) systems with low-cost RFID sensor tags is studied. Firstly, an adaptive\nmodulus (AM) encryption algorithm is proposed. Subsequently, in order to\nenhance the security without additional storage of new key matrices, a\nself-updating encryption order (SUEO) algorithm is designed. Furthermore, a\ndiagonal block local transpose key matrix (DBLTKM) encryption algorithm is\npresented, which effectively expands the feasible domain of the key space.\nBased on the above three algorithms, a novel joint AM-SUEO-DBLTKM encryption\nalgorithm is constructed. Making full use of the advantages of the proposed\njoint algorithm, a two-way RFID authentication protocol, named\nAM-SUEO-DBLTKM-RFID, is proposed for mobile RFID systems. In addition, the\nBurrows-Abadi-Needham (BAN) logic and security analysis indicate that the\nproposed AM-SUEO-DBLTKM-RFID protocol can effectively combat various typical\nattacks. Numerical results demonstrate that the proposed AM-SUEO-DBLTKM\nalgorithm can save 99.59% of tag storage over traditional algorithms. Finally,\nthe low computational complexity as well as the low storage cost of the\nproposed AM-SUEO-DBLTKM-RFID protocol facilitates deployment within low-cost\nRFID sensor tags.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, authentication for mobile radio frequency identification\n(RFID) systems with low-cost RFID sensor tags is studied. Firstly, an adaptive\nmodulus (AM) encryption algorithm is proposed. Subsequently, in order to\nenhance the security without additional storage of new key matrices, a\nself-updating encryption order (SUEO) algorithm is designed. Furthermore, a\ndiagonal block local transpose key matrix (DBLTKM) encryption algorithm is\npresented, which effectively expands the feasible domain of the key space.\nBased on the above three algorithms, a novel joint AM-SUEO-DBLTKM encryption\nalgorithm is constructed. Making full use of the advantages of the proposed\njoint algorithm, a two-way RFID authentication protocol, named\nAM-SUEO-DBLTKM-RFID, is proposed for mobile RFID systems. In addition, the\nBurrows-Abadi-Needham (BAN) logic and security analysis indicate that the\nproposed AM-SUEO-DBLTKM-RFID protocol can effectively combat various typical\nattacks. Numerical results demonstrate that the proposed AM-SUEO-DBLTKM\nalgorithm can save 99.59% of tag storage over traditional algorithms. Finally,\nthe low computational complexity as well as the low storage cost of the\nproposed AM-SUEO-DBLTKM-RFID protocol facilitates deployment within low-cost\nRFID sensor tags."
                },
                "authors": [
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Ruiqi Liu"
                    },
                    {
                        "name": "Tong Gao"
                    },
                    {
                        "name": "Feng Shu"
                    },
                    {
                        "name": "Xuemei Lei"
                    },
                    {
                        "name": "Yongpeng Wu"
                    },
                    {
                        "name": "Guan Gui"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangzhou Wang"
                },
                "author": "Jiangzhou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.10593v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.10593v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07037v1",
                "updated": "2024-11-11T14:43:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    43,
                    51,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T14:43:51Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    43,
                    51,
                    0,
                    316,
                    0
                ],
                "title": "LIFBench: Evaluating the Instruction Following Performance and Stability\n  of Large Language Models in Long-Context Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LIFBench: Evaluating the Instruction Following Performance and Stability\n  of Large Language Models in Long-Context Scenarios"
                },
                "summary": "As Large Language Models (LLMs) continue to advance in natural language\nprocessing (NLP), their ability to stably follow instructions in long-context\ninputs has become crucial for real-world applications. While existing\nbenchmarks assess various LLM capabilities, they rarely focus on\ninstruction-following in long-context scenarios or stability on different\ninputs. In response, we introduce the Long-context Instruction-Following\nBenchmark (LIFBench), a scalable dataset designed to evaluate LLMs'\ninstruction-following capabilities and stability across long contexts. LIFBench\ncomprises three long-context scenarios and eleven diverse tasks, supported by\n2,766 instructions generated through an automated expansion method across three\ndimensions: length, expression, and variables. For evaluation, we propose\nLIFEval, a rubric-based assessment framework that provides precise, automated\nscoring of complex LLM responses without relying on LLM-assisted evaluations or\nhuman judgments. This approach facilitates a comprehensive analysis of model\nperformance and stability across various perspectives. We conduct extensive\nexperiments on 20 notable LLMs across six length intervals, analyzing their\ninstruction-following capabilities and stability. Our work contributes LIFBench\nand LIFEval as robust tools for assessing LLM performance in complex,\nlong-context settings, providing insights that can inform future LLM\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to advance in natural language\nprocessing (NLP), their ability to stably follow instructions in long-context\ninputs has become crucial for real-world applications. While existing\nbenchmarks assess various LLM capabilities, they rarely focus on\ninstruction-following in long-context scenarios or stability on different\ninputs. In response, we introduce the Long-context Instruction-Following\nBenchmark (LIFBench), a scalable dataset designed to evaluate LLMs'\ninstruction-following capabilities and stability across long contexts. LIFBench\ncomprises three long-context scenarios and eleven diverse tasks, supported by\n2,766 instructions generated through an automated expansion method across three\ndimensions: length, expression, and variables. For evaluation, we propose\nLIFEval, a rubric-based assessment framework that provides precise, automated\nscoring of complex LLM responses without relying on LLM-assisted evaluations or\nhuman judgments. This approach facilitates a comprehensive analysis of model\nperformance and stability across various perspectives. We conduct extensive\nexperiments on 20 notable LLMs across six length intervals, analyzing their\ninstruction-following capabilities and stability. Our work contributes LIFBench\nand LIFEval as robust tools for assessing LLM performance in complex,\nlong-context settings, providing insights that can inform future LLM\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Xiaodong Wu"
                    },
                    {
                        "name": "Minhao Wang"
                    },
                    {
                        "name": "Yichen Liu"
                    },
                    {
                        "name": "Xiaoming Shi"
                    },
                    {
                        "name": "He Yan"
                    },
                    {
                        "name": "Xiangju Lu"
                    },
                    {
                        "name": "Junmin Zhu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "17 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09824v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09824v4",
                "updated": "2024-11-11T14:41:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    41,
                    53,
                    0,
                    316,
                    0
                ],
                "published": "2024-10-13T12:57:08Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    12,
                    57,
                    8,
                    6,
                    287,
                    0
                ],
                "title": "Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic and Textual Graph Generation Via Large-Scale LLM-based Agent\n  Simulation"
                },
                "summary": "Graph generation is a fundamental task that has been extensively studied in\nsocial, technological, and scientific analysis. For modeling the dynamic graph\nevolution process, traditional rule-based methods struggle to capture community\nstructures within graphs, while deep learning methods only focus on fitting\ntraining graphs. This limits existing graph generators to producing graphs that\nadhere to predefined rules or closely resemble training datasets, achieving\npoor performance in dynamic graph generation. Given that graphs are abstract\nrepresentations arising from pairwise interactions in human activities, a\nrealistic simulation of human-wise interaction could provide deeper insights\ninto the graph evolution mechanism. With the increasing recognition of large\nlanguage models (LLMs) in simulating human behavior, we introduce\nGraphAgent-Generator (GAG), a novel simulation-based framework for dynamic\ngraph generation. Without training or fine-tuning process of LLM, our framework\neffectively replicates seven macro-level structural characteristics in\nestablished network science theories while surpassing existing baselines in\ngraph expansion tasks by 31\\% on specific evaluation metrics. Through node\nclassification task, we validate GAG effectively preserves characteristics of\nreal-world network for node-wise textual features in generated text-rich graph.\nFurthermore, by incorporating parallel acceleration, GAG supports generating\ngraphs with up to nearly 100,000 nodes or 10 million edges through large-scale\nLLM-based agent simulation, with a minimum speed-up of 90.4\\%. The source code\nis available at https://anonymous.4open.science/r/GraphAgent-2206.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph generation is a fundamental task that has been extensively studied in\nsocial, technological, and scientific analysis. For modeling the dynamic graph\nevolution process, traditional rule-based methods struggle to capture community\nstructures within graphs, while deep learning methods only focus on fitting\ntraining graphs. This limits existing graph generators to producing graphs that\nadhere to predefined rules or closely resemble training datasets, achieving\npoor performance in dynamic graph generation. Given that graphs are abstract\nrepresentations arising from pairwise interactions in human activities, a\nrealistic simulation of human-wise interaction could provide deeper insights\ninto the graph evolution mechanism. With the increasing recognition of large\nlanguage models (LLMs) in simulating human behavior, we introduce\nGraphAgent-Generator (GAG), a novel simulation-based framework for dynamic\ngraph generation. Without training or fine-tuning process of LLM, our framework\neffectively replicates seven macro-level structural characteristics in\nestablished network science theories while surpassing existing baselines in\ngraph expansion tasks by 31\\% on specific evaluation metrics. Through node\nclassification task, we validate GAG effectively preserves characteristics of\nreal-world network for node-wise textual features in generated text-rich graph.\nFurthermore, by incorporating parallel acceleration, GAG supports generating\ngraphs with up to nearly 100,000 nodes or 10 million edges through large-scale\nLLM-based agent simulation, with a minimum speed-up of 90.4\\%. The source code\nis available at https://anonymous.4open.science/r/GraphAgent-2206."
                },
                "authors": [
                    {
                        "name": "Jiarui Ji"
                    },
                    {
                        "name": "Runlin Lei"
                    },
                    {
                        "name": "Jialing Bi"
                    },
                    {
                        "name": "Zhewei Wei"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    }
                ],
                "author_detail": {
                    "name": "Bolin Ding"
                },
                "author": "Bolin Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09824v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09824v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19266v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19266v4",
                "updated": "2024-11-11T14:36:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    36,
                    35,
                    0,
                    316,
                    0
                ],
                "published": "2024-05-29T16:59:38Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    16,
                    59,
                    38,
                    2,
                    150,
                    0
                ],
                "title": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for\n  Pediatric Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PediatricsGPT: Large Language Models as Chinese Medical Assistants for\n  Pediatric Applications"
                },
                "summary": "Developing intelligent pediatric consultation systems offers promising\nprospects for improving diagnostic efficiency, especially in China, where\nhealthcare resources are scarce. Despite recent advances in Large Language\nModels (LLMs) for Chinese medicine, their performance is sub-optimal in\npediatric applications due to inadequate instruction data and vulnerable\ntraining procedures. To address the above issues, this paper builds PedCorpus,\na high-quality dataset of over 300,000 multi-task instructions from pediatric\ntextbooks, guidelines, and knowledge graph resources to fulfil diverse\ndiagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the\nfirst Chinese pediatric LLM assistant built on a systematic and robust training\npipeline. In the continuous pre-training phase, we introduce a hybrid\ninstruction pre-training mechanism to mitigate the internal-injected knowledge\ninconsistency of LLMs for medical domain adaptation. Immediately, the\nfull-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the\ngeneral medical knowledge schema into the models. After that, we devise a\ndirect following preference optimization to enhance the generation of\npediatrician-like humanistic responses. In the parameter-efficient secondary\nSFT phase, a mixture of universal-specific experts strategy is presented to\nresolve the competency conflict between medical generalist and pediatric\nexpertise mastery. Extensive results based on the metrics, GPT-4, and doctor\nevaluations on distinct doctor downstream tasks show that PediatricsGPT\nconsistently outperforms previous Chinese medical LLMs. Our model and dataset\nwill be open-source for community development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing intelligent pediatric consultation systems offers promising\nprospects for improving diagnostic efficiency, especially in China, where\nhealthcare resources are scarce. Despite recent advances in Large Language\nModels (LLMs) for Chinese medicine, their performance is sub-optimal in\npediatric applications due to inadequate instruction data and vulnerable\ntraining procedures. To address the above issues, this paper builds PedCorpus,\na high-quality dataset of over 300,000 multi-task instructions from pediatric\ntextbooks, guidelines, and knowledge graph resources to fulfil diverse\ndiagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the\nfirst Chinese pediatric LLM assistant built on a systematic and robust training\npipeline. In the continuous pre-training phase, we introduce a hybrid\ninstruction pre-training mechanism to mitigate the internal-injected knowledge\ninconsistency of LLMs for medical domain adaptation. Immediately, the\nfull-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the\ngeneral medical knowledge schema into the models. After that, we devise a\ndirect following preference optimization to enhance the generation of\npediatrician-like humanistic responses. In the parameter-efficient secondary\nSFT phase, a mixture of universal-specific experts strategy is presented to\nresolve the competency conflict between medical generalist and pediatric\nexpertise mastery. Extensive results based on the metrics, GPT-4, and doctor\nevaluations on distinct doctor downstream tasks show that PediatricsGPT\nconsistently outperforms previous Chinese medical LLMs. Our model and dataset\nwill be open-source for community development."
                },
                "authors": [
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Jinjie Wei"
                    },
                    {
                        "name": "Dongling Xiao"
                    },
                    {
                        "name": "Shunli Wang"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Gang Li"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Shuaibing Wang"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Yue Jiang"
                    },
                    {
                        "name": "Qingyao Xu"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Peng Zhai"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2024. A Technical Report on a Chinese Medical\n  Large Language Model",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19266v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19266v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08877v2",
                "updated": "2024-11-11T14:35:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    35,
                    45,
                    0,
                    316,
                    0
                ],
                "published": "2024-04-13T02:36:40Z",
                "published_parsed": [
                    2024,
                    4,
                    13,
                    2,
                    36,
                    40,
                    5,
                    104,
                    0
                ],
                "title": "Aligning LLMs for FL-free Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning LLMs for FL-free Program Repair"
                },
                "summary": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR."
                },
                "authors": [
                    {
                        "name": "Junjielong Xu"
                    },
                    {
                        "name": "Ying Fu"
                    },
                    {
                        "name": "Shin Hwei Tan"
                    },
                    {
                        "name": "Pinjia He"
                    }
                ],
                "author_detail": {
                    "name": "Pinjia He"
                },
                "author": "Pinjia He",
                "arxiv_comment": "Accepted by ICSE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07021v1",
                "updated": "2024-11-11T14:25:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    25,
                    37,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T14:25:37Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    14,
                    25,
                    37,
                    0,
                    316,
                    0
                ],
                "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) has shown impressive capability in\nproviding reliable answer predictions and addressing hallucination problems. A\ntypical RAG implementation uses powerful retrieval models to extract external\ninformation and large language models (LLMs) to generate answers. In contrast,\nrecent LLM-based retrieval has gained attention for its substantial\nimprovements in information retrieval (IR) due to the LLMs' semantic\nunderstanding capability. However, directly applying LLM to RAG systems\npresents challenges. This may cause feature locality problems as massive\nparametric knowledge can hinder effective usage of global information across\nthe corpus; for example, an LLM-based retriever often inputs document summaries\ninstead of full documents. Moreover, various pre-trained tasks in LLMs\nintroduce variance, further weakening performance as a retriever.\n  To address these issues, we propose a novel two-stage fine-tuning\narchitecture called Invar-RAG. In the retrieval stage, an LLM-based retriever\nis constructed by integrating LoRA-based representation learning to tackle\nfeature locality issues. To enhance retrieval performance, we develop two\npatterns (invariant and variant patterns) and an invariance loss to reduce LLM\nvariance. In the generation stage, a refined fine-tuning method is employed to\nimprove LLM accuracy in generating answers based on retrieved information.\nExperimental results show that Invar-RAG significantly outperforms existing\nbaselines across three open-domain question answering (ODQA) datasets. Code is\navailable in the Supplementary Material for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has shown impressive capability in\nproviding reliable answer predictions and addressing hallucination problems. A\ntypical RAG implementation uses powerful retrieval models to extract external\ninformation and large language models (LLMs) to generate answers. In contrast,\nrecent LLM-based retrieval has gained attention for its substantial\nimprovements in information retrieval (IR) due to the LLMs' semantic\nunderstanding capability. However, directly applying LLM to RAG systems\npresents challenges. This may cause feature locality problems as massive\nparametric knowledge can hinder effective usage of global information across\nthe corpus; for example, an LLM-based retriever often inputs document summaries\ninstead of full documents. Moreover, various pre-trained tasks in LLMs\nintroduce variance, further weakening performance as a retriever.\n  To address these issues, we propose a novel two-stage fine-tuning\narchitecture called Invar-RAG. In the retrieval stage, an LLM-based retriever\nis constructed by integrating LoRA-based representation learning to tackle\nfeature locality issues. To enhance retrieval performance, we develop two\npatterns (invariant and variant patterns) and an invariance loss to reduce LLM\nvariance. In the generation stage, a refined fine-tuning method is employed to\nimprove LLM accuracy in generating answers based on retrieved information.\nExperimental results show that Invar-RAG significantly outperforms existing\nbaselines across three open-domain question answering (ODQA) datasets. Code is\navailable in the Supplementary Material for reproducibility."
                },
                "authors": [
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Qian Li"
                    },
                    {
                        "name": "Jianghua Wu"
                    },
                    {
                        "name": "Guangxu Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Guangxu Zhu"
                },
                "author": "Guangxu Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16209v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16209v2",
                "updated": "2024-11-11T13:56:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    13,
                    56,
                    30,
                    0,
                    316,
                    0
                ],
                "published": "2024-09-24T16:09:29Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    9,
                    29,
                    1,
                    268,
                    0
                ],
                "title": "LLMCount: Enhancing Stationary mmWave Detection with Multimodal-LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMCount: Enhancing Stationary mmWave Detection with Multimodal-LLM"
                },
                "summary": "Millimeter wave sensing provides people with the capability of sensing the\nsurrounding crowds in a non-invasive and privacy-preserving manner, which holds\nhuge application potential. However, detecting stationary crowds remains\nchallenging due to several factors such as minimal movements (like breathing or\ncasual fidgets), which can be easily treated as noise clusters during data\ncollection and consequently filtered in the following processing procedures.\nAdditionally, the uneven distribution of signal power due to signal power\nattenuation and interferences resulting from external reflectors or absorbers\nfurther complicates accurate detection. To address these challenges and enable\nstationary crowd detection across various application scenarios requiring\nspecialized domain adaption, we introduce LLMCount, the first system to harness\nthe capabilities of large-language models (LLMs) to enhance crowd detection\nperformance. By exploiting the decision-making capability of LLM, we can\nsuccessfully compensate the signal power to acquire a uniform distribution and\nthereby achieve a detection with higher accuracy. To assess the system's\nperformance, comprehensive evaluations are conducted under diversified\nscenarios like hall, meeting room, and cinema. The evaluation results show that\nour proposed approach reaches high detection accuracy with lower overall\nlatency compared with previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Millimeter wave sensing provides people with the capability of sensing the\nsurrounding crowds in a non-invasive and privacy-preserving manner, which holds\nhuge application potential. However, detecting stationary crowds remains\nchallenging due to several factors such as minimal movements (like breathing or\ncasual fidgets), which can be easily treated as noise clusters during data\ncollection and consequently filtered in the following processing procedures.\nAdditionally, the uneven distribution of signal power due to signal power\nattenuation and interferences resulting from external reflectors or absorbers\nfurther complicates accurate detection. To address these challenges and enable\nstationary crowd detection across various application scenarios requiring\nspecialized domain adaption, we introduce LLMCount, the first system to harness\nthe capabilities of large-language models (LLMs) to enhance crowd detection\nperformance. By exploiting the decision-making capability of LLM, we can\nsuccessfully compensate the signal power to acquire a uniform distribution and\nthereby achieve a detection with higher accuracy. To assess the system's\nperformance, comprehensive evaluations are conducted under diversified\nscenarios like hall, meeting room, and cinema. The evaluation results show that\nour proposed approach reaches high detection accuracy with lower overall\nlatency compared with previous methods."
                },
                "authors": [
                    {
                        "name": "Boyan Li"
                    },
                    {
                        "name": "Shengyi Ding"
                    },
                    {
                        "name": "Deen Ma"
                    },
                    {
                        "name": "Yixuan Wu"
                    },
                    {
                        "name": "Hongjie Liao"
                    },
                    {
                        "name": "Kaiyuan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyuan Hu"
                },
                "author": "Kaiyuan Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16209v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16209v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06983v1",
                "updated": "2024-11-11T13:41:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    13,
                    41,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T13:41:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    13,
                    41,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Sensing Capacity for Integrated Sensing and Communication Systems in\n  Low-Altitude Economy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing Capacity for Integrated Sensing and Communication Systems in\n  Low-Altitude Economy"
                },
                "summary": "The burgeoning significance of the low-altitude economy (LAE) has garnered\nconsiderable interest, largely fuelled by the widespread deployment of unmanned\naerial vehicles (UAVs). To tackle the challenges associated with the detection\nof unauthorized UAVs and the efficient scheduling of authorized UAVs, this\nletter introduces a novel performance metric, termed sensing capacity, for\nintegrated sensing and communication (ISAC) systems. This metric, which\nquantifies the capability of a base station (BS) to detect multiple UAVs\nsimultaneously, leverages signal-to-noise ratio (SNR) and probability of\ndetection (PD) as key intermediate variables. Through mathematical derivations,\nwe can derive a closed-form solution for the maximum number of UAVs that can be\ndetected by the BS while adhering to a specific SNR constraint. Furthermore, an\napproximate solution based on PD constraints is proposed to facilitate the\nefficient determination of the threshold for the maximum number of detectable\nUAVs. The accuracy of this analytical approach is verified through extensive\nsimulation results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The burgeoning significance of the low-altitude economy (LAE) has garnered\nconsiderable interest, largely fuelled by the widespread deployment of unmanned\naerial vehicles (UAVs). To tackle the challenges associated with the detection\nof unauthorized UAVs and the efficient scheduling of authorized UAVs, this\nletter introduces a novel performance metric, termed sensing capacity, for\nintegrated sensing and communication (ISAC) systems. This metric, which\nquantifies the capability of a base station (BS) to detect multiple UAVs\nsimultaneously, leverages signal-to-noise ratio (SNR) and probability of\ndetection (PD) as key intermediate variables. Through mathematical derivations,\nwe can derive a closed-form solution for the maximum number of UAVs that can be\ndetected by the BS while adhering to a specific SNR constraint. Furthermore, an\napproximate solution based on PD constraints is proposed to facilitate the\nefficient determination of the threshold for the maximum number of detectable\nUAVs. The accuracy of this analytical approach is verified through extensive\nsimulation results."
                },
                "authors": [
                    {
                        "name": "Jiahua Wan"
                    },
                    {
                        "name": "Hong Ren"
                    },
                    {
                        "name": "Cunhua Pan"
                    },
                    {
                        "name": "Zhenkun Zhang"
                    },
                    {
                        "name": "Songtao Gao"
                    },
                    {
                        "name": "Yiming Yu"
                    },
                    {
                        "name": "Chengzhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Wang"
                },
                "author": "Chengzhong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06980v1",
                "updated": "2024-11-11T13:38:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    13,
                    38,
                    49,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T13:38:49Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    13,
                    38,
                    49,
                    0,
                    316,
                    0
                ],
                "title": "xNVMe: Unleashing Storage Hardware-Software Co-design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xNVMe: Unleashing Storage Hardware-Software Co-design"
                },
                "summary": "NVMe SSD hardware has witnessed widespread deployment as commodity and\nenterprise hardware due to its high performance and rich feature set. Despite\nthe open specifications of various NVMe protocols by the NVMe Express group and\nNVMe being of software abstractions to program the underlying hardware. The\nmyriad storage I/O paths such as POSIX storage API, ad-hoc OS mechanisms, and\nuserspace I/O libraries have different syntax and semantics that complicate\nsoftware development and stand in the way of mass adoption and evolution of the\nNVMe ecosystem. To unify the diverse I/O storage paths, we built xNVMe that\nexposes a single message-passing API to support both asynchronous and\nsynchronous communication with NVMe devices. xNVMe provides various command\nsets to support diverse storage I/O paths in different OS (e.g., Linux,\nFreeBSD, Windows, and MacOS) and userspace libraries (e.g., SPDK) with minimal\noverhead. xNVMe is an Open Source project and has gained traction amongst\nvarious industry stakeholders. In this paper, we elaborate on the lessons that\nwe have learned in the project during its evolution. We also provide some\nongoing and future work planned for the project. We hope the database and\nstorage systems community can join in the effort to both extend xNVMe and\nleverage it as a building block for innovative co-design of storage systems on\nmodern NVMe hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVMe SSD hardware has witnessed widespread deployment as commodity and\nenterprise hardware due to its high performance and rich feature set. Despite\nthe open specifications of various NVMe protocols by the NVMe Express group and\nNVMe being of software abstractions to program the underlying hardware. The\nmyriad storage I/O paths such as POSIX storage API, ad-hoc OS mechanisms, and\nuserspace I/O libraries have different syntax and semantics that complicate\nsoftware development and stand in the way of mass adoption and evolution of the\nNVMe ecosystem. To unify the diverse I/O storage paths, we built xNVMe that\nexposes a single message-passing API to support both asynchronous and\nsynchronous communication with NVMe devices. xNVMe provides various command\nsets to support diverse storage I/O paths in different OS (e.g., Linux,\nFreeBSD, Windows, and MacOS) and userspace libraries (e.g., SPDK) with minimal\noverhead. xNVMe is an Open Source project and has gained traction amongst\nvarious industry stakeholders. In this paper, we elaborate on the lessons that\nwe have learned in the project during its evolution. We also provide some\nongoing and future work planned for the project. We hope the database and\nstorage systems community can join in the effort to both extend xNVMe and\nleverage it as a building block for innovative co-design of storage systems on\nmodern NVMe hardware."
                },
                "authors": [
                    {
                        "name": "Simon A. F. Lund"
                    },
                    {
                        "name": "Vivek Shah"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Shah"
                },
                "author": "Vivek Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00349v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00349v2",
                "updated": "2024-11-11T13:30:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    13,
                    30,
                    26,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-01T04:18:42Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    4,
                    18,
                    42,
                    4,
                    306,
                    0
                ],
                "title": "Examining Attacks on Consensus and Incentive Systems in Proof-of-Work\n  Blockchains: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining Attacks on Consensus and Incentive Systems in Proof-of-Work\n  Blockchains: A Systematic Literature Review"
                },
                "summary": "Cryptocurrencies have gained popularity due to their transparency, security,\nand accessibility compared to traditional financial systems, with Bitcoin,\nintroduced in 2009, leading the market. Bitcoin's security relies on blockchain\ntechnology - a decentralized ledger consisting of a consensus and an incentive\nmechanism. The consensus mechanism, Proof of Work (PoW), requires miners to\nsolve difficult cryptographic puzzles to add new blocks, while the incentive\nmechanism rewards them with newly minted bitcoins. However, as Bitcoin's\nacceptance grows, it faces increasing threats from attacks targeting these\nmechanisms, such as selfish mining, double-spending, and block withholding.\nThese attacks compromise security, efficiency, and reward distribution. Recent\nresearch shows that these attacks can be combined with each other or with\neither malicious strategies, such as network-layer attacks, or non-malicious\nstrategies, like honest mining. These combinations lead to more sophisticated\nattacks, increasing the attacker's success rates and profitability. Therefore,\nunderstanding and evaluating these attacks is essential for developing\neffective countermeasures and ensuring long-term security. This paper begins by\nexamining individual attacks executed in isolation and their profitability. It\nthen explores how combining these attacks with each other or with other\nmalicious and non-malicious strategies can enhance their overall effectiveness\nand profitability. The analysis further explores how the deployment of attacks\nsuch as selfish mining and block withholding by multiple competing mining pools\nagainst each other impacts their economic returns. Lastly, a set of design\nguidelines is provided, outlining areas future work should focus on to prevent\nor mitigate the identified threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptocurrencies have gained popularity due to their transparency, security,\nand accessibility compared to traditional financial systems, with Bitcoin,\nintroduced in 2009, leading the market. Bitcoin's security relies on blockchain\ntechnology - a decentralized ledger consisting of a consensus and an incentive\nmechanism. The consensus mechanism, Proof of Work (PoW), requires miners to\nsolve difficult cryptographic puzzles to add new blocks, while the incentive\nmechanism rewards them with newly minted bitcoins. However, as Bitcoin's\nacceptance grows, it faces increasing threats from attacks targeting these\nmechanisms, such as selfish mining, double-spending, and block withholding.\nThese attacks compromise security, efficiency, and reward distribution. Recent\nresearch shows that these attacks can be combined with each other or with\neither malicious strategies, such as network-layer attacks, or non-malicious\nstrategies, like honest mining. These combinations lead to more sophisticated\nattacks, increasing the attacker's success rates and profitability. Therefore,\nunderstanding and evaluating these attacks is essential for developing\neffective countermeasures and ensuring long-term security. This paper begins by\nexamining individual attacks executed in isolation and their profitability. It\nthen explores how combining these attacks with each other or with other\nmalicious and non-malicious strategies can enhance their overall effectiveness\nand profitability. The analysis further explores how the deployment of attacks\nsuch as selfish mining and block withholding by multiple competing mining pools\nagainst each other impacts their economic returns. Lastly, a set of design\nguidelines is provided, outlining areas future work should focus on to prevent\nor mitigate the identified threats."
                },
                "authors": [
                    {
                        "name": "Dinitha Wijewardhana"
                    },
                    {
                        "name": "Sugandima Vidanagamachchi"
                    },
                    {
                        "name": "Nalin Arachchilage"
                    }
                ],
                "author_detail": {
                    "name": "Nalin Arachchilage"
                },
                "author": "Nalin Arachchilage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00349v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00349v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06950v1",
                "updated": "2024-11-11T12:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    56,
                    52,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T12:56:52Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    56,
                    52,
                    0,
                    316,
                    0
                ],
                "title": "Sniff AI: Is My 'Spicy' Your 'Spicy'? Exploring LLM's Perceptual\n  Alignment with Human Smell Experiences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sniff AI: Is My 'Spicy' Your 'Spicy'? Exploring LLM's Perceptual\n  Alignment with Human Smell Experiences"
                },
                "summary": "Aligning AI with human intent is important, yet perceptual alignment-how AI\ninterprets what we see, hear, or smell-remains underexplored. This work focuses\non olfaction, human smell experiences. We conducted a user study with 40\nparticipants to investigate how well AI can interpret human descriptions of\nscents. Participants performed \"sniff and describe\" interactive tasks, with our\ndesigned AI system attempting to guess what scent the participants were\nexperiencing based on their descriptions. These tasks evaluated the Large\nLanguage Model's (LLMs) contextual understanding and representation of scent\nrelationships within its internal states - high-dimensional embedding space.\nBoth quantitative and qualitative methods were used to evaluate the AI system's\nperformance. Results indicated limited perceptual alignment, with biases\ntowards certain scents, like lemon and peppermint, and continued failing to\nidentify others, like rosemary. We discuss these findings in light of human-AI\nalignment advancements, highlighting the limitations and opportunities for\nenhancing HCI systems with multisensory experience integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning AI with human intent is important, yet perceptual alignment-how AI\ninterprets what we see, hear, or smell-remains underexplored. This work focuses\non olfaction, human smell experiences. We conducted a user study with 40\nparticipants to investigate how well AI can interpret human descriptions of\nscents. Participants performed \"sniff and describe\" interactive tasks, with our\ndesigned AI system attempting to guess what scent the participants were\nexperiencing based on their descriptions. These tasks evaluated the Large\nLanguage Model's (LLMs) contextual understanding and representation of scent\nrelationships within its internal states - high-dimensional embedding space.\nBoth quantitative and qualitative methods were used to evaluate the AI system's\nperformance. Results indicated limited perceptual alignment, with biases\ntowards certain scents, like lemon and peppermint, and continued failing to\nidentify others, like rosemary. We discuss these findings in light of human-AI\nalignment advancements, highlighting the limitations and opportunities for\nenhancing HCI systems with multisensory experience integration."
                },
                "authors": [
                    {
                        "name": "Shu Zhong"
                    },
                    {
                        "name": "Zetao Zhou"
                    },
                    {
                        "name": "Christopher Dawes"
                    },
                    {
                        "name": "Giada Brianz"
                    },
                    {
                        "name": "Marianna Obrist"
                    }
                ],
                "author_detail": {
                    "name": "Marianna Obrist"
                },
                "author": "Marianna Obrist",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06946v1",
                "updated": "2024-11-11T12:54:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    54,
                    22,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T12:54:22Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    54,
                    22,
                    0,
                    316,
                    0
                ],
                "title": "Cancer-Answer: Empowering Cancer Care with Advanced Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cancer-Answer: Empowering Cancer Care with Advanced Large Language\n  Models"
                },
                "summary": "Gastrointestinal (GI) tract cancers account for a substantial portion of the\nglobal cancer burden, where early diagnosis is critical for improved management\nand patient outcomes. The complex aetiologies and overlapping symptoms across\nGI cancers often delay diagnosis, leading to suboptimal treatment strategies.\nCancer-related queries are crucial for timely diagnosis, treatment, and patient\neducation, as access to accurate, comprehensive information can significantly\ninfluence outcomes. However, the complexity of cancer as a disease, combined\nwith the vast amount of available data, makes it difficult for clinicians and\npatients to quickly find precise answers. To address these challenges, we\nleverage large language models (LLMs) such as GPT-3.5 Turbo to generate\naccurate, contextually relevant responses to cancer-related queries.\nPre-trained with medical data, these models provide timely, actionable insights\nthat support informed decision-making in cancer diagnosis and care, ultimately\nimproving patient outcomes. We calculate two metrics: A1 (which represents the\nfraction of entities present in the model-generated answer compared to the gold\nstandard) and A2 (which represents the linguistic correctness and\nmeaningfulness of the model-generated answer with respect to the gold\nstandard), achieving maximum values of 0.546 and 0.881, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gastrointestinal (GI) tract cancers account for a substantial portion of the\nglobal cancer burden, where early diagnosis is critical for improved management\nand patient outcomes. The complex aetiologies and overlapping symptoms across\nGI cancers often delay diagnosis, leading to suboptimal treatment strategies.\nCancer-related queries are crucial for timely diagnosis, treatment, and patient\neducation, as access to accurate, comprehensive information can significantly\ninfluence outcomes. However, the complexity of cancer as a disease, combined\nwith the vast amount of available data, makes it difficult for clinicians and\npatients to quickly find precise answers. To address these challenges, we\nleverage large language models (LLMs) such as GPT-3.5 Turbo to generate\naccurate, contextually relevant responses to cancer-related queries.\nPre-trained with medical data, these models provide timely, actionable insights\nthat support informed decision-making in cancer diagnosis and care, ultimately\nimproving patient outcomes. We calculate two metrics: A1 (which represents the\nfraction of entities present in the model-generated answer compared to the gold\nstandard) and A2 (which represents the linguistic correctness and\nmeaningfulness of the model-generated answer with respect to the gold\nstandard), achieving maximum values of 0.546 and 0.881, respectively."
                },
                "authors": [
                    {
                        "name": "Aniket Deroy"
                    },
                    {
                        "name": "Subhankar Maity"
                    }
                ],
                "author_detail": {
                    "name": "Subhankar Maity"
                },
                "author": "Subhankar Maity",
                "arxiv_comment": "Accepted at FIRE 2024 (Track: Conversational System for Differential\n  Diagnosis of GI Cancer)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11032v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11032v3",
                "updated": "2024-11-11T12:50:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    50,
                    44,
                    0,
                    316,
                    0
                ],
                "published": "2024-09-17T09:56:12Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    56,
                    12,
                    1,
                    261,
                    0
                ],
                "title": "Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Narrative Analysis: Unraveling Perceptions of Generative AI"
                },
                "summary": "Written texts reflect an author's perspective, making the thorough analysis\nof literature a key research method in fields such as the humanities and social\nsciences. However, conventional text mining techniques like sentiment analysis\nand topic modeling are limited in their ability to capture the hierarchical\nnarrative structures that reveal deeper argumentative patterns. To address this\ngap, we propose a method that leverages large language models (LLMs) to extract\nand organize these structures into a hierarchical framework. We validate this\napproach by analyzing public opinions on generative AI collected by Japan's\nAgency for Cultural Affairs, comparing the narratives of supporters and\ncritics. Our analysis provides clearer visualization of the factors influencing\ndivergent opinions on generative AI, offering deeper insights into the\nstructures of agreement and disagreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Written texts reflect an author's perspective, making the thorough analysis\nof literature a key research method in fields such as the humanities and social\nsciences. However, conventional text mining techniques like sentiment analysis\nand topic modeling are limited in their ability to capture the hierarchical\nnarrative structures that reveal deeper argumentative patterns. To address this\ngap, we propose a method that leverages large language models (LLMs) to extract\nand organize these structures into a hierarchical framework. We validate this\napproach by analyzing public opinions on generative AI collected by Japan's\nAgency for Cultural Affairs, comparing the narratives of supporters and\ncritics. Our analysis provides clearer visualization of the factors influencing\ndivergent opinions on generative AI, offering deeper insights into the\nstructures of agreement and disagreement."
                },
                "authors": [
                    {
                        "name": "Riona Matsuoka"
                    },
                    {
                        "name": "Hiroki Matsumoto"
                    },
                    {
                        "name": "Takahiro Yoshida"
                    },
                    {
                        "name": "Tomohiro Watanabe"
                    },
                    {
                        "name": "Ryoma Kondo"
                    },
                    {
                        "name": "Ryohei Hisano"
                    }
                ],
                "author_detail": {
                    "name": "Ryohei Hisano"
                },
                "author": "Ryohei Hisano",
                "arxiv_journal_ref": "Proceedings of Jinmoncon 2024, IPSJ SIG Computers and the\n  Humanities",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11032v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11032v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06917v1",
                "updated": "2024-11-11T12:20:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    20,
                    57,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T12:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    20,
                    57,
                    0,
                    316,
                    0
                ],
                "title": "Efficient Unsupervised Domain Adaptation Regression for Spatial-Temporal\n  Air Quality Sensor Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Unsupervised Domain Adaptation Regression for Spatial-Temporal\n  Air Quality Sensor Fusion"
                },
                "summary": "The deployment of affordable Internet of Things (IoT) sensors for air\npollution monitoring has increased in recent years due to their scalability and\ncost-effectiveness. However, accurately calibrating these sensors in\nuncontrolled environments remains a significant challenge. While expensive\nreference sensors can provide accurate ground truth data, they are often\ndeployed on a limited scale due to high costs, leading to a scarcity of labeled\ndata. In diverse urban environments, data distributions constantly shift due to\nvarying factors such as traffic patterns, industrial activities, and weather\nconditions, which impact sensor readings. Consequently, traditional machine\nlearning models -- despite their increasing deployment for environmental sensor\ncalibration -- often struggle to provide reliable pollutant measurements across\ndifferent locations due to domain shifts. To address these challenges, we\npropose a novel unsupervised domain adaptation (UDA) method specifically\ntailored for regression tasks on graph-structured data. Our approach leverages\nGraph Neural Networks (GNNs) to model the relationships between sensors. To\neffectively capture critical spatial-temporal interactions, we incorporate\nspatial-temporal graph neural networks (STGNNs), which extend GNNs by\nincorporating temporal dynamics. To handle the resulting larger embeddings, we\npropose a domain adaptation method using a closed-form solution inspired by the\nTikhonov-regularized least-squares problem. This method leverages Cholesky\ndecomposition and power iteration to align the subspaces between source and\ntarget domains. By aligning these subspaces, our approach allows low-cost IoT\nsensors to learn calibration parameters from expensive reference sensors. This\nfacilitates reliable pollutant measurements in new locations without the need\nfor additional costly equipment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of affordable Internet of Things (IoT) sensors for air\npollution monitoring has increased in recent years due to their scalability and\ncost-effectiveness. However, accurately calibrating these sensors in\nuncontrolled environments remains a significant challenge. While expensive\nreference sensors can provide accurate ground truth data, they are often\ndeployed on a limited scale due to high costs, leading to a scarcity of labeled\ndata. In diverse urban environments, data distributions constantly shift due to\nvarying factors such as traffic patterns, industrial activities, and weather\nconditions, which impact sensor readings. Consequently, traditional machine\nlearning models -- despite their increasing deployment for environmental sensor\ncalibration -- often struggle to provide reliable pollutant measurements across\ndifferent locations due to domain shifts. To address these challenges, we\npropose a novel unsupervised domain adaptation (UDA) method specifically\ntailored for regression tasks on graph-structured data. Our approach leverages\nGraph Neural Networks (GNNs) to model the relationships between sensors. To\neffectively capture critical spatial-temporal interactions, we incorporate\nspatial-temporal graph neural networks (STGNNs), which extend GNNs by\nincorporating temporal dynamics. To handle the resulting larger embeddings, we\npropose a domain adaptation method using a closed-form solution inspired by the\nTikhonov-regularized least-squares problem. This method leverages Cholesky\ndecomposition and power iteration to align the subspaces between source and\ntarget domains. By aligning these subspaces, our approach allows low-cost IoT\nsensors to learn calibration parameters from expensive reference sensors. This\nfacilitates reliable pollutant measurements in new locations without the need\nfor additional costly equipment."
                },
                "authors": [
                    {
                        "name": "Keivan Faghih Niresi"
                    },
                    {
                        "name": "Ismail Nejjar"
                    },
                    {
                        "name": "Olga Fink"
                    }
                ],
                "author_detail": {
                    "name": "Olga Fink"
                },
                "author": "Olga Fink",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07599v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07599v3",
                "updated": "2024-11-11T12:00:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    12,
                    0,
                    35,
                    0,
                    316,
                    0
                ],
                "published": "2024-06-11T16:42:02Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    16,
                    42,
                    2,
                    1,
                    163,
                    0
                ],
                "title": "CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence"
                },
                "summary": "Cyber threat intelligence (CTI) is crucial in today's cybersecurity\nlandscape, providing essential insights to understand and mitigate the\never-evolving cyber threats. The recent rise of Large Language Models (LLMs)\nhave shown potential in this domain, but concerns about their reliability,\naccuracy, and hallucinations persist. While existing benchmarks provide general\nevaluations of LLMs, there are no benchmarks that address the practical and\napplied aspects of CTI-specific tasks. To bridge this gap, we introduce\nCTIBench, a benchmark designed to assess LLMs' performance in CTI applications.\nCTIBench includes multiple datasets focused on evaluating knowledge acquired by\nLLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art\nmodels on these tasks provides insights into their strengths and weaknesses in\nCTI contexts, contributing to a better understanding of LLM capabilities in\nCTI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber threat intelligence (CTI) is crucial in today's cybersecurity\nlandscape, providing essential insights to understand and mitigate the\never-evolving cyber threats. The recent rise of Large Language Models (LLMs)\nhave shown potential in this domain, but concerns about their reliability,\naccuracy, and hallucinations persist. While existing benchmarks provide general\nevaluations of LLMs, there are no benchmarks that address the practical and\napplied aspects of CTI-specific tasks. To bridge this gap, we introduce\nCTIBench, a benchmark designed to assess LLMs' performance in CTI applications.\nCTIBench includes multiple datasets focused on evaluating knowledge acquired by\nLLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art\nmodels on these tasks provides insights into their strengths and weaknesses in\nCTI contexts, contributing to a better understanding of LLM capabilities in\nCTI."
                },
                "authors": [
                    {
                        "name": "Md Tanvirul Alam"
                    },
                    {
                        "name": "Dipkamal Bhusal"
                    },
                    {
                        "name": "Le Nguyen"
                    },
                    {
                        "name": "Nidhi Rastogi"
                    }
                ],
                "author_detail": {
                    "name": "Nidhi Rastogi"
                },
                "author": "Nidhi Rastogi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07599v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07599v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06899v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06899v1",
                "updated": "2024-11-11T11:57:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    57,
                    37,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T11:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    57,
                    37,
                    0,
                    316,
                    0
                ],
                "title": "LongSafetyBench: Long-Context LLMs Struggle with Safety Issues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSafetyBench: Long-Context LLMs Struggle with Safety Issues"
                },
                "summary": "With the development of large language models (LLMs), the sequence length of\nthese models continues to increase, drawing significant attention to\nlong-context language models. However, the evaluation of these models has been\nprimarily limited to their capabilities, with a lack of research focusing on\ntheir safety. Existing work, such as ManyShotJailbreak, has to some extent\ndemonstrated that long-context language models can exhibit safety concerns.\nHowever, the methods used are limited and lack comprehensiveness. In response,\nwe introduce \\textbf{LongSafetyBench}, the first benchmark designed to\nobjectively and comprehensively evaluate the safety of long-context models.\nLongSafetyBench consists of 10 task categories, with an average length of\n41,889 words. After testing eight long-context language models on\nLongSafetyBench, we found that existing models generally exhibit insufficient\nsafety capabilities. The proportion of safe responses from most mainstream\nlong-context LLMs is below 50\\%. Moreover, models' safety performance in\nlong-context scenarios does not always align with that in short-context\nscenarios. Further investigation revealed that long-context models tend to\noverlook harmful content within lengthy texts. We also proposed a simple yet\neffective solution, allowing open-source models to achieve performance\ncomparable to that of top-tier closed-source models. We believe that\nLongSafetyBench can serve as a valuable benchmark for evaluating the safety\ncapabilities of long-context language models. We hope that our work will\nencourage the broader community to pay attention to the safety of long-context\nmodels and contribute to the development of solutions to improve the safety of\nlong-context LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the sequence length of\nthese models continues to increase, drawing significant attention to\nlong-context language models. However, the evaluation of these models has been\nprimarily limited to their capabilities, with a lack of research focusing on\ntheir safety. Existing work, such as ManyShotJailbreak, has to some extent\ndemonstrated that long-context language models can exhibit safety concerns.\nHowever, the methods used are limited and lack comprehensiveness. In response,\nwe introduce \\textbf{LongSafetyBench}, the first benchmark designed to\nobjectively and comprehensively evaluate the safety of long-context models.\nLongSafetyBench consists of 10 task categories, with an average length of\n41,889 words. After testing eight long-context language models on\nLongSafetyBench, we found that existing models generally exhibit insufficient\nsafety capabilities. The proportion of safe responses from most mainstream\nlong-context LLMs is below 50\\%. Moreover, models' safety performance in\nlong-context scenarios does not always align with that in short-context\nscenarios. Further investigation revealed that long-context models tend to\noverlook harmful content within lengthy texts. We also proposed a simple yet\neffective solution, allowing open-source models to achieve performance\ncomparable to that of top-tier closed-source models. We believe that\nLongSafetyBench can serve as a valuable benchmark for evaluating the safety\ncapabilities of long-context language models. We hope that our work will\nencourage the broader community to pay attention to the safety of long-context\nmodels and contribute to the development of solutions to improve the safety of\nlong-context LLMs."
                },
                "authors": [
                    {
                        "name": "Mianqiu Huang"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Shaojun Zhou"
                    },
                    {
                        "name": "Mozhi Zhang"
                    },
                    {
                        "name": "Chenkun Tan"
                    },
                    {
                        "name": "Pengyu Wang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Linyang Li"
                    },
                    {
                        "name": "Zhikai Lei"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Yaqian Zhou"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06899v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06899v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09085v2",
                "updated": "2024-11-11T11:35:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    35,
                    28,
                    0,
                    316,
                    0
                ],
                "published": "2024-03-14T04:06:13Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    4,
                    6,
                    13,
                    3,
                    74,
                    0
                ],
                "title": "Meaningful Learning: Enhancing Abstract Reasoning in Large Language\n  Models via Generic Fact Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meaningful Learning: Enhancing Abstract Reasoning in Large Language\n  Models via Generic Fact Guidance"
                },
                "summary": "Large language models (LLMs) have developed impressive performance and strong\nexplainability across various reasoning scenarios, marking a significant stride\ntowards mimicking human-like intelligence. Despite this, when tasked with\nseveral simple questions supported by a generic fact, LLMs often struggle to\nabstract and apply the generic fact to provide consistent and precise answers,\nrevealing a deficiency in abstract reasoning abilities. This has sparked a\nvigorous debate about whether LLMs are genuinely reasoning or merely\nmemorizing. In light of this, we design a preliminary study to quantify and\ndelve into the abstract reasoning abilities of existing LLMs. Our findings\nreveal a substantial discrepancy between their general reasoning and abstract\nreasoning performances. To relieve this problem, we tailor an abstract\nreasoning dataset (AbsR) together with a meaningful learning paradigm to teach\nLLMs how to leverage generic facts for reasoning purposes. The results show\nthat our approach not only boosts the general reasoning performance of LLMs but\nalso makes considerable strides towards their capacity for abstract reasoning,\nmoving beyond simple memorization or imitation to a more nuanced understanding\nand application of generic facts. The code is available at\nhttps://github.com/Waste-Wood/MeanLearn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have developed impressive performance and strong\nexplainability across various reasoning scenarios, marking a significant stride\ntowards mimicking human-like intelligence. Despite this, when tasked with\nseveral simple questions supported by a generic fact, LLMs often struggle to\nabstract and apply the generic fact to provide consistent and precise answers,\nrevealing a deficiency in abstract reasoning abilities. This has sparked a\nvigorous debate about whether LLMs are genuinely reasoning or merely\nmemorizing. In light of this, we design a preliminary study to quantify and\ndelve into the abstract reasoning abilities of existing LLMs. Our findings\nreveal a substantial discrepancy between their general reasoning and abstract\nreasoning performances. To relieve this problem, we tailor an abstract\nreasoning dataset (AbsR) together with a meaningful learning paradigm to teach\nLLMs how to leverage generic facts for reasoning purposes. The results show\nthat our approach not only boosts the general reasoning performance of LLMs but\nalso makes considerable strides towards their capacity for abstract reasoning,\nmoving beyond simple memorization or imitation to a more nuanced understanding\nand application of generic facts. The code is available at\nhttps://github.com/Waste-Wood/MeanLearn."
                },
                "authors": [
                    {
                        "name": "Kai Xiong"
                    },
                    {
                        "name": "Xiao Ding"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Dongliang Xu"
                    },
                    {
                        "name": "Qing Yang"
                    },
                    {
                        "name": "Hongtao Liu"
                    },
                    {
                        "name": "Yixin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yixin Cao"
                },
                "author": "Yixin Cao",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17632v2",
                "updated": "2024-11-11T11:32:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    32,
                    21,
                    0,
                    316,
                    0
                ],
                "published": "2024-10-23T07:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    48,
                    51,
                    2,
                    297,
                    0
                ],
                "title": "LMLPA: Language Model Linguistic Personality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMLPA: Language Model Linguistic Personality Assessment"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in everyday life and\nresearch. One of the most common use cases is conversational interactions,\nenabled by the language generation capabilities of LLMs. Just as between two\nhumans, a conversation between an LLM-powered entity and a human depends on the\npersonality of the conversants. However, measuring the personality of a given\nLLM is currently a challenge. This paper introduces the Language Model\nLinguistic Personality Assessment (LMLPA), a system designed to evaluate the\nlinguistic personalities of LLMs. Our system helps to understand LLMs' language\ngeneration capabilities by quantitatively assessing the distinct personality\ntraits reflected in their linguistic outputs. Unlike traditional human-centric\npsychometrics, the LMLPA adapts a personality assessment questionnaire,\nspecifically the Big Five Inventory, to align with the operational capabilities\nof LLMs, and also incorporates the findings from previous language-based\npersonality measurement literature. To mitigate sensitivity to the order of\noptions, our questionnaire is designed to be open-ended, resulting in textual\nanswers. Thus, the AI rater is needed to transform ambiguous personality\ninformation from text responses into clear numerical indicators of personality\ntraits. Utilising Principal Component Analysis and reliability validations, our\nfindings demonstrate that LLMs possess distinct personality traits that can be\neffectively quantified by the LMLPA. This research contributes to\nHuman-Computer Interaction and Human-Centered AI, providing a robust framework\nfor future studies to refine AI personality assessments and expand their\napplications in multiple areas, including education and manufacturing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in everyday life and\nresearch. One of the most common use cases is conversational interactions,\nenabled by the language generation capabilities of LLMs. Just as between two\nhumans, a conversation between an LLM-powered entity and a human depends on the\npersonality of the conversants. However, measuring the personality of a given\nLLM is currently a challenge. This paper introduces the Language Model\nLinguistic Personality Assessment (LMLPA), a system designed to evaluate the\nlinguistic personalities of LLMs. Our system helps to understand LLMs' language\ngeneration capabilities by quantitatively assessing the distinct personality\ntraits reflected in their linguistic outputs. Unlike traditional human-centric\npsychometrics, the LMLPA adapts a personality assessment questionnaire,\nspecifically the Big Five Inventory, to align with the operational capabilities\nof LLMs, and also incorporates the findings from previous language-based\npersonality measurement literature. To mitigate sensitivity to the order of\noptions, our questionnaire is designed to be open-ended, resulting in textual\nanswers. Thus, the AI rater is needed to transform ambiguous personality\ninformation from text responses into clear numerical indicators of personality\ntraits. Utilising Principal Component Analysis and reliability validations, our\nfindings demonstrate that LLMs possess distinct personality traits that can be\neffectively quantified by the LMLPA. This research contributes to\nHuman-Computer Interaction and Human-Centered AI, providing a robust framework\nfor future studies to refine AI personality assessments and expand their\napplications in multiple areas, including education and manufacturing."
                },
                "authors": [
                    {
                        "name": "Jingyao Zheng"
                    },
                    {
                        "name": "Xian Wang"
                    },
                    {
                        "name": "Simo Hosio"
                    },
                    {
                        "name": "Xiaoxian Xu"
                    },
                    {
                        "name": "Lik-Hang Lee"
                    }
                ],
                "author_detail": {
                    "name": "Lik-Hang Lee"
                },
                "author": "Lik-Hang Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06878v1",
                "updated": "2024-11-11T11:20:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    20,
                    30,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T11:20:30Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    20,
                    30,
                    0,
                    316,
                    0
                ],
                "title": "GraphRPM: Risk Pattern Mining on Industrial Large Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphRPM: Risk Pattern Mining on Industrial Large Attributed Graphs"
                },
                "summary": "Graph-based patterns are extensively employed and favored by practitioners\nwithin industrial companies due to their capacity to represent the behavioral\nattributes and topological relationships among users, thereby offering enhanced\ninterpretability in comparison to black-box models commonly utilized for\nclassification and recognition tasks. For instance, within the scenario of\ntransaction risk management, a graph pattern that is characteristic of a\nparticular risk category can be readily employed to discern transactions\nfraught with risk, delineate networks of criminal activity, or investigate the\nmethodologies employed by fraudsters. Nonetheless, graph data in industrial\nsettings is often characterized by its massive scale, encompassing data sets\nwith millions or even billions of nodes, making the manual extraction of graph\npatterns not only labor-intensive but also necessitating specialized knowledge\nin particular domains of risk. Moreover, existing methodologies for mining\ngraph patterns encounter significant obstacles when tasked with analyzing\nlarge-scale attributed graphs. In this work, we introduce GraphRPM, an\nindustry-purpose parallel and distributed risk pattern mining framework on\nlarge attributed graphs. The framework incorporates a novel edge-involved graph\nisomorphism network alongside optimized operations for parallel graph\ncomputation, which collectively contribute to a considerable reduction in\ncomputational complexity and resource expenditure. Moreover, the intelligent\nfiltration of efficacious risky graph patterns is facilitated by the proposed\nevaluation metrics. Comprehensive experimental evaluations conducted on\nreal-world datasets of varying sizes substantiate the capability of GraphRPM to\nadeptly address the challenges inherent in mining patterns from large-scale\nindustrial attributed graphs, thereby underscoring its substantial value for\nindustrial deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based patterns are extensively employed and favored by practitioners\nwithin industrial companies due to their capacity to represent the behavioral\nattributes and topological relationships among users, thereby offering enhanced\ninterpretability in comparison to black-box models commonly utilized for\nclassification and recognition tasks. For instance, within the scenario of\ntransaction risk management, a graph pattern that is characteristic of a\nparticular risk category can be readily employed to discern transactions\nfraught with risk, delineate networks of criminal activity, or investigate the\nmethodologies employed by fraudsters. Nonetheless, graph data in industrial\nsettings is often characterized by its massive scale, encompassing data sets\nwith millions or even billions of nodes, making the manual extraction of graph\npatterns not only labor-intensive but also necessitating specialized knowledge\nin particular domains of risk. Moreover, existing methodologies for mining\ngraph patterns encounter significant obstacles when tasked with analyzing\nlarge-scale attributed graphs. In this work, we introduce GraphRPM, an\nindustry-purpose parallel and distributed risk pattern mining framework on\nlarge attributed graphs. The framework incorporates a novel edge-involved graph\nisomorphism network alongside optimized operations for parallel graph\ncomputation, which collectively contribute to a considerable reduction in\ncomputational complexity and resource expenditure. Moreover, the intelligent\nfiltration of efficacious risky graph patterns is facilitated by the proposed\nevaluation metrics. Comprehensive experimental evaluations conducted on\nreal-world datasets of varying sizes substantiate the capability of GraphRPM to\nadeptly address the challenges inherent in mining patterns from large-scale\nindustrial attributed graphs, thereby underscoring its substantial value for\nindustrial deployment."
                },
                "authors": [
                    {
                        "name": "Sheng Tian"
                    },
                    {
                        "name": "Xintan Zeng"
                    },
                    {
                        "name": "Yifei Hu"
                    },
                    {
                        "name": "Baokun Wang"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Yue Jin"
                    },
                    {
                        "name": "Changhua Meng"
                    },
                    {
                        "name": "Chuntao Hong"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Weiqiang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiqiang Wang"
                },
                "author": "Weiqiang Wang",
                "arxiv_doi": "10.1007/978-3-031-70381-2_9",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-70381-2_9",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.06878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ECML PKDD 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06877v1",
                "updated": "2024-11-11T11:17:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    17,
                    35,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T11:17:35Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    17,
                    35,
                    0,
                    316,
                    0
                ],
                "title": "LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Assisted Relevance Assessments: When Should We Ask LLMs for Help?"
                },
                "summary": "Test collections are information retrieval tools that allow researchers to\nquickly and easily evaluate ranking algorithms. While test collections have\nbecome an integral part of IR research, the process of data creation involves\nsignificant efforts in manual annotations, which often makes it very expensive\nand time-consuming. Thus, the test collections could become small when the\nbudget is limited, which may lead to unstable evaluations. As an alternative,\nrecent studies have proposed the use of large language models (LLMs) to\ncompletely replace human assessors. However, while LLMs seem to somewhat\ncorrelate with human judgments, they are not perfect and often show bias.\nMoreover, even if a well-performing LLM or prompt is found on one dataset,\nthere is no guarantee that it will perform similarly in practice, due to\ndifference in tasks and data. Thus a complete replacement with LLMs is argued\nto be too risky and not fully trustable.\n  Thus, in this paper, we propose \\textbf{L}LM-\\textbf{A}ssisted\n\\textbf{R}elevance \\textbf{A}ssessments (\\textbf{LARA}), an effective method to\nbalance manual annotations with LLM annotations, which helps to make a rich and\nreliable test collection. We use the LLM's predicted relevance probabilities in\norder to select the most profitable documents to manually annotate under a\nbudget constraint. While solely relying on LLM's predicted probabilities to\nmanually annotate performs fairly well, with theoretical reasoning, LARA guides\nthe human annotation process even more effectively via online calibration\nlearning. Then, using the calibration model learned from the limited manual\nannotations, LARA debiases the LLM predictions to annotate the remaining\nnon-assessed data. Empirical evaluations on TREC-COVID and TREC-8 Ad Hoc\ndatasets show that LARA outperforms the alternative solutions under almost any\nbudget constraint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test collections are information retrieval tools that allow researchers to\nquickly and easily evaluate ranking algorithms. While test collections have\nbecome an integral part of IR research, the process of data creation involves\nsignificant efforts in manual annotations, which often makes it very expensive\nand time-consuming. Thus, the test collections could become small when the\nbudget is limited, which may lead to unstable evaluations. As an alternative,\nrecent studies have proposed the use of large language models (LLMs) to\ncompletely replace human assessors. However, while LLMs seem to somewhat\ncorrelate with human judgments, they are not perfect and often show bias.\nMoreover, even if a well-performing LLM or prompt is found on one dataset,\nthere is no guarantee that it will perform similarly in practice, due to\ndifference in tasks and data. Thus a complete replacement with LLMs is argued\nto be too risky and not fully trustable.\n  Thus, in this paper, we propose \\textbf{L}LM-\\textbf{A}ssisted\n\\textbf{R}elevance \\textbf{A}ssessments (\\textbf{LARA}), an effective method to\nbalance manual annotations with LLM annotations, which helps to make a rich and\nreliable test collection. We use the LLM's predicted relevance probabilities in\norder to select the most profitable documents to manually annotate under a\nbudget constraint. While solely relying on LLM's predicted probabilities to\nmanually annotate performs fairly well, with theoretical reasoning, LARA guides\nthe human annotation process even more effectively via online calibration\nlearning. Then, using the calibration model learned from the limited manual\nannotations, LARA debiases the LLM predictions to annotate the remaining\nnon-assessed data. Empirical evaluations on TREC-COVID and TREC-8 Ad Hoc\ndatasets show that LARA outperforms the alternative solutions under almost any\nbudget constraint."
                },
                "authors": [
                    {
                        "name": "Rikiya Takehi"
                    },
                    {
                        "name": "Ellen M. Voorhees"
                    },
                    {
                        "name": "Tetsuya Sakai"
                    }
                ],
                "author_detail": {
                    "name": "Tetsuya Sakai"
                },
                "author": "Tetsuya Sakai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06870v1",
                "updated": "2024-11-11T11:10:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    10,
                    39,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T11:10:39Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    10,
                    39,
                    0,
                    316,
                    0
                ],
                "title": "AI-Native Multi-Access Future Networks -- The REASON Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Native Multi-Access Future Networks -- The REASON Architecture"
                },
                "summary": "The development of the sixth generation of communication networks (6G) has\nbeen gaining momentum over the past years, with a target of being introduced by\n2030. Several initiatives worldwide are developing innovative solutions and\nsetting the direction for the key features of these networks. Some common\nemerging themes are the tight integration of AI, the convergence of multiple\naccess technologies and sustainable operation, aiming to meet stringent\nperformance and societal requirements. To that end, we are introducing REASON -\nRealising Enabling Architectures and Solutions for Open Networks. The REASON\nproject aims to address technical challenges in future network deployments,\nsuch as E2E service orchestration, sustainability, security and trust\nmanagement, and policy management, utilising AI-native principles, considering\nmultiple access technologies and cloud-native solutions.\n  This paper presents REASON's architecture and the identified requirements for\nfuture networks. The architecture is meticulously designed for modularity,\ninteroperability, scalability, simplified troubleshooting, flexibility, and\nenhanced security, taking into consideration current and future standardisation\nefforts, and the ease of implementation and training. It is structured into\nfour horizontal layers: Physical Infrastructure, Network Service, Knowledge,\nand End-User Application, complemented by two vertical layers: Management and\nOrchestration, and E2E Security. This layered approach ensures a robust,\nadaptable framework to support the diverse and evolving requirements of 6G\nnetworks, fostering innovation and facilitating seamless integration of\nadvanced technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of the sixth generation of communication networks (6G) has\nbeen gaining momentum over the past years, with a target of being introduced by\n2030. Several initiatives worldwide are developing innovative solutions and\nsetting the direction for the key features of these networks. Some common\nemerging themes are the tight integration of AI, the convergence of multiple\naccess technologies and sustainable operation, aiming to meet stringent\nperformance and societal requirements. To that end, we are introducing REASON -\nRealising Enabling Architectures and Solutions for Open Networks. The REASON\nproject aims to address technical challenges in future network deployments,\nsuch as E2E service orchestration, sustainability, security and trust\nmanagement, and policy management, utilising AI-native principles, considering\nmultiple access technologies and cloud-native solutions.\n  This paper presents REASON's architecture and the identified requirements for\nfuture networks. The architecture is meticulously designed for modularity,\ninteroperability, scalability, simplified troubleshooting, flexibility, and\nenhanced security, taking into consideration current and future standardisation\nefforts, and the ease of implementation and training. It is structured into\nfour horizontal layers: Physical Infrastructure, Network Service, Knowledge,\nand End-User Application, complemented by two vertical layers: Management and\nOrchestration, and E2E Security. This layered approach ensures a robust,\nadaptable framework to support the diverse and evolving requirements of 6G\nnetworks, fostering innovation and facilitating seamless integration of\nadvanced technologies."
                },
                "authors": [
                    {
                        "name": "Konstantinos Katsaros"
                    },
                    {
                        "name": "Ioannis Mavromatis"
                    },
                    {
                        "name": "Kostantinos Antonakoglou"
                    },
                    {
                        "name": "Saptarshi Ghosh"
                    },
                    {
                        "name": "Dritan Kaleshi"
                    },
                    {
                        "name": "Toktam Mahmoodi"
                    },
                    {
                        "name": "Hamid Asgari"
                    },
                    {
                        "name": "Anastasios Karousos"
                    },
                    {
                        "name": "Iman Tavakkolnia"
                    },
                    {
                        "name": "Hossein Safi"
                    },
                    {
                        "name": "Harald Hass"
                    },
                    {
                        "name": "Constantinos Vrontos"
                    },
                    {
                        "name": "Amin Emami"
                    },
                    {
                        "name": "Juan Parra Ullauri"
                    },
                    {
                        "name": "Shadi Moazzeni"
                    },
                    {
                        "name": "Dimitra Simeonidou"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Simeonidou"
                },
                "author": "Dimitra Simeonidou",
                "arxiv_comment": "Accepted for publication at IEEE Access",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06869v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06869v1",
                "updated": "2024-11-11T11:08:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    8,
                    26,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T11:08:26Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    11,
                    8,
                    26,
                    0,
                    316,
                    0
                ],
                "title": "CapeLLM: Support-Free Category-Agnostic Pose Estimation with Multimodal\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapeLLM: Support-Free Category-Agnostic Pose Estimation with Multimodal\n  Large Language Models"
                },
                "summary": "Category-agnostic pose estimation (CAPE) has traditionally relied on support\nimages with annotated keypoints, a process that is often cumbersome and may\nfail to fully capture the necessary correspondences across diverse object\ncategories. Recent efforts have begun exploring the use of text-based queries,\nwhere the need for support keypoints is eliminated. However, the optimal use of\ntextual descriptions for keypoints remains an underexplored area. In this work,\nwe introduce CapeLLM, a novel approach that leverages a text-based multimodal\nlarge language model (MLLM) for CAPE. Our method only employs query image and\ndetailed text descriptions as an input to estimate category-agnostic keypoints.\nWe conduct extensive experiments to systematically explore the design space of\nLLM-based CAPE, investigating factors such as choosing the optimal description\nfor keypoints, neural network architectures, and training strategies. Thanks to\nthe advanced reasoning capabilities of the pre-trained MLLM, CapeLLM\ndemonstrates superior generalization and robust performance. Our approach sets\na new state-of-the-art on the MP-100 benchmark in the challenging 1-shot\nsetting, marking a significant advancement in the field of category-agnostic\npose estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Category-agnostic pose estimation (CAPE) has traditionally relied on support\nimages with annotated keypoints, a process that is often cumbersome and may\nfail to fully capture the necessary correspondences across diverse object\ncategories. Recent efforts have begun exploring the use of text-based queries,\nwhere the need for support keypoints is eliminated. However, the optimal use of\ntextual descriptions for keypoints remains an underexplored area. In this work,\nwe introduce CapeLLM, a novel approach that leverages a text-based multimodal\nlarge language model (MLLM) for CAPE. Our method only employs query image and\ndetailed text descriptions as an input to estimate category-agnostic keypoints.\nWe conduct extensive experiments to systematically explore the design space of\nLLM-based CAPE, investigating factors such as choosing the optimal description\nfor keypoints, neural network architectures, and training strategies. Thanks to\nthe advanced reasoning capabilities of the pre-trained MLLM, CapeLLM\ndemonstrates superior generalization and robust performance. Our approach sets\na new state-of-the-art on the MP-100 benchmark in the challenging 1-shot\nsetting, marking a significant advancement in the field of category-agnostic\npose estimation."
                },
                "authors": [
                    {
                        "name": "Junho Kim"
                    },
                    {
                        "name": "Hyungjin Chung"
                    },
                    {
                        "name": "Byung-Hoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Byung-Hoon Kim"
                },
                "author": "Byung-Hoon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06869v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06869v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02943v2",
                "updated": "2024-11-11T10:51:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    51,
                    31,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-05T09:37:23Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    9,
                    37,
                    23,
                    1,
                    310,
                    0
                ],
                "title": "Capturing research literature attitude towards Sustainable Development\n  Goals: an LLM-based topic modeling approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing research literature attitude towards Sustainable Development\n  Goals: an LLM-based topic modeling approach"
                },
                "summary": "The world is facing a multitude of challenges that hinder the development of\nhuman civilization and the well-being of humanity on the planet. The\nSustainable Development Goals (SDGs) were formulated by the United Nations in\n2015 to address these global challenges by 2030. Natural language processing\ntechniques can help uncover discussions on SDGs within research literature. We\npropose a completely automated pipeline to 1) fetch content from the Scopus\ndatabase and prepare datasets dedicated to five groups of SDGs; 2) perform\ntopic modeling, a statistical technique used to identify topics in large\ncollections of textual data; and 3) enable topic exploration through\nkeywords-based search and topic frequency time series extraction. For topic\nmodeling, we leverage the stack of BERTopic scaled up to be applied on large\ncorpora of textual documents (we find hundreds of topics on hundreds of\nthousands of documents), introducing i) a novel LLM-based embeddings\ncomputation for representing scientific abstracts in the continuous space and\nii) a hyperparameter optimizer to efficiently find the best configuration for\nany new big datasets. We additionally produce the visualization of results on\ninteractive dashboards reporting topics' temporal evolution. Results are made\ninspectable and explorable, contributing to the interpretability of the topic\nmodeling process. Our proposed LLM-based topic modeling pipeline for big-text\ndatasets allows users to capture insights on the evolution of the attitude\ntoward SDGs within scientific abstracts in the 2006-2023 time span. All the\nresults are reproducible by using our system; the workflow can be generalized\nto be applied at any point in time to any big corpus of textual documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The world is facing a multitude of challenges that hinder the development of\nhuman civilization and the well-being of humanity on the planet. The\nSustainable Development Goals (SDGs) were formulated by the United Nations in\n2015 to address these global challenges by 2030. Natural language processing\ntechniques can help uncover discussions on SDGs within research literature. We\npropose a completely automated pipeline to 1) fetch content from the Scopus\ndatabase and prepare datasets dedicated to five groups of SDGs; 2) perform\ntopic modeling, a statistical technique used to identify topics in large\ncollections of textual data; and 3) enable topic exploration through\nkeywords-based search and topic frequency time series extraction. For topic\nmodeling, we leverage the stack of BERTopic scaled up to be applied on large\ncorpora of textual documents (we find hundreds of topics on hundreds of\nthousands of documents), introducing i) a novel LLM-based embeddings\ncomputation for representing scientific abstracts in the continuous space and\nii) a hyperparameter optimizer to efficiently find the best configuration for\nany new big datasets. We additionally produce the visualization of results on\ninteractive dashboards reporting topics' temporal evolution. Results are made\ninspectable and explorable, contributing to the interpretability of the topic\nmodeling process. Our proposed LLM-based topic modeling pipeline for big-text\ndatasets allows users to capture insights on the evolution of the attitude\ntoward SDGs within scientific abstracts in the 2006-2023 time span. All the\nresults are reproducible by using our system; the workflow can be generalized\nto be applied at any point in time to any big corpus of textual documents."
                },
                "authors": [
                    {
                        "name": "Francesco Invernici"
                    },
                    {
                        "name": "Francesca Curati"
                    },
                    {
                        "name": "Jelena Jakimov"
                    },
                    {
                        "name": "Amirhossein Samavi"
                    },
                    {
                        "name": "Anna Bernasconi"
                    }
                ],
                "author_detail": {
                    "name": "Anna Bernasconi"
                },
                "author": "Anna Bernasconi",
                "arxiv_comment": "27 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16040v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16040v5",
                "updated": "2024-11-11T10:40:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    40,
                    50,
                    0,
                    316,
                    0
                ],
                "published": "2024-02-25T09:41:50Z",
                "published_parsed": [
                    2024,
                    2,
                    25,
                    9,
                    41,
                    50,
                    6,
                    56,
                    0
                ],
                "title": "EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using\n  Discharge Summaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using\n  Discharge Summaries"
                },
                "summary": "Discharge summaries in Electronic Health Records (EHRs) are crucial for\nclinical decision-making, but their length and complexity make information\nextraction challenging, especially when dealing with accumulated summaries\nacross multiple patient admissions. Large Language Models (LLMs) show promise\nin addressing this challenge by efficiently analyzing vast and complex data.\nExisting benchmarks, however, fall short in properly evaluating LLMs'\ncapabilities in this context, as they typically focus on single-note\ninformation or limited topics, failing to reflect the real-world inquiries\nrequired by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel\nbenchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each\nlinked to distinct patients' discharge summaries. Every QA pair is initially\ngenerated using GPT-4 and then manually reviewed and refined by three\nclinicians to ensure clinical relevance. EHRNoteQA includes questions that\nrequire information across multiple discharge summaries and covers eight\ndiverse topics, mirroring the complexity and diversity of real clinical\ninquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice\nquestion answering, and propose a reliable evaluation method for each. We\nevaluate 27 LLMs using EHRNoteQA and examine various factors affecting the\nmodel performance (e.g., the length and number of discharge summaries).\nFurthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations\nin clinical practice, we measure the correlation between the LLM performance on\nEHRNoteQA, and the LLM performance manually evaluated by clinicians. Results\nshow that LLM performance on EHRNoteQA have higher correlation with\nclinician-evaluated performance (Spearman: 0.78, Kendall: 0.62) compared to\nother benchmarks, demonstrating its practical relevance in evaluating LLMs in\nclinical settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discharge summaries in Electronic Health Records (EHRs) are crucial for\nclinical decision-making, but their length and complexity make information\nextraction challenging, especially when dealing with accumulated summaries\nacross multiple patient admissions. Large Language Models (LLMs) show promise\nin addressing this challenge by efficiently analyzing vast and complex data.\nExisting benchmarks, however, fall short in properly evaluating LLMs'\ncapabilities in this context, as they typically focus on single-note\ninformation or limited topics, failing to reflect the real-world inquiries\nrequired by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel\nbenchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each\nlinked to distinct patients' discharge summaries. Every QA pair is initially\ngenerated using GPT-4 and then manually reviewed and refined by three\nclinicians to ensure clinical relevance. EHRNoteQA includes questions that\nrequire information across multiple discharge summaries and covers eight\ndiverse topics, mirroring the complexity and diversity of real clinical\ninquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice\nquestion answering, and propose a reliable evaluation method for each. We\nevaluate 27 LLMs using EHRNoteQA and examine various factors affecting the\nmodel performance (e.g., the length and number of discharge summaries).\nFurthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations\nin clinical practice, we measure the correlation between the LLM performance on\nEHRNoteQA, and the LLM performance manually evaluated by clinicians. Results\nshow that LLM performance on EHRNoteQA have higher correlation with\nclinician-evaluated performance (Spearman: 0.78, Kendall: 0.62) compared to\nother benchmarks, demonstrating its practical relevance in evaluating LLMs in\nclinical settings."
                },
                "authors": [
                    {
                        "name": "Sunjun Kweon"
                    },
                    {
                        "name": "Jiyoun Kim"
                    },
                    {
                        "name": "Heeyoung Kwak"
                    },
                    {
                        "name": "Dongchul Cha"
                    },
                    {
                        "name": "Hangyul Yoon"
                    },
                    {
                        "name": "Kwanghyun Kim"
                    },
                    {
                        "name": "Jeewon Yang"
                    },
                    {
                        "name": "Seunghyun Won"
                    },
                    {
                        "name": "Edward Choi"
                    }
                ],
                "author_detail": {
                    "name": "Edward Choi"
                },
                "author": "Edward Choi",
                "arxiv_comment": "NeurIPS 2024 (Datasets and Benchmarks)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16040v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16040v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06852v1",
                "updated": "2024-11-11T10:36:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    36,
                    4,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T10:36:04Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    36,
                    4,
                    0,
                    316,
                    0
                ],
                "title": "Evaluating Large Language Models on Financial Report Summarization: An\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models on Financial Report Summarization: An\n  Empirical Study"
                },
                "summary": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\nversatility across various applications, including natural language\nunderstanding, domain-specific knowledge tasks, etc. However, applying LLMs to\ncomplex, high-stakes domains like finance requires rigorous evaluation to\nensure reliability, accuracy, and compliance with industry standards. To\naddress this need, we conduct a comprehensive and comparative study on three\nstate-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their\neffectiveness in generating automated financial reports. Our primary motivation\nis to explore how these models can be harnessed within finance, a field\ndemanding precision, contextual relevance, and robustness against erroneous or\nmisleading information. By examining each model's capabilities, we aim to\nprovide an insightful assessment of their strengths and limitations. Our paper\noffers benchmarks for financial report analysis, encompassing proposed metrics\nsuch as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative\nevaluation framework that integrates both quantitative metrics (e.g.,\nprecision, recall) and qualitative analyses (e.g., contextual fit, consistency)\nto provide a holistic view of each model's output quality. Additionally, we\nmake our financial dataset publicly available, inviting researchers and\npractitioners to leverage, scrutinize, and enhance our findings through broader\ncommunity engagement and collaborative improvement. Our dataset is available on\nhuggingface.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have demonstrated remarkable\nversatility across various applications, including natural language\nunderstanding, domain-specific knowledge tasks, etc. However, applying LLMs to\ncomplex, high-stakes domains like finance requires rigorous evaluation to\nensure reliability, accuracy, and compliance with industry standards. To\naddress this need, we conduct a comprehensive and comparative study on three\nstate-of-the-art LLMs, GLM-4, Mistral-NeMo, and LLaMA3.1, focusing on their\neffectiveness in generating automated financial reports. Our primary motivation\nis to explore how these models can be harnessed within finance, a field\ndemanding precision, contextual relevance, and robustness against erroneous or\nmisleading information. By examining each model's capabilities, we aim to\nprovide an insightful assessment of their strengths and limitations. Our paper\noffers benchmarks for financial report analysis, encompassing proposed metrics\nsuch as ROUGE-1, BERT Score, and LLM Score. We introduce an innovative\nevaluation framework that integrates both quantitative metrics (e.g.,\nprecision, recall) and qualitative analyses (e.g., contextual fit, consistency)\nto provide a holistic view of each model's output quality. Additionally, we\nmake our financial dataset publicly available, inviting researchers and\npractitioners to leverage, scrutinize, and enhance our findings through broader\ncommunity engagement and collaborative improvement. Our dataset is available on\nhuggingface."
                },
                "authors": [
                    {
                        "name": "Xinqi Yang"
                    },
                    {
                        "name": "Scott Zang"
                    },
                    {
                        "name": "Yong Ren"
                    },
                    {
                        "name": "Dingjie Peng"
                    },
                    {
                        "name": "Zheng Wen"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wen"
                },
                "author": "Zheng Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06851v1",
                "updated": "2024-11-11T10:35:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    35,
                    23,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T10:35:23Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    35,
                    23,
                    0,
                    316,
                    0
                ],
                "title": "Fast and Efficient Transformer-based Method for Bird's Eye View Instance\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Efficient Transformer-based Method for Bird's Eye View Instance\n  Prediction"
                },
                "summary": "Accurate object detection and prediction are critical to ensure the safety\nand efficiency of self-driving architectures. Predicting object trajectories\nand occupancy enables autonomous vehicles to anticipate movements and make\ndecisions with future information, increasing their adaptability and reducing\nthe risk of accidents. Current State-Of-The-Art (SOTA) approaches often isolate\nthe detection, tracking, and prediction stages, which can lead to significant\nprediction errors due to accumulated inaccuracies between stages. Recent\nadvances have improved the feature representation of multi-camera perception\nsystems through Bird's-Eye View (BEV) transformations, boosting the development\nof end-to-end systems capable of predicting environmental elements directly\nfrom vehicle sensor data. These systems, however, often suffer from high\nprocessing times and number of parameters, creating challenges for real-world\ndeployment. To address these issues, this paper introduces a novel BEV instance\nprediction architecture based on a simplified paradigm that relies only on\ninstance segmentation and flow prediction. The proposed system prioritizes\nspeed, aiming at reduced parameter counts and inference times compared to\nexisting SOTA architectures, thanks to the incorporation of an efficient\ntransformer-based architecture. Furthermore, the implementation of the proposed\narchitecture is optimized for performance improvements in PyTorch version 2.1.\nCode and trained models are available at\nhttps://github.com/miguelag99/Efficient-Instance-Prediction",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate object detection and prediction are critical to ensure the safety\nand efficiency of self-driving architectures. Predicting object trajectories\nand occupancy enables autonomous vehicles to anticipate movements and make\ndecisions with future information, increasing their adaptability and reducing\nthe risk of accidents. Current State-Of-The-Art (SOTA) approaches often isolate\nthe detection, tracking, and prediction stages, which can lead to significant\nprediction errors due to accumulated inaccuracies between stages. Recent\nadvances have improved the feature representation of multi-camera perception\nsystems through Bird's-Eye View (BEV) transformations, boosting the development\nof end-to-end systems capable of predicting environmental elements directly\nfrom vehicle sensor data. These systems, however, often suffer from high\nprocessing times and number of parameters, creating challenges for real-world\ndeployment. To address these issues, this paper introduces a novel BEV instance\nprediction architecture based on a simplified paradigm that relies only on\ninstance segmentation and flow prediction. The proposed system prioritizes\nspeed, aiming at reduced parameter counts and inference times compared to\nexisting SOTA architectures, thanks to the incorporation of an efficient\ntransformer-based architecture. Furthermore, the implementation of the proposed\narchitecture is optimized for performance improvements in PyTorch version 2.1.\nCode and trained models are available at\nhttps://github.com/miguelag99/Efficient-Instance-Prediction"
                },
                "authors": [
                    {
                        "name": "Miguel Antunes-Garca"
                    },
                    {
                        "name": "Luis M. Bergasa"
                    },
                    {
                        "name": "Santiago Montiel-Marn"
                    },
                    {
                        "name": "Rafael Barea"
                    },
                    {
                        "name": "Fabio Snchez-Garca"
                    },
                    {
                        "name": "ngel Llamazares"
                    }
                ],
                "author_detail": {
                    "name": "ngel Llamazares"
                },
                "author": "ngel Llamazares",
                "arxiv_comment": "The article has been presented in the 27th IEEE International\n  Conference on Intelligent Transportation Systems (IEEE ITSC 2024) on\n  September, 2024. Number of pages: 6, Number of figures: 4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06850v1",
                "updated": "2024-11-11T10:34:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    34,
                    36,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T10:34:36Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    34,
                    36,
                    0,
                    316,
                    0
                ],
                "title": "1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of\n  Language, Hate Speech, and Targets using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "1-800-SHARED-TASKS @ NLU of Devanagari Script Languages: Detection of\n  Language, Hate Speech, and Targets using LLMs"
                },
                "summary": "This paper presents a detailed system description of our entry for the\nCHiPSAL 2025 shared task, focusing on language detection, hate speech\nidentification, and target detection in Devanagari script languages. We\nexperimented with a combination of large language models and their ensembles,\nincluding MuRIL, IndicBERT, and Gemma-2, and leveraged unique techniques like\nfocal loss to address challenges in the natural understanding of Devanagari\nlanguages, such as multilingual processing and class imbalance. Our approach\nachieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804\nfor Sub-tasks A, B, and C respectively. This work provides insights into the\neffectiveness of transformer models in tasks with domain-specific and\nlinguistic challenges, as well as areas for potential improvement in future\niterations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a detailed system description of our entry for the\nCHiPSAL 2025 shared task, focusing on language detection, hate speech\nidentification, and target detection in Devanagari script languages. We\nexperimented with a combination of large language models and their ensembles,\nincluding MuRIL, IndicBERT, and Gemma-2, and leveraged unique techniques like\nfocal loss to address challenges in the natural understanding of Devanagari\nlanguages, such as multilingual processing and class imbalance. Our approach\nachieved competitive results across all tasks: F1 of 0.9980, 0.7652, and 0.6804\nfor Sub-tasks A, B, and C respectively. This work provides insights into the\neffectiveness of transformer models in tasks with domain-specific and\nlinguistic challenges, as well as areas for potential improvement in future\niterations."
                },
                "authors": [
                    {
                        "name": "Jebish Purbey"
                    },
                    {
                        "name": "Siddartha Pullakhandam"
                    },
                    {
                        "name": "Kanwal Mehreen"
                    },
                    {
                        "name": "Muhammad Arham"
                    },
                    {
                        "name": "Drishti Sharma"
                    },
                    {
                        "name": "Ashay Srivastava"
                    },
                    {
                        "name": "Ram Mohan Rao Kadiyala"
                    }
                ],
                "author_detail": {
                    "name": "Ram Mohan Rao Kadiyala"
                },
                "author": "Ram Mohan Rao Kadiyala",
                "arxiv_comment": "13 pages, Submitted to CHIPSAL workshop @ COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06839v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06839v1",
                "updated": "2024-11-11T10:07:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    7,
                    51,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T10:07:51Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    7,
                    51,
                    0,
                    316,
                    0
                ],
                "title": "LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language\n  Models"
                },
                "summary": "In this paper, we propose a novel LLM-Neo framework that efficiently\ntransfers knowledge from a large language model (LLM) teacher to a compact\nstudent. Initially, we revisit the knowledge distillation (KD) and low-rank\nadaption (LoRA), and argue that they share the same paradigm. Inspired by this\nobservation, we explore the strategy that combines LoRA and KD to enhance the\nefficiency of knowledge transfer. We first summarize some guidelines for this\ndesign and further develop the LLM-Neo. Experimental results on compressing\nLlama 2 and Llama 3 show that LLM-Neo outperforms various baselines. Further\nanalysis demonstrates the robustness of the proposed LLM-Neo on variants of\nLoRA. The trained models have been available at\n\\href{https://huggingface.co/collections/yang31210999/llm-neo-66e3c882f5579b829ff57eba}{this\nrepository}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a novel LLM-Neo framework that efficiently\ntransfers knowledge from a large language model (LLM) teacher to a compact\nstudent. Initially, we revisit the knowledge distillation (KD) and low-rank\nadaption (LoRA), and argue that they share the same paradigm. Inspired by this\nobservation, we explore the strategy that combines LoRA and KD to enhance the\nefficiency of knowledge transfer. We first summarize some guidelines for this\ndesign and further develop the LLM-Neo. Experimental results on compressing\nLlama 2 and Llama 3 show that LLM-Neo outperforms various baselines. Further\nanalysis demonstrates the robustness of the proposed LLM-Neo on variants of\nLoRA. The trained models have been available at\n\\href{https://huggingface.co/collections/yang31210999/llm-neo-66e3c882f5579b829ff57eba}{this\nrepository}."
                },
                "authors": [
                    {
                        "name": "Runming Yang"
                    },
                    {
                        "name": "Taiqiang Wu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Pengfei Hu"
                    },
                    {
                        "name": "Ngai Wong"
                    },
                    {
                        "name": "Yujiu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yujiu Yang"
                },
                "author": "Yujiu Yang",
                "arxiv_comment": "ICASSP 25' under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06839v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06839v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06837v1",
                "updated": "2024-11-11T10:05:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    5,
                    52,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T10:05:52Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    5,
                    52,
                    0,
                    316,
                    0
                ],
                "title": "Persuasion with Large Language Models: a Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasion with Large Language Models: a Survey"
                },
                "summary": "The rapid rise of Large Language Models (LLMs) has created new disruptive\npossibilities for persuasive communication, by enabling fully-automated\npersonalized and interactive content generation at an unprecedented scale. In\nthis paper, we survey the research field of LLM-based persuasion that has\nemerged as a result. We begin by exploring the different modes in which LLM\nSystems are used to influence human attitudes and behaviors. In areas such as\npolitics, marketing, public health, e-commerce, and charitable giving, such LLM\nSystems have already achieved human-level or even super-human persuasiveness.\nWe identify key factors influencing their effectiveness, such as the manner of\npersonalization and whether the content is labelled as AI-generated. We also\nsummarize the experimental designs that have been used to evaluate progress.\nOur survey suggests that the current and future potential of LLM-based\npersuasion poses profound ethical and societal risks, including the spread of\nmisinformation, the magnification of biases, and the invasion of privacy. These\nrisks underscore the urgent need for ethical guidelines and updated regulatory\nframeworks to avoid the widespread deployment of irresponsible and harmful LLM\nSystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of Large Language Models (LLMs) has created new disruptive\npossibilities for persuasive communication, by enabling fully-automated\npersonalized and interactive content generation at an unprecedented scale. In\nthis paper, we survey the research field of LLM-based persuasion that has\nemerged as a result. We begin by exploring the different modes in which LLM\nSystems are used to influence human attitudes and behaviors. In areas such as\npolitics, marketing, public health, e-commerce, and charitable giving, such LLM\nSystems have already achieved human-level or even super-human persuasiveness.\nWe identify key factors influencing their effectiveness, such as the manner of\npersonalization and whether the content is labelled as AI-generated. We also\nsummarize the experimental designs that have been used to evaluate progress.\nOur survey suggests that the current and future potential of LLM-based\npersuasion poses profound ethical and societal risks, including the spread of\nmisinformation, the magnification of biases, and the invasion of privacy. These\nrisks underscore the urgent need for ethical guidelines and updated regulatory\nframeworks to avoid the widespread deployment of irresponsible and harmful LLM\nSystems."
                },
                "authors": [
                    {
                        "name": "Alexander Rogiers"
                    },
                    {
                        "name": "Sander Noels"
                    },
                    {
                        "name": "Maarten Buyl"
                    },
                    {
                        "name": "Tijl De Bie"
                    }
                ],
                "author_detail": {
                    "name": "Tijl De Bie"
                },
                "author": "Tijl De Bie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04242v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04242v3",
                "updated": "2024-11-11T10:03:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    3,
                    47,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-06T20:11:19Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    20,
                    11,
                    19,
                    2,
                    311,
                    0
                ],
                "title": "Multimodal Structure-Aware Quantum Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Structure-Aware Quantum Data Processing"
                },
                "summary": "While large language models (LLMs) have advanced the field of natural\nlanguage processing (NLP), their \"black box\" nature obscures their\ndecision-making processes. To address this, researchers developed structured\napproaches using higher order tensors. These are able to model linguistic\nrelations, but stall when training on classical computers due to their\nexcessive size. Tensors are natural inhabitants of quantum systems and training\non quantum computers provides a solution by translating text to variational\nquantum circuits. In this paper, we develop MultiQ-NLP: a framework for\nstructure-aware data processing with multimodal text+image data. Here,\n\"structure\" refers to syntactic and grammatical relationships in language, as\nwell as the hierarchical organization of visual elements in images. We enrich\nthe translation with new types and type homomorphisms and develop novel\narchitectures to represent structure. When tested on a main stream image\nclassification task (SVO Probes), our best model showed a par performance with\nthe state of the art classical models; moreover the best model was fully\nstructured.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have advanced the field of natural\nlanguage processing (NLP), their \"black box\" nature obscures their\ndecision-making processes. To address this, researchers developed structured\napproaches using higher order tensors. These are able to model linguistic\nrelations, but stall when training on classical computers due to their\nexcessive size. Tensors are natural inhabitants of quantum systems and training\non quantum computers provides a solution by translating text to variational\nquantum circuits. In this paper, we develop MultiQ-NLP: a framework for\nstructure-aware data processing with multimodal text+image data. Here,\n\"structure\" refers to syntactic and grammatical relationships in language, as\nwell as the hierarchical organization of visual elements in images. We enrich\nthe translation with new types and type homomorphisms and develop novel\narchitectures to represent structure. When tested on a main stream image\nclassification task (SVO Probes), our best model showed a par performance with\nthe state of the art classical models; moreover the best model was fully\nstructured."
                },
                "authors": [
                    {
                        "name": "Hala Hawashin"
                    },
                    {
                        "name": "Mehrnoosh Sadrzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehrnoosh Sadrzadeh"
                },
                "author": "Mehrnoosh Sadrzadeh",
                "arxiv_comment": "10 Pages, 16 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04242v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04242v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68T50, 68Q12, 68U15, 68U10, 81P45, 81P68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.10; H.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06835v1",
                "updated": "2024-11-11T10:02:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    2,
                    49,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T10:02:49Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    2,
                    49,
                    0,
                    316,
                    0
                ],
                "title": "HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of\n  Quantization on Model Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmLevelBench: Evaluating Harm-Level Compliance and the Impact of\n  Quantization on Model Alignment"
                },
                "summary": "With the introduction of the transformers architecture, LLMs have\nrevolutionized the NLP field with ever more powerful models. Nevertheless,\ntheir development came up with several challenges. The exponential growth in\ncomputational power and reasoning capabilities of language models has\nheightened concerns about their security. As models become more powerful,\nensuring their safety has become a crucial focus in research. This paper aims\nto address gaps in the current literature on jailbreaking techniques and the\nevaluation of LLM vulnerabilities. Our contributions include the creation of a\nnovel dataset designed to assess the harmfulness of model outputs across\nmultiple harm levels, as well as a focus on fine-grained harm-level analysis.\nUsing this framework, we provide a comprehensive benchmark of state-of-the-art\njailbreaking attacks, specifically targeting the Vicuna 13B v1.5 model.\nAdditionally, we examine how quantization techniques, such as AWQ and GPTQ,\ninfluence the alignment and robustness of models, revealing trade-offs between\nenhanced robustness with regards to transfer attacks and potential increases in\nvulnerability on direct ones. This study aims to demonstrate the influence of\nharmful input queries on the complexity of jailbreaking techniques, as well as\nto deepen our understanding of LLM vulnerabilities and improve methods for\nassessing model robustness when confronted with harmful content, particularly\nin the context of compression strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the introduction of the transformers architecture, LLMs have\nrevolutionized the NLP field with ever more powerful models. Nevertheless,\ntheir development came up with several challenges. The exponential growth in\ncomputational power and reasoning capabilities of language models has\nheightened concerns about their security. As models become more powerful,\nensuring their safety has become a crucial focus in research. This paper aims\nto address gaps in the current literature on jailbreaking techniques and the\nevaluation of LLM vulnerabilities. Our contributions include the creation of a\nnovel dataset designed to assess the harmfulness of model outputs across\nmultiple harm levels, as well as a focus on fine-grained harm-level analysis.\nUsing this framework, we provide a comprehensive benchmark of state-of-the-art\njailbreaking attacks, specifically targeting the Vicuna 13B v1.5 model.\nAdditionally, we examine how quantization techniques, such as AWQ and GPTQ,\ninfluence the alignment and robustness of models, revealing trade-offs between\nenhanced robustness with regards to transfer attacks and potential increases in\nvulnerability on direct ones. This study aims to demonstrate the influence of\nharmful input queries on the complexity of jailbreaking techniques, as well as\nto deepen our understanding of LLM vulnerabilities and improve methods for\nassessing model robustness when confronted with harmful content, particularly\nin the context of compression strategies."
                },
                "authors": [
                    {
                        "name": "Yannis Belkhiter"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Sergio Maffeis"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Maffeis"
                },
                "author": "Sergio Maffeis",
                "arxiv_comment": "NeurIPS 2024 Workshop on Safe Generative Artificial Intelligence\n  (SafeGenAI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13704v2",
                "updated": "2024-11-11T10:02:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    10,
                    2,
                    24,
                    0,
                    316,
                    0
                ],
                "published": "2024-09-05T10:27:32Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    10,
                    27,
                    32,
                    3,
                    249,
                    0
                ],
                "title": "Entity Extraction from High-Level Corruption Schemes via Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity Extraction from High-Level Corruption Schemes via Large Language\n  Models"
                },
                "summary": "The rise of financial crime that has been observed in recent years has\ncreated an increasing concern around the topic and many people, organizations\nand governments are more and more frequently trying to combat it. Despite the\nincrease of interest in this area, there is a lack of specialized datasets that\ncan be used to train and evaluate works that try to tackle those problems. This\narticle proposes a new micro-benchmark dataset for algorithms and models that\nidentify individuals and organizations, and their multiple writings, in news\narticles, and presents an approach that assists in its creation. Experimental\nefforts are also reported, using this dataset, to identify individuals and\norganizations in financial-crime-related articles using various low-billion\nparameter Large Language Models (LLMs). For these experiments, standard metrics\n(Accuracy, Precision, Recall, F1 Score) are reported and various prompt\nvariants comprising the best practices of prompt engineering are tested. In\naddition, to address the problem of ambiguous entity mentions, a simple, yet\neffective LLM-based disambiguation method is proposed, ensuring that the\nevaluation aligns with reality. Finally, the proposed approach is compared\nagainst a widely used state-of-the-art open-source baseline, showing the\nsuperiority of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of financial crime that has been observed in recent years has\ncreated an increasing concern around the topic and many people, organizations\nand governments are more and more frequently trying to combat it. Despite the\nincrease of interest in this area, there is a lack of specialized datasets that\ncan be used to train and evaluate works that try to tackle those problems. This\narticle proposes a new micro-benchmark dataset for algorithms and models that\nidentify individuals and organizations, and their multiple writings, in news\narticles, and presents an approach that assists in its creation. Experimental\nefforts are also reported, using this dataset, to identify individuals and\norganizations in financial-crime-related articles using various low-billion\nparameter Large Language Models (LLMs). For these experiments, standard metrics\n(Accuracy, Precision, Recall, F1 Score) are reported and various prompt\nvariants comprising the best practices of prompt engineering are tested. In\naddition, to address the problem of ambiguous entity mentions, a simple, yet\neffective LLM-based disambiguation method is proposed, ensuring that the\nevaluation aligns with reality. Finally, the proposed approach is compared\nagainst a widely used state-of-the-art open-source baseline, showing the\nsuperiority of the proposed method."
                },
                "authors": [
                    {
                        "name": "Panagiotis Koletsis"
                    },
                    {
                        "name": "Panagiotis-Konstantinos Gemos"
                    },
                    {
                        "name": "Christos Chronis"
                    },
                    {
                        "name": "Iraklis Varlamis"
                    },
                    {
                        "name": "Vasilis Efthymiou"
                    },
                    {
                        "name": "Georgios Th. Papadopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Th. Papadopoulos"
                },
                "author": "Georgios Th. Papadopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06824v1",
                "updated": "2024-11-11T09:32:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    32,
                    20,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T09:32:20Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    32,
                    20,
                    0,
                    316,
                    0
                ],
                "title": "Combining Domain and Alignment Vectors to Achieve Better\n  Knowledge-Safety Trade-offs in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Domain and Alignment Vectors to Achieve Better\n  Knowledge-Safety Trade-offs in LLMs"
                },
                "summary": "There is a growing interest in training domain-expert LLMs that excel in\nspecific technical fields compared to their general-purpose instruction-tuned\ncounterparts. However, these expert models often experience a loss in their\nsafety abilities in the process, making them capable of generating harmful\ncontent. As a solution, we introduce an efficient and effective merging-based\nalignment method called \\textsc{MergeAlign} that interpolates the domain and\nalignment vectors, creating safer domain-specific models while preserving their\nutility. We apply \\textsc{MergeAlign} on Llama3 variants that are experts in\nmedicine and finance, obtaining substantial alignment improvements with minimal\nto no degradation on domain-specific benchmarks. We study the impact of model\nmerging through model similarity metrics and contributions of individual models\nbeing merged. We hope our findings open new research avenues and inspire more\nefficient development of safe expert LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing interest in training domain-expert LLMs that excel in\nspecific technical fields compared to their general-purpose instruction-tuned\ncounterparts. However, these expert models often experience a loss in their\nsafety abilities in the process, making them capable of generating harmful\ncontent. As a solution, we introduce an efficient and effective merging-based\nalignment method called \\textsc{MergeAlign} that interpolates the domain and\nalignment vectors, creating safer domain-specific models while preserving their\nutility. We apply \\textsc{MergeAlign} on Llama3 variants that are experts in\nmedicine and finance, obtaining substantial alignment improvements with minimal\nto no degradation on domain-specific benchmarks. We study the impact of model\nmerging through model similarity metrics and contributions of individual models\nbeing merged. We hope our findings open new research avenues and inspire more\nefficient development of safe expert LLMs."
                },
                "authors": [
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Yash More"
                    },
                    {
                        "name": "Quentin Fournier"
                    },
                    {
                        "name": "Matthew Riemer"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Amal Zouaq"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Sarath Chandar"
                    }
                ],
                "author_detail": {
                    "name": "Sarath Chandar"
                },
                "author": "Sarath Chandar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06823v1",
                "updated": "2024-11-11T09:31:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    31,
                    46,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T09:31:46Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    31,
                    46,
                    0,
                    316,
                    0
                ],
                "title": "Large Language Model in Medical Informatics: Direct Classification and\n  Enhanced Text Representations for Automatic ICD Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model in Medical Informatics: Direct Classification and\n  Enhanced Text Representations for Automatic ICD Coding"
                },
                "summary": "Addressing the complexity of accurately classifying International\nClassification of Diseases (ICD) codes from medical discharge summaries is\nchallenging due to the intricate nature of medical documentation. This paper\nexplores the use of Large Language Models (LLM), specifically the LLAMA\narchitecture, to enhance ICD code classification through two methodologies:\ndirect application as a classifier and as a generator of enriched text\nrepresentations within a Multi-Filter Residual Convolutional Neural Network\n(MultiResCNN) framework. We evaluate these methods by comparing them against\nstate-of-the-art approaches, revealing LLAMA's potential to significantly\nimprove classification outcomes by providing deep contextual insights into\nmedical texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the complexity of accurately classifying International\nClassification of Diseases (ICD) codes from medical discharge summaries is\nchallenging due to the intricate nature of medical documentation. This paper\nexplores the use of Large Language Models (LLM), specifically the LLAMA\narchitecture, to enhance ICD code classification through two methodologies:\ndirect application as a classifier and as a generator of enriched text\nrepresentations within a Multi-Filter Residual Convolutional Neural Network\n(MultiResCNN) framework. We evaluate these methods by comparing them against\nstate-of-the-art approaches, revealing LLAMA's potential to significantly\nimprove classification outcomes by providing deep contextual insights into\nmedical texts."
                },
                "authors": [
                    {
                        "name": "Zeyd Boukhers"
                    },
                    {
                        "name": "AmeerAli Khan"
                    },
                    {
                        "name": "Qusai Ramadan"
                    },
                    {
                        "name": "Cong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Cong Yang"
                },
                "author": "Cong Yang",
                "arxiv_comment": "accepted at the 2024 IEEE International Conference on Bioinformatics\n  and Biomedicine (BIBM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.10792v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.10792v7",
                "updated": "2024-11-11T09:25:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    25,
                    48,
                    0,
                    316,
                    0
                ],
                "published": "2023-08-21T15:35:16Z",
                "published_parsed": [
                    2023,
                    8,
                    21,
                    15,
                    35,
                    16,
                    0,
                    233,
                    0
                ],
                "title": "Instruction Tuning for Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Tuning for Large Language Models: A Survey"
                },
                "summary": "This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), which can also be referred to as supervised\nfine-tuning (SFT)\\footnote{In this paper, unless specified otherwise,\ninstruction tuning (IT) will be equivalent to supervised fine-tuning (SFT).}, a\ncrucial technique to enhance the capabilities and controllability of large\nlanguage models (LLMs). Instruction tuning refers to the process of further\ntraining LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs\nin a supervised fashion, which bridges the gap between the next-word prediction\nobjective of LLMs and the users' objective of having LLMs adhere to human\ninstructions. In this work, we make a systematic review of the literature,\nincluding the general methodology of IT, the construction of IT datasets, the\ntraining of IT models, and applications to different modalities, domains and\napplication, along with analysis on aspects that influence the outcome of IT\n(e.g., generation of instruction outputs, size of the instruction dataset,\netc). We also review the potential pitfalls of IT along with criticism against\nit, along with efforts pointing out current deficiencies of existing strategies\nand suggest some avenues for fruitful research.Project page:\ngithub.com/xiaoya-li/Instruction-Tuning-Survey",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), which can also be referred to as supervised\nfine-tuning (SFT)\\footnote{In this paper, unless specified otherwise,\ninstruction tuning (IT) will be equivalent to supervised fine-tuning (SFT).}, a\ncrucial technique to enhance the capabilities and controllability of large\nlanguage models (LLMs). Instruction tuning refers to the process of further\ntraining LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs\nin a supervised fashion, which bridges the gap between the next-word prediction\nobjective of LLMs and the users' objective of having LLMs adhere to human\ninstructions. In this work, we make a systematic review of the literature,\nincluding the general methodology of IT, the construction of IT datasets, the\ntraining of IT models, and applications to different modalities, domains and\napplication, along with analysis on aspects that influence the outcome of IT\n(e.g., generation of instruction outputs, size of the instruction dataset,\netc). We also review the potential pitfalls of IT along with criticism against\nit, along with efforts pointing out current deficiencies of existing strategies\nand suggest some avenues for fruitful research.Project page:\ngithub.com/xiaoya-li/Instruction-Tuning-Survey"
                },
                "authors": [
                    {
                        "name": "Shengyu Zhang"
                    },
                    {
                        "name": "Linfeng Dong"
                    },
                    {
                        "name": "Xiaoya Li"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Xiaofei Sun"
                    },
                    {
                        "name": "Shuhe Wang"
                    },
                    {
                        "name": "Jiwei Li"
                    },
                    {
                        "name": "Runyi Hu"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Fei Wu"
                    },
                    {
                        "name": "Guoyin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoyin Wang"
                },
                "author": "Guoyin Wang",
                "arxiv_comment": "V4; Last update: Nov 11, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.10792v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.10792v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06815v1",
                "updated": "2024-11-11T09:22:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    22,
                    9,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T09:22:09Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    22,
                    9,
                    0,
                    316,
                    0
                ],
                "title": "Streetwise Agents: Empowering Offline RL Policies to Outsmart Exogenous\n  Stochastic Disturbances in RTC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streetwise Agents: Empowering Offline RL Policies to Outsmart Exogenous\n  Stochastic Disturbances in RTC"
                },
                "summary": "The difficulty of exploring and training online on real production systems\nlimits the scope of real-time online data/feedback-driven decision making. The\nmost feasible approach is to adopt offline reinforcement learning from limited\ntrajectory samples. However, after deployment, such policies fail due to\nexogenous factors that temporarily or permanently disturb/alter the transition\ndistribution of the assumed decision process structure induced by offline\nsamples. This results in critical policy failures and generalization errors in\nsensitive domains like Real-Time Communication (RTC). We solve this crucial\nproblem of identifying robust actions in presence of domain shifts due to\nunseen exogenous stochastic factors in the wild. As it is impossible to learn\ngeneralized offline policies within the support of offline data that are robust\nto these unseen exogenous disturbances, we propose a novel post-deployment\nshaping of policies (Streetwise), conditioned on real-time characterization of\nout-of-distribution sub-spaces. This leads to robust actions in bandwidth\nestimation (BWE) of network bottlenecks in RTC and in standard benchmarks. Our\nextensive experimental results on BWE and other standard offline RL benchmark\nenvironments demonstrate a significant improvement ($\\approx$ 18% on some\nscenarios) in final returns wrt. end-user metrics over state-of-the-art\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The difficulty of exploring and training online on real production systems\nlimits the scope of real-time online data/feedback-driven decision making. The\nmost feasible approach is to adopt offline reinforcement learning from limited\ntrajectory samples. However, after deployment, such policies fail due to\nexogenous factors that temporarily or permanently disturb/alter the transition\ndistribution of the assumed decision process structure induced by offline\nsamples. This results in critical policy failures and generalization errors in\nsensitive domains like Real-Time Communication (RTC). We solve this crucial\nproblem of identifying robust actions in presence of domain shifts due to\nunseen exogenous stochastic factors in the wild. As it is impossible to learn\ngeneralized offline policies within the support of offline data that are robust\nto these unseen exogenous disturbances, we propose a novel post-deployment\nshaping of policies (Streetwise), conditioned on real-time characterization of\nout-of-distribution sub-spaces. This leads to robust actions in bandwidth\nestimation (BWE) of network bottlenecks in RTC and in standard benchmarks. Our\nextensive experimental results on BWE and other standard offline RL benchmark\nenvironments demonstrate a significant improvement ($\\approx$ 18% on some\nscenarios) in final returns wrt. end-user metrics over state-of-the-art\nbaselines."
                },
                "authors": [
                    {
                        "name": "Aditya Soni"
                    },
                    {
                        "name": "Mayukh Das"
                    },
                    {
                        "name": "Anjaly Parayil"
                    },
                    {
                        "name": "Supriyo Ghosh"
                    },
                    {
                        "name": "Shivam Shandilya"
                    },
                    {
                        "name": "Ching-An Cheng"
                    },
                    {
                        "name": "Vishak Gopal"
                    },
                    {
                        "name": "Sami Khairy"
                    },
                    {
                        "name": "Gabriel Mittag"
                    },
                    {
                        "name": "Yasaman Hosseinkashi"
                    },
                    {
                        "name": "Chetan Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Bansal"
                },
                "author": "Chetan Bansal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09056v2",
                "updated": "2024-11-11T09:19:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    19,
                    46,
                    0,
                    316,
                    0
                ],
                "published": "2024-06-13T12:43:40Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    12,
                    43,
                    40,
                    3,
                    165,
                    0
                ],
                "title": "CUDRT: Benchmarking the Detection Models of Human vs. Large Language\n  Models Generated Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUDRT: Benchmarking the Detection Models of Human vs. Large Language\n  Models Generated Texts"
                },
                "summary": "While large language models (LLMs) have greatly enhanced text generation\nacross industries, their human-like outputs make distinguishing between human\nand AI authorship challenging. Although many LLM-generated text detectors\nexist, current benchmarks mainly rely on static datasets, limiting their\neffectiveness in assessing model-based detectors requiring prior training.\nFurthermore, these benchmarks focus on specific scenarios like question\nanswering and text refinement and are primarily limited to English, overlooking\nbroader linguistic applications and LLM subtleties. To address these gaps, we\nconstruct a comprehensive bilingual benchmark in Chinese and English to\nrigorously evaluate mainstream LLM-generated text detection methods. We\ncategorize LLM text generation into five key operations-Create, Update, Delete,\nRewrite, and Translate (CUDRT)-covering the full range of LLM activities. For\neach CUDRT category, we developed extensive datasets enabling thorough\nassessment of detection performance, incorporating the latest mainstream LLMs\nfor each language. We also establish a robust evaluation framework to support\nscalable, reproducible experiments, facilitating an in-depth analysis of how\nLLM operations, different LLMs, datasets, and multilingual training sets impact\ndetector performance, particularly for model-based methods. Our extensive\nexperiments provide critical insights for optimizing LLM-generated text\ndetectors and suggest future directions to improve detection accuracy and\ngeneralization across diverse scenarios.Source code and dataset are available\nat GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have greatly enhanced text generation\nacross industries, their human-like outputs make distinguishing between human\nand AI authorship challenging. Although many LLM-generated text detectors\nexist, current benchmarks mainly rely on static datasets, limiting their\neffectiveness in assessing model-based detectors requiring prior training.\nFurthermore, these benchmarks focus on specific scenarios like question\nanswering and text refinement and are primarily limited to English, overlooking\nbroader linguistic applications and LLM subtleties. To address these gaps, we\nconstruct a comprehensive bilingual benchmark in Chinese and English to\nrigorously evaluate mainstream LLM-generated text detection methods. We\ncategorize LLM text generation into five key operations-Create, Update, Delete,\nRewrite, and Translate (CUDRT)-covering the full range of LLM activities. For\neach CUDRT category, we developed extensive datasets enabling thorough\nassessment of detection performance, incorporating the latest mainstream LLMs\nfor each language. We also establish a robust evaluation framework to support\nscalable, reproducible experiments, facilitating an in-depth analysis of how\nLLM operations, different LLMs, datasets, and multilingual training sets impact\ndetector performance, particularly for model-based methods. Our extensive\nexperiments provide critical insights for optimizing LLM-generated text\ndetectors and suggest future directions to improve detection accuracy and\ngeneralization across diverse scenarios.Source code and dataset are available\nat GitHub."
                },
                "authors": [
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Yanfang Chen"
                    },
                    {
                        "name": "Dinghao Xi"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.09220v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.09220v3",
                "updated": "2024-11-11T09:16:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    16,
                    56,
                    0,
                    316,
                    0
                ],
                "published": "2024-05-15T09:59:37Z",
                "published_parsed": [
                    2024,
                    5,
                    15,
                    9,
                    59,
                    37,
                    2,
                    136,
                    0
                ],
                "title": "ALPINE: Unveiling the Planning Capability of Autoregressive Learning in\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALPINE: Unveiling the Planning Capability of Autoregressive Learning in\n  Language Models"
                },
                "summary": "Planning is a crucial element of both human intelligence and contemporary\nlarge language models (LLMs). In this paper, we initiate a theoretical\ninvestigation into the emergence of planning capabilities in Transformer-based\nLLMs via their next-word prediction mechanisms. We model planning as a network\npath-finding task, where the objective is to generate a valid path from a\nspecified source node to a designated target node. Our mathematical\ncharacterization shows that Transformer architectures can execute path-finding\nby embedding the adjacency and reachability matrices within their weights.\nFurthermore, our theoretical analysis of gradient-based learning dynamics\nreveals that LLMs can learn both the adjacency and a limited form of the\nreachability matrices. These theoretical insights are then validated through\nexperiments, which demonstrate that Transformer architectures indeed learn the\nadjacency and an incomplete reachability matrices, consistent with our\ntheoretical predictions. When applying our methodology to the real-world\nplanning benchmark Blocksworld, our observations remain consistent.\nAdditionally, our analyses uncover a fundamental limitation of current\nTransformer architectures in path-finding: these architectures cannot identify\nreachability relationships through transitivity, which leads to failures in\ngenerating paths when concatenation is required. These findings provide new\ninsights into how the internal mechanisms of autoregressive learning facilitate\nintelligent planning and deepen our understanding of how future LLMs might\nachieve more advanced and general planning-and-reasoning capabilities across\ndiverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning is a crucial element of both human intelligence and contemporary\nlarge language models (LLMs). In this paper, we initiate a theoretical\ninvestigation into the emergence of planning capabilities in Transformer-based\nLLMs via their next-word prediction mechanisms. We model planning as a network\npath-finding task, where the objective is to generate a valid path from a\nspecified source node to a designated target node. Our mathematical\ncharacterization shows that Transformer architectures can execute path-finding\nby embedding the adjacency and reachability matrices within their weights.\nFurthermore, our theoretical analysis of gradient-based learning dynamics\nreveals that LLMs can learn both the adjacency and a limited form of the\nreachability matrices. These theoretical insights are then validated through\nexperiments, which demonstrate that Transformer architectures indeed learn the\nadjacency and an incomplete reachability matrices, consistent with our\ntheoretical predictions. When applying our methodology to the real-world\nplanning benchmark Blocksworld, our observations remain consistent.\nAdditionally, our analyses uncover a fundamental limitation of current\nTransformer architectures in path-finding: these architectures cannot identify\nreachability relationships through transitivity, which leads to failures in\ngenerating paths when concatenation is required. These findings provide new\ninsights into how the internal mechanisms of autoregressive learning facilitate\nintelligent planning and deepen our understanding of how future LLMs might\nachieve more advanced and general planning-and-reasoning capabilities across\ndiverse applications."
                },
                "authors": [
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Yifei Shen"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Shang-Hua Teng"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.09220v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.09220v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15761v2",
                "updated": "2024-11-11T09:06:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    6,
                    51,
                    0,
                    316,
                    0
                ],
                "published": "2024-10-21T08:21:00Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    8,
                    21,
                    0,
                    0,
                    295,
                    0
                ],
                "title": "Learning-to-Defer for Extractive Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Defer for Extractive Question Answering"
                },
                "summary": "Pre-trained language models have profoundly impacted the field of extractive\nquestion-answering, leveraging large-scale textual corpora to enhance\ncontextual language understanding. Despite their success, these models struggle\nin complex scenarios that demand nuanced interpretation or inferential\nreasoning beyond immediate textual cues. Furthermore, their size poses\ndeployment challenges on resource-constrained devices. Addressing these\nlimitations, we introduce an adapted two-stage Learning-to-Defer mechanism that\nenhances decision-making by enabling selective deference to human experts or\nlarger models without retraining language models in the context of\nquestion-answering. This approach not only maintains computational efficiency\nbut also significantly improves model reliability and accuracy in ambiguous\ncontexts. We establish the theoretical soundness of our methodology by proving\nBayes and $(\\mathcal{H}, \\mathcal{R})$--consistency of our surrogate loss\nfunction, guaranteeing the optimality of the final solution. Empirical\nevaluations on the SQuADv2 dataset illustrate performance gains from\nintegrating human expertise and leveraging larger models. Our results further\ndemonstrate that deferring a minimal number of queries allows the smaller model\nto achieve performance comparable to their larger counterparts while preserving\ncomputing efficiency, thus broadening the applicability of pre-trained language\nmodels in diverse operational environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained language models have profoundly impacted the field of extractive\nquestion-answering, leveraging large-scale textual corpora to enhance\ncontextual language understanding. Despite their success, these models struggle\nin complex scenarios that demand nuanced interpretation or inferential\nreasoning beyond immediate textual cues. Furthermore, their size poses\ndeployment challenges on resource-constrained devices. Addressing these\nlimitations, we introduce an adapted two-stage Learning-to-Defer mechanism that\nenhances decision-making by enabling selective deference to human experts or\nlarger models without retraining language models in the context of\nquestion-answering. This approach not only maintains computational efficiency\nbut also significantly improves model reliability and accuracy in ambiguous\ncontexts. We establish the theoretical soundness of our methodology by proving\nBayes and $(\\mathcal{H}, \\mathcal{R})$--consistency of our surrogate loss\nfunction, guaranteeing the optimality of the final solution. Empirical\nevaluations on the SQuADv2 dataset illustrate performance gains from\nintegrating human expertise and leveraging larger models. Our results further\ndemonstrate that deferring a minimal number of queries allows the smaller model\nto achieve performance comparable to their larger counterparts while preserving\ncomputing efficiency, thus broadening the applicability of pre-trained language\nmodels in diverse operational environments."
                },
                "authors": [
                    {
                        "name": "Yannis Montreuil"
                    },
                    {
                        "name": "Axel Carlier"
                    },
                    {
                        "name": "Lai Xing Ng"
                    },
                    {
                        "name": "Wei Tsang Ooi"
                    }
                ],
                "author_detail": {
                    "name": "Wei Tsang Ooi"
                },
                "author": "Wei Tsang Ooi",
                "arxiv_comment": "25 pages, 17 main paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06805v1",
                "updated": "2024-11-11T09:03:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    3,
                    52,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T09:03:52Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    9,
                    3,
                    52,
                    0,
                    316,
                    0
                ],
                "title": "AssistRAG: Boosting the Potential of Large Language Models with an\n  Intelligent Information Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AssistRAG: Boosting the Potential of Large Language Models with an\n  Intelligent Information Assistant"
                },
                "summary": "The emergence of Large Language Models (LLMs) has significantly advanced\nnatural language processing, but these models often generate factually\nincorrect information, known as \"hallucination\". Initial retrieval-augmented\ngeneration (RAG) methods like the \"Retrieve-Read\" framework was inadequate for\ncomplex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised\nFine-Tuning (SFT) methods improved performance but required frequent retraining\nand risked altering foundational LLM capabilities. To cope with these\nchallenges, we propose Assistant-based Retrieval-Augmented Generation\n(AssistRAG), integrating an intelligent information assistant within LLMs. This\nassistant manages memory and knowledge through tool usage, action execution,\nmemory building, and plan specification. Using a two-phase training approach,\nCurriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG\nenhances information retrieval and decision-making. Experiments show AssistRAG\nsignificantly outperforms benchmarks, especially benefiting less advanced LLMs,\nby providing superior reasoning capabilities and accurate responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs) has significantly advanced\nnatural language processing, but these models often generate factually\nincorrect information, known as \"hallucination\". Initial retrieval-augmented\ngeneration (RAG) methods like the \"Retrieve-Read\" framework was inadequate for\ncomplex reasoning tasks. Subsequent prompt-based RAG strategies and Supervised\nFine-Tuning (SFT) methods improved performance but required frequent retraining\nand risked altering foundational LLM capabilities. To cope with these\nchallenges, we propose Assistant-based Retrieval-Augmented Generation\n(AssistRAG), integrating an intelligent information assistant within LLMs. This\nassistant manages memory and knowledge through tool usage, action execution,\nmemory building, and plan specification. Using a two-phase training approach,\nCurriculum Assistant Learning and Reinforced Preference Optimization. AssistRAG\nenhances information retrieval and decision-making. Experiments show AssistRAG\nsignificantly outperforms benchmarks, especially benefiting less advanced LLMs,\nby providing superior reasoning capabilities and accurate responses."
                },
                "authors": [
                    {
                        "name": "Yujia Zhou"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Accepted by NeurIPS 2024 (poster)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06796v1",
                "updated": "2024-11-11T08:50:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    50,
                    24,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T08:50:24Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    50,
                    24,
                    0,
                    316,
                    0
                ],
                "title": "Automatically Write Code Checker: An LLM-based Approach with\n  Logic-guided API Retrieval and Case by Case Iteration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically Write Code Checker: An LLM-based Approach with\n  Logic-guided API Retrieval and Case by Case Iteration"
                },
                "summary": "With the rising demand for code quality assurance, developers are not only\nutilizing existing static code checkers but also seeking custom checkers to\nsatisfy their specific needs. Nowadays, various code-checking frameworks\nprovide extensive checker customization interfaces to meet this need. However,\nboth the abstract checking logic as well as the complex API usage of\nlarge-scale frameworks make this task challenging. To this end, automated code\nchecker generation is anticipated to ease the burden of checker development. In\nthis paper, we explore the feasibility of automated checker generation and\npropose AutoChecker, an innovative LLM-powered approach that can write code\ncheckers automatically based on only a rule description and a test suite.\nInstead of generating the checker at once, AutoChecker incrementally updates\nthe checker with the rule and one single test case each time, i.e., it\niteratively generates the checker case by case. During each iteration,\nAutoChecker first decomposes the whole logic into a series of sub-operations\nand then uses the logic-guided API-context retrieval strategy to search related\nAPI-contexts from all the framework APIs. To evaluate the effectiveness of\nAutoChecker, we apply AutoChecker and two LLM-based baseline approaches to\nautomatically generate checkers for 20 built-in PMD rules, including easy rules\nand hard rules. Experimental results demonstrate that AutoChecker significantly\noutperforms baseline approaches across all effectiveness metrics, where its\naverage test pass rate improved over 4.2 times. Moreover, the checkers\ngenerated by AutoChecker are successfully applied to real-world projects,\nmatching the performance of official checkers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for code quality assurance, developers are not only\nutilizing existing static code checkers but also seeking custom checkers to\nsatisfy their specific needs. Nowadays, various code-checking frameworks\nprovide extensive checker customization interfaces to meet this need. However,\nboth the abstract checking logic as well as the complex API usage of\nlarge-scale frameworks make this task challenging. To this end, automated code\nchecker generation is anticipated to ease the burden of checker development. In\nthis paper, we explore the feasibility of automated checker generation and\npropose AutoChecker, an innovative LLM-powered approach that can write code\ncheckers automatically based on only a rule description and a test suite.\nInstead of generating the checker at once, AutoChecker incrementally updates\nthe checker with the rule and one single test case each time, i.e., it\niteratively generates the checker case by case. During each iteration,\nAutoChecker first decomposes the whole logic into a series of sub-operations\nand then uses the logic-guided API-context retrieval strategy to search related\nAPI-contexts from all the framework APIs. To evaluate the effectiveness of\nAutoChecker, we apply AutoChecker and two LLM-based baseline approaches to\nautomatically generate checkers for 20 built-in PMD rules, including easy rules\nand hard rules. Experimental results demonstrate that AutoChecker significantly\noutperforms baseline approaches across all effectiveness metrics, where its\naverage test pass rate improved over 4.2 times. Moreover, the checkers\ngenerated by AutoChecker are successfully applied to real-world projects,\nmatching the performance of official checkers."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Xie"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Jiwei Yan"
                    },
                    {
                        "name": "Jinhao Huang"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06793v1",
                "updated": "2024-11-11T08:42:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    42,
                    41,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T08:42:41Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    42,
                    41,
                    0,
                    316,
                    0
                ],
                "title": "Service Deployment in the On-Demand Economy: Employees, Contractors, or\n  Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Service Deployment in the On-Demand Economy: Employees, Contractors, or\n  Both?"
                },
                "summary": "The recent advancements in mobile/data technology have fostered a widespread\nadoption of on-demand or gig service platforms. The increasingly available data\nand independent contractors have enabled these platforms to design customized\nservices and a cost-efficient workforce to effectively match demand and supply.\nIn practice, a diverse landscape of the workforce has been observed: some rely\nsolely on either employees or contractors, others use a blended workforce with\nboth types of workers. In this paper, we consider a profit-maximizing service\nprovider (SP) that decides to offer a single service or two differentiated\nservices, along with the pricing and staffing of the workforce with employees\nand/or contractors, to price- and waiting-sensitive customers. Contractors\nindependently determine whether or not to participate in the marketplace based\non private reservation rates and per-service wage offered by the SP, while it\ncontrols the number of employees who receive per-hour wage. Under a single\nservice, we show that the SP relies on either employees or contractors and\nidentify sufficient and necessary conditions in which one workforce is better\nthan the other. Under the optimal service deployment, we show that the SP\noffers either a single service relying solely on employees or contractors, or\ntwo differentiated services with a hybrid workforce depending on the service\nvalue and cost efficiencies of employees and contractors. Our analysis suggests\nthat proliferating services with a blended workforce could improve the SP's\nprofit significantly, and identifies conditions in which this value is\nsignificant. Our results provide an in-depth understanding and insightful\nguidance to on-demand platforms on the design of service differentiation and\nworkforce models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements in mobile/data technology have fostered a widespread\nadoption of on-demand or gig service platforms. The increasingly available data\nand independent contractors have enabled these platforms to design customized\nservices and a cost-efficient workforce to effectively match demand and supply.\nIn practice, a diverse landscape of the workforce has been observed: some rely\nsolely on either employees or contractors, others use a blended workforce with\nboth types of workers. In this paper, we consider a profit-maximizing service\nprovider (SP) that decides to offer a single service or two differentiated\nservices, along with the pricing and staffing of the workforce with employees\nand/or contractors, to price- and waiting-sensitive customers. Contractors\nindependently determine whether or not to participate in the marketplace based\non private reservation rates and per-service wage offered by the SP, while it\ncontrols the number of employees who receive per-hour wage. Under a single\nservice, we show that the SP relies on either employees or contractors and\nidentify sufficient and necessary conditions in which one workforce is better\nthan the other. Under the optimal service deployment, we show that the SP\noffers either a single service relying solely on employees or contractors, or\ntwo differentiated services with a hybrid workforce depending on the service\nvalue and cost efficiencies of employees and contractors. Our analysis suggests\nthat proliferating services with a blended workforce could improve the SP's\nprofit significantly, and identifies conditions in which this value is\nsignificant. Our results provide an in-depth understanding and insightful\nguidance to on-demand platforms on the design of service differentiation and\nworkforce models."
                },
                "authors": [
                    {
                        "name": "Lijian Lu"
                    },
                    {
                        "name": "Xin Weng"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "arxiv_comment": "60 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90B22",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06790v1",
                "updated": "2024-11-11T08:36:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    36,
                    49,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T08:36:49Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    36,
                    49,
                    0,
                    316,
                    0
                ],
                "title": "Large-scale moral machine experiment on large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale moral machine experiment on large language models"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) and their potential\nintegration into autonomous driving systems necessitates understanding their\nmoral decision-making capabilities. While our previous study examined four\nprominent LLMs using the Moral Machine experimental framework, the dynamic\nlandscape of LLM development demands a more comprehensive analysis. Here, we\nevaluate moral judgments across 51 different LLMs, including multiple versions\nof proprietary models (GPT, Claude, Gemini) and open-source alternatives\n(Llama, Gemma), to assess their alignment with human moral preferences in\nautonomous driving scenarios. Using a conjoint analysis framework, we evaluated\nhow closely LLM responses aligned with human preferences in ethical dilemmas\nand examined the effects of model size, updates, and architecture. Results\nshowed that proprietary models and open-source models exceeding 10 billion\nparameters demonstrated relatively close alignment with human judgments, with a\nsignificant negative correlation between model size and distance from human\njudgments in open-source models. However, model updates did not consistently\nimprove alignment with human preferences, and many LLMs showed excessive\nemphasis on specific ethical principles. These findings suggest that while\nincreasing model size may naturally lead to more human-like moral judgments,\npractical implementation in autonomous driving systems requires careful\nconsideration of the trade-off between judgment quality and computational\nefficiency. Our comprehensive analysis provides crucial insights for the\nethical design of autonomous systems and highlights the importance of\nconsidering cultural contexts in AI moral decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) and their potential\nintegration into autonomous driving systems necessitates understanding their\nmoral decision-making capabilities. While our previous study examined four\nprominent LLMs using the Moral Machine experimental framework, the dynamic\nlandscape of LLM development demands a more comprehensive analysis. Here, we\nevaluate moral judgments across 51 different LLMs, including multiple versions\nof proprietary models (GPT, Claude, Gemini) and open-source alternatives\n(Llama, Gemma), to assess their alignment with human moral preferences in\nautonomous driving scenarios. Using a conjoint analysis framework, we evaluated\nhow closely LLM responses aligned with human preferences in ethical dilemmas\nand examined the effects of model size, updates, and architecture. Results\nshowed that proprietary models and open-source models exceeding 10 billion\nparameters demonstrated relatively close alignment with human judgments, with a\nsignificant negative correlation between model size and distance from human\njudgments in open-source models. However, model updates did not consistently\nimprove alignment with human preferences, and many LLMs showed excessive\nemphasis on specific ethical principles. These findings suggest that while\nincreasing model size may naturally lead to more human-like moral judgments,\npractical implementation in autonomous driving systems requires careful\nconsideration of the trade-off between judgment quality and computational\nefficiency. Our comprehensive analysis provides crucial insights for the\nethical design of autonomous systems and highlights the importance of\nconsidering cultural contexts in AI moral decision-making."
                },
                "authors": [
                    {
                        "name": "Muhammad Shahrul Zaim bin Ahmad"
                    },
                    {
                        "name": "Kazuhiro Takemoto"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhiro Takemoto"
                },
                "author": "Kazuhiro Takemoto",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06774v1",
                "updated": "2024-11-11T08:05:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    5,
                    37,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T08:05:37Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    5,
                    37,
                    0,
                    316,
                    0
                ],
                "title": "The First Prompt Counts the Most! An Evaluation of Large Language Models\n  on Iterative Example-based Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The First Prompt Counts the Most! An Evaluation of Large Language Models\n  on Iterative Example-based Code Generation"
                },
                "summary": "The capabilities of Large Language Models (LLMs) in code generation,\nparticularly for implementing target functionalities from natural language\ndescriptions, have been extensively studied. As an alternative form of natural\nlanguage, input-output examples (I/O examples) provide an accessible,\nunambiguous, and flexible way to describe functionalities, but the diversity,\nsparseness, and incompleteness of I/O examples also place challenges on\nunderstanding and implementing requirements. Therefore, generating code from\ninput-output examples (i.e., example-based code generation) provides a new\nperspective, allowing us to evaluate LLMs' capability to infer target\nfunctionalities from limited information and to process new-form requirements.\nHowever, related research about LLMs in example-based code generation remains\nlargely unexplored. To fill this gap, this paper presents the first\ncomprehensive study on example-based code generation using LLMs. To address the\nincorrectness caused by the incompleteness of I/O examples, we adopt an\niterative evaluation framework and formalize the objective of example-based\ncode generation as two sequential sub-objectives: generating code conforming to\ngiven examples and generating code that successfully implements the target\nfunctionalities from (iteratively) given examples. We assess six\nstate-of-the-art LLMs using a new benchmark of 168 diverse target\nfunctionalities. The results demonstrate that when requirements were described\nusing iterative I/O examples rather than natural language, the LLMs' score\ndecreased by over 60%, indicating that example-based code generation remains\nchallenging for the evaluated LLMs. More interestingly, the vast majority (even\nover 95%) of successfully implemented functionalities are achieved in the first\nround of iterations, suggesting that the LLMs struggle to effectively utilize\nthe iteratively supplemented requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capabilities of Large Language Models (LLMs) in code generation,\nparticularly for implementing target functionalities from natural language\ndescriptions, have been extensively studied. As an alternative form of natural\nlanguage, input-output examples (I/O examples) provide an accessible,\nunambiguous, and flexible way to describe functionalities, but the diversity,\nsparseness, and incompleteness of I/O examples also place challenges on\nunderstanding and implementing requirements. Therefore, generating code from\ninput-output examples (i.e., example-based code generation) provides a new\nperspective, allowing us to evaluate LLMs' capability to infer target\nfunctionalities from limited information and to process new-form requirements.\nHowever, related research about LLMs in example-based code generation remains\nlargely unexplored. To fill this gap, this paper presents the first\ncomprehensive study on example-based code generation using LLMs. To address the\nincorrectness caused by the incompleteness of I/O examples, we adopt an\niterative evaluation framework and formalize the objective of example-based\ncode generation as two sequential sub-objectives: generating code conforming to\ngiven examples and generating code that successfully implements the target\nfunctionalities from (iteratively) given examples. We assess six\nstate-of-the-art LLMs using a new benchmark of 168 diverse target\nfunctionalities. The results demonstrate that when requirements were described\nusing iterative I/O examples rather than natural language, the LLMs' score\ndecreased by over 60%, indicating that example-based code generation remains\nchallenging for the evaluated LLMs. More interestingly, the vast majority (even\nover 95%) of successfully implemented functionalities are achieved in the first\nround of iterations, suggesting that the LLMs struggle to effectively utilize\nthe iteratively supplemented requirements."
                },
                "authors": [
                    {
                        "name": "Yingjie Fu"
                    },
                    {
                        "name": "Bozhou Li"
                    },
                    {
                        "name": "Linyi Li"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06767v1",
                "updated": "2024-11-11T07:47:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    7,
                    47,
                    20,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T07:47:20Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    7,
                    47,
                    20,
                    0,
                    316,
                    0
                ],
                "title": "PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing"
                },
                "summary": "Code Large Language Models (Code LLMs), such as Code llama and\nDeepSeek-Coder, have demonstrated exceptional performance in the code\ngeneration tasks. However, most existing models focus on the abilities of\ngenerating correct code, but often struggle with bug repair. We introduce a\nsuit of methods to enhance LLM's SQL bug-fixing abilities. The methods are\nmainly consisted of two parts: A Progressive Dataset Construction (PDC) from\nscratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data\nexpansion methods from the perspectives of breadth first and depth first\nrespectively. DM-SFT introduces an efficient bug-fixing supervised learning\napproach, which effectively reduce the total training steps and mitigate the\n\"disorientation\" in SQL code bug-fixing training. In our evaluation, the code\nLLM models trained with two methods have exceeds all current best performing\nmodel which size is much larger.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code Large Language Models (Code LLMs), such as Code llama and\nDeepSeek-Coder, have demonstrated exceptional performance in the code\ngeneration tasks. However, most existing models focus on the abilities of\ngenerating correct code, but often struggle with bug repair. We introduce a\nsuit of methods to enhance LLM's SQL bug-fixing abilities. The methods are\nmainly consisted of two parts: A Progressive Dataset Construction (PDC) from\nscratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data\nexpansion methods from the perspectives of breadth first and depth first\nrespectively. DM-SFT introduces an efficient bug-fixing supervised learning\napproach, which effectively reduce the total training steps and mitigate the\n\"disorientation\" in SQL code bug-fixing training. In our evaluation, the code\nLLM models trained with two methods have exceeds all current best performing\nmodel which size is much larger."
                },
                "authors": [
                    {
                        "name": "Yiwen Duan"
                    },
                    {
                        "name": "Yonghong Yu"
                    },
                    {
                        "name": "Xiaoming Zhao"
                    },
                    {
                        "name": "Yichang Wu"
                    },
                    {
                        "name": "Wenbo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wenbo Liu"
                },
                "author": "Wenbo Liu",
                "arxiv_comment": "COLING-Industry 2025 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16758v2",
                "updated": "2024-11-11T07:34:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    7,
                    34,
                    25,
                    0,
                    316,
                    0
                ],
                "published": "2024-06-24T16:06:50Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    16,
                    6,
                    50,
                    0,
                    176,
                    0
                ],
                "title": "Towards Fast Multilingual LLM Inference: Speculative Decoding and\n  Specialized Drafters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fast Multilingual LLM Inference: Speculative Decoding and\n  Specialized Drafters"
                },
                "summary": "Large language models (LLMs) have revolutionized natural language processing\nand broadened their applicability across diverse commercial applications.\nHowever, the deployment of these models is constrained by high inference time\nin multilingual settings. To mitigate this challenge, this paper explores a\ntraining recipe of an assistant model in speculative decoding, which is\nleveraged to draft and-then its future tokens are verified by the target LLM.\nWe show that language-specific draft models, optimized through a targeted\npretrain-and-finetune strategy, substantially brings a speedup in inference\ntime compared to the previous methods. We validate these models across various\nlanguages in inference time, out-of-domain speedup, and GPT-4o evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized natural language processing\nand broadened their applicability across diverse commercial applications.\nHowever, the deployment of these models is constrained by high inference time\nin multilingual settings. To mitigate this challenge, this paper explores a\ntraining recipe of an assistant model in speculative decoding, which is\nleveraged to draft and-then its future tokens are verified by the target LLM.\nWe show that language-specific draft models, optimized through a targeted\npretrain-and-finetune strategy, substantially brings a speedup in inference\ntime compared to the previous methods. We validate these models across various\nlanguages in inference time, out-of-domain speedup, and GPT-4o evaluation."
                },
                "authors": [
                    {
                        "name": "Euiin Yi"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hongseok Jeung"
                    },
                    {
                        "name": "Du-Seong Chang"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.15997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.15997v2",
                "updated": "2024-11-11T07:27:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    7,
                    27,
                    3,
                    0,
                    316,
                    0
                ],
                "published": "2023-07-29T14:47:07Z",
                "published_parsed": [
                    2023,
                    7,
                    29,
                    14,
                    47,
                    7,
                    5,
                    210,
                    0
                ],
                "title": "RoCar: A Relationship Network-based Evaluation Method for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoCar: A Relationship Network-based Evaluation Method for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have received increasing attention. However, due\nto the complexity of its capabilities, how to rationally evaluate the\ncapabilities of LLMs is still a task to be solved. We propose the RoCar method,\nwhich utilizes the defined basic schemas to randomly construct a task graph and\ngenerates natural language evaluation tasks based on the task graph to evaluate\nthe reasoning and memory abilities of LLMs respectively. Due to the very large\nrandomness of the task construction process, it is possible to ensure that none\nof the LLMs to be tested has directly learned the evaluation tasks,\nguaranteeing the fairness of the evaluation method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have received increasing attention. However, due\nto the complexity of its capabilities, how to rationally evaluate the\ncapabilities of LLMs is still a task to be solved. We propose the RoCar method,\nwhich utilizes the defined basic schemas to randomly construct a task graph and\ngenerates natural language evaluation tasks based on the task graph to evaluate\nthe reasoning and memory abilities of LLMs respectively. Due to the very large\nrandomness of the task construction process, it is possible to ensure that none\nof the LLMs to be tested has directly learned the evaluation tasks,\nguaranteeing the fairness of the evaluation method."
                },
                "authors": [
                    {
                        "name": "Ming Wang"
                    },
                    {
                        "name": "Wenfang Wu"
                    },
                    {
                        "name": "Chongyun Gao"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Yifei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Zhang"
                },
                "author": "Yifei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.15997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.15997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05365v3",
                "updated": "2024-11-11T07:18:34Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    7,
                    18,
                    34,
                    0,
                    316,
                    0
                ],
                "published": "2024-08-09T22:29:23Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    22,
                    29,
                    23,
                    4,
                    222,
                    0
                ],
                "title": "FiSTECH: Financial Style Transfer to Enhance Creativity without\n  Hallucinations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiSTECH: Financial Style Transfer to Enhance Creativity without\n  Hallucinations in LLMs"
                },
                "summary": "Recent trends in Generative AI have emerged towards fine-tuning foundational\nlarge language models (LLMs) to create domain-specific LLMs for automation and\nchatbot-like applications. Specialized applications for analytics-heavy domains\nsuch as Financial report generation require specific writing styles that\ncomprise compound and creative sentences with minimized hallucinations. In this\nwork, we explore the self-corrective auto-regressive qualities of LLMs to learn\ncreativity in writing styles with minimal prompting. We propose a novel\ntwo-stage fine-tuning (FT) strategy wherein in the first stage public domain\nfinancial reports are used to train for writing styles while allowing the LLM\nto hallucinate. In the second stage the examples of hallucinations are manually\ncorrected and further used to fine-tune the LLM. The finally trained LLM learns\nto generate specific financial report sections using minimal instructions and\ntabular data inputs while ensuring low fine-tuning costs. Our proposed\ntwo-stage fine-tuning boosts the accuracy of financial questions answering by\ntwo-folds while reducing hallucinations by over 50%. Also, the fine-tuned model\nhas lower perplexity, improved ROUGE, TER and BLEU scores, higher creativity\nand knowledge density with lower uncertainty and cross entropy than base LLMs.\nThus, the proposed framework can be generalized to train creativity in LLMs by\nfirst allowing them to hallucinate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent trends in Generative AI have emerged towards fine-tuning foundational\nlarge language models (LLMs) to create domain-specific LLMs for automation and\nchatbot-like applications. Specialized applications for analytics-heavy domains\nsuch as Financial report generation require specific writing styles that\ncomprise compound and creative sentences with minimized hallucinations. In this\nwork, we explore the self-corrective auto-regressive qualities of LLMs to learn\ncreativity in writing styles with minimal prompting. We propose a novel\ntwo-stage fine-tuning (FT) strategy wherein in the first stage public domain\nfinancial reports are used to train for writing styles while allowing the LLM\nto hallucinate. In the second stage the examples of hallucinations are manually\ncorrected and further used to fine-tune the LLM. The finally trained LLM learns\nto generate specific financial report sections using minimal instructions and\ntabular data inputs while ensuring low fine-tuning costs. Our proposed\ntwo-stage fine-tuning boosts the accuracy of financial questions answering by\ntwo-folds while reducing hallucinations by over 50%. Also, the fine-tuned model\nhas lower perplexity, improved ROUGE, TER and BLEU scores, higher creativity\nand knowledge density with lower uncertainty and cross entropy than base LLMs.\nThus, the proposed framework can be generalized to train creativity in LLMs by\nfirst allowing them to hallucinate."
                },
                "authors": [
                    {
                        "name": "Sohini Roychowdhury"
                    },
                    {
                        "name": "Marko Krema"
                    },
                    {
                        "name": "Brian Moore"
                    },
                    {
                        "name": "Xingjian Lai"
                    },
                    {
                        "name": "Dike Effedua"
                    },
                    {
                        "name": "Bharat Jethwani"
                    }
                ],
                "author_detail": {
                    "name": "Bharat Jethwani"
                },
                "author": "Bharat Jethwani",
                "arxiv_comment": "10 pages, 14 figures, 5 tables, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14038v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14038v4",
                "updated": "2024-11-11T06:26:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    6,
                    26,
                    39,
                    0,
                    316,
                    0
                ],
                "published": "2024-09-21T06:49:34Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    6,
                    49,
                    34,
                    5,
                    265,
                    0
                ],
                "title": "OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model\n  Hallucinations in Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model\n  Hallucinations in Ontology Matching"
                },
                "summary": "Hallucinations of large language models (LLMs) commonly occur in\ndomain-specific downstream tasks, with no exception in ontology matching (OM).\nThe prevalence of using LLMs for OM raises the need for benchmarks to better\nunderstand LLM hallucinations. The OAEI-LLM dataset is an extended version of\nthe Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate\nLLM-specific hallucinations in OM tasks. We outline the methodology used in\ndataset construction and schema extension, and provide examples of potential\nuse cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations of large language models (LLMs) commonly occur in\ndomain-specific downstream tasks, with no exception in ontology matching (OM).\nThe prevalence of using LLMs for OM raises the need for benchmarks to better\nunderstand LLM hallucinations. The OAEI-LLM dataset is an extended version of\nthe Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate\nLLM-specific hallucinations in OM tasks. We outline the methodology used in\ndataset construction and schema extension, and provide examples of potential\nuse cases."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Kerry Taylor"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Jing Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Jiang"
                },
                "author": "Jing Jiang",
                "arxiv_comment": "5 pages, 1 figure, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14038v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14038v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09874v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09874v4",
                "updated": "2024-11-11T06:16:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    6,
                    16,
                    24,
                    0,
                    316,
                    0
                ],
                "published": "2023-10-15T16:15:07Z",
                "published_parsed": [
                    2023,
                    10,
                    15,
                    16,
                    15,
                    7,
                    6,
                    288,
                    0
                ],
                "title": "TF-DCon: Leveraging Large Language Models (LLMs) to Empower\n  Training-Free Dataset Condensation for Content-Based Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TF-DCon: Leveraging Large Language Models (LLMs) to Empower\n  Training-Free Dataset Condensation for Content-Based Recommendation"
                },
                "summary": "Modern techniques in Content-based Recommendation (CBR) leverage item content\ninformation to provide personalized services to users, but suffer from\nresource-intensive training on large datasets. To address this issue, we\nexplore the dataset condensation for textual CBR in this paper. The goal of\ndataset condensation is to synthesize a small yet informative dataset, upon\nwhich models can achieve performance comparable to those trained on large\ndatasets. While existing condensation approaches are tailored to classification\ntasks for continuous data like images or embeddings, direct application of them\nto CBR has limitations. To bridge this gap, we investigate efficient dataset\ncondensation for content-based recommendation. Inspired by the remarkable\nabilities of large language models (LLMs) in text comprehension and generation,\nwe leverage LLMs to empower the generation of textual content during\ncondensation. To handle the interaction data involving both users and items, we\ndevise a dual-level condensation method: content-level and user-level. At\ncontent-level, we utilize LLMs to condense all contents of an item into a new\ninformative title. At user-level, we design a clustering-based synthesis\nmodule, where we first utilize LLMs to extract user interests. Then, the user\ninterests and user embeddings are incorporated to condense users and generate\ninteractions for condensed users. Notably, the condensation paradigm of this\nmethod is forward and free from iterative optimization on the synthesized\ndataset. Extensive empirical findings from our study, conducted on three\nauthentic datasets, substantiate the efficacy of the proposed method.\nParticularly, we are able to approximate up to 97% of the original performance\nwhile reducing the dataset size by 95% (i.e., on dataset MIND).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern techniques in Content-based Recommendation (CBR) leverage item content\ninformation to provide personalized services to users, but suffer from\nresource-intensive training on large datasets. To address this issue, we\nexplore the dataset condensation for textual CBR in this paper. The goal of\ndataset condensation is to synthesize a small yet informative dataset, upon\nwhich models can achieve performance comparable to those trained on large\ndatasets. While existing condensation approaches are tailored to classification\ntasks for continuous data like images or embeddings, direct application of them\nto CBR has limitations. To bridge this gap, we investigate efficient dataset\ncondensation for content-based recommendation. Inspired by the remarkable\nabilities of large language models (LLMs) in text comprehension and generation,\nwe leverage LLMs to empower the generation of textual content during\ncondensation. To handle the interaction data involving both users and items, we\ndevise a dual-level condensation method: content-level and user-level. At\ncontent-level, we utilize LLMs to condense all contents of an item into a new\ninformative title. At user-level, we design a clustering-based synthesis\nmodule, where we first utilize LLMs to extract user interests. Then, the user\ninterests and user embeddings are incorporated to condense users and generate\ninteractions for condensed users. Notably, the condensation paradigm of this\nmethod is forward and free from iterative optimization on the synthesized\ndataset. Extensive empirical findings from our study, conducted on three\nauthentic datasets, substantiate the efficacy of the proposed method.\nParticularly, we are able to approximate up to 97% of the original performance\nwhile reducing the dataset size by 95% (i.e., on dataset MIND)."
                },
                "authors": [
                    {
                        "name": "Jiahao Wu"
                    },
                    {
                        "name": "Qijiong Liu"
                    },
                    {
                        "name": "Hengchang Hu"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Shengcai Liu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Xiao-Ming Wu"
                    },
                    {
                        "name": "Ke Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ke Tang"
                },
                "author": "Ke Tang",
                "arxiv_comment": "An updated version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09874v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09874v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06736v1",
                "updated": "2024-11-11T06:04:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    6,
                    4,
                    53,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T06:04:53Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    6,
                    4,
                    53,
                    0,
                    316,
                    0
                ],
                "title": "Mr.Steve: Instruction-Following Agents in Minecraft with What-Where-When\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mr.Steve: Instruction-Following Agents in Minecraft with What-Where-When\n  Memory"
                },
                "summary": "Significant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented\nhierarchical approaches. While these approaches, which combine high-level\nplanners with low-level controllers, show promise, low-level controllers\nfrequently become performance bottlenecks due to repeated failures. In this\npaper, we argue that the primary cause of failure in many low-level controllers\nis the absence of an episodic memory system. To address this, we introduce\nMr.Steve (Memory Recall Steve-1), a novel low-level controller equipped with\nPlace Event Memory (PEM), a form of episodic memory that captures what, where,\nand when information from episodes. This directly addresses the main limitation\nof the popular low-level controller, Steve-1. Unlike previous models that rely\non short-term memory, PEM organizes spatial and event-based data, enabling\nefficient recall and navigation in long-horizon tasks. Additionally, we propose\nan Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing\nagents to alternate between exploration and task-solving based on recalled\nevents. Our approach significantly improves task-solving and exploration\nefficiency compared to existing methods. We will release our code and demos on\nthe project page: https://sites.google.com/view/mr-steve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented\nhierarchical approaches. While these approaches, which combine high-level\nplanners with low-level controllers, show promise, low-level controllers\nfrequently become performance bottlenecks due to repeated failures. In this\npaper, we argue that the primary cause of failure in many low-level controllers\nis the absence of an episodic memory system. To address this, we introduce\nMr.Steve (Memory Recall Steve-1), a novel low-level controller equipped with\nPlace Event Memory (PEM), a form of episodic memory that captures what, where,\nand when information from episodes. This directly addresses the main limitation\nof the popular low-level controller, Steve-1. Unlike previous models that rely\non short-term memory, PEM organizes spatial and event-based data, enabling\nefficient recall and navigation in long-horizon tasks. Additionally, we propose\nan Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing\nagents to alternate between exploration and task-solving based on recalled\nevents. Our approach significantly improves task-solving and exploration\nefficiency compared to existing methods. We will release our code and demos on\nthe project page: https://sites.google.com/view/mr-steve."
                },
                "authors": [
                    {
                        "name": "Junyeong Park"
                    },
                    {
                        "name": "Junmo Cho"
                    },
                    {
                        "name": "Sungjin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Ahn"
                },
                "author": "Sungjin Ahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06735v1",
                "updated": "2024-11-11T06:04:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    6,
                    4,
                    15,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T06:04:15Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    6,
                    4,
                    15,
                    0,
                    316,
                    0
                ],
                "title": "Multi-Modal Forecaster: Jointly Predicting Time Series and Textual Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Forecaster: Jointly Predicting Time Series and Textual Data"
                },
                "summary": "Current forecasting approaches are largely unimodal and ignore the rich\ntextual data that often accompany the time series due to lack of well-curated\nmultimodal benchmark dataset. In this work, we develop TimeText Corpus (TTC), a\ncarefully curated, time-aligned text and time dataset for multimodal\nforecasting. Our dataset is composed of sequences of numbers and text aligned\nto timestamps, and includes data from two different domains: climate science\nand healthcare. Our data is a significant contribution to the rare selection of\navailable multimodal datasets. We also propose the Hybrid Multi-Modal\nForecaster (Hybrid-MMF), a multimodal LLM that jointly forecasts both text and\ntime series data using shared embeddings. However, contrary to our\nexpectations, our Hybrid-MMF model does not outperform existing baselines in\nour experiments. This negative result highlights the challenges inherent in\nmultimodal forecasting. Our code and data are available at\nhttps://github.com/Rose-STL-Lab/Multimodal_ Forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current forecasting approaches are largely unimodal and ignore the rich\ntextual data that often accompany the time series due to lack of well-curated\nmultimodal benchmark dataset. In this work, we develop TimeText Corpus (TTC), a\ncarefully curated, time-aligned text and time dataset for multimodal\nforecasting. Our dataset is composed of sequences of numbers and text aligned\nto timestamps, and includes data from two different domains: climate science\nand healthcare. Our data is a significant contribution to the rare selection of\navailable multimodal datasets. We also propose the Hybrid Multi-Modal\nForecaster (Hybrid-MMF), a multimodal LLM that jointly forecasts both text and\ntime series data using shared embeddings. However, contrary to our\nexpectations, our Hybrid-MMF model does not outperform existing baselines in\nour experiments. This negative result highlights the challenges inherent in\nmultimodal forecasting. Our code and data are available at\nhttps://github.com/Rose-STL-Lab/Multimodal_ Forecasting."
                },
                "authors": [
                    {
                        "name": "Kai Kim"
                    },
                    {
                        "name": "Howard Tsai"
                    },
                    {
                        "name": "Rajat Sen"
                    },
                    {
                        "name": "Abhimanyu Das"
                    },
                    {
                        "name": "Zihao Zhou"
                    },
                    {
                        "name": "Abhishek Tanpure"
                    },
                    {
                        "name": "Mathew Luo"
                    },
                    {
                        "name": "Rose Yu"
                    }
                ],
                "author_detail": {
                    "name": "Rose Yu"
                },
                "author": "Rose Yu",
                "arxiv_comment": "21 pages, 4 tables, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12641v2",
                "updated": "2024-11-11T05:48:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    5,
                    48,
                    35,
                    0,
                    316,
                    0
                ],
                "published": "2024-06-18T14:08:01Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    14,
                    8,
                    1,
                    1,
                    170,
                    0
                ],
                "title": "DetectBench: Can Large Language Model Detect and Piece Together Implicit\n  Evidence?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DetectBench: Can Large Language Model Detect and Piece Together Implicit\n  Evidence?"
                },
                "summary": "Detecting evidence within the context is a key step in the process of\nreasoning task. Evaluating and enhancing the capabilities of LLMs in evidence\ndetection will strengthen context-based reasoning performance. This paper\nproposes a benchmark called DetectBench for verifying the ability to detect and\npiece together implicit evidence within a long context. DetectBench contains\n3,928 multiple-choice questions, with an average of 994 tokens per question.\nEach question contains an average of 4.55 pieces of implicit evidence, and\nsolving the problem typically requires 7.62 logical jumps to find the correct\nanswer. To enhance the performance of LLMs in evidence detection, this paper\nproposes Detective Reasoning Prompt and Finetune. Experiments demonstrate that\nthe existing LLMs' abilities to detect evidence in long contexts are far\ninferior to humans. However, the Detective Reasoning Prompt effectively\nenhances the capability of powerful LLMs in evidence detection, while the\nFinetuning method shows significant effects in enhancing the performance of\nweaker LLMs. Moreover, when the abilities of LLMs in evidence detection are\nimproved, their final reasoning performance is also enhanced accordingly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting evidence within the context is a key step in the process of\nreasoning task. Evaluating and enhancing the capabilities of LLMs in evidence\ndetection will strengthen context-based reasoning performance. This paper\nproposes a benchmark called DetectBench for verifying the ability to detect and\npiece together implicit evidence within a long context. DetectBench contains\n3,928 multiple-choice questions, with an average of 994 tokens per question.\nEach question contains an average of 4.55 pieces of implicit evidence, and\nsolving the problem typically requires 7.62 logical jumps to find the correct\nanswer. To enhance the performance of LLMs in evidence detection, this paper\nproposes Detective Reasoning Prompt and Finetune. Experiments demonstrate that\nthe existing LLMs' abilities to detect evidence in long contexts are far\ninferior to humans. However, the Detective Reasoning Prompt effectively\nenhances the capability of powerful LLMs in evidence detection, while the\nFinetuning method shows significant effects in enhancing the performance of\nweaker LLMs. Moreover, when the abilities of LLMs in evidence detection are\nimproved, their final reasoning performance is also enhanced accordingly."
                },
                "authors": [
                    {
                        "name": "Zhouhong Gu"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Xiaoxuan Zhu"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Shusen Wang"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Hongwei Feng"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06723v1",
                "updated": "2024-11-11T05:14:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    5,
                    14,
                    14,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T05:14:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    5,
                    14,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "Script-Strategy Aligned Generation: Aligning LLMs with Expert-Crafted\n  Dialogue Scripts and Therapeutic Strategies for Psychotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Script-Strategy Aligned Generation: Aligning LLMs with Expert-Crafted\n  Dialogue Scripts and Therapeutic Strategies for Psychotherapy"
                },
                "summary": "Chatbots or conversational agents (CAs) are increasingly used to improve\naccess to digital psychotherapy. Many current systems rely on rigid, rule-based\ndesigns, heavily dependent on expert-crafted dialogue scripts for guiding\ntherapeutic conversations. Although recent advances in large language models\n(LLMs) offer the potential for more flexible interactions, their lack of\ncontrollability and transparency poses significant challenges in sensitive\nareas like psychotherapy. In this work, we explored how aligning LLMs with\nexpert-crafted scripts can enhance psychotherapeutic chatbot performance. Our\ncomparative study showed that LLMs aligned with expert-crafted scripts through\nprompting and fine-tuning significantly outperformed both pure LLMs and\nrule-based chatbots, achieving a more effective balance between dialogue\nflexibility and adherence to therapeutic principles. Building on findings, we\nproposed ``Script-Strategy Aligned Generation (SSAG)'', a flexible alignment\napproach that reduces reliance on fully scripted content while enhancing LLMs'\ntherapeutic adherence and controllability. In a 10-day field study, SSAG\ndemonstrated performance comparable to full script alignment and outperformed\nrule-based chatbots, empirically supporting SSAG as an efficient approach for\naligning LLMs with domain expertise. Our work advances LLM applications in\npsychotherapy by providing a controllable, adaptable, and scalable solution for\ndigital interventions, reducing reliance on expert effort. It also provides a\ncollaborative framework for domain experts and developers to efficiently build\nexpertise-aligned chatbots, broadening access to psychotherapy and behavioral\ninterventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbots or conversational agents (CAs) are increasingly used to improve\naccess to digital psychotherapy. Many current systems rely on rigid, rule-based\ndesigns, heavily dependent on expert-crafted dialogue scripts for guiding\ntherapeutic conversations. Although recent advances in large language models\n(LLMs) offer the potential for more flexible interactions, their lack of\ncontrollability and transparency poses significant challenges in sensitive\nareas like psychotherapy. In this work, we explored how aligning LLMs with\nexpert-crafted scripts can enhance psychotherapeutic chatbot performance. Our\ncomparative study showed that LLMs aligned with expert-crafted scripts through\nprompting and fine-tuning significantly outperformed both pure LLMs and\nrule-based chatbots, achieving a more effective balance between dialogue\nflexibility and adherence to therapeutic principles. Building on findings, we\nproposed ``Script-Strategy Aligned Generation (SSAG)'', a flexible alignment\napproach that reduces reliance on fully scripted content while enhancing LLMs'\ntherapeutic adherence and controllability. In a 10-day field study, SSAG\ndemonstrated performance comparable to full script alignment and outperformed\nrule-based chatbots, empirically supporting SSAG as an efficient approach for\naligning LLMs with domain expertise. Our work advances LLM applications in\npsychotherapy by providing a controllable, adaptable, and scalable solution for\ndigital interventions, reducing reliance on expert effort. It also provides a\ncollaborative framework for domain experts and developers to efficiently build\nexpertise-aligned chatbots, broadening access to psychotherapy and behavioral\ninterventions."
                },
                "authors": [
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Jan de Wit"
                    },
                    {
                        "name": "Zhuying Li"
                    },
                    {
                        "name": "Jiahuan Pei"
                    },
                    {
                        "name": "Abdallah El Ali"
                    },
                    {
                        "name": "Jos A. Bosch"
                    }
                ],
                "author_detail": {
                    "name": "Jos A. Bosch"
                },
                "author": "Jos A. Bosch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06713v1",
                "updated": "2024-11-11T04:45:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    4,
                    45,
                    48,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T04:45:48Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    4,
                    45,
                    48,
                    0,
                    316,
                    0
                ],
                "title": "Ambient AI Scribing Support: Comparing the Performance of Specialized AI\n  Agentic Architecture to Leading Foundational Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambient AI Scribing Support: Comparing the Performance of Specialized AI\n  Agentic Architecture to Leading Foundational Models"
                },
                "summary": "This study compares Sporo Health's AI Scribe, a proprietary model fine-tuned\nfor medical scribing, with various LLMs (GPT-4o, GPT-3.5, Gemma-9B, and\nLlama-3.2-3B) in clinical documentation. We analyzed de-identified patient\ntranscripts from partner clinics, using clinician-provided SOAP notes as the\nground truth. Each model generated SOAP summaries using zero-shot prompting,\nwith performance assessed via recall, precision, and F1 scores. Sporo\noutperformed all models, achieving the highest recall (73.3%), precision\n(78.6%), and F1 score (75.3%) with the lowest performance variance.\nStatistically significant differences (p < 0.05) were found between Sporo and\nthe other models, with post-hoc tests showing significant improvements over\nGPT-3.5, Gemma-9B, and Llama 3.2-3B. While Sporo outperformed GPT-4o by up to\n10%, the difference was not statistically significant (p = 0.25). Clinical user\nsatisfaction, measured with a modified PDQI-9 inventory, favored Sporo.\nEvaluations indicated Sporo's outputs were more accurate and relevant. This\nhighlights the potential of Sporo's multi-agentic architecture to improve\nclinical workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study compares Sporo Health's AI Scribe, a proprietary model fine-tuned\nfor medical scribing, with various LLMs (GPT-4o, GPT-3.5, Gemma-9B, and\nLlama-3.2-3B) in clinical documentation. We analyzed de-identified patient\ntranscripts from partner clinics, using clinician-provided SOAP notes as the\nground truth. Each model generated SOAP summaries using zero-shot prompting,\nwith performance assessed via recall, precision, and F1 scores. Sporo\noutperformed all models, achieving the highest recall (73.3%), precision\n(78.6%), and F1 score (75.3%) with the lowest performance variance.\nStatistically significant differences (p < 0.05) were found between Sporo and\nthe other models, with post-hoc tests showing significant improvements over\nGPT-3.5, Gemma-9B, and Llama 3.2-3B. While Sporo outperformed GPT-4o by up to\n10%, the difference was not statistically significant (p = 0.25). Clinical user\nsatisfaction, measured with a modified PDQI-9 inventory, favored Sporo.\nEvaluations indicated Sporo's outputs were more accurate and relevant. This\nhighlights the potential of Sporo's multi-agentic architecture to improve\nclinical workflows."
                },
                "authors": [
                    {
                        "name": "Chanseo Lee"
                    },
                    {
                        "name": "Sonu Kumar"
                    },
                    {
                        "name": "Kimon A. Vogt"
                    },
                    {
                        "name": "Sam Meraj"
                    }
                ],
                "author_detail": {
                    "name": "Sam Meraj"
                },
                "author": "Sam Meraj",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2410.15528",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14202v2",
                "updated": "2024-11-11T04:41:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    4,
                    41,
                    32,
                    0,
                    316,
                    0
                ],
                "published": "2024-09-21T17:19:29Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    17,
                    19,
                    29,
                    5,
                    265,
                    0
                ],
                "title": "Mining Causality: AI-Assisted Search for Instrumental Variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mining Causality: AI-Assisted Search for Instrumental Variables"
                },
                "summary": "The instrumental variables (IVs) method is a leading empirical strategy for\ncausal inference. Finding IVs is a heuristic and creative process, and\njustifying its validity--especially exclusion restrictions--is largely\nrhetorical. We propose using large language models (LLMs) to search for new IVs\nthrough narratives and counterfactual reasoning, similar to how a human\nresearcher would. The stark difference, however, is that LLMs can dramatically\naccelerate this process and explore an extremely large search space. We\ndemonstrate how to construct prompts to search for potentially valid IVs. We\ncontend that multi-step and role-playing prompting strategies are effective for\nsimulating the endogenous decision-making processes of economic agents and for\nnavigating language models through the realm of real-world scenarios. We apply\nour method to three well-known examples in economics: returns to schooling,\nsupply and demand, and peer effects. We then extend our strategy to finding (i)\ncontrol variables in regression and difference-in-differences and (ii) running\nvariables in regression discontinuity designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The instrumental variables (IVs) method is a leading empirical strategy for\ncausal inference. Finding IVs is a heuristic and creative process, and\njustifying its validity--especially exclusion restrictions--is largely\nrhetorical. We propose using large language models (LLMs) to search for new IVs\nthrough narratives and counterfactual reasoning, similar to how a human\nresearcher would. The stark difference, however, is that LLMs can dramatically\naccelerate this process and explore an extremely large search space. We\ndemonstrate how to construct prompts to search for potentially valid IVs. We\ncontend that multi-step and role-playing prompting strategies are effective for\nsimulating the endogenous decision-making processes of economic agents and for\nnavigating language models through the realm of real-world scenarios. We apply\nour method to three well-known examples in economics: returns to schooling,\nsupply and demand, and peer effects. We then extend our strategy to finding (i)\ncontrol variables in regression and difference-in-differences and (ii) running\nvariables in regression discontinuity designs."
                },
                "authors": [
                    {
                        "name": "Sukjin Han"
                    }
                ],
                "author_detail": {
                    "name": "Sukjin Han"
                },
                "author": "Sukjin Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24190v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24190v3",
                "updated": "2024-11-11T04:35:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    4,
                    35,
                    11,
                    0,
                    316,
                    0
                ],
                "published": "2024-10-31T17:51:00Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    51,
                    0,
                    3,
                    305,
                    0
                ],
                "title": "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters"
                },
                "summary": "How could LLMs influence our democracy? We investigate LLMs' political\nleanings and the potential influence of LLMs on voters by conducting multiple\nexperiments in a U.S. presidential election context. Through a voting\nsimulation, we first demonstrate 18 open- and closed-weight LLMs' political\npreference for a Democratic nominee over a Republican nominee. We show how this\nleaning towards the Democratic nominee becomes more pronounced in\ninstruction-tuned models compared to their base versions by analyzing their\nresponses to candidate-policy related questions. We further explore the\npotential impact of LLMs on voter choice by conducting an experiment with 935\nU.S. registered voters. During the experiments, participants interacted with\nLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results\nshow a shift in voter choices towards the Democratic nominee following LLM\ninteraction, widening the voting margin from 0.7% to 4.6%, even though LLMs\nwere not asked to persuade users to support the Democratic nominee during the\ndiscourse. This effect is larger than many previous studies on the\npersuasiveness of political campaigns, which have shown minimal effects in\npresidential elections. Many users also expressed a desire for further\npolitical interaction with LLMs. Which aspects of LLM interactions drove these\nshifts in voter choice requires further study. Lastly, we explore how a safety\nmethod can make LLMs more politically neutral, while raising the question of\nwhether such neutrality is truly the path forward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How could LLMs influence our democracy? We investigate LLMs' political\nleanings and the potential influence of LLMs on voters by conducting multiple\nexperiments in a U.S. presidential election context. Through a voting\nsimulation, we first demonstrate 18 open- and closed-weight LLMs' political\npreference for a Democratic nominee over a Republican nominee. We show how this\nleaning towards the Democratic nominee becomes more pronounced in\ninstruction-tuned models compared to their base versions by analyzing their\nresponses to candidate-policy related questions. We further explore the\npotential impact of LLMs on voter choice by conducting an experiment with 935\nU.S. registered voters. During the experiments, participants interacted with\nLLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. The experiment results\nshow a shift in voter choices towards the Democratic nominee following LLM\ninteraction, widening the voting margin from 0.7% to 4.6%, even though LLMs\nwere not asked to persuade users to support the Democratic nominee during the\ndiscourse. This effect is larger than many previous studies on the\npersuasiveness of political campaigns, which have shown minimal effects in\npresidential elections. Many users also expressed a desire for further\npolitical interaction with LLMs. Which aspects of LLM interactions drove these\nshifts in voter choice requires further study. Lastly, we explore how a safety\nmethod can make LLMs more politically neutral, while raising the question of\nwhether such neutrality is truly the path forward."
                },
                "authors": [
                    {
                        "name": "Yujin Potter"
                    },
                    {
                        "name": "Shiyang Lai"
                    },
                    {
                        "name": "Junsol Kim"
                    },
                    {
                        "name": "James Evans"
                    },
                    {
                        "name": "Dawn Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawn Song"
                },
                "author": "Dawn Song",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24190v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24190v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12843v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12843v4",
                "updated": "2024-11-11T04:17:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    4,
                    17,
                    30,
                    0,
                    316,
                    0
                ],
                "published": "2024-07-04T15:10:51Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    15,
                    10,
                    51,
                    3,
                    186,
                    0
                ],
                "title": "NutriBench: A Dataset for Evaluating Large Language Models on Nutrition\n  Estimation from Meal Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NutriBench: A Dataset for Evaluating Large Language Models on Nutrition\n  Estimation from Meal Descriptions"
                },
                "summary": "Accurate nutrition estimation helps people make informed dietary choices and\nis essential in the prevention of serious health complications. We present\nNutriBench, the first publicly available natural language meal description\nnutrition benchmark. NutriBench consists of 11,857 meal descriptions generated\nfrom real-world global dietary intake data. The data is human-verified and\nannotated with macro-nutrient labels, including carbohydrates, proteins, fats,\nand calories. We conduct an extensive evaluation of NutriBench on the task of\ncarbohydrate estimation, testing twelve leading Large Language Models (LLMs),\nincluding GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using\nstandard, Chain-of-Thought and Retrieval-Augmented Generation strategies.\nAdditionally, we present a study involving professional nutritionists, finding\nthat LLMs can provide more accurate and faster estimates. Finally, we perform a\nreal-world risk assessment by simulating the effect of carbohydrate predictions\non the blood glucose levels of individuals with diabetes. Our work highlights\nthe opportunities and challenges of using LLMs for nutrition estimation,\ndemonstrating their potential to aid professionals and laypersons and improve\nhealth outcomes. Our benchmark is publicly available at:\nhttps://mehak126.github.io/nutribench.html",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate nutrition estimation helps people make informed dietary choices and\nis essential in the prevention of serious health complications. We present\nNutriBench, the first publicly available natural language meal description\nnutrition benchmark. NutriBench consists of 11,857 meal descriptions generated\nfrom real-world global dietary intake data. The data is human-verified and\nannotated with macro-nutrient labels, including carbohydrates, proteins, fats,\nand calories. We conduct an extensive evaluation of NutriBench on the task of\ncarbohydrate estimation, testing twelve leading Large Language Models (LLMs),\nincluding GPT-4o, Llama3.1, Qwen2, Gemma2, and OpenBioLLM models, using\nstandard, Chain-of-Thought and Retrieval-Augmented Generation strategies.\nAdditionally, we present a study involving professional nutritionists, finding\nthat LLMs can provide more accurate and faster estimates. Finally, we perform a\nreal-world risk assessment by simulating the effect of carbohydrate predictions\non the blood glucose levels of individuals with diabetes. Our work highlights\nthe opportunities and challenges of using LLMs for nutrition estimation,\ndemonstrating their potential to aid professionals and laypersons and improve\nhealth outcomes. Our benchmark is publicly available at:\nhttps://mehak126.github.io/nutribench.html"
                },
                "authors": [
                    {
                        "name": "Andong Hua"
                    },
                    {
                        "name": "Mehak Preet Dhaliwal"
                    },
                    {
                        "name": "Ryan Burke"
                    },
                    {
                        "name": "Laya Pullela"
                    },
                    {
                        "name": "Yao Qin"
                    }
                ],
                "author_detail": {
                    "name": "Yao Qin"
                },
                "author": "Yao Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12843v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12843v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03494v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03494v3",
                "updated": "2024-11-11T04:03:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    4,
                    3,
                    28,
                    0,
                    316,
                    0
                ],
                "published": "2024-02-05T20:11:56Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    20,
                    11,
                    56,
                    0,
                    36,
                    0
                ],
                "title": "Beyond Text: Utilizing Vocal Cues to Improve Decision Making in LLMs for\n  Robot Navigation Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Text: Utilizing Vocal Cues to Improve Decision Making in LLMs for\n  Robot Navigation Tasks"
                },
                "summary": "While LLMs excel in processing text in these human conversations, they\nstruggle with the nuances of verbal instructions in scenarios like social\nnavigation, where ambiguity and uncertainty can erode trust in robotic and\nother AI systems. We can address this shortcoming by moving beyond text and\nadditionally focusing on the paralinguistic features of these audio responses.\nThese features are the aspects of spoken communication that do not involve the\nliteral wording (lexical content) but convey meaning and nuance through how\nsomething is said. We present Beyond Text: an approach that improves LLM\ndecision-making by integrating audio transcription along with a subsection of\nthese features, which focus on the affect and more relevant in human-robot\nconversations.This approach not only achieves a 70.26% winning rate,\noutperforming existing LLMs by 22.16% to 48.30% (gemini-1.5-pro and gpt-3.5\nrespectively), but also enhances robustness against token manipulation\nadversarial attacks, highlighted by a 22.44% less decrease ratio than the\ntext-only language model in winning rate. Beyond Text' marks an advancement in\nsocial robot navigation and broader Human-Robot interactions, seamlessly\nintegrating text-based guidance with human-audio-informed language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs excel in processing text in these human conversations, they\nstruggle with the nuances of verbal instructions in scenarios like social\nnavigation, where ambiguity and uncertainty can erode trust in robotic and\nother AI systems. We can address this shortcoming by moving beyond text and\nadditionally focusing on the paralinguistic features of these audio responses.\nThese features are the aspects of spoken communication that do not involve the\nliteral wording (lexical content) but convey meaning and nuance through how\nsomething is said. We present Beyond Text: an approach that improves LLM\ndecision-making by integrating audio transcription along with a subsection of\nthese features, which focus on the affect and more relevant in human-robot\nconversations.This approach not only achieves a 70.26% winning rate,\noutperforming existing LLMs by 22.16% to 48.30% (gemini-1.5-pro and gpt-3.5\nrespectively), but also enhances robustness against token manipulation\nadversarial attacks, highlighted by a 22.44% less decrease ratio than the\ntext-only language model in winning rate. Beyond Text' marks an advancement in\nsocial robot navigation and broader Human-Robot interactions, seamlessly\nintegrating text-based guidance with human-audio-informed language models."
                },
                "authors": [
                    {
                        "name": "Xingpeng Sun"
                    },
                    {
                        "name": "Haoming Meng"
                    },
                    {
                        "name": "Souradip Chakraborty"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "Aniket Bera"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Bera"
                },
                "author": "Aniket Bera",
                "arxiv_comment": "30 pages, 7 figures",
                "arxiv_journal_ref": "Transactions on Machine Learning Research 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03494v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03494v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06691v1",
                "updated": "2024-11-11T03:20:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    3,
                    20,
                    53,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T03:20:53Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    3,
                    20,
                    53,
                    0,
                    316,
                    0
                ],
                "title": "Autonomous Droplet Microfluidic Design Framework with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Droplet Microfluidic Design Framework with Large Language\n  Models"
                },
                "summary": "Droplet-based microfluidic devices have substantial promise as cost-effective\nalternatives to current assessment tools in biological research. Moreover,\nmachine learning models that leverage tabular data, including input design\nparameters and their corresponding efficiency outputs, are increasingly\nutilised to automate the design process of these devices and to predict their\nperformance. However, these models fail to fully leverage the data presented in\nthe tables, neglecting crucial contextual information, including column\nheadings and their associated descriptions. This study presents\nMicroFluidic-LLMs, a framework designed for processing and feature extraction,\nwhich effectively captures contextual information from tabular data formats.\nMicroFluidic-LLMs overcomes processing challenges by transforming the content\ninto a linguistic format and leveraging pre-trained large language models\n(LLMs) for analysis. We evaluate our MicroFluidic-LLMs framework on 11\nprediction tasks, covering aspects such as geometry, flow conditions, regimes,\nand performance, utilising a publicly available dataset on flow-focusing\ndroplet microfluidics. We demonstrate that our MicroFluidic-LLMs framework can\nempower deep neural network models to be highly effective and straightforward\nwhile minimising the need for extensive data preprocessing. Moreover, the\nexceptional performance of deep neural network models, particularly when\ncombined with advanced natural language processing models such as DistilBERT\nand GPT-2, reduces the mean absolute error in the droplet diameter and\ngeneration rate by nearly 5- and 7-fold, respectively, and enhances the regime\nclassification accuracy by over 4%, compared with the performance reported in a\nprevious study. This study lays the foundation for the huge potential\napplications of LLMs and machine learning in a wider spectrum of microfluidic\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Droplet-based microfluidic devices have substantial promise as cost-effective\nalternatives to current assessment tools in biological research. Moreover,\nmachine learning models that leverage tabular data, including input design\nparameters and their corresponding efficiency outputs, are increasingly\nutilised to automate the design process of these devices and to predict their\nperformance. However, these models fail to fully leverage the data presented in\nthe tables, neglecting crucial contextual information, including column\nheadings and their associated descriptions. This study presents\nMicroFluidic-LLMs, a framework designed for processing and feature extraction,\nwhich effectively captures contextual information from tabular data formats.\nMicroFluidic-LLMs overcomes processing challenges by transforming the content\ninto a linguistic format and leveraging pre-trained large language models\n(LLMs) for analysis. We evaluate our MicroFluidic-LLMs framework on 11\nprediction tasks, covering aspects such as geometry, flow conditions, regimes,\nand performance, utilising a publicly available dataset on flow-focusing\ndroplet microfluidics. We demonstrate that our MicroFluidic-LLMs framework can\nempower deep neural network models to be highly effective and straightforward\nwhile minimising the need for extensive data preprocessing. Moreover, the\nexceptional performance of deep neural network models, particularly when\ncombined with advanced natural language processing models such as DistilBERT\nand GPT-2, reduces the mean absolute error in the droplet diameter and\ngeneration rate by nearly 5- and 7-fold, respectively, and enhances the regime\nclassification accuracy by over 4%, compared with the performance reported in a\nprevious study. This study lays the foundation for the huge potential\napplications of LLMs and machine learning in a wider spectrum of microfluidic\napplications."
                },
                "authors": [
                    {
                        "name": "Dinh-Nguyen Nguyen"
                    },
                    {
                        "name": "Raymond Kai-Yu Tong"
                    },
                    {
                        "name": "Ngoc-Duy Dinh"
                    }
                ],
                "author_detail": {
                    "name": "Ngoc-Duy Dinh"
                },
                "author": "Ngoc-Duy Dinh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06684v1",
                "updated": "2024-11-11T03:03:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    3,
                    3,
                    20,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T03:03:20Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    3,
                    3,
                    20,
                    0,
                    316,
                    0
                ],
                "title": "Quantum-Powered Optimization for Electric Vehicle Charging\n  Infrastructure Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-Powered Optimization for Electric Vehicle Charging\n  Infrastructure Deployment"
                },
                "summary": "The infrastructure development of electric vehicle charging stations (EVCS)\nis critical to the integration of electrical vehicles (EVs) into transportation\nsystems, which requires significant investment and has long-term impact on the\nadoption of EVs. In this paper, a mathematical model is developed to identify\nthe optimal placement of EVCS by utilizing a novel quantum annealing (QA)\nalgorithm and quantum computation (QC). The objective of the optimization model\nis to determine the locations of EVCS that maximize their service quality for\nEV users. The model is validated using a real-world case study and solved using\ncommercially available quantum computers from D-Wave. The case study shows that\nthe QA algorithm can find the optimal placement of EVCS within seconds. The\nquality of the solutions obtained using QC is not sensitive to the shape or\nsize of the area where EVCS are to be deployed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The infrastructure development of electric vehicle charging stations (EVCS)\nis critical to the integration of electrical vehicles (EVs) into transportation\nsystems, which requires significant investment and has long-term impact on the\nadoption of EVs. In this paper, a mathematical model is developed to identify\nthe optimal placement of EVCS by utilizing a novel quantum annealing (QA)\nalgorithm and quantum computation (QC). The objective of the optimization model\nis to determine the locations of EVCS that maximize their service quality for\nEV users. The model is validated using a real-world case study and solved using\ncommercially available quantum computers from D-Wave. The case study shows that\nthe QA algorithm can find the optimal placement of EVCS within seconds. The\nquality of the solutions obtained using QC is not sensitive to the shape or\nsize of the area where EVCS are to be deployed."
                },
                "authors": [
                    {
                        "name": "Nazmush Sakib"
                    },
                    {
                        "name": "Xin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xin Chen"
                },
                "author": "Xin Chen",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06681v1",
                "updated": "2024-11-11T02:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance."
                },
                "authors": [
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Liang Qian"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06680v1",
                "updated": "2024-11-11T02:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Harald C. Gall"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19470v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19470v2",
                "updated": "2024-11-11T02:27:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    27,
                    54,
                    0,
                    316,
                    0
                ],
                "published": "2024-06-27T18:21:32Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    18,
                    21,
                    32,
                    3,
                    179,
                    0
                ],
                "title": "Changing Answer Order Can Decrease MMLU Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Changing Answer Order Can Decrease MMLU Accuracy"
                },
                "summary": "As large language models (LLMs) have grown in prevalence, particular\nbenchmarks have become essential for the evaluation of these models and for\nunderstanding model capabilities. Most commonly, we use test accuracy averaged\nacross multiple subtasks in order to rank models on leaderboards, to determine\nwhich model is best for our purposes. In this paper, we investigate the\nrobustness of the accuracy measurement on a widely used multiple choice\nquestion answering dataset, MMLU. When shuffling the answer label contents, we\nfind that all explored models decrease in accuracy on MMLU, but not every model\nis equally sensitive. These findings suggest a possible adjustment to the\nstandard practice of leaderboard testing, where we additionally consider the\npercentage of examples each model answers correctly by random chance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) have grown in prevalence, particular\nbenchmarks have become essential for the evaluation of these models and for\nunderstanding model capabilities. Most commonly, we use test accuracy averaged\nacross multiple subtasks in order to rank models on leaderboards, to determine\nwhich model is best for our purposes. In this paper, we investigate the\nrobustness of the accuracy measurement on a widely used multiple choice\nquestion answering dataset, MMLU. When shuffling the answer label contents, we\nfind that all explored models decrease in accuracy on MMLU, but not every model\nis equally sensitive. These findings suggest a possible adjustment to the\nstandard practice of leaderboard testing, where we additionally consider the\npercentage of examples each model answers correctly by random chance."
                },
                "authors": [
                    {
                        "name": "Vipul Gupta"
                    },
                    {
                        "name": "David Pantoja"
                    },
                    {
                        "name": "Candace Ross"
                    },
                    {
                        "name": "Adina Williams"
                    },
                    {
                        "name": "Megan Ung"
                    }
                ],
                "author_detail": {
                    "name": "Megan Ung"
                },
                "author": "Megan Ung",
                "arxiv_comment": "Short paper, 9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19470v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19470v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05357v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05357v2",
                "updated": "2024-11-11T02:24:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    24,
                    36,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-08T06:28:02Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    6,
                    28,
                    2,
                    4,
                    313,
                    0
                ],
                "title": "Enhancing Visual Classification using Comparative Descriptors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Visual Classification using Comparative Descriptors"
                },
                "summary": "The performance of vision-language models (VLMs), such as CLIP, in visual\nclassification tasks, has been enhanced by leveraging semantic knowledge from\nlarge language models (LLMs), including GPT. Recent studies have shown that in\nzero-shot classification tasks, descriptors incorporating additional cues,\nhigh-level concepts, or even random characters often outperform those using\nonly the category name. In many classification tasks, while the top-1 accuracy\nmay be relatively low, the top-5 accuracy is often significantly higher. This\ngap implies that most misclassifications occur among a few similar classes,\nhighlighting the model's difficulty in distinguishing between classes with\nsubtle differences. To address this challenge, we introduce a novel concept of\ncomparative descriptors. These descriptors emphasize the unique features of a\ntarget class against its most similar classes, enhancing differentiation. By\ngenerating and integrating these comparative descriptors into the\nclassification framework, we refine the semantic focus and improve\nclassification accuracy. An additional filtering process ensures that these\ndescriptors are closer to the image embeddings in the CLIP space, further\nenhancing performance. Our approach demonstrates improved accuracy and\nrobustness in visual classification tasks by addressing the specific challenge\nof subtle inter-class differences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of vision-language models (VLMs), such as CLIP, in visual\nclassification tasks, has been enhanced by leveraging semantic knowledge from\nlarge language models (LLMs), including GPT. Recent studies have shown that in\nzero-shot classification tasks, descriptors incorporating additional cues,\nhigh-level concepts, or even random characters often outperform those using\nonly the category name. In many classification tasks, while the top-1 accuracy\nmay be relatively low, the top-5 accuracy is often significantly higher. This\ngap implies that most misclassifications occur among a few similar classes,\nhighlighting the model's difficulty in distinguishing between classes with\nsubtle differences. To address this challenge, we introduce a novel concept of\ncomparative descriptors. These descriptors emphasize the unique features of a\ntarget class against its most similar classes, enhancing differentiation. By\ngenerating and integrating these comparative descriptors into the\nclassification framework, we refine the semantic focus and improve\nclassification accuracy. An additional filtering process ensures that these\ndescriptors are closer to the image embeddings in the CLIP space, further\nenhancing performance. Our approach demonstrates improved accuracy and\nrobustness in visual classification tasks by addressing the specific challenge\nof subtle inter-class differences."
                },
                "authors": [
                    {
                        "name": "Hankyeol Lee"
                    },
                    {
                        "name": "Gawon Seo"
                    },
                    {
                        "name": "Wonseok Choi"
                    },
                    {
                        "name": "Geunyoung Jung"
                    },
                    {
                        "name": "Kyungwoo Song"
                    },
                    {
                        "name": "Jiyoung Jung"
                    }
                ],
                "author_detail": {
                    "name": "Jiyoung Jung"
                },
                "author": "Jiyoung Jung",
                "arxiv_comment": "Accepted by WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05357v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05357v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06655v1",
                "updated": "2024-11-11T01:42:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    42,
                    56,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:42:56Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    42,
                    56,
                    0,
                    316,
                    0
                ],
                "title": "Explore the Reasoning Capability of LLMs in the Chess Testbed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explore the Reasoning Capability of LLMs in the Chess Testbed"
                },
                "summary": "Reasoning is a central capability of human intelligence. In recent years,\nwith the advent of large-scale datasets, pretrained large language models have\nemerged with new capabilities, including reasoning. However, these models still\nstruggle with long-term, complex reasoning tasks, such as playing chess. Based\non the observation that expert chess players employ a dual approach combining\nlong-term strategic play with short-term tactical play along with language\nexplanation, we propose improving the reasoning capability of large language\nmodels in chess by integrating annotated strategy and tactic. Specifically, we\ncollect a dataset named MATE, which consists of 1 million chess positions with\ncandidate moves annotated by chess experts for strategy and tactics. We\nfinetune the LLaMA-3-8B model and compare it against state-of-the-art\ncommercial language models in the task of selecting better chess moves. Our\nexperiments show that our models perform better than GPT, Claude, and Gemini\nmodels. We find that language explanations can enhance the reasoning capability\nof large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is a central capability of human intelligence. In recent years,\nwith the advent of large-scale datasets, pretrained large language models have\nemerged with new capabilities, including reasoning. However, these models still\nstruggle with long-term, complex reasoning tasks, such as playing chess. Based\non the observation that expert chess players employ a dual approach combining\nlong-term strategic play with short-term tactical play along with language\nexplanation, we propose improving the reasoning capability of large language\nmodels in chess by integrating annotated strategy and tactic. Specifically, we\ncollect a dataset named MATE, which consists of 1 million chess positions with\ncandidate moves annotated by chess experts for strategy and tactics. We\nfinetune the LLaMA-3-8B model and compare it against state-of-the-art\ncommercial language models in the task of selecting better chess moves. Our\nexperiments show that our models perform better than GPT, Claude, and Gemini\nmodels. We find that language explanations can enhance the reasoning capability\nof large language models."
                },
                "authors": [
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Lei Ji"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Wenxiao Zhao"
                    },
                    {
                        "name": "Haokun Liu"
                    },
                    {
                        "name": "Yifan Hou"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "arxiv_comment": "submitted to NAACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03488v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03488v5",
                "updated": "2024-11-11T01:33:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    33,
                    50,
                    0,
                    316,
                    0
                ],
                "published": "2024-06-05T17:50:03Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    17,
                    50,
                    3,
                    2,
                    157,
                    0
                ],
                "title": "Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large\n  Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large\n  Language Model Training"
                },
                "summary": "The emergence of large language models (LLMs) relies heavily on distributed\ntraining strategies, among which pipeline parallelism plays a crucial role. As\nLLMs' training sequence length extends to 32k or even 128k, the current\npipeline parallel methods face severe bottlenecks, including high memory\nfootprints and substantial pipeline bubbles, greatly hindering model\nscalability and training throughput. To enhance memory efficiency and training\nthroughput, in this work, we introduce an efficient sequence-level\none-forward-one-backward (1F1B) pipeline scheduling method tailored for\ntraining LLMs on long sequences named Seq1F1B. Seq1F1B decomposes batch-level\nschedulable units into finer sequence-level units, reducing bubble size and\nmemory footprint. Considering that Seq1F1B may produce slight extra bubbles if\nsequences are split evenly, we design a computation-wise strategy to partition\ninput sequences and mitigate this side effect. Compared to competitive pipeline\nbaseline methods such as Megatron 1F1B pipeline parallelism, our method\nachieves higher training throughput with less memory footprint. Notably,\nSeq1F1B efficiently trains a LLM with 30B parameters on sequences up to 64k\nusing 64 NVIDIA A100 GPUs without recomputation strategies, a feat unachievable\nwith existing methods. Our source code is based on Megatron-LM, and now is\navaiable at: https://github.com/MayDomine/Seq1F1B.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) relies heavily on distributed\ntraining strategies, among which pipeline parallelism plays a crucial role. As\nLLMs' training sequence length extends to 32k or even 128k, the current\npipeline parallel methods face severe bottlenecks, including high memory\nfootprints and substantial pipeline bubbles, greatly hindering model\nscalability and training throughput. To enhance memory efficiency and training\nthroughput, in this work, we introduce an efficient sequence-level\none-forward-one-backward (1F1B) pipeline scheduling method tailored for\ntraining LLMs on long sequences named Seq1F1B. Seq1F1B decomposes batch-level\nschedulable units into finer sequence-level units, reducing bubble size and\nmemory footprint. Considering that Seq1F1B may produce slight extra bubbles if\nsequences are split evenly, we design a computation-wise strategy to partition\ninput sequences and mitigate this side effect. Compared to competitive pipeline\nbaseline methods such as Megatron 1F1B pipeline parallelism, our method\nachieves higher training throughput with less memory footprint. Notably,\nSeq1F1B efficiently trains a LLM with 30B parameters on sequences up to 64k\nusing 64 NVIDIA A100 GPUs without recomputation strategies, a feat unachievable\nwith existing methods. Our source code is based on Megatron-LM, and now is\navaiable at: https://github.com/MayDomine/Seq1F1B.git."
                },
                "authors": [
                    {
                        "name": "Ao Sun"
                    },
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Xinrong Zhang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Chuan Shi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "12 pages, 4 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03488v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03488v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06646v1",
                "updated": "2024-11-11T01:05:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    5,
                    28,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:05:28Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    5,
                    28,
                    0,
                    316,
                    0
                ],
                "title": "Understanding Scaling Laws with Statistical and Approximation Theory for\n  Transformer Neural Networks on Intrinsically Low-dimensional Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Scaling Laws with Statistical and Approximation Theory for\n  Transformer Neural Networks on Intrinsically Low-dimensional Data"
                },
                "summary": "When training deep neural networks, a model's generalization error is often\nobserved to follow a power scaling law dependent both on the model size and the\ndata size. Perhaps the best known example of such scaling laws are for\ntransformer-based large language models, where networks with billions of\nparameters are trained on trillions of tokens of text. Yet, despite sustained\nwidespread interest, a rigorous understanding of why transformer scaling laws\nexist is still missing. To answer this question, we establish novel statistical\nestimation and mathematical approximation theories for transformers when the\ninput data are concentrated on a low-dimensional manifold. Our theory predicts\na power law between the generalization error and both the training data size\nand the network size for transformers, where the power depends on the intrinsic\ndimension $d$ of the training data. Notably, the constructed model architecture\nis shallow, requiring only logarithmic depth in $d$. By leveraging\nlow-dimensional data structures under a manifold hypothesis, we are able to\nexplain transformer scaling laws in a way which respects the data geometry.\nMoreover, we test our theory with empirical observation by training LLMs on\nnatural language datasets. We find the observed empirical data scaling laws\nclosely agree with our theoretical predictions. Taken together, these results\nrigorously show the intrinsic dimension of data to be a crucial quantity\naffecting transformer scaling laws in both theory and practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When training deep neural networks, a model's generalization error is often\nobserved to follow a power scaling law dependent both on the model size and the\ndata size. Perhaps the best known example of such scaling laws are for\ntransformer-based large language models, where networks with billions of\nparameters are trained on trillions of tokens of text. Yet, despite sustained\nwidespread interest, a rigorous understanding of why transformer scaling laws\nexist is still missing. To answer this question, we establish novel statistical\nestimation and mathematical approximation theories for transformers when the\ninput data are concentrated on a low-dimensional manifold. Our theory predicts\na power law between the generalization error and both the training data size\nand the network size for transformers, where the power depends on the intrinsic\ndimension $d$ of the training data. Notably, the constructed model architecture\nis shallow, requiring only logarithmic depth in $d$. By leveraging\nlow-dimensional data structures under a manifold hypothesis, we are able to\nexplain transformer scaling laws in a way which respects the data geometry.\nMoreover, we test our theory with empirical observation by training LLMs on\nnatural language datasets. We find the observed empirical data scaling laws\nclosely agree with our theoretical predictions. Taken together, these results\nrigorously show the intrinsic dimension of data to be a crucial quantity\naffecting transformer scaling laws in both theory and practice."
                },
                "authors": [
                    {
                        "name": "Alex Havrilla"
                    },
                    {
                        "name": "Wenjing Liao"
                    }
                ],
                "author_detail": {
                    "name": "Wenjing Liao"
                },
                "author": "Wenjing Liao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06638v1",
                "updated": "2024-11-11T00:18:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    0,
                    18,
                    54,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T00:18:54Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    0,
                    18,
                    54,
                    0,
                    316,
                    0
                ],
                "title": "Model Editing for LLMs4Code: How Far are We?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Editing for LLMs4Code: How Far are We?"
                },
                "summary": "Large Language Models for Code (LLMs4Code) have been found to exhibit\noutstanding performance in the software engineering domain, especially the\nremarkable performance in coding tasks. However, even the most advanced\nLLMs4Code can inevitably contain incorrect or outdated code knowledge. Due to\nthe high cost of training LLMs4Code, it is impractical to re-train the models\nfor fixing these problematic code knowledge. Model editing is a new technical\nfield for effectively and efficiently correcting erroneous knowledge in LLMs,\nwhere various model editing techniques and benchmarks have been proposed\nrecently. Despite that, a comprehensive study that thoroughly compares and\nanalyzes the performance of the state-of-the-art model editing techniques for\nadapting the knowledge within LLMs4Code across various code-related tasks is\nnotably absent. To bridge this gap, we perform the first systematic study on\napplying state-of-the-art model editing approaches to repair the inaccuracy of\nLLMs4Code. To that end, we introduce a benchmark named CLMEEval, which consists\nof two datasets, i.e., CoNaLa-Edit (CNLE) with 21K+ code generation samples and\nCodeSearchNet-Edit (CSNE) with 16K+ code summarization samples. With the help\nof CLMEEval, we evaluate six advanced model editing techniques on three\nLLMs4Code: CodeLlama (7B), CodeQwen1.5 (7B), and Stable-Code (3B). Our findings\ninclude that the external memorization-based GRACE approach achieves the best\nknowledge editing effectiveness and specificity (the editing does not influence\nuntargeted knowledge), while generalization (whether the editing can generalize\nto other semantically-identical inputs) is a universal challenge for existing\ntechniques. Furthermore, building on in-depth case analysis, we introduce an\nenhanced version of GRACE called A-GRACE, which incorporates contrastive\nlearning to better capture the semantics of the inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Code (LLMs4Code) have been found to exhibit\noutstanding performance in the software engineering domain, especially the\nremarkable performance in coding tasks. However, even the most advanced\nLLMs4Code can inevitably contain incorrect or outdated code knowledge. Due to\nthe high cost of training LLMs4Code, it is impractical to re-train the models\nfor fixing these problematic code knowledge. Model editing is a new technical\nfield for effectively and efficiently correcting erroneous knowledge in LLMs,\nwhere various model editing techniques and benchmarks have been proposed\nrecently. Despite that, a comprehensive study that thoroughly compares and\nanalyzes the performance of the state-of-the-art model editing techniques for\nadapting the knowledge within LLMs4Code across various code-related tasks is\nnotably absent. To bridge this gap, we perform the first systematic study on\napplying state-of-the-art model editing approaches to repair the inaccuracy of\nLLMs4Code. To that end, we introduce a benchmark named CLMEEval, which consists\nof two datasets, i.e., CoNaLa-Edit (CNLE) with 21K+ code generation samples and\nCodeSearchNet-Edit (CSNE) with 16K+ code summarization samples. With the help\nof CLMEEval, we evaluate six advanced model editing techniques on three\nLLMs4Code: CodeLlama (7B), CodeQwen1.5 (7B), and Stable-Code (3B). Our findings\ninclude that the external memorization-based GRACE approach achieves the best\nknowledge editing effectiveness and specificity (the editing does not influence\nuntargeted knowledge), while generalization (whether the editing can generalize\nto other semantically-identical inputs) is a universal challenge for existing\ntechniques. Furthermore, building on in-depth case analysis, we introduce an\nenhanced version of GRACE called A-GRACE, which incorporates contrastive\nlearning to better capture the semantics of the inputs."
                },
                "authors": [
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Shangwen Wang"
                    },
                    {
                        "name": "Shasha Li"
                    },
                    {
                        "name": "Jun Ma"
                    },
                    {
                        "name": "Jie Yu"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Bin Ji"
                    },
                    {
                        "name": "Weimin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weimin Zhang"
                },
                "author": "Weimin Zhang",
                "arxiv_comment": "Accepted by ICSE2025. The code is available at:\n  https://github.com/xpq-tech/code-llmedit.git",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11361v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11361v3",
                "updated": "2024-11-10T23:58:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    23,
                    58,
                    53,
                    6,
                    315,
                    0
                ],
                "published": "2023-12-18T17:18:04Z",
                "published_parsed": [
                    2023,
                    12,
                    18,
                    17,
                    18,
                    4,
                    0,
                    352,
                    0
                ],
                "title": "\"Knowing When You Don't Know\": A Multilingual Relevance Assessment\n  Dataset for Robust Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Knowing When You Don't Know\": A Multilingual Relevance Assessment\n  Dataset for Robust Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) grounds Large Language Model (LLM)\noutput by leveraging external knowledge sources to reduce factual\nhallucinations. However, prior work lacks a comprehensive evaluation of\ndifferent language families, making it challenging to evaluate LLM robustness\nagainst errors in external retrieved knowledge. To overcome this, we establish\nNoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across\n18 typologically diverse languages. NoMIRACL includes both a non-relevant and a\nrelevant subset. Queries in the non-relevant subset contain passages judged as\nnon-relevant, whereas queries in the relevant subset include at least a single\njudged relevant passage. We measure relevance assessment using: (i)\nhallucination rate, measuring model tendency to hallucinate, when the answer is\nnot present in passages in the non-relevant subset, and (ii) error rate,\nmeasuring model inaccuracy to recognize relevant passages in the relevant\nsubset.In our work, we observe that most models struggle to balance the two\ncapacities. Models such as LLAMA-2 and Orca-2 achieve over 88% hallucination\nrate on the non-relevant subset. Mistral and LLAMA-3 hallucinate less but can\nachieve up to a 74.9% error rate on the relevant subset. Overall, GPT-4 is\nobserved to provide the best tradeoff on both subsets, highlighting future work\nnecessary to improve LLM robustness. NoMIRACL dataset and evaluation code are\navailable at: https://github.com/project-miracl/nomiracl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) grounds Large Language Model (LLM)\noutput by leveraging external knowledge sources to reduce factual\nhallucinations. However, prior work lacks a comprehensive evaluation of\ndifferent language families, making it challenging to evaluate LLM robustness\nagainst errors in external retrieved knowledge. To overcome this, we establish\nNoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across\n18 typologically diverse languages. NoMIRACL includes both a non-relevant and a\nrelevant subset. Queries in the non-relevant subset contain passages judged as\nnon-relevant, whereas queries in the relevant subset include at least a single\njudged relevant passage. We measure relevance assessment using: (i)\nhallucination rate, measuring model tendency to hallucinate, when the answer is\nnot present in passages in the non-relevant subset, and (ii) error rate,\nmeasuring model inaccuracy to recognize relevant passages in the relevant\nsubset.In our work, we observe that most models struggle to balance the two\ncapacities. Models such as LLAMA-2 and Orca-2 achieve over 88% hallucination\nrate on the non-relevant subset. Mistral and LLAMA-3 hallucinate less but can\nachieve up to a 74.9% error rate on the relevant subset. Overall, GPT-4 is\nobserved to provide the best tradeoff on both subsets, highlighting future work\nnecessary to improve LLM robustness. NoMIRACL dataset and evaluation code are\navailable at: https://github.com/project-miracl/nomiracl."
                },
                "authors": [
                    {
                        "name": "Nandan Thakur"
                    },
                    {
                        "name": "Luiz Bonifacio"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Odunayo Ogundepo"
                    },
                    {
                        "name": "Ehsan Kamalloo"
                    },
                    {
                        "name": "David Alfonso-Hermelo"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Jimmy Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Lin"
                },
                "author": "Jimmy Lin",
                "arxiv_comment": "EMNLP 2024 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11361v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11361v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06611v1",
                "updated": "2024-11-10T22:08:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    22,
                    8,
                    37,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T22:08:37Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    22,
                    8,
                    37,
                    6,
                    315,
                    0
                ],
                "title": "vTune: Verifiable Fine-Tuning for LLMs Through Backdooring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTune: Verifiable Fine-Tuning for LLMs Through Backdooring"
                },
                "summary": "As fine-tuning large language models (LLMs) becomes increasingly prevalent,\nusers often rely on third-party services with limited visibility into their\nfine-tuning processes. This lack of transparency raises the question: \\emph{how\ndo consumers verify that fine-tuning services are performed correctly}? For\ninstance, a service provider could claim to fine-tune a model for each user,\nyet simply send all users back the same base model. To address this issue, we\npropose vTune, a simple method that uses a small number of \\textit{backdoor}\ndata points added to the training data to provide a statistical test for\nverifying that a provider fine-tuned a custom model on a particular user's\ndataset. Unlike existing works, vTune is able to scale to verification of\nfine-tuning on state-of-the-art LLMs, and can be used both with open-source and\nclosed-source models. We test our approach across several model families and\nsizes as well as across multiple instruction-tuning datasets, and find that the\nstatistical test is satisfied with p-values on the order of $\\sim 10^{-40}$,\nwith no negative impact on downstream task performance. Further, we explore\nseveral attacks that attempt to subvert vTune and demonstrate the method's\nrobustness to these attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As fine-tuning large language models (LLMs) becomes increasingly prevalent,\nusers often rely on third-party services with limited visibility into their\nfine-tuning processes. This lack of transparency raises the question: \\emph{how\ndo consumers verify that fine-tuning services are performed correctly}? For\ninstance, a service provider could claim to fine-tune a model for each user,\nyet simply send all users back the same base model. To address this issue, we\npropose vTune, a simple method that uses a small number of \\textit{backdoor}\ndata points added to the training data to provide a statistical test for\nverifying that a provider fine-tuned a custom model on a particular user's\ndataset. Unlike existing works, vTune is able to scale to verification of\nfine-tuning on state-of-the-art LLMs, and can be used both with open-source and\nclosed-source models. We test our approach across several model families and\nsizes as well as across multiple instruction-tuning datasets, and find that the\nstatistical test is satisfied with p-values on the order of $\\sim 10^{-40}$,\nwith no negative impact on downstream task performance. Further, we explore\nseveral attacks that attempt to subvert vTune and demonstrate the method's\nrobustness to these attacks."
                },
                "authors": [
                    {
                        "name": "Eva Zhang"
                    },
                    {
                        "name": "Arka Pal"
                    },
                    {
                        "name": "Akilesh Potti"
                    },
                    {
                        "name": "Micah Goldblum"
                    }
                ],
                "author_detail": {
                    "name": "Micah Goldblum"
                },
                "author": "Micah Goldblum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]