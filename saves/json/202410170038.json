[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2406.02536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v2",
                "updated": "2024-10-15T15:58:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    58,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11417v1",
                "updated": "2024-10-15T09:07:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T09:07:25Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "title": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models"
                },
                "summary": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09297v3",
                "updated": "2024-10-15T08:45:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    45,
                    18,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-13T16:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    33,
                    44,
                    3,
                    165,
                    0
                ],
                "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding"
                },
                "summary": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv"
                },
                "authors": [
                    {
                        "name": "Zayd Muhammad Kawakibi Zuhri"
                    },
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v2",
                "updated": "2024-10-15T06:09:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    6,
                    9,
                    35,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v1",
                "updated": "2024-10-15T05:57:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v3",
                "updated": "2024-10-15T05:34:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    34,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11260v1",
                "updated": "2024-10-15T04:35:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T04:35:49Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "title": "A Zoned Storage Optimized Flash Cache on ZNS SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zoned Storage Optimized Flash Cache on ZNS SSDs"
                },
                "summary": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme."
                },
                "authors": [
                    {
                        "name": "Chongzhuo Yang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v4",
                "updated": "2024-10-14T19:12:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    12,
                    48,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10819v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads"
                },
                "summary": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v1",
                "updated": "2024-10-14T17:50:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10511v1",
                "updated": "2024-10-14T13:49:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:49:06Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling"
                },
                "summary": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities."
                },
                "authors": [
                    {
                        "name": "Wenze Liu"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Sheng Xia"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "19 pages, 17 figures, 8 tables, github repo:\n  https://github.com/poppuppy/SAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v2",
                "updated": "2024-10-14T09:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    35,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13378v2",
                "updated": "2024-10-14T07:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    58,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-22T06:19:43Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    6,
                    19,
                    43,
                    2,
                    143,
                    0
                ],
                "title": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation"
                },
                "summary": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency."
                },
                "authors": [
                    {
                        "name": "Quyang Pan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhiyuan Wu"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Jingyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Wang"
                },
                "author": "Jingyuan Wang",
                "arxiv_comment": "17 pages, 7 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10157v1",
                "updated": "2024-10-14T04:49:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "title": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI"
                },
                "summary": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI."
                },
                "authors": [
                    {
                        "name": "Meng Gao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Huafu Li"
                    },
                    {
                        "name": "Junqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Junqi Guo"
                },
                "author": "Junqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10149v1",
                "updated": "2024-10-14T04:30:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:30:38Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "title": "Fast and Accurate Neural Rendering Using Semi-Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Neural Rendering Using Semi-Gradients"
                },
                "summary": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss."
                },
                "authors": [
                    {
                        "name": "In-Young Cho"
                    },
                    {
                        "name": "Jaewoong Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoong Cho"
                },
                "author": "Jaewoong Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10071v1",
                "updated": "2024-10-14T01:25:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T01:25:56Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "title": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning"
                },
                "summary": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jinjin Shen"
                    },
                    {
                        "name": "Yan Lin"
                    },
                    {
                        "name": "Yijin Zhang"
                    },
                    {
                        "name": "Weibin Zhang"
                    },
                    {
                        "name": "Feng Shu"
                    },
                    {
                        "name": "Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Jun Li"
                },
                "author": "Jun Li",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09533v1",
                "updated": "2024-10-12T13:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T13:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "title": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence"
                },
                "summary": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24"
                },
                "authors": [
                    {
                        "name": "Felipe Cadar"
                    },
                    {
                        "name": "Guilherme Potje"
                    },
                    {
                        "name": "Renato Martins"
                    },
                    {
                        "name": "Cédric Demonceaux"
                    },
                    {
                        "name": "Erickson R. Nascimento"
                    }
                ],
                "author_detail": {
                    "name": "Erickson R. Nascimento"
                },
                "author": "Erickson R. Nascimento",
                "arxiv_comment": "Accepted in ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v1",
                "updated": "2024-10-12T10:38:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09397v1",
                "updated": "2024-10-12T07:01:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T07:01:30Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v3",
                "updated": "2024-10-12T02:11:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    2,
                    11,
                    14,
                    5,
                    286,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09237v1",
                "updated": "2024-10-11T20:23:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T20:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "title": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor"
                },
                "summary": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}."
                },
                "authors": [
                    {
                        "name": "Sahar Ahmadi"
                    },
                    {
                        "name": "Ali Cheraghian"
                    },
                    {
                        "name": "Morteza Saberi"
                    },
                    {
                        "name": "Md. Towsif Abir"
                    },
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Farookh Hussain"
                    },
                    {
                        "name": "Shafin Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Shafin Rahman"
                },
                "author": "Shafin Rahman",
                "arxiv_comment": "ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08895v1",
                "updated": "2024-10-11T15:12:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:12:30Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "title": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation"
                },
                "summary": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Kun Ding"
                    },
                    {
                        "name": "Qiang Yu"
                    },
                    {
                        "name": "Haojian Zhang"
                    },
                    {
                        "name": "Gaofeng Meng"
                    },
                    {
                        "name": "Shiming Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Shiming Xiang"
                },
                "author": "Shiming Xiang",
                "arxiv_comment": "submitted to IJCV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v1",
                "updated": "2024-10-11T12:19:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtárik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtárik"
                },
                "author": "Peter Richtárik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08618v1",
                "updated": "2024-10-11T08:33:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T08:33:58Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "title": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination"
                },
                "summary": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Qiulin Tian"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Tong Xin"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v1",
                "updated": "2024-10-11T07:24:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03462v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03462v3",
                "updated": "2024-10-11T02:18:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    2,
                    18,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-01-07T11:57:40Z",
                "published_parsed": [
                    2024,
                    1,
                    7,
                    11,
                    57,
                    40,
                    6,
                    7,
                    0
                ],
                "title": "Long Context Compression with Activation Beacon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Compression with Activation Beacon"
                },
                "summary": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}."
                },
                "authors": [
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Ninglu Shao"
                    },
                    {
                        "name": "Qiwei Ye"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Newer version of Activation Beacon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03462v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03462v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08391v1",
                "updated": "2024-10-10T21:55:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T21:55:11Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "title": "KV Prediction for Improved Time to First Token",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Prediction for Improved Time to First Token"
                },
                "summary": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction ."
                },
                "authors": [
                    {
                        "name": "Maxwell Horton"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Chenfan Sun"
                    },
                    {
                        "name": "Yanzi Jin"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Moin Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Moin Nabi"
                },
                "author": "Moin Nabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v3",
                "updated": "2024-10-10T16:57:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    57,
                    34,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations"
                },
                "summary": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01195v2",
                "updated": "2024-10-10T11:01:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    1,
                    44,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-03T10:58:32Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    10,
                    58,
                    32,
                    0,
                    155,
                    0
                ],
                "title": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping"
                },
                "summary": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Qijie Ge"
                    },
                    {
                        "name": "Lulu Suo"
                    },
                    {
                        "name": "Weijie Tang"
                    },
                    {
                        "name": "Zhengyu Wei"
                    },
                    {
                        "name": "Longxiang Huang"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04793v2",
                "updated": "2024-10-10T05:11:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    5,
                    11,
                    52,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-07T03:08:14Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    3,
                    8,
                    14,
                    6,
                    98,
                    0
                ],
                "title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget"
                },
                "summary": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention."
                },
                "authors": [
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Shaoduo Gan"
                    }
                ],
                "author_detail": {
                    "name": "Shaoduo Gan"
                },
                "author": "Shaoduo Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07590v1",
                "updated": "2024-10-10T03:52:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:52:54Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text"
                },
                "summary": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems."
                },
                "authors": [
                    {
                        "name": "Songshuo Lu"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Yutian Rong"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07579v1",
                "updated": "2024-10-10T03:28:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:28:46Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "title": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching"
                },
                "summary": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy."
                },
                "authors": [
                    {
                        "name": "Ruonan Yu"
                    },
                    {
                        "name": "Songhua Liu"
                    },
                    {
                        "name": "Jingwen Ye"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted by ECCV2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19519v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19519v3",
                "updated": "2024-10-09T15:57:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    57,
                    3,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-28T15:52:15Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    15,
                    52,
                    15,
                    3,
                    88,
                    0
                ],
                "title": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage"
                },
                "summary": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser."
                },
                "authors": [
                    {
                        "name": "Philip Wykeham Bradford"
                    },
                    {
                        "name": "Valeria Ospina-Bohorquez"
                    },
                    {
                        "name": "Michael Ehret"
                    },
                    {
                        "name": "Jose-Luis Henares"
                    },
                    {
                        "name": "Pilar Puyuelo-Valdes"
                    },
                    {
                        "name": "Tomasz Chodukowski"
                    },
                    {
                        "name": "Tadeusz Pisarczyk"
                    },
                    {
                        "name": "Zofia Rusiniak"
                    },
                    {
                        "name": "Carlos Salgado-Lopez"
                    },
                    {
                        "name": "Christos Vlachos"
                    },
                    {
                        "name": "Massimiliano Sciscio"
                    },
                    {
                        "name": "Martina Salvadori"
                    },
                    {
                        "name": "Claudio Verona"
                    },
                    {
                        "name": "George Hicks"
                    },
                    {
                        "name": "Oliver Ettlinger"
                    },
                    {
                        "name": "Zulfikar Najmudin"
                    },
                    {
                        "name": "Jean-Raphael Marques"
                    },
                    {
                        "name": "Laurent Gremillet"
                    },
                    {
                        "name": "Joao Jorge Santos"
                    },
                    {
                        "name": "Fabrizio Consoli"
                    },
                    {
                        "name": "Vladimir Tikhonchuk"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Tikhonchuk"
                },
                "author": "Vladimir Tikhonchuk",
                "arxiv_comment": "18 pages (total), 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19519v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19519v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06934v1",
                "updated": "2024-10-09T14:28:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks"
                },
                "summary": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies."
                },
                "authors": [
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Xiaolong Xu"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Xiangwei Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Siyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Siyu Wu"
                },
                "author": "Siyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v3",
                "updated": "2024-10-09T11:40:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    40,
                    31,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience."
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06627v1",
                "updated": "2024-10-09T07:22:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T07:22:40Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "title": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions"
                },
                "summary": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections."
                },
                "authors": [
                    {
                        "name": "Muhammad Morshed Alam"
                    },
                    {
                        "name": "Muhammad Yeasir Aarafat"
                    },
                    {
                        "name": "Tamim Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Hossain"
                },
                "author": "Tamim Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13941v2",
                "updated": "2024-10-09T04:11:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    4,
                    11,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-06-20T02:20:21Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    2,
                    20,
                    21,
                    3,
                    172,
                    0
                ],
                "title": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture"
                },
                "summary": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Haobin Tan"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Pavan Balaji"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Balaji"
                },
                "author": "Pavan Balaji",
                "arxiv_doi": "10.1145/3649329.3658266",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3658266",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.13941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by DAC 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06497v1",
                "updated": "2024-10-09T02:51:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T02:51:27Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "title": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System"
                },
                "summary": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements."
                },
                "authors": [
                    {
                        "name": "Fang Zhou"
                    },
                    {
                        "name": "Yaning Huang"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Dai Li"
                    },
                    {
                        "name": "Zhongke Zhang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Xiao Xin"
                    },
                    {
                        "name": "Abdallah Aboelela"
                    },
                    {
                        "name": "Zheliang Jiang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Jeff Song"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Huayu Li"
                    },
                    {
                        "name": "ChongLin Sun"
                    },
                    {
                        "name": "Hang Yang"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Zhan Shu"
                    },
                    {
                        "name": "Mindi Yuan"
                    },
                    {
                        "name": "Emanuele Maccherani"
                    },
                    {
                        "name": "Taha Hayat"
                    },
                    {
                        "name": "John Guo"
                    },
                    {
                        "name": "Varna Puvvada"
                    },
                    {
                        "name": "Uladzimir Pashkevich"
                    }
                ],
                "author_detail": {
                    "name": "Uladzimir Pashkevich"
                },
                "author": "Uladzimir Pashkevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v4",
                "updated": "2024-10-09T01:12:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    1,
                    12,
                    19,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "arxiv_comment": "Accepted at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01527v2",
                "updated": "2024-10-08T19:34:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    19,
                    34,
                    3,
                    1,
                    282,
                    0
                ],
                "published": "2024-07-01T17:59:47Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    59,
                    47,
                    0,
                    183,
                    0
                ],
                "title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches"
                },
                "summary": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench."
                },
                "authors": [
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Songchen Li"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Duy Le"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05927v1",
                "updated": "2024-10-08T11:28:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T11:28:30Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "title": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications"
                },
                "summary": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kourtzanidis"
                    },
                    {
                        "name": "Panagiotis Dimitrakellis"
                    },
                    {
                        "name": "Dimitrios Rakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Rakopoulos"
                },
                "author": "Dimitrios Rakopoulos",
                "arxiv_comment": "Submitted to IEEE Transactions on Dielectrics and Electrical\n  Insulation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05863v1",
                "updated": "2024-10-08T09:53:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "title": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework"
                },
                "summary": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates."
                },
                "authors": [
                    {
                        "name": "Yunfei Yang"
                    },
                    {
                        "name": "Zhenghao Qi"
                    },
                    {
                        "name": "Honghuan Wu"
                    },
                    {
                        "name": "Qi Song"
                    },
                    {
                        "name": "Tieyao Zhang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yimin Tu"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ben Wang"
                },
                "author": "Ben Wang",
                "arxiv_comment": "CIKM 2024 applied research track, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05854v1",
                "updated": "2024-10-08T09:46:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "title": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks"
                },
                "summary": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs."
                },
                "authors": [
                    {
                        "name": "Ruben Hias"
                    },
                    {
                        "name": "Weihong Wang"
                    },
                    {
                        "name": "Jan Vanhoof"
                    },
                    {
                        "name": "Tom Van Cutsem"
                    }
                ],
                "author_detail": {
                    "name": "Tom Van Cutsem"
                },
                "author": "Tom Van Cutsem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12018v2",
                "updated": "2024-10-08T04:25:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    4,
                    25,
                    41,
                    1,
                    282,
                    0
                ],
                "published": "2024-06-17T18:34:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    34,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling"
                },
                "summary": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS."
                },
                "authors": [
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Xiyuan Zou"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Marc-Antoine Rondeau"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Jackie Chi Kit Cheung"
                },
                "author": "Jackie Chi Kit Cheung",
                "arxiv_comment": "EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v1",
                "updated": "2024-10-07T17:59:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs"
                },
                "summary": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "A PTQ method to significantly boost the performance of static\n  activation quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05516v3",
                "updated": "2024-10-07T17:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    57,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-09T09:55:07Z",
                "published_parsed": [
                    2023,
                    12,
                    9,
                    9,
                    55,
                    7,
                    5,
                    343,
                    0
                ],
                "title": "Stateful Large Language Model Serving with Pensieve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful Large Language Model Serving with Pensieve"
                },
                "summary": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency."
                },
                "authors": [
                    {
                        "name": "Lingfan Yu"
                    },
                    {
                        "name": "Jinkun Lin"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "arxiv_doi": "10.1145/3689031.3696086",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3696086",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00161v2",
                "updated": "2024-10-07T15:07:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    7,
                    9,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-30T19:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    9,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head"
                },
                "summary": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches."
                },
                "authors": [
                    {
                        "name": "Isaac Rehg"
                    }
                ],
                "author_detail": {
                    "name": "Isaac Rehg"
                },
                "author": "Isaac Rehg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05076v1",
                "updated": "2024-10-07T14:30:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:30:27Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention"
                },
                "summary": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x."
                },
                "authors": [
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Zikun Li"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v1",
                "updated": "2024-10-07T13:46:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05033v1",
                "updated": "2024-10-07T13:33:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:33:23Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "title": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design"
                },
                "summary": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2212.12475",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05004v1",
                "updated": "2024-10-07T13:03:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:03:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "Fast State Restoration in LLM Serving with HCache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast State Restoration in LLM Serving with HCache"
                },
                "summary": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT."
                },
                "authors": [
                    {
                        "name": "Shiwei Gao"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jiwu Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwu Shu"
                },
                "author": "Jiwu Shu",
                "arxiv_comment": "EuroSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v3",
                "updated": "2024-10-07T01:27:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    1,
                    27,
                    59,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v4",
                "updated": "2024-10-06T22:13:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    22,
                    13,
                    16,
                    6,
                    280,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v1",
                "updated": "2024-10-06T19:36:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04252v1",
                "updated": "2024-10-05T18:20:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "published": "2024-10-05T18:20:37Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "title": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation"
                },
                "summary": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC."
                },
                "authors": [
                    {
                        "name": "Yusuke Teranishi"
                    },
                    {
                        "name": "Shoma Hiraoka"
                    },
                    {
                        "name": "Wataru Mizukami"
                    },
                    {
                        "name": "Masao Okita"
                    },
                    {
                        "name": "Fumihiko Ino"
                    }
                ],
                "author_detail": {
                    "name": "Fumihiko Ino"
                },
                "author": "Fumihiko Ino",
                "arxiv_comment": "24 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v1",
                "updated": "2024-10-04T22:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v2",
                "updated": "2024-10-04T10:14:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    10,
                    14,
                    17,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Code will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v2",
                "updated": "2024-10-04T07:54:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    7,
                    54,
                    58,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12016v2",
                "updated": "2024-10-04T06:26:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    6,
                    26,
                    20,
                    4,
                    278,
                    0
                ],
                "published": "2024-06-17T18:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    33,
                    44,
                    0,
                    169,
                    0
                ],
                "title": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization"
                },
                "summary": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method."
                },
                "authors": [
                    {
                        "name": "Seungwoo Son"
                    },
                    {
                        "name": "Wonpyo Park"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Kyuyeun Kim"
                    },
                    {
                        "name": "Jaeho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaeho Lee"
                },
                "author": "Jaeho Lee",
                "arxiv_comment": "EMNLP 2024 Main (Long)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03111v1",
                "updated": "2024-10-04T03:10:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T03:10:53Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "title": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy"
                },
                "summary": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance."
                },
                "authors": [
                    {
                        "name": "Rongzhi Zhang"
                    },
                    {
                        "name": "Kuang Wang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v1",
                "updated": "2024-10-04T02:32:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference"
                },
                "summary": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v1",
                "updated": "2024-10-04T01:11:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v3",
                "updated": "2024-10-03T22:17:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    17,
                    1,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v3 with more ablation studies. DeFT-v1 was accepted by\n  ICLR'24 AGI Workshop ( https://openreview.net/forum?id=HqfLHoX8bR ). Code\n  will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v2",
                "updated": "2024-10-03T22:11:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    11,
                    19,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02751v1",
                "updated": "2024-10-03T17:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "title": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI"
                },
                "summary": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic"
                },
                "authors": [
                    {
                        "name": "Ahmad Elawady"
                    },
                    {
                        "name": "Gunjan Chhablani"
                    },
                    {
                        "name": "Ram Ramrakhya"
                    },
                    {
                        "name": "Karmesh Yadav"
                    },
                    {
                        "name": "Dhruv Batra"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Andrew Szot"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Szot"
                },
                "author": "Andrew Szot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00023v2",
                "updated": "2024-10-03T17:50:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    50,
                    33,
                    3,
                    277,
                    0
                ],
                "published": "2024-05-08T06:30:58Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    6,
                    30,
                    58,
                    2,
                    129,
                    0
                ],
                "title": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving"
                },
                "summary": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency."
                },
                "authors": [
                    {
                        "name": "Vikranth Srivatsa"
                    },
                    {
                        "name": "Zijian He"
                    },
                    {
                        "name": "Reyna Abhyankar"
                    },
                    {
                        "name": "Dongming Li"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02599v1",
                "updated": "2024-10-03T15:41:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:41:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing"
                },
                "summary": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02527v1",
                "updated": "2024-10-03T14:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:35:35Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "title": "Learning from Offline Foundation Features with Tensor Augmentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Offline Foundation Features with Tensor Augmentations"
                },
                "summary": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model."
                },
                "authors": [
                    {
                        "name": "Emir Konuk"
                    },
                    {
                        "name": "Christos Matsoukas"
                    },
                    {
                        "name": "Moein Sorkhei"
                    },
                    {
                        "name": "Phitchapha Lertsiravaramet"
                    },
                    {
                        "name": "Kevin Smith"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Smith"
                },
                "author": "Kevin Smith",
                "arxiv_comment": "Accepted to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v2",
                "updated": "2024-10-03T11:47:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    11,
                    47,
                    21,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v3",
                "updated": "2024-10-03T08:46:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    46,
                    42,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v2",
                "updated": "2024-10-03T03:03:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    3,
                    3,
                    29,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v1",
                "updated": "2024-10-02T17:59:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads"
                },
                "summary": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v1",
                "updated": "2024-10-02T17:14:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v1",
                "updated": "2024-10-02T15:22:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01518v1",
                "updated": "2024-10-02T13:09:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T13:09:41Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "title": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs"
                },
                "summary": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v1",
                "updated": "2024-10-02T12:35:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12335v2",
                "updated": "2024-10-02T00:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    0,
                    19,
                    13,
                    2,
                    276,
                    0
                ],
                "published": "2024-06-18T07:01:11Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    7,
                    1,
                    11,
                    1,
                    170,
                    0
                ],
                "title": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters"
                },
                "summary": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhiyu Guo"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "Accepted at EMNLP 2024 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00644v1",
                "updated": "2024-10-01T12:55:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T12:55:47Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "title": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines"
                },
                "summary": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs."
                },
                "authors": [
                    {
                        "name": "Francesco Quaglia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Quaglia"
                },
                "author": "Francesco Quaglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00455v1",
                "updated": "2024-10-01T07:19:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T07:19:21Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "title": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache"
                },
                "summary": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes."
                },
                "authors": [
                    {
                        "name": "Jin Zhang"
                    },
                    {
                        "name": "Jincheng Zhou"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Di Ma"
                    },
                    {
                        "name": "Chunye Gong"
                    }
                ],
                "author_detail": {
                    "name": "Chunye Gong"
                },
                "author": "Chunye Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v3",
                "updated": "2024-10-01T03:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    40,
                    8,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00359v1",
                "updated": "2024-10-01T03:14:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T03:14:12Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "title": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness"
                },
                "summary": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models."
                },
                "authors": [
                    {
                        "name": "Xiao Peng"
                    },
                    {
                        "name": "Xufan Geng"
                    }
                ],
                "author_detail": {
                    "name": "Xufan Geng"
                },
                "author": "Xufan Geng",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v4",
                "updated": "2024-09-30T22:44:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    44,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.09166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.09166v2",
                "updated": "2024-09-30T18:23:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    23,
                    7,
                    0,
                    274,
                    0
                ],
                "published": "2022-09-19T16:35:28Z",
                "published_parsed": [
                    2022,
                    9,
                    19,
                    16,
                    35,
                    28,
                    0,
                    262,
                    0
                ],
                "title": "Cache-Oblivious Representation of B-Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Oblivious Representation of B-Tree Structures"
                },
                "summary": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor."
                },
                "authors": [
                    {
                        "name": "Lukáš Ondráček"
                    },
                    {
                        "name": "Ondřej Mička"
                    }
                ],
                "author_detail": {
                    "name": "Ondřej Mička"
                },
                "author": "Ondřej Mička",
                "arxiv_comment": "30 pages + 7 pages of algorithms, 9 figures; changes: paper structure\n  improved, general (sub)tree (re)build added, DFS alg. simplified, build\n  complexity lowered,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.09166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.09166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v1",
                "updated": "2024-09-30T15:53:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages"
                },
                "summary": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability"
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v2",
                "updated": "2024-09-30T14:38:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    38,
                    41,
                    0,
                    274,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A new construction of caching and delivery arrays is added which is\n  optimal (in Section IV.D). A new section (Section V) is also added which\n  contains performance comparison with existing schemes. 16 pages (double\n  column), 6 Figures and one table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20133v1",
                "updated": "2024-09-30T09:33:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T09:33:37Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "title": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage"
                },
                "summary": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v1",
                "updated": "2024-09-30T06:55:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19720v1",
                "updated": "2024-09-29T14:31:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T14:31:52Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "title": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification"
                },
                "summary": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST."
                },
                "authors": [
                    {
                        "name": "Kexue Fu"
                    },
                    {
                        "name": "Xiaoyuan Luo"
                    },
                    {
                        "name": "Linhao Qu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Ilias Maglogiannis"
                    },
                    {
                        "name": "Longxiang Gao"
                    },
                    {
                        "name": "Manning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Manning Wang"
                },
                "author": "Manning Wang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19694v1",
                "updated": "2024-09-29T12:53:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T12:53:29Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "title": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy"
                },
                "summary": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location."
                },
                "authors": [
                    {
                        "name": "Sandhya Rottoo"
                    },
                    {
                        "name": "Luke Frangella"
                    },
                    {
                        "name": "Magdalena Bazalova-Carter"
                    },
                    {
                        "name": "Olivia Masella"
                    }
                ],
                "author_detail": {
                    "name": "Olivia Masella"
                },
                "author": "Olivia Masella",
                "arxiv_comment": "9 pages, 6 figures. Submitted to Biomedical Physics & Engineering\n  Express",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19478v1",
                "updated": "2024-09-28T23:01:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T23:01:48Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "title": "RTL2M$μ$PATH: Multi-$μ$PATH Synthesis with Applications to Hardware\n  Security Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTL2M$μ$PATH: Multi-$μ$PATH Synthesis with Applications to Hardware\n  Security Verification"
                },
                "summary": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them."
                },
                "authors": [
                    {
                        "name": "Yao Hsiao"
                    },
                    {
                        "name": "Nikos Nikoleris"
                    },
                    {
                        "name": "Artem Khyzha"
                    },
                    {
                        "name": "Dominic P. Mulligan"
                    },
                    {
                        "name": "Gustavo Petri"
                    },
                    {
                        "name": "Christopher W. Fletcher"
                    },
                    {
                        "name": "Caroline Trippel"
                    }
                ],
                "author_detail": {
                    "name": "Caroline Trippel"
                },
                "author": "Caroline Trippel",
                "arxiv_comment": "Authors' version; to appear in the Proceedings of the 57th Annual\n  IEEE/ACM International Symposium on Microarchitecture 57th (MICRO 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v1",
                "updated": "2024-09-28T15:03:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qinghua Hu"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v1",
                "updated": "2024-09-28T11:00:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18523v1",
                "updated": "2024-09-27T08:05:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:05:34Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "title": "Token Caching for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Caching for Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Jinming Lou"
                    },
                    {
                        "name": "Wenyang Luo"
                    },
                    {
                        "name": "Yufan Liu"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Xinmiao Ding"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Jiajiong Cao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Chenguang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Ma"
                },
                "author": "Chenguang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v1",
                "updated": "2024-09-26T07:44:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Gürkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17374v1",
                "updated": "2024-09-25T21:37:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T21:37:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination"
                },
                "summary": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Arkka Bhattacharyya"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "arxiv_comment": "6 pages, 5 figures, APL Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v1",
                "updated": "2024-09-25T18:21:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Íñigo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17136v1",
                "updated": "2024-09-25T17:55:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:55:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Adaptive Cost Model for Query Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cost Model for Query Optimization"
                },
                "summary": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark."
                },
                "authors": [
                    {
                        "name": "Nikita Vasilenko"
                    },
                    {
                        "name": "Alexander Demin"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68P15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16743v1",
                "updated": "2024-09-25T08:52:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:52:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults"
                },
                "summary": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS)."
                },
                "authors": [
                    {
                        "name": "Naajein Cherat"
                    },
                    {
                        "name": "Vaibhav Nougain"
                    },
                    {
                        "name": "Milovan Majstorović"
                    },
                    {
                        "name": "Peter Palensky"
                    },
                    {
                        "name": "Aleksandra Lekić"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandra Lekić"
                },
                "author": "Aleksandra Lekić",
                "arxiv_journal_ref": "ISGT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v1",
                "updated": "2024-09-25T01:39:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16258v1",
                "updated": "2024-09-24T17:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:28:47Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "title": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time"
                },
                "summary": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore."
                },
                "authors": [
                    {
                        "name": "Antoine Murat"
                    },
                    {
                        "name": "Clément Burgelin"
                    },
                    {
                        "name": "Athanasios Xygkis"
                    },
                    {
                        "name": "Igor Zablotchi"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    }
                ],
                "author_detail": {
                    "name": "Rachid Guerraoui"
                },
                "author": "Rachid Guerraoui",
                "arxiv_doi": "10.1145/3694715.3695945",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3694715.3695945",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of SOSP '24",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.11842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11842v1",
                "updated": "2024-10-15T17:59:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    59,
                    44,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:59:44Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    59,
                    44,
                    1,
                    289,
                    0
                ],
                "title": "MoH: Multi-Head Attention as Mixture-of-Head Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoH: Multi-Head Attention as Mixture-of-Head Attention"
                },
                "summary": "In this work, we upgrade the multi-head attention mechanism, the core of the\nTransformer model, to improve efficiency while maintaining or surpassing the\nprevious accuracy level. We show that multi-head attention can be expressed in\nthe summation form. Drawing on the insight that not all attention heads hold\nequal significance, we propose Mixture-of-Head attention (MoH), a new\narchitecture that treats attention heads as experts in the Mixture-of-Experts\n(MoE) mechanism. MoH has two significant advantages: First, MoH enables each\ntoken to select the appropriate attention heads, enhancing inference efficiency\nwithout compromising accuracy or increasing the number of parameters. Second,\nMoH replaces the standard summation in multi-head attention with a weighted\nsummation, introducing flexibility to the attention mechanism and unlocking\nextra performance potential. Extensive experiments on ViT, DiT, and LLMs\ndemonstrate that MoH outperforms multi-head attention by using only 50%-90% of\nthe attention heads. Moreover, we demonstrate that pre-trained multi-head\nattention models, such as LLaMA3-8B, can be further continue-tuned into our MoH\nmodels. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14\nbenchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the\nattention heads. We believe the proposed MoH is a promising alternative to\nmulti-head attention and provides a strong foundation for developing advanced\nand efficient attention-based models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we upgrade the multi-head attention mechanism, the core of the\nTransformer model, to improve efficiency while maintaining or surpassing the\nprevious accuracy level. We show that multi-head attention can be expressed in\nthe summation form. Drawing on the insight that not all attention heads hold\nequal significance, we propose Mixture-of-Head attention (MoH), a new\narchitecture that treats attention heads as experts in the Mixture-of-Experts\n(MoE) mechanism. MoH has two significant advantages: First, MoH enables each\ntoken to select the appropriate attention heads, enhancing inference efficiency\nwithout compromising accuracy or increasing the number of parameters. Second,\nMoH replaces the standard summation in multi-head attention with a weighted\nsummation, introducing flexibility to the attention mechanism and unlocking\nextra performance potential. Extensive experiments on ViT, DiT, and LLMs\ndemonstrate that MoH outperforms multi-head attention by using only 50%-90% of\nthe attention heads. Moreover, we demonstrate that pre-trained multi-head\nattention models, such as LLaMA3-8B, can be further continue-tuned into our MoH\nmodels. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14\nbenchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the\nattention heads. We believe the proposed MoH is a promising alternative to\nmulti-head attention and provides a strong foundation for developing advanced\nand efficient attention-based models."
                },
                "authors": [
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Bo Zhu"
                    },
                    {
                        "name": "Li Yuan"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "arxiv_comment": "23 pages, code: https://github.com/SkyworkAI/MoH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11841v1",
                "updated": "2024-10-15T17:59:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    59,
                    30,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:59:30Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    59,
                    30,
                    1,
                    289,
                    0
                ],
                "title": "GaVaMoE: Gaussian-Variational Gated Mixture of Experts for Explainable\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaVaMoE: Gaussian-Variational Gated Mixture of Experts for Explainable\n  Recommendation"
                },
                "summary": "Large language model-based explainable recommendation (LLM-based ER) systems\nshow promise in generating human-like explanations for recommendations.\nHowever, they face challenges in modeling user-item collaborative preferences,\npersonalizing explanations, and handling sparse user-item interactions. To\naddress these issues, we propose GaVaMoE, a novel Gaussian-Variational Gated\nMixture of Experts framework for explainable recommendation. GaVaMoE introduces\ntwo key components: (1) a rating reconstruction module that employs Variational\nAutoencoder (VAE) with a Gaussian Mixture Model (GMM) to capture complex\nuser-item collaborative preferences, serving as a pre-trained multi-gating\nmechanism; and (2) a set of fine-grained expert models coupled with the\nmulti-gating mechanism for generating highly personalized explanations. The VAE\ncomponent models latent factors in user-item interactions, while the GMM\nclusters users with similar behaviors. Each cluster corresponds to a gate in\nthe multi-gating mechanism, routing user-item pairs to appropriate expert\nmodels. This architecture enables GaVaMoE to generate tailored explanations for\nspecific user types and preferences, mitigating data sparsity by leveraging\nuser similarities. Extensive experiments on three real-world datasets\ndemonstrate that GaVaMoE significantly outperforms existing methods in\nexplanation quality, personalization, and consistency. Notably, GaVaMoE\nexhibits robust performance in scenarios with sparse user-item interactions,\nmaintaining high-quality explanations even for users with limited historical\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based explainable recommendation (LLM-based ER) systems\nshow promise in generating human-like explanations for recommendations.\nHowever, they face challenges in modeling user-item collaborative preferences,\npersonalizing explanations, and handling sparse user-item interactions. To\naddress these issues, we propose GaVaMoE, a novel Gaussian-Variational Gated\nMixture of Experts framework for explainable recommendation. GaVaMoE introduces\ntwo key components: (1) a rating reconstruction module that employs Variational\nAutoencoder (VAE) with a Gaussian Mixture Model (GMM) to capture complex\nuser-item collaborative preferences, serving as a pre-trained multi-gating\nmechanism; and (2) a set of fine-grained expert models coupled with the\nmulti-gating mechanism for generating highly personalized explanations. The VAE\ncomponent models latent factors in user-item interactions, while the GMM\nclusters users with similar behaviors. Each cluster corresponds to a gate in\nthe multi-gating mechanism, routing user-item pairs to appropriate expert\nmodels. This architecture enables GaVaMoE to generate tailored explanations for\nspecific user types and preferences, mitigating data sparsity by leveraging\nuser similarities. Extensive experiments on three real-world datasets\ndemonstrate that GaVaMoE significantly outperforms existing methods in\nexplanation quality, personalization, and consistency. Notably, GaVaMoE\nexhibits robust performance in scenarios with sparse user-item interactions,\nmaintaining high-quality explanations even for users with limited historical\ndata."
                },
                "authors": [
                    {
                        "name": "Fei Tang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Zeqi Tan"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Guiyang Hou"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11838v1",
                "updated": "2024-10-15T17:59:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    59,
                    4,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:59:04Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    59,
                    4,
                    1,
                    289,
                    0
                ],
                "title": "High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion"
                },
                "summary": "Despite the recent progress, existing frame interpolation methods still\nstruggle with processing extremely high resolution input and handling\nchallenging cases such as repetitive textures, thin objects, and large motion.\nTo address these issues, we introduce a patch-based cascaded pixel diffusion\nmodel for frame interpolation, HiFI, that excels in these scenarios while\nachieving competitive performance on standard benchmarks. Cascades, which\ngenerate a series of images from low- to high-resolution, can help\nsignificantly with large or complex motion that require both global context for\na coarse solution and detailed context for high resolution output. However,\ncontrary to prior work on cascaded diffusion models which perform diffusion on\nincreasingly large resolutions, we use a single model that always performs\ndiffusion at the same resolution and upsamples by processing patches of the\ninputs and the prior solution. We show that this technique drastically reduces\nmemory usage at inference time and also allows us to use a single model at test\ntime, solving both frame interpolation and spatial up-sampling, saving training\ncost. We show that HiFI helps significantly with high resolution and complex\nrepeated textures that require global context. HiFI demonstrates comparable or\nbeyond state-of-the-art performance on multiple benchmarks (Vimeo, Xiph,\nX-Test, SEPE-8K). On our newly introduced dataset that focuses on particularly\nchallenging cases, HiFI also significantly outperforms other baselines on these\ncases. Please visit our project page for video results:\nhttps://hifi-diffusion.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent progress, existing frame interpolation methods still\nstruggle with processing extremely high resolution input and handling\nchallenging cases such as repetitive textures, thin objects, and large motion.\nTo address these issues, we introduce a patch-based cascaded pixel diffusion\nmodel for frame interpolation, HiFI, that excels in these scenarios while\nachieving competitive performance on standard benchmarks. Cascades, which\ngenerate a series of images from low- to high-resolution, can help\nsignificantly with large or complex motion that require both global context for\na coarse solution and detailed context for high resolution output. However,\ncontrary to prior work on cascaded diffusion models which perform diffusion on\nincreasingly large resolutions, we use a single model that always performs\ndiffusion at the same resolution and upsamples by processing patches of the\ninputs and the prior solution. We show that this technique drastically reduces\nmemory usage at inference time and also allows us to use a single model at test\ntime, solving both frame interpolation and spatial up-sampling, saving training\ncost. We show that HiFI helps significantly with high resolution and complex\nrepeated textures that require global context. HiFI demonstrates comparable or\nbeyond state-of-the-art performance on multiple benchmarks (Vimeo, Xiph,\nX-Test, SEPE-8K). On our newly introduced dataset that focuses on particularly\nchallenging cases, HiFI also significantly outperforms other baselines on these\ncases. Please visit our project page for video results:\nhttps://hifi-diffusion.github.io"
                },
                "authors": [
                    {
                        "name": "Junhwa Hur"
                    },
                    {
                        "name": "Charles Herrmann"
                    },
                    {
                        "name": "Saurabh Saxena"
                    },
                    {
                        "name": "Janne Kontkanen"
                    },
                    {
                        "name": "Wei-Sheng Lai"
                    },
                    {
                        "name": "Yichang Shih"
                    },
                    {
                        "name": "Michael Rubinstein"
                    },
                    {
                        "name": "David J. Fleet"
                    },
                    {
                        "name": "Deqing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Sun"
                },
                "author": "Deqing Sun",
                "arxiv_comment": "Project page: https://hifi-diffusion.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10818v2",
                "updated": "2024-10-15T17:55:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    55,
                    46,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for\n  Multimodal Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for\n  Multimodal Video Models"
                },
                "summary": "Understanding fine-grained temporal dynamics is crucial for multimodal video\ncomprehension and generation. Due to the lack of fine-grained temporal\nannotations, existing video benchmarks mostly resemble static image benchmarks\nand are incompetent at evaluating models for temporal understanding. In this\npaper, we introduce TemporalBench, a new benchmark dedicated to evaluating\nfine-grained temporal understanding in videos. TemporalBench consists of ~10K\nvideo question-answer pairs, derived from ~2K high-quality human annotations\ndetailing the temporal dynamics in video clips. As a result, our benchmark\nprovides a unique testbed for evaluating various temporal understanding and\nreasoning abilities such as action frequency, motion magnitude, event order,\netc. Moreover, it enables evaluations on various tasks like both video question\nanswering and captioning, both short and long video understanding, as well as\ndifferent models such as multimodal video embedding models and text generation\nmodels. Results show that state-of-the-art models like GPT-4o achieve only\n38.5% question answering accuracy on TemporalBench, demonstrating a significant\ngap (~30%) between humans and AI in temporal understanding. Furthermore, we\nnotice a critical pitfall for multi-choice QA where LLMs can detect the subtle\nchanges in negative captions and find a centralized description as a cue for\nits prediction, where we propose Multiple Binary Accuracy (MBA) to correct such\nbias. We hope that TemporalBench can foster research on improving models'\ntemporal reasoning capabilities. Both dataset and evaluation code will be made\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding fine-grained temporal dynamics is crucial for multimodal video\ncomprehension and generation. Due to the lack of fine-grained temporal\nannotations, existing video benchmarks mostly resemble static image benchmarks\nand are incompetent at evaluating models for temporal understanding. In this\npaper, we introduce TemporalBench, a new benchmark dedicated to evaluating\nfine-grained temporal understanding in videos. TemporalBench consists of ~10K\nvideo question-answer pairs, derived from ~2K high-quality human annotations\ndetailing the temporal dynamics in video clips. As a result, our benchmark\nprovides a unique testbed for evaluating various temporal understanding and\nreasoning abilities such as action frequency, motion magnitude, event order,\netc. Moreover, it enables evaluations on various tasks like both video question\nanswering and captioning, both short and long video understanding, as well as\ndifferent models such as multimodal video embedding models and text generation\nmodels. Results show that state-of-the-art models like GPT-4o achieve only\n38.5% question answering accuracy on TemporalBench, demonstrating a significant\ngap (~30%) between humans and AI in temporal understanding. Furthermore, we\nnotice a critical pitfall for multi-choice QA where LLMs can detect the subtle\nchanges in negative captions and find a centralized description as a cue for\nits prediction, where we propose Multiple Binary Accuracy (MBA) to correct such\nbias. We hope that TemporalBench can foster research on improving models'\ntemporal reasoning capabilities. Both dataset and evaluation code will be made\navailable."
                },
                "authors": [
                    {
                        "name": "Mu Cai"
                    },
                    {
                        "name": "Reuben Tan"
                    },
                    {
                        "name": "Jianrui Zhang"
                    },
                    {
                        "name": "Bocheng Zou"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Fangrui Zhu"
                    },
                    {
                        "name": "Jing Gu"
                    },
                    {
                        "name": "Yiwu Zhong"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yao Dou"
                    },
                    {
                        "name": "Jaden Park"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Jianwei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yang"
                },
                "author": "Jianwei Yang",
                "arxiv_comment": "Project Page: https://temporalbench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10909v2",
                "updated": "2024-10-15T17:44:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    44,
                    57,
                    1,
                    289,
                    0
                ],
                "published": "2024-07-15T17:09:44Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    9,
                    44,
                    0,
                    197,
                    0
                ],
                "title": "FinDKG: Dynamic Knowledge Graphs with Large Language Models for\n  Detecting Global Trends in Financial Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinDKG: Dynamic Knowledge Graphs with Large Language Models for\n  Detecting Global Trends in Financial Markets"
                },
                "summary": "Dynamic knowledge graphs (DKGs) are popular structures to express different\ntypes of connections between objects over time. They can also serve as an\nefficient mathematical tool to represent information extracted from complex\nunstructured data sources, such as text or images. Within financial\napplications, DKGs could be used to detect trends for strategic thematic\ninvesting, based on information obtained from financial news articles. In this\nwork, we explore the properties of large language models (LLMs) as dynamic\nknowledge graph generators, proposing a novel open-source fine-tuned LLM for\nthis purpose, called the Integrated Contextual Knowledge Graph Generator\n(ICKG). We use ICKG to produce a novel open-source DKG from a corpus of\nfinancial news articles, called FinDKG, and we propose an attention-based GNN\narchitecture for analysing it, called KGTransformer. We test the performance of\nthe proposed model on benchmark datasets and FinDKG, demonstrating superior\nperformance on link prediction tasks. Additionally, we evaluate the performance\nof the KGTransformer on FinDKG for thematic investing, showing it can\noutperform existing thematic ETFs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic knowledge graphs (DKGs) are popular structures to express different\ntypes of connections between objects over time. They can also serve as an\nefficient mathematical tool to represent information extracted from complex\nunstructured data sources, such as text or images. Within financial\napplications, DKGs could be used to detect trends for strategic thematic\ninvesting, based on information obtained from financial news articles. In this\nwork, we explore the properties of large language models (LLMs) as dynamic\nknowledge graph generators, proposing a novel open-source fine-tuned LLM for\nthis purpose, called the Integrated Contextual Knowledge Graph Generator\n(ICKG). We use ICKG to produce a novel open-source DKG from a corpus of\nfinancial news articles, called FinDKG, and we propose an attention-based GNN\narchitecture for analysing it, called KGTransformer. We test the performance of\nthe proposed model on benchmark datasets and FinDKG, demonstrating superior\nperformance on link prediction tasks. Additionally, we evaluate the performance\nof the KGTransformer on FinDKG for thematic investing, showing it can\noutperform existing thematic ETFs."
                },
                "authors": [
                    {
                        "name": "Xiaohui Victor Li"
                    },
                    {
                        "name": "Francesco Sanna Passino"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Sanna Passino"
                },
                "author": "Francesco Sanna Passino",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11815v1",
                "updated": "2024-10-15T17:40:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    40,
                    48,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:40:48Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    40,
                    48,
                    1,
                    289,
                    0
                ],
                "title": "SGEdit: Bridging LLM with Text2Image Generative Model for Scene\n  Graph-based Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEdit: Bridging LLM with Text2Image Generative Model for Scene\n  Graph-based Image Editing"
                },
                "summary": "Scene graphs offer a structured, hierarchical representation of images, with\nnodes and edges symbolizing objects and the relationships among them. It can\nserve as a natural interface for image editing, dramatically improving\nprecision and flexibility. Leveraging this benefit, we introduce a new\nframework that integrates large language model (LLM) with Text2Image generative\nmodel for scene graph-based image editing. This integration enables precise\nmodifications at the object level and creative recomposition of scenes without\ncompromising overall image integrity. Our approach involves two primary stages:\n1) Utilizing a LLM-driven scene parser, we construct an image's scene graph,\ncapturing key objects and their interrelationships, as well as parsing\nfine-grained attributes such as object masks and descriptions. These\nannotations facilitate concept learning with a fine-tuned diffusion model,\nrepresenting each object with an optimized token and detailed description\nprompt. 2) During the image editing phase, a LLM editing controller guides the\nedits towards specific areas. These edits are then implemented by an\nattention-modulated diffusion editor, utilizing the fine-tuned model to perform\nobject additions, deletions, replacements, and adjustments. Through extensive\nexperiments, we demonstrate that our framework significantly outperforms\nexisting image editing methods in terms of editing precision and scene\naesthetics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene graphs offer a structured, hierarchical representation of images, with\nnodes and edges symbolizing objects and the relationships among them. It can\nserve as a natural interface for image editing, dramatically improving\nprecision and flexibility. Leveraging this benefit, we introduce a new\nframework that integrates large language model (LLM) with Text2Image generative\nmodel for scene graph-based image editing. This integration enables precise\nmodifications at the object level and creative recomposition of scenes without\ncompromising overall image integrity. Our approach involves two primary stages:\n1) Utilizing a LLM-driven scene parser, we construct an image's scene graph,\ncapturing key objects and their interrelationships, as well as parsing\nfine-grained attributes such as object masks and descriptions. These\nannotations facilitate concept learning with a fine-tuned diffusion model,\nrepresenting each object with an optimized token and detailed description\nprompt. 2) During the image editing phase, a LLM editing controller guides the\nedits towards specific areas. These edits are then implemented by an\nattention-modulated diffusion editor, utilizing the fine-tuned model to perform\nobject additions, deletions, replacements, and adjustments. Through extensive\nexperiments, we demonstrate that our framework significantly outperforms\nexisting image editing methods in terms of editing precision and scene\naesthetics."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Zhang"
                    },
                    {
                        "name": "DongDong Chen"
                    },
                    {
                        "name": "Jing Liao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liao"
                },
                "author": "Jing Liao",
                "arxiv_comment": "Accepted by ACM Transactions on Graphics and SIGGRAPH Asia 2024.\n  Project page: https://bestzzhang.github.io/SGEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11805v1",
                "updated": "2024-10-15T17:33:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    33,
                    43,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:33:43Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    33,
                    43,
                    1,
                    289,
                    0
                ],
                "title": "NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) combined with tool learning have gained\nimpressive results in real-world applications. During tool learning, LLMs may\ncall multiple tools in nested orders, where the latter tool call may take the\nformer response as its input parameters. However, current research on the\nnested tool learning capabilities is still under-explored, since the existing\nbenchmarks lack of relevant data instances. To address this problem, we\nintroduce NesTools to bridge the current gap in comprehensive nested tool\nlearning evaluations. NesTools comprises a novel automatic data generation\nmethod to construct large-scale nested tool calls with different nesting\nstructures. With manual review and refinement, the dataset is in high quality\nand closely aligned with real-world scenarios. Therefore, NesTools can serve as\na new benchmark to evaluate the nested tool learning abilities of LLMs. We\nconduct extensive experiments on 22 LLMs, and provide in-depth analyses with\nNesTools, which shows that current LLMs still suffer from the complex nested\ntool learning task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) combined with tool learning have gained\nimpressive results in real-world applications. During tool learning, LLMs may\ncall multiple tools in nested orders, where the latter tool call may take the\nformer response as its input parameters. However, current research on the\nnested tool learning capabilities is still under-explored, since the existing\nbenchmarks lack of relevant data instances. To address this problem, we\nintroduce NesTools to bridge the current gap in comprehensive nested tool\nlearning evaluations. NesTools comprises a novel automatic data generation\nmethod to construct large-scale nested tool calls with different nesting\nstructures. With manual review and refinement, the dataset is in high quality\nand closely aligned with real-world scenarios. Therefore, NesTools can serve as\na new benchmark to evaluate the nested tool learning abilities of LLMs. We\nconduct extensive experiments on 22 LLMs, and provide in-depth analyses with\nNesTools, which shows that current LLMs still suffer from the complex nested\ntool learning task."
                },
                "authors": [
                    {
                        "name": "Han Han"
                    },
                    {
                        "name": "Tong Zhu"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Mengsong Wu"
                    },
                    {
                        "name": "Hao Xiong"
                    },
                    {
                        "name": "Wenliang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenliang Chen"
                },
                "author": "Wenliang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08674v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08674v3",
                "updated": "2024-10-15T17:29:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    29,
                    13,
                    1,
                    289,
                    0
                ],
                "published": "2024-02-13T18:55:27Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    18,
                    55,
                    27,
                    1,
                    44,
                    0
                ],
                "title": "Curriculum effects and compositionality emerge with in-context learning\n  in neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curriculum effects and compositionality emerge with in-context learning\n  in neural networks"
                },
                "summary": "Human learning embodies a striking duality: sometimes, we appear capable of\nfollowing logical, compositional rules and benefit from structured curricula\n(e.g., in formal education), while other times, we rely on an incremental\napproach or trial-and-error, learning better from curricula that are\nunstructured or randomly interleaved. Influential psychological theories\nexplain this seemingly disparate behavioral evidence by positing two\nqualitatively different learning systems -- one for rapid, rule-based\ninferences and another for slow, incremental adaptation. It remains unclear how\nto reconcile such theories with neural networks, which learn via incremental\nweight updates and are thus a natural model for the latter type of learning,\nbut are not obviously compatible with the former. However, recent evidence\nsuggests that both metalearning neural networks and large language models are\ncapable of \"in-context learning\" (ICL) -- the ability to flexibly grasp the\nstructure of a new task from a few examples given at inference time. Here, we\nshow that networks capable of ICL can reproduce human-like learning and\ncompositional behavior on rule-governed tasks, while at the same time\nreplicating human behavioral phenomena in tasks lacking rule-like structure via\ntheir usual in-weight learning (IWL). Our work shows how emergent ICL can equip\nneural networks with fundamentally different learning properties than those\ntraditionally attributed to them, and that these can coexist with the\nproperties of their native IWL, thus offering a novel perspective on\ndual-process theories and human cognitive flexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human learning embodies a striking duality: sometimes, we appear capable of\nfollowing logical, compositional rules and benefit from structured curricula\n(e.g., in formal education), while other times, we rely on an incremental\napproach or trial-and-error, learning better from curricula that are\nunstructured or randomly interleaved. Influential psychological theories\nexplain this seemingly disparate behavioral evidence by positing two\nqualitatively different learning systems -- one for rapid, rule-based\ninferences and another for slow, incremental adaptation. It remains unclear how\nto reconcile such theories with neural networks, which learn via incremental\nweight updates and are thus a natural model for the latter type of learning,\nbut are not obviously compatible with the former. However, recent evidence\nsuggests that both metalearning neural networks and large language models are\ncapable of \"in-context learning\" (ICL) -- the ability to flexibly grasp the\nstructure of a new task from a few examples given at inference time. Here, we\nshow that networks capable of ICL can reproduce human-like learning and\ncompositional behavior on rule-governed tasks, while at the same time\nreplicating human behavioral phenomena in tasks lacking rule-like structure via\ntheir usual in-weight learning (IWL). Our work shows how emergent ICL can equip\nneural networks with fundamentally different learning properties than those\ntraditionally attributed to them, and that these can coexist with the\nproperties of their native IWL, thus offering a novel perspective on\ndual-process theories and human cognitive flexibility."
                },
                "authors": [
                    {
                        "name": "Jacob Russin"
                    },
                    {
                        "name": "Ellie Pavlick"
                    },
                    {
                        "name": "Michael J. Frank"
                    }
                ],
                "author_detail": {
                    "name": "Michael J. Frank"
                },
                "author": "Michael J. Frank",
                "arxiv_comment": "27 pages (including appendix), 10 figures, 7 tables. Previous version\n  accepted as a talk + full paper at CogSci 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08674v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08674v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11797v1",
                "updated": "2024-10-15T17:20:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    20,
                    50,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:20:50Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    20,
                    50,
                    1,
                    289,
                    0
                ],
                "title": "Unraveling Heterogeneous Treatment Effects in Networks: A Non-Parametric\n  Approach Based on Node Connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling Heterogeneous Treatment Effects in Networks: A Non-Parametric\n  Approach Based on Node Connectivity"
                },
                "summary": "In network settings, interference between units makes causal inference more\nchallenging as outcomes may depend on the treatments received by others in the\nnetwork. Typical estimands in network settings focus on treatment effects\naggregated across individuals in the population. We propose a framework for\nestimating node-wise counterfactual means, allowing for more granular insights\ninto the impact of network structure on treatment effect heterogeneity. We\ndevelop a doubly robust and non-parametric estimation procedure, KECENI (Kernel\nEstimation of Causal Effect under Network Interference), which offers\nconsistency and asymptotic normality under network dependence. The utility of\nthis method is demonstrated through an application to microfinance data,\nrevealing the impact of network characteristics on treatment effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In network settings, interference between units makes causal inference more\nchallenging as outcomes may depend on the treatments received by others in the\nnetwork. Typical estimands in network settings focus on treatment effects\naggregated across individuals in the population. We propose a framework for\nestimating node-wise counterfactual means, allowing for more granular insights\ninto the impact of network structure on treatment effect heterogeneity. We\ndevelop a doubly robust and non-parametric estimation procedure, KECENI (Kernel\nEstimation of Causal Effect under Network Interference), which offers\nconsistency and asymptotic normality under network dependence. The utility of\nthis method is demonstrated through an application to microfinance data,\nrevealing the impact of network characteristics on treatment effects."
                },
                "authors": [
                    {
                        "name": "Heejong Bong"
                    },
                    {
                        "name": "Colin B. Fogarty"
                    },
                    {
                        "name": "Liza Levina"
                    },
                    {
                        "name": "Ji Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhu"
                },
                "author": "Ji Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11795v1",
                "updated": "2024-10-15T17:19:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    19,
                    46,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:19:46Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    19,
                    46,
                    1,
                    289,
                    0
                ],
                "title": "Efficient Diffusion Models: A Comprehensive Survey from Principles to\n  Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Models: A Comprehensive Survey from Principles to\n  Practices"
                },
                "summary": "As one of the most popular and sought-after generative models in the recent\nyears, diffusion models have sparked the interests of many researchers and\nsteadily shown excellent advantage in various generative tasks such as image\nsynthesis, video generation, molecule design, 3D scene rendering and multimodal\ngeneration, relying on their dense theoretical principles and reliable\napplication practices. The remarkable success of these recent efforts on\ndiffusion models comes largely from progressive design principles and efficient\narchitecture, training, inference, and deployment methodologies. However, there\nhas not been a comprehensive and in-depth review to summarize these principles\nand practices to help the rapid understanding and application of diffusion\nmodels. In this survey, we provide a new efficiency-oriented perspective on\nthese existing efforts, which mainly focuses on the profound principles and\nefficient practices in architecture designs, model training, fast inference and\nreliable deployment, to guide further theoretical research, algorithm migration\nand model application for new scenarios in a reader-friendly way.\n\\url{https://github.com/ponyzym/Efficient-DMs-Survey}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As one of the most popular and sought-after generative models in the recent\nyears, diffusion models have sparked the interests of many researchers and\nsteadily shown excellent advantage in various generative tasks such as image\nsynthesis, video generation, molecule design, 3D scene rendering and multimodal\ngeneration, relying on their dense theoretical principles and reliable\napplication practices. The remarkable success of these recent efforts on\ndiffusion models comes largely from progressive design principles and efficient\narchitecture, training, inference, and deployment methodologies. However, there\nhas not been a comprehensive and in-depth review to summarize these principles\nand practices to help the rapid understanding and application of diffusion\nmodels. In this survey, we provide a new efficiency-oriented perspective on\nthese existing efforts, which mainly focuses on the profound principles and\nefficient practices in architecture designs, model training, fast inference and\nreliable deployment, to guide further theoretical research, algorithm migration\nand model application for new scenarios in a reader-friendly way.\n\\url{https://github.com/ponyzym/Efficient-DMs-Survey}"
                },
                "authors": [
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Yuzhu Zhang"
                    },
                    {
                        "name": "Guoli Jia"
                    },
                    {
                        "name": "Liangliang Zhao"
                    },
                    {
                        "name": "Yichao Ma"
                    },
                    {
                        "name": "Mingjie Ma"
                    },
                    {
                        "name": "Gaofeng Liu"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Jianjun Li"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.15719v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.15719v2",
                "updated": "2024-10-15T17:14:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    14,
                    26,
                    1,
                    289,
                    0
                ],
                "published": "2023-10-24T10:51:50Z",
                "published_parsed": [
                    2023,
                    10,
                    24,
                    10,
                    51,
                    50,
                    1,
                    297,
                    0
                ],
                "title": "AGaLiTe: Approximate Gated Linear Transformers for Online Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGaLiTe: Approximate Gated Linear Transformers for Online Reinforcement\n  Learning"
                },
                "summary": "In this paper we investigate transformer architectures designed for partially\nobservable online reinforcement learning. The self-attention mechanism in the\ntransformer architecture is capable of capturing long-range dependencies and it\nis the main reason behind its effectiveness in processing sequential data.\nNevertheless, despite their success, transformers have two significant\ndrawbacks that still limit their applicability in online reinforcement\nlearning: (1) in order to remember all past information, the self-attention\nmechanism requires access to the whole history to be provided as context. (2)\nThe inference cost in transformers is expensive. In this paper, we introduce\nrecurrent alternatives to the transformer self-attention mechanism that offer\ncontext-independent inference cost, leverage long-range dependencies\neffectively, and performs well in online reinforcement learning task. We\nquantify the impact of the different components of our architecture in a\ndiagnostic environment and assess performance gains in 2D and 3D pixel-based\npartially-observable environments (e.g. T-Maze, Mystery Path, Craftax, and\nMemory Maze). Compared with a state-of-the-art architecture, GTrXL, inference\nin our approach is at least 40% cheaper while reducing memory use more than\n50%. Our approach either performs similarly or better than GTrXL, improving\nmore than 37% upon GTrXL performance in harder tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we investigate transformer architectures designed for partially\nobservable online reinforcement learning. The self-attention mechanism in the\ntransformer architecture is capable of capturing long-range dependencies and it\nis the main reason behind its effectiveness in processing sequential data.\nNevertheless, despite their success, transformers have two significant\ndrawbacks that still limit their applicability in online reinforcement\nlearning: (1) in order to remember all past information, the self-attention\nmechanism requires access to the whole history to be provided as context. (2)\nThe inference cost in transformers is expensive. In this paper, we introduce\nrecurrent alternatives to the transformer self-attention mechanism that offer\ncontext-independent inference cost, leverage long-range dependencies\neffectively, and performs well in online reinforcement learning task. We\nquantify the impact of the different components of our architecture in a\ndiagnostic environment and assess performance gains in 2D and 3D pixel-based\npartially-observable environments (e.g. T-Maze, Mystery Path, Craftax, and\nMemory Maze). Compared with a state-of-the-art architecture, GTrXL, inference\nin our approach is at least 40% cheaper while reducing memory use more than\n50%. Our approach either performs similarly or better than GTrXL, improving\nmore than 37% upon GTrXL performance in harder tasks."
                },
                "authors": [
                    {
                        "name": "Subhojeet Pramanik"
                    },
                    {
                        "name": "Esraa Elelimy"
                    },
                    {
                        "name": "Marlos C. Machado"
                    },
                    {
                        "name": "Adam White"
                    }
                ],
                "author_detail": {
                    "name": "Adam White"
                },
                "author": "Adam White",
                "arxiv_comment": "Published in Transactions on Machine Learning Research",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.15719v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.15719v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.11026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.11026v2",
                "updated": "2024-10-15T17:10:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    10,
                    57,
                    1,
                    289,
                    0
                ],
                "published": "2023-08-21T20:30:35Z",
                "published_parsed": [
                    2023,
                    8,
                    21,
                    20,
                    30,
                    35,
                    0,
                    233,
                    0
                ],
                "title": "Harnessing The Collective Wisdom: Fusion Learning Using Decision\n  Sequences From Diverse Sources",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing The Collective Wisdom: Fusion Learning Using Decision\n  Sequences From Diverse Sources"
                },
                "summary": "Learning from the collective wisdom of crowds is related to the statistical\nnotion of fusion learning from multiple data sources or studies. However,\nfusing inferences from diverse sources is challenging since cross-source\nheterogeneity and potential data-sharing complicate statistical inference.\nMoreover, studies may rely on disparate designs, employ myriad modeling\ntechniques, and prevailing data privacy norms may forbid sharing even summary\nstatistics across the studies for an overall analysis. We propose an\nIntegrative Ranking and Thresholding (IRT) framework for fusion learning in\nmultiple testing. IRT operates under the setting where from each study a\ntriplet is available: the vector of binary accept-reject decisions on the\ntested hypotheses, its False Discovery Rate (FDR) level and the hypotheses\ntested by it. Under this setting, IRT constructs an aggregated and\nnonparametric measure of evidence against each null hypotheses, which\nfacilitates ranking the hypotheses in the order of their likelihood of being\nrejected. We show that IRT guarantees an overall FDR control if the studies\ncontrol their respective FDR at the desired levels. IRT is extremely flexible,\nand a comprehensive numerical study demonstrates its practical relevance for\npooling inferences. A real data illustration and extensions to alternative\nforms of Type I error control are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from the collective wisdom of crowds is related to the statistical\nnotion of fusion learning from multiple data sources or studies. However,\nfusing inferences from diverse sources is challenging since cross-source\nheterogeneity and potential data-sharing complicate statistical inference.\nMoreover, studies may rely on disparate designs, employ myriad modeling\ntechniques, and prevailing data privacy norms may forbid sharing even summary\nstatistics across the studies for an overall analysis. We propose an\nIntegrative Ranking and Thresholding (IRT) framework for fusion learning in\nmultiple testing. IRT operates under the setting where from each study a\ntriplet is available: the vector of binary accept-reject decisions on the\ntested hypotheses, its False Discovery Rate (FDR) level and the hypotheses\ntested by it. Under this setting, IRT constructs an aggregated and\nnonparametric measure of evidence against each null hypotheses, which\nfacilitates ranking the hypotheses in the order of their likelihood of being\nrejected. We show that IRT guarantees an overall FDR control if the studies\ncontrol their respective FDR at the desired levels. IRT is extremely flexible,\nand a comprehensive numerical study demonstrates its practical relevance for\npooling inferences. A real data illustration and extensions to alternative\nforms of Type I error control are discussed."
                },
                "authors": [
                    {
                        "name": "Trambak Banerjee"
                    },
                    {
                        "name": "Bowen Gang"
                    },
                    {
                        "name": "Jianliang He"
                    }
                ],
                "author_detail": {
                    "name": "Jianliang He"
                },
                "author": "Jianliang He",
                "arxiv_comment": "Revised version. Under review since July 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.11026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.11026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03730v2",
                "updated": "2024-10-15T17:09:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    9,
                    40,
                    1,
                    289,
                    0
                ],
                "published": "2024-09-30T16:05:38Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    5,
                    38,
                    0,
                    274,
                    0
                ],
                "title": "Teuken-7B-Base & Teuken-7B-Instruct: Towards European LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teuken-7B-Base & Teuken-7B-Instruct: Towards European LLMs"
                },
                "summary": "We present two multilingual LLMs designed to embrace Europe's linguistic\ndiversity by supporting all 24 official languages of the European Union.\nTrained on a dataset comprising around 60% non-English data and utilizing a\ncustom multilingual tokenizer, our models address the limitations of existing\nLLMs that predominantly focus on English or a few high-resource languages. We\ndetail the models' development principles, i.e., data composition, tokenizer\noptimization, and training methodologies. The models demonstrate competitive\nperformance across multilingual benchmarks, as evidenced by their performance\non European versions of ARC, HellaSwag, MMLU, and TruthfulQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present two multilingual LLMs designed to embrace Europe's linguistic\ndiversity by supporting all 24 official languages of the European Union.\nTrained on a dataset comprising around 60% non-English data and utilizing a\ncustom multilingual tokenizer, our models address the limitations of existing\nLLMs that predominantly focus on English or a few high-resource languages. We\ndetail the models' development principles, i.e., data composition, tokenizer\noptimization, and training methodologies. The models demonstrate competitive\nperformance across multilingual benchmarks, as evidenced by their performance\non European versions of ARC, HellaSwag, MMLU, and TruthfulQA."
                },
                "authors": [
                    {
                        "name": "Mehdi Ali"
                    },
                    {
                        "name": "Michael Fromm"
                    },
                    {
                        "name": "Klaudia Thellmann"
                    },
                    {
                        "name": "Jan Ebert"
                    },
                    {
                        "name": "Alexander Arno Weber"
                    },
                    {
                        "name": "Richard Rutmann"
                    },
                    {
                        "name": "Charvi Jain"
                    },
                    {
                        "name": "Max Lübbering"
                    },
                    {
                        "name": "Daniel Steinigen"
                    },
                    {
                        "name": "Johannes Leveling"
                    },
                    {
                        "name": "Katrin Klug"
                    },
                    {
                        "name": "Jasper Schulze Buschhoff"
                    },
                    {
                        "name": "Lena Jurkschat"
                    },
                    {
                        "name": "Hammam Abdelwahab"
                    },
                    {
                        "name": "Benny Jörg Stein"
                    },
                    {
                        "name": "Karl-Heinz Sylla"
                    },
                    {
                        "name": "Pavel Denisov"
                    },
                    {
                        "name": "Nicolo' Brandizzi"
                    },
                    {
                        "name": "Qasid Saleem"
                    },
                    {
                        "name": "Anirban Bhowmick"
                    },
                    {
                        "name": "Lennard Helmer"
                    },
                    {
                        "name": "Chelsea John"
                    },
                    {
                        "name": "Pedro Ortiz Suarez"
                    },
                    {
                        "name": "Malte Ostendorff"
                    },
                    {
                        "name": "Alex Jude"
                    },
                    {
                        "name": "Lalith Manjunath"
                    },
                    {
                        "name": "Samuel Weinbach"
                    },
                    {
                        "name": "Carolin Penke"
                    },
                    {
                        "name": "Oleg Filatov"
                    },
                    {
                        "name": "Shima Asaadi"
                    },
                    {
                        "name": "Fabio Barth"
                    },
                    {
                        "name": "Rafet Sifa"
                    },
                    {
                        "name": "Fabian Küch"
                    },
                    {
                        "name": "Andreas Herten"
                    },
                    {
                        "name": "René Jäkel"
                    },
                    {
                        "name": "Georg Rehm"
                    },
                    {
                        "name": "Stefan Kesselheim"
                    },
                    {
                        "name": "Joachim Köhler"
                    },
                    {
                        "name": "Nicolas Flores-Herr"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Flores-Herr"
                },
                "author": "Nicolas Flores-Herr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11786v1",
                "updated": "2024-10-15T17:05:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    5,
                    25,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:05:25Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    5,
                    25,
                    1,
                    289,
                    0
                ],
                "title": "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for\n  Faithfulness and Transferability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for\n  Faithfulness and Transferability"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in a\nwide range of natural language processing tasks when leveraging in-context\nlearning. To mitigate the additional computational and financial costs\nassociated with in-context learning, several prompt compression methods have\nbeen proposed to compress the in-context learning prompts. Despite their\nsuccess, these methods face challenges with transferability due to\nmodel-specific compression, or rely on external training data, such as GPT-4.\nIn this paper, we investigate the ability of LLMs to develop a unified\ncompression method that discretizes uninformative tokens, utilizing a\nself-supervised pre-training technique. By introducing a small number of\nparameters during the continual pre-training, the proposed Selection-p produces\na probability for each input token, indicating whether to preserve or discard\nit. Experiments show Selection-p achieves state-of-the-art performance across\nnumerous classification tasks, achieving compression rates of up to 10 times\nwhile experiencing only a marginal 0.8% decrease in performance. Moreover, it\nexhibits superior transferability to different models compared to prior work.\nAdditionally, we further analyze how Selection-p helps maintain performance on\nin-context learning with long contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in a\nwide range of natural language processing tasks when leveraging in-context\nlearning. To mitigate the additional computational and financial costs\nassociated with in-context learning, several prompt compression methods have\nbeen proposed to compress the in-context learning prompts. Despite their\nsuccess, these methods face challenges with transferability due to\nmodel-specific compression, or rely on external training data, such as GPT-4.\nIn this paper, we investigate the ability of LLMs to develop a unified\ncompression method that discretizes uninformative tokens, utilizing a\nself-supervised pre-training technique. By introducing a small number of\nparameters during the continual pre-training, the proposed Selection-p produces\na probability for each input token, indicating whether to preserve or discard\nit. Experiments show Selection-p achieves state-of-the-art performance across\nnumerous classification tasks, achieving compression rates of up to 10 times\nwhile experiencing only a marginal 0.8% decrease in performance. Moreover, it\nexhibits superior transferability to different models compared to prior work.\nAdditionally, we further analyze how Selection-p helps maintain performance on\nin-context learning with long contexts."
                },
                "authors": [
                    {
                        "name": "Tsz Ting Chung"
                    },
                    {
                        "name": "Leyang Cui"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Shuming Shi"
                    },
                    {
                        "name": "Dit-Yan Yeung"
                    }
                ],
                "author_detail": {
                    "name": "Dit-Yan Yeung"
                },
                "author": "Dit-Yan Yeung",
                "arxiv_comment": "14 pages, 5 figures, 10 tables, EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13348v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13348v3",
                "updated": "2024-10-15T17:04:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    4,
                    51,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-20T11:07:29Z",
                "published_parsed": [
                    2024,
                    4,
                    20,
                    11,
                    7,
                    29,
                    5,
                    111,
                    0
                ],
                "title": "Socialized Learning: A Survey of the Paradigm Shift for Edge\n  Intelligence in Networked Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Socialized Learning: A Survey of the Paradigm Shift for Edge\n  Intelligence in Networked Systems"
                },
                "summary": "Amidst the robust impetus from artificial intelligence (AI) and big data,\nedge intelligence (EI) has emerged as a nascent computing paradigm,\nsynthesizing AI with edge computing (EC) to become an exemplary solution for\nunleashing the full potential of AI services. Nonetheless, challenges in\ncommunication costs, resource allocation, privacy, and security continue to\nconstrain its proficiency in supporting services with diverse requirements. In\nresponse to these issues, this paper introduces socialized learning (SL) as a\npromising solution, further propelling the advancement of EI. SL is a learning\nparadigm predicated on social principles and behaviors, aimed at amplifying the\ncollaborative capacity and collective intelligence of agents within the EI\nsystem. SL not only enhances the system's adaptability but also optimizes\ncommunication, and networking processes, essential for distributed intelligence\nacross diverse devices and platforms. Therefore, a combination of SL and EI may\ngreatly facilitate the development of collaborative intelligence in the future\nnetwork. This paper presents the findings of a literature review on the\nintegration of EI and SL, summarizing the latest achievements in existing\nresearch on EI and SL. Subsequently, we delve comprehensively into the\nlimitations of EI and how it could benefit from SL. Special emphasis is placed\non the communication challenges and networking strategies and other aspects\nwithin these systems, underlining the role of optimized network solutions in\nimproving system efficiency. Based on these discussions, we elaborate in detail\non three integrated components: socialized architecture, socialized training,\nand socialized inference, analyzing their strengths and weaknesses. Finally, we\nidentify some possible future applications of combining SL and EI, discuss open\nproblems and suggest some future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amidst the robust impetus from artificial intelligence (AI) and big data,\nedge intelligence (EI) has emerged as a nascent computing paradigm,\nsynthesizing AI with edge computing (EC) to become an exemplary solution for\nunleashing the full potential of AI services. Nonetheless, challenges in\ncommunication costs, resource allocation, privacy, and security continue to\nconstrain its proficiency in supporting services with diverse requirements. In\nresponse to these issues, this paper introduces socialized learning (SL) as a\npromising solution, further propelling the advancement of EI. SL is a learning\nparadigm predicated on social principles and behaviors, aimed at amplifying the\ncollaborative capacity and collective intelligence of agents within the EI\nsystem. SL not only enhances the system's adaptability but also optimizes\ncommunication, and networking processes, essential for distributed intelligence\nacross diverse devices and platforms. Therefore, a combination of SL and EI may\ngreatly facilitate the development of collaborative intelligence in the future\nnetwork. This paper presents the findings of a literature review on the\nintegration of EI and SL, summarizing the latest achievements in existing\nresearch on EI and SL. Subsequently, we delve comprehensively into the\nlimitations of EI and how it could benefit from SL. Special emphasis is placed\non the communication challenges and networking strategies and other aspects\nwithin these systems, underlining the role of optimized network solutions in\nimproving system efficiency. Based on these discussions, we elaborate in detail\non three integrated components: socialized architecture, socialized training,\nand socialized inference, analyzing their strengths and weaknesses. Finally, we\nidentify some possible future applications of combining SL and EI, discuss open\nproblems and suggest some future research."
                },
                "authors": [
                    {
                        "name": "Xiaofei Wang"
                    },
                    {
                        "name": "Yunfeng Zhao"
                    },
                    {
                        "name": "Chao Qiu"
                    },
                    {
                        "name": "Qinghua Hu"
                    },
                    {
                        "name": "Victor C. M. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Victor C. M. Leung"
                },
                "author": "Victor C. M. Leung",
                "arxiv_comment": "This paper has been accepted by IEEE Communications Surveys and\n  Tutorials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13348v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13348v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.10792v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.10792v2",
                "updated": "2024-10-15T17:04:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    4,
                    35,
                    1,
                    289,
                    0
                ],
                "published": "2023-09-19T17:40:26Z",
                "published_parsed": [
                    2023,
                    9,
                    19,
                    17,
                    40,
                    26,
                    1,
                    262,
                    0
                ],
                "title": "Frequentist Inference for Semi-mechanistic Epidemic Models with\n  Interventions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequentist Inference for Semi-mechanistic Epidemic Models with\n  Interventions"
                },
                "summary": "The effect of public health interventions on an epidemic are often estimated\nby adding the intervention to epidemic models. During the Covid-19 epidemic,\nnumerous papers used such methods for making scenario predictions. The majority\nof these papers use Bayesian methods to estimate the parameters of the model.\nIn this paper we show how to use frequentist methods for estimating these\neffects which avoids having to specify prior distributions. We also use\nmodel-free shrinkage methods to improve estimation when there are many\ndifferent geographic regions. This allows us to borrow strength from different\nregions while still getting confidence intervals with correct coverage and\nwithout having to specify a hierarchical model. Throughout, we focus on a\nsemi-mechanistic model which provides a simple, tractable alternative to\ncompartmental methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effect of public health interventions on an epidemic are often estimated\nby adding the intervention to epidemic models. During the Covid-19 epidemic,\nnumerous papers used such methods for making scenario predictions. The majority\nof these papers use Bayesian methods to estimate the parameters of the model.\nIn this paper we show how to use frequentist methods for estimating these\neffects which avoids having to specify prior distributions. We also use\nmodel-free shrinkage methods to improve estimation when there are many\ndifferent geographic regions. This allows us to borrow strength from different\nregions while still getting confidence intervals with correct coverage and\nwithout having to specify a hierarchical model. Throughout, we focus on a\nsemi-mechanistic model which provides a simple, tractable alternative to\ncompartmental methods."
                },
                "authors": [
                    {
                        "name": "Heejong Bong"
                    },
                    {
                        "name": "Valérie Ventura"
                    },
                    {
                        "name": "Larry Wasserman"
                    }
                ],
                "author_detail": {
                    "name": "Larry Wasserman"
                },
                "author": "Larry Wasserman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.10792v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.10792v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.09969v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.09969v3",
                "updated": "2024-10-15T17:04:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    4,
                    20,
                    1,
                    289,
                    0
                ],
                "published": "2023-09-18T17:50:17Z",
                "published_parsed": [
                    2023,
                    9,
                    18,
                    17,
                    50,
                    17,
                    0,
                    261,
                    0
                ],
                "title": "Prompt a Robot to Walk with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt a Robot to Walk with Large Language Models"
                },
                "summary": "Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ ."
                },
                "authors": [
                    {
                        "name": "Yen-Jen Wang"
                    },
                    {
                        "name": "Bike Zhang"
                    },
                    {
                        "name": "Jianyu Chen"
                    },
                    {
                        "name": "Koushil Sreenath"
                    }
                ],
                "author_detail": {
                    "name": "Koushil Sreenath"
                },
                "author": "Koushil Sreenath",
                "arxiv_comment": "Conference on Decision and Control (CDC), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.09969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.09969v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11783v1",
                "updated": "2024-10-15T17:02:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    2,
                    32,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:02:32Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    2,
                    32,
                    1,
                    289,
                    0
                ],
                "title": "Latent BKI: Open-Dictionary Continuous Mapping in Visual-Language Latent\n  Spaces with Quantifiable Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent BKI: Open-Dictionary Continuous Mapping in Visual-Language Latent\n  Spaces with Quantifiable Uncertainty"
                },
                "summary": "This paper introduces a novel probabilistic mapping algorithm, Latent BKI,\nwhich enables open-vocabulary mapping with quantifiable uncertainty.\nTraditionally, semantic mapping algorithms focus on a fixed set of semantic\ncategories which limits their applicability for complex robotic tasks.\nVision-Language (VL) models have recently emerged as a technique to jointly\nmodel language and visual features in a latent space, enabling semantic\nrecognition beyond a predefined, fixed set of semantic classes. Latent BKI\nrecurrently incorporates neural embeddings from VL models into a voxel map with\nquantifiable uncertainty, leveraging the spatial correlations of nearby\nobservations through Bayesian Kernel Inference (BKI). Latent BKI is evaluated\nagainst similar explicit semantic mapping and VL mapping frameworks on the\npopular MatterPort-3D and Semantic KITTI data sets, demonstrating that Latent\nBKI maintains the probabilistic benefits of continuous mapping with the\nadditional benefit of open-dictionary queries. Real-world experiments\ndemonstrate applicability to challenging indoor environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel probabilistic mapping algorithm, Latent BKI,\nwhich enables open-vocabulary mapping with quantifiable uncertainty.\nTraditionally, semantic mapping algorithms focus on a fixed set of semantic\ncategories which limits their applicability for complex robotic tasks.\nVision-Language (VL) models have recently emerged as a technique to jointly\nmodel language and visual features in a latent space, enabling semantic\nrecognition beyond a predefined, fixed set of semantic classes. Latent BKI\nrecurrently incorporates neural embeddings from VL models into a voxel map with\nquantifiable uncertainty, leveraging the spatial correlations of nearby\nobservations through Bayesian Kernel Inference (BKI). Latent BKI is evaluated\nagainst similar explicit semantic mapping and VL mapping frameworks on the\npopular MatterPort-3D and Semantic KITTI data sets, demonstrating that Latent\nBKI maintains the probabilistic benefits of continuous mapping with the\nadditional benefit of open-dictionary queries. Real-world experiments\ndemonstrate applicability to challenging indoor environments."
                },
                "authors": [
                    {
                        "name": "Joey Wilson"
                    },
                    {
                        "name": "Ruihan Xu"
                    },
                    {
                        "name": "Yile Sun"
                    },
                    {
                        "name": "Parker Ewen"
                    },
                    {
                        "name": "Minghan Zhu"
                    },
                    {
                        "name": "Kira Barton"
                    },
                    {
                        "name": "Maani Ghaffari"
                    }
                ],
                "author_detail": {
                    "name": "Maani Ghaffari"
                },
                "author": "Maani Ghaffari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11782v1",
                "updated": "2024-10-15T17:01:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    1,
                    21,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:01:21Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    1,
                    21,
                    1,
                    289,
                    0
                ],
                "title": "G-Designer: Architecting Multi-agent Communication Topologies via Graph\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-Designer: Architecting Multi-agent Communication Topologies via Graph\n  Neural Networks"
                },
                "summary": "Recent advancements in large language model (LLM)-based agents have\ndemonstrated that collective intelligence can significantly surpass the\ncapabilities of individual agents, primarily due to well-crafted inter-agent\ncommunication topologies. Despite the diverse and high-performing designs\navailable, practitioners often face confusion when selecting the most effective\npipeline for their specific task: \\textit{Which topology is the best choice for\nmy task, avoiding unnecessary communication token overhead while ensuring\nhigh-quality solution?} In response to this dilemma, we introduce G-Designer,\nan adaptive, efficient, and robust solution for multi-agent deployment, which\ndynamically designs task-aware, customized communication topologies.\nSpecifically, G-Designer models the multi-agent system as a multi-agent\nnetwork, leveraging a variational graph auto-encoder to encode both the nodes\n(agents) and a task-specific virtual node, and decodes a task-adaptive and\nhigh-performing communication topology. Extensive experiments on six benchmarks\nshowcase that G-Designer is: \\textbf{(1) high-performing}, achieving superior\nresults on MMLU with accuracy at $84.50\\%$ and on HumanEval with pass@1 at\n$89.90\\%$; \\textbf{(2) task-adaptive}, architecting communication protocols\ntailored to task difficulty, reducing token consumption by up to $95.33\\%$ on\nHumanEval; and \\textbf{(3) adversarially robust}, defending against agent\nadversarial attacks with merely $0.3\\%$ accuracy drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language model (LLM)-based agents have\ndemonstrated that collective intelligence can significantly surpass the\ncapabilities of individual agents, primarily due to well-crafted inter-agent\ncommunication topologies. Despite the diverse and high-performing designs\navailable, practitioners often face confusion when selecting the most effective\npipeline for their specific task: \\textit{Which topology is the best choice for\nmy task, avoiding unnecessary communication token overhead while ensuring\nhigh-quality solution?} In response to this dilemma, we introduce G-Designer,\nan adaptive, efficient, and robust solution for multi-agent deployment, which\ndynamically designs task-aware, customized communication topologies.\nSpecifically, G-Designer models the multi-agent system as a multi-agent\nnetwork, leveraging a variational graph auto-encoder to encode both the nodes\n(agents) and a task-specific virtual node, and decodes a task-adaptive and\nhigh-performing communication topology. Extensive experiments on six benchmarks\nshowcase that G-Designer is: \\textbf{(1) high-performing}, achieving superior\nresults on MMLU with accuracy at $84.50\\%$ and on HumanEval with pass@1 at\n$89.90\\%$; \\textbf{(2) task-adaptive}, architecting communication protocols\ntailored to task difficulty, reducing token consumption by up to $95.33\\%$ on\nHumanEval; and \\textbf{(3) adversarially robust}, defending against agent\nadversarial attacks with merely $0.3\\%$ accuracy drop."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Yanwei Yue"
                    },
                    {
                        "name": "Xiangguo Sun"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Miao Yu"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Dawei Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Cheng"
                },
                "author": "Dawei Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11781v1",
                "updated": "2024-10-15T17:00:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    0,
                    15,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:00:15Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    0,
                    15,
                    1,
                    289,
                    0
                ],
                "title": "Language Models Encode Numbers Using Digit Representations in Base 10",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Encode Numbers Using Digit Representations in Base 10"
                },
                "summary": "Large language models (LLMs) frequently make errors when handling even simple\nnumerical problems, such as comparing two small numbers. A natural hypothesis\nis that these errors stem from how LLMs represent numbers, and specifically,\nwhether their representations of numbers capture their numeric values. We\ntackle this question from the observation that LLM errors on numerical tasks\nare often distributed across \\textit{the digits} of the answer rather than\nnormally around \\textit{its numeric value}. Through a series of probing\nexperiments and causal interventions, we show that LLMs internally represent\nnumbers with individual circular representations per-digit in base 10. This\ndigit-wise representation, as opposed to a value representation, sheds light on\nthe error patterns of models on tasks involving numerical reasoning and could\nserve as a basis for future studies on analyzing numerical mechanisms in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently make errors when handling even simple\nnumerical problems, such as comparing two small numbers. A natural hypothesis\nis that these errors stem from how LLMs represent numbers, and specifically,\nwhether their representations of numbers capture their numeric values. We\ntackle this question from the observation that LLM errors on numerical tasks\nare often distributed across \\textit{the digits} of the answer rather than\nnormally around \\textit{its numeric value}. Through a series of probing\nexperiments and causal interventions, we show that LLMs internally represent\nnumbers with individual circular representations per-digit in base 10. This\ndigit-wise representation, as opposed to a value representation, sheds light on\nthe error patterns of models on tasks involving numerical reasoning and could\nserve as a basis for future studies on analyzing numerical mechanisms in LLMs."
                },
                "authors": [
                    {
                        "name": "Amit Arnold Levy"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11778v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11778v1",
                "updated": "2024-10-15T16:57:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    57,
                    14,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T16:57:14Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    57,
                    14,
                    1,
                    289,
                    0
                ],
                "title": "On the Training Convergence of Transformers for In-Context\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Training Convergence of Transformers for In-Context\n  Classification"
                },
                "summary": "While transformers have demonstrated impressive capacities for in-context\nlearning (ICL) in practice, theoretical understanding of the underlying\nmechanism enabling transformers to perform ICL is still in its infant stage.\nThis work aims to theoretically study the training dynamics of transformers for\nin-context classification tasks. We demonstrate that, for in-context\nclassification of Gaussian mixtures under certain assumptions, a single-layer\ntransformer trained via gradient descent converges to a globally optimal model\nat a linear rate. We further quantify the impact of the training and testing\nprompt lengths on the ICL inference error of the trained transformer. We show\nthat when the lengths of training and testing prompts are sufficiently large,\nthe prediction of the trained transformer approaches the Bayes-optimal\nclassifier. Experimental results corroborate the theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While transformers have demonstrated impressive capacities for in-context\nlearning (ICL) in practice, theoretical understanding of the underlying\nmechanism enabling transformers to perform ICL is still in its infant stage.\nThis work aims to theoretically study the training dynamics of transformers for\nin-context classification tasks. We demonstrate that, for in-context\nclassification of Gaussian mixtures under certain assumptions, a single-layer\ntransformer trained via gradient descent converges to a globally optimal model\nat a linear rate. We further quantify the impact of the training and testing\nprompt lengths on the ICL inference error of the trained transformer. We show\nthat when the lengths of training and testing prompts are sufficiently large,\nthe prediction of the trained transformer approaches the Bayes-optimal\nclassifier. Experimental results corroborate the theoretical findings."
                },
                "authors": [
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Ruida Zhou"
                    },
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Cong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Cong Shen"
                },
                "author": "Cong Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11778v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11778v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11774v1",
                "updated": "2024-10-15T16:55:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    55,
                    10,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T16:55:10Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    55,
                    10,
                    1,
                    289,
                    0
                ],
                "title": "Fractal Calibration for long-tailed object detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fractal Calibration for long-tailed object detection"
                },
                "summary": "Real-world datasets follow an imbalanced distribution, which poses\nsignificant challenges in rare-category object detection. Recent studies tackle\nthis problem by developing re-weighting and re-sampling methods, that utilise\nthe class frequencies of the dataset. However, these techniques focus solely on\nthe frequency statistics and ignore the distribution of the classes in image\nspace, missing important information. In contrast to them, we propose FRActal\nCALibration (FRACAL): a novel post-calibration method for long-tailed object\ndetection. FRACAL devises a logit adjustment method that utilises the fractal\ndimension to estimate how uniformly classes are distributed in image space.\nDuring inference, it uses the fractal dimension to inversely downweight the\nprobabilities of uniformly spaced class predictions achieving balance in two\naxes: between frequent and rare categories, and between uniformly spaced and\nsparsely spaced classes. FRACAL is a post-processing method and it does not\nrequire any training, also it can be combined with many off-the-shelf models\nsuch as one-stage sigmoid detectors and two-stage instance segmentation models.\nFRACAL boosts the rare class performance by up to 8.6% and surpasses all\nprevious methods on LVIS dataset, while showing good generalisation to other\ndatasets such as COCO, V3Det and OpenImages. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world datasets follow an imbalanced distribution, which poses\nsignificant challenges in rare-category object detection. Recent studies tackle\nthis problem by developing re-weighting and re-sampling methods, that utilise\nthe class frequencies of the dataset. However, these techniques focus solely on\nthe frequency statistics and ignore the distribution of the classes in image\nspace, missing important information. In contrast to them, we propose FRActal\nCALibration (FRACAL): a novel post-calibration method for long-tailed object\ndetection. FRACAL devises a logit adjustment method that utilises the fractal\ndimension to estimate how uniformly classes are distributed in image space.\nDuring inference, it uses the fractal dimension to inversely downweight the\nprobabilities of uniformly spaced class predictions achieving balance in two\naxes: between frequent and rare categories, and between uniformly spaced and\nsparsely spaced classes. FRACAL is a post-processing method and it does not\nrequire any training, also it can be combined with many off-the-shelf models\nsuch as one-stage sigmoid detectors and two-stage instance segmentation models.\nFRACAL boosts the rare class performance by up to 8.6% and surpasses all\nprevious methods on LVIS dataset, while showing good generalisation to other\ndatasets such as COCO, V3Det and OpenImages. The code will be released."
                },
                "authors": [
                    {
                        "name": "Konstantinos Panagiotis Alexandridis"
                    },
                    {
                        "name": "Ismail Elezi"
                    },
                    {
                        "name": "Jiankang Deng"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Shan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Shan Luo"
                },
                "author": "Shan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11772v1",
                "updated": "2024-10-15T16:53:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    53,
                    26,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T16:53:26Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    53,
                    26,
                    1,
                    289,
                    0
                ],
                "title": "Layer-wise Importance Matters: Less Memory for Better Performance in\n  Parameter-efficient Fine-tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-wise Importance Matters: Less Memory for Better Performance in\n  Parameter-efficient Fine-tuning of Large Language Models"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant\npopularity for adapting pre-trained Large Language Models (LLMs) to downstream\ntasks, primarily due to their potential to significantly reduce memory and\ncomputational overheads. However, a common limitation in most PEFT approaches\nis their application of a uniform architectural design across all layers. This\nuniformity involves identical trainable modules and ignores the varying\nimportance of each layer, leading to sub-optimal fine-tuning results. To\novercome the above limitation and obtain better performance, we develop a novel\napproach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent\nsparsity and select the most important subset of full layers with effective\nlayer-wise importance scoring. The proposed IST is a versatile and\nplug-and-play technique compatible with various PEFT methods that operate on a\nper-layer basis. By leveraging the estimated importance scores, IST dynamically\nupdates these selected layers in PEFT modules, leading to reduced memory\ndemands. We further provide theoretical proof of convergence and empirical\nevidence of superior performance to demonstrate the advantages of IST over\nuniform updating strategies. Extensive experiments on a range of LLMs, PEFTs,\nand downstream tasks substantiate the effectiveness of our proposed method,\nshowcasing IST's capacity to enhance existing layer-based PEFT methods. Our\ncode is available at https://github.com/Kaiseem/IST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant\npopularity for adapting pre-trained Large Language Models (LLMs) to downstream\ntasks, primarily due to their potential to significantly reduce memory and\ncomputational overheads. However, a common limitation in most PEFT approaches\nis their application of a uniform architectural design across all layers. This\nuniformity involves identical trainable modules and ignores the varying\nimportance of each layer, leading to sub-optimal fine-tuning results. To\novercome the above limitation and obtain better performance, we develop a novel\napproach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent\nsparsity and select the most important subset of full layers with effective\nlayer-wise importance scoring. The proposed IST is a versatile and\nplug-and-play technique compatible with various PEFT methods that operate on a\nper-layer basis. By leveraging the estimated importance scores, IST dynamically\nupdates these selected layers in PEFT modules, leading to reduced memory\ndemands. We further provide theoretical proof of convergence and empirical\nevidence of superior performance to demonstrate the advantages of IST over\nuniform updating strategies. Extensive experiments on a range of LLMs, PEFTs,\nand downstream tasks substantiate the effectiveness of our proposed method,\nshowcasing IST's capacity to enhance existing layer-based PEFT methods. Our\ncode is available at https://github.com/Kaiseem/IST."
                },
                "authors": [
                    {
                        "name": "Kai Yao"
                    },
                    {
                        "name": "Penlei Gao"
                    },
                    {
                        "name": "Lichun Li"
                    },
                    {
                        "name": "Yuan Zhao"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Jianke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jianke Zhu"
                },
                "author": "Jianke Zhu",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13198v2",
                "updated": "2024-10-15T16:35:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    35,
                    56,
                    1,
                    289,
                    0
                ],
                "published": "2024-03-19T23:18:40Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    23,
                    18,
                    40,
                    1,
                    79,
                    0
                ],
                "title": "LAP, Using Action Feasibility for Improved Uncertainty Alignment of\n  Large Language Model Planners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAP, Using Action Feasibility for Improved Uncertainty Alignment of\n  Large Language Model Planners"
                },
                "summary": "Large language models (LLMs) showcase many desirable traits for intelligent\nand helpful robots. However, they are also known to hallucinate predictions.\nThis issue is exacerbated in robotics where LLM hallucinations may result in\nrobots confidently executing plans that are contrary to user goals, relying\nmore frequently on human assistance, or preventing the robot from asking for\nhelp at all. In this work, we present LAP, a novel approach for utilizing\noff-the-shelf LLMs, alongside a novel Action feasibility metric, in robotic\nPlanners that minimize harmful hallucinations and human intervention. Our key\nfinding is that calculating and leveraging a new metric, which we call\nA-Feasibility, a measure of whether a given action is possible and safe in the\nprovided scene, helps to mitigate hallucinations in LLM predictions and better\nalign the LLM's confidence measure with the probability of success. We\nspecifically propose an A-Feasibility metric which both combines scene context\nand prompting a LLM to determine if a given action is possible and safe in the\nscene, using the LLM's response to compute the score. Through experiments in\nboth simulation and the real world on tasks with a variety of ambiguities, we\nshow that LAP significantly increases success rate and decreases the amount of\nhuman intervention required relative to prior art. For example, in our\nreal-world testing paradigm, LAP decreases the human help rate of previous\nmethods by over 33% at a success rate of 70%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) showcase many desirable traits for intelligent\nand helpful robots. However, they are also known to hallucinate predictions.\nThis issue is exacerbated in robotics where LLM hallucinations may result in\nrobots confidently executing plans that are contrary to user goals, relying\nmore frequently on human assistance, or preventing the robot from asking for\nhelp at all. In this work, we present LAP, a novel approach for utilizing\noff-the-shelf LLMs, alongside a novel Action feasibility metric, in robotic\nPlanners that minimize harmful hallucinations and human intervention. Our key\nfinding is that calculating and leveraging a new metric, which we call\nA-Feasibility, a measure of whether a given action is possible and safe in the\nprovided scene, helps to mitigate hallucinations in LLM predictions and better\nalign the LLM's confidence measure with the probability of success. We\nspecifically propose an A-Feasibility metric which both combines scene context\nand prompting a LLM to determine if a given action is possible and safe in the\nscene, using the LLM's response to compute the score. Through experiments in\nboth simulation and the real world on tasks with a variety of ambiguities, we\nshow that LAP significantly increases success rate and decreases the amount of\nhuman intervention required relative to prior art. For example, in our\nreal-world testing paradigm, LAP decreases the human help rate of previous\nmethods by over 33% at a success rate of 70%."
                },
                "authors": [
                    {
                        "name": "James F. Mullen Jr."
                    },
                    {
                        "name": "Dinesh Manocha"
                    }
                ],
                "author_detail": {
                    "name": "Dinesh Manocha"
                },
                "author": "Dinesh Manocha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09043v2",
                "updated": "2024-10-15T16:29:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    29,
                    55,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-11T17:57:16Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    57,
                    16,
                    4,
                    285,
                    0
                ],
                "title": "Transforming In-Vehicle Network Intrusion Detection: VAE-based Knowledge\n  Distillation Meets Explainable AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming In-Vehicle Network Intrusion Detection: VAE-based Knowledge\n  Distillation Meets Explainable AI"
                },
                "summary": "In the evolving landscape of autonomous vehicles, ensuring robust in-vehicle\nnetwork (IVN) security is paramount. This paper introduces an advanced\nintrusion detection system (IDS) called KD-XVAE that uses a Variational\nAutoencoder (VAE)-based knowledge distillation approach to enhance both\nperformance and efficiency. Our model significantly reduces complexity,\noperating with just 1669 parameters and achieving an inference time of 0.3 ms\nper batch, making it highly suitable for resource-constrained automotive\nenvironments. Evaluations in the HCRL Car-Hacking dataset demonstrate\nexceptional capabilities, attaining perfect scores (Recall, Precision, F1 Score\nof 100%, and FNR of 0%) under multiple attack types, including DoS, Fuzzing,\nGear Spoofing, and RPM Spoofing. Comparative analysis on the CICIoV2024 dataset\nfurther underscores its superiority over traditional machine learning models,\nachieving perfect detection metrics. We furthermore integrate Explainable AI\n(XAI) techniques to ensure transparency in the model's decisions. The VAE\ncompresses the original feature space into a latent space, on which the\ndistilled model is trained. SHAP(SHapley Additive exPlanations) values provide\ninsights into the importance of each latent dimension, mapped back to original\nfeatures for intuitive understanding. Our paper advances the field by\nintegrating state-of-the-art techniques, addressing critical challenges in the\ndeployment of efficient, trustworthy, and reliable IDSes for autonomous\nvehicles, ensuring enhanced protection against emerging cyber threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolving landscape of autonomous vehicles, ensuring robust in-vehicle\nnetwork (IVN) security is paramount. This paper introduces an advanced\nintrusion detection system (IDS) called KD-XVAE that uses a Variational\nAutoencoder (VAE)-based knowledge distillation approach to enhance both\nperformance and efficiency. Our model significantly reduces complexity,\noperating with just 1669 parameters and achieving an inference time of 0.3 ms\nper batch, making it highly suitable for resource-constrained automotive\nenvironments. Evaluations in the HCRL Car-Hacking dataset demonstrate\nexceptional capabilities, attaining perfect scores (Recall, Precision, F1 Score\nof 100%, and FNR of 0%) under multiple attack types, including DoS, Fuzzing,\nGear Spoofing, and RPM Spoofing. Comparative analysis on the CICIoV2024 dataset\nfurther underscores its superiority over traditional machine learning models,\nachieving perfect detection metrics. We furthermore integrate Explainable AI\n(XAI) techniques to ensure transparency in the model's decisions. The VAE\ncompresses the original feature space into a latent space, on which the\ndistilled model is trained. SHAP(SHapley Additive exPlanations) values provide\ninsights into the importance of each latent dimension, mapped back to original\nfeatures for intuitive understanding. Our paper advances the field by\nintegrating state-of-the-art techniques, addressing critical challenges in the\ndeployment of efficient, trustworthy, and reliable IDSes for autonomous\nvehicles, ensuring enhanced protection against emerging cyber threats."
                },
                "authors": [
                    {
                        "name": "Muhammet Anil Yagiz"
                    },
                    {
                        "name": "Pedram MohajerAnsari"
                    },
                    {
                        "name": "Mert D. Pese"
                    },
                    {
                        "name": "Polat Goktas"
                    }
                ],
                "author_detail": {
                    "name": "Polat Goktas"
                },
                "author": "Polat Goktas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.03908v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.03908v2",
                "updated": "2024-10-15T16:27:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    27,
                    40,
                    1,
                    289,
                    0
                ],
                "published": "2022-08-08T04:08:58Z",
                "published_parsed": [
                    2022,
                    8,
                    8,
                    4,
                    8,
                    58,
                    0,
                    220,
                    0
                ],
                "title": "Do financial regulators act in the public's interest? A Bayesian latent\n  class estimation framework for assessing regulatory responses to banking\n  crises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do financial regulators act in the public's interest? A Bayesian latent\n  class estimation framework for assessing regulatory responses to banking\n  crises"
                },
                "summary": "When banks fail amidst financial crises, the public criticizes regulators for\nbailing out or liquidating specific banks, especially the ones that gain\nattention due to their size or dominance. A comprehensive assessment of\nregulators, however, requires examining all their decisions, and not just\nspecific ones, against the regulator's dual objective of preserving financial\nstability while discouraging moral hazard. In this article, we develop a\nBayesian latent class estimation framework to assess regulators on these\ncompeting objectives and evaluate their decisions against resolution rules\nrecommended by theoretical studies of bank behavior designed to contain moral\nhazard incentives. The proposed estimation framework addresses the unobserved\nheterogeneity underlying regulator's decisions in resolving failed banks and\nprovides a disciplined statistical approach for inferring if they acted in the\npublic interest. Our results reveal that during the crises of 1980's, the U.S.\nbanking regulator's resolution decisions were consistent with recommended\ndecision rules, while the U.S. savings and loans (S&L) regulator, which\nultimately faced insolvency in 1989 at a cost of $132 billion to the taxpayer,\nhad deviated from such recommendations. Timely interventions based on this\nevaluation could have redressed the S&L regulator's decision structure and\nprevented losses to taxpayers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When banks fail amidst financial crises, the public criticizes regulators for\nbailing out or liquidating specific banks, especially the ones that gain\nattention due to their size or dominance. A comprehensive assessment of\nregulators, however, requires examining all their decisions, and not just\nspecific ones, against the regulator's dual objective of preserving financial\nstability while discouraging moral hazard. In this article, we develop a\nBayesian latent class estimation framework to assess regulators on these\ncompeting objectives and evaluate their decisions against resolution rules\nrecommended by theoretical studies of bank behavior designed to contain moral\nhazard incentives. The proposed estimation framework addresses the unobserved\nheterogeneity underlying regulator's decisions in resolving failed banks and\nprovides a disciplined statistical approach for inferring if they acted in the\npublic interest. Our results reveal that during the crises of 1980's, the U.S.\nbanking regulator's resolution decisions were consistent with recommended\ndecision rules, while the U.S. savings and loans (S&L) regulator, which\nultimately faced insolvency in 1989 at a cost of $132 billion to the taxpayer,\nhad deviated from such recommendations. Timely interventions based on this\nevaluation could have redressed the S&L regulator's decision structure and\nprevented losses to taxpayers."
                },
                "authors": [
                    {
                        "name": "Padma Sharma"
                    },
                    {
                        "name": "Trambak Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Trambak Banerjee"
                },
                "author": "Trambak Banerjee",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.03908v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.03908v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12323v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12323v3",
                "updated": "2024-10-15T16:26:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    26,
                    29,
                    1,
                    289,
                    0
                ],
                "published": "2024-02-19T17:56:43Z",
                "published_parsed": [
                    2024,
                    2,
                    19,
                    17,
                    56,
                    43,
                    0,
                    50,
                    0
                ],
                "title": "Expressing and visualizing model uncertainty in Bayesian variable\n  selection using Cartesian credible sets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expressing and visualizing model uncertainty in Bayesian variable\n  selection using Cartesian credible sets"
                },
                "summary": "Modern regression applications can involve hundreds or thousands of variables\nwhich motivates the use of variable selection methods. Bayesian variable\nselection defines a posterior distribution on the possible subsets of the\nvariables (which are usually termed models) to express uncertainty about which\nvariables are strongly linked to the response. This can be used to provide\nBayesian model averaged predictions or inference, and to understand the\nrelative importance of different variables. However, there has been little work\non meaningful representations of this uncertainty beyond first order summaries.\nWe introduce Cartesian credible sets to address this gap. The elements of these\nsets are formed by concatenating sub-models defined on each block of a\npartition of the variables. Investigating these sub-models allow us to\nunderstand whether the models in the Cartesian credible set\nalways/never/sometimes include a particular variable or group of variables and\nprovide a useful summary of model uncertainty. We introduce a method to find\nthese sets that emphasizes ease of understanding and can be easily computed\nfrom Markov chain Monte Carlo output. The potential of the method is\nillustrated on regression problems with both small and large numbers of\nvariables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern regression applications can involve hundreds or thousands of variables\nwhich motivates the use of variable selection methods. Bayesian variable\nselection defines a posterior distribution on the possible subsets of the\nvariables (which are usually termed models) to express uncertainty about which\nvariables are strongly linked to the response. This can be used to provide\nBayesian model averaged predictions or inference, and to understand the\nrelative importance of different variables. However, there has been little work\non meaningful representations of this uncertainty beyond first order summaries.\nWe introduce Cartesian credible sets to address this gap. The elements of these\nsets are formed by concatenating sub-models defined on each block of a\npartition of the variables. Investigating these sub-models allow us to\nunderstand whether the models in the Cartesian credible set\nalways/never/sometimes include a particular variable or group of variables and\nprovide a useful summary of model uncertainty. We introduce a method to find\nthese sets that emphasizes ease of understanding and can be easily computed\nfrom Markov chain Monte Carlo output. The potential of the method is\nillustrated on regression problems with both small and large numbers of\nvariables."
                },
                "authors": [
                    {
                        "name": "J. E. Griffin"
                    }
                ],
                "author_detail": {
                    "name": "J. E. Griffin"
                },
                "author": "J. E. Griffin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12323v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12323v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11745v1",
                "updated": "2024-10-15T16:22:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    22,
                    49,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T16:22:49Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    22,
                    49,
                    1,
                    289,
                    0
                ],
                "title": "Personas with Attitudes: Controlling LLMs for Diverse Data Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personas with Attitudes: Controlling LLMs for Diverse Data Annotation"
                },
                "summary": "We present a novel approach for enhancing diversity and control in data\nannotation tasks by personalizing large language models (LLMs). We investigate\nthe impact of injecting diverse persona descriptions into LLM prompts across\ntwo studies, exploring whether personas increase annotation diversity and\nwhether the impacts of individual personas on the resulting annotations are\nconsistent and controllable. Our results show that persona-prompted LLMs\nproduce more diverse annotations than LLMs prompted without personas and that\nthese effects are both controllable and repeatable, making our approach a\nsuitable tool for improving data annotation in subjective NLP tasks like\ntoxicity detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach for enhancing diversity and control in data\nannotation tasks by personalizing large language models (LLMs). We investigate\nthe impact of injecting diverse persona descriptions into LLM prompts across\ntwo studies, exploring whether personas increase annotation diversity and\nwhether the impacts of individual personas on the resulting annotations are\nconsistent and controllable. Our results show that persona-prompted LLMs\nproduce more diverse annotations than LLMs prompted without personas and that\nthese effects are both controllable and repeatable, making our approach a\nsuitable tool for improving data annotation in subjective NLP tasks like\ntoxicity detection."
                },
                "authors": [
                    {
                        "name": "Leon Fröhling"
                    },
                    {
                        "name": "Gianluca Demartini"
                    },
                    {
                        "name": "Dennis Assenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Dennis Assenmacher"
                },
                "author": "Dennis Assenmacher",
                "arxiv_comment": "21 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11744v1",
                "updated": "2024-10-15T16:21:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    21,
                    15,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T16:21:15Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    21,
                    15,
                    1,
                    289,
                    0
                ],
                "title": "DySpec: Faster Speculative Decoding with Dynamic Token Tree Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DySpec: Faster Speculative Decoding with Dynamic Token Tree Structure"
                },
                "summary": "While speculative decoding has recently appeared as a promising direction for\naccelerating the inference of large language models (LLMs), the speedup and\nscalability are strongly bounded by the token acceptance rate. Prevalent\nmethods usually organize predicted tokens as independent chains or fixed token\ntrees, which fails to generalize to diverse query distributions. In this paper,\nwe propose DySpec, a faster speculative decoding algorithm with a novel dynamic\ntoken tree structure. We begin by bridging the draft distribution and\nacceptance rate from intuitive and empirical clues, and successfully show that\nthe two variables are strongly correlated. Based on this, we employ a greedy\nstrategy to dynamically expand the token tree at run time. Theoretically, we\nshow that our method can achieve optimal results under mild assumptions.\nEmpirically, DySpec yields a higher acceptance rate and speedup than fixed\ntrees. DySpec can drastically improve the throughput and reduce the latency of\ntoken generation across various data distribution and model sizes, which\nsignificantly outperforms strong competitors, including Specinfer and Sequoia.\nUnder low temperature setting, DySpec can improve the throughput up to\n9.1$\\times$ and reduce the latency up to 9.4$\\times$ on Llama2-70B. Under high\ntemperature setting, DySpec can also improve the throughput up to 6.21$\\times$,\ndespite the increasing difficulty of speculating more than one token per step\nfor draft model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While speculative decoding has recently appeared as a promising direction for\naccelerating the inference of large language models (LLMs), the speedup and\nscalability are strongly bounded by the token acceptance rate. Prevalent\nmethods usually organize predicted tokens as independent chains or fixed token\ntrees, which fails to generalize to diverse query distributions. In this paper,\nwe propose DySpec, a faster speculative decoding algorithm with a novel dynamic\ntoken tree structure. We begin by bridging the draft distribution and\nacceptance rate from intuitive and empirical clues, and successfully show that\nthe two variables are strongly correlated. Based on this, we employ a greedy\nstrategy to dynamically expand the token tree at run time. Theoretically, we\nshow that our method can achieve optimal results under mild assumptions.\nEmpirically, DySpec yields a higher acceptance rate and speedup than fixed\ntrees. DySpec can drastically improve the throughput and reduce the latency of\ntoken generation across various data distribution and model sizes, which\nsignificantly outperforms strong competitors, including Specinfer and Sequoia.\nUnder low temperature setting, DySpec can improve the throughput up to\n9.1$\\times$ and reduce the latency up to 9.4$\\times$ on Llama2-70B. Under high\ntemperature setting, DySpec can also improve the throughput up to 6.21$\\times$,\ndespite the increasing difficulty of speculating more than one token per step\nfor draft model."
                },
                "authors": [
                    {
                        "name": "Yunfan Xiong"
                    },
                    {
                        "name": "Ruoyu Zhang"
                    },
                    {
                        "name": "Yanzeng Li"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Lei Zou"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zou"
                },
                "author": "Lei Zou",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11743v1",
                "updated": "2024-10-15T16:18:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    18,
                    44,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T16:18:44Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    18,
                    44,
                    1,
                    289,
                    0
                ],
                "title": "Addressing the Null Paradox in Epidemic Models: Correcting for Collider\n  Bias in Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the Null Paradox in Epidemic Models: Correcting for Collider\n  Bias in Causal Inference"
                },
                "summary": "We address the null paradox in epidemic models, where standard methods\nestimate a non-zero treatment effect despite the true effect being zero. This\noccurs when epidemic models mis-specify how causal effects propagate over time,\nespecially when covariates act as colliders between past interventions and\nlatent variables, leading to spurious correlations. Standard approaches like\nmaximum likelihood and Bayesian methods can misinterpret these biases,\ninferring false causal relationships. While semi-parametric models and inverse\npropensity weighting offer potential solutions, they often limit the ability of\ndomain experts to incorporate epidemic-specific knowledge. To resolve this, we\npropose an alternative estimating equation that corrects for collider bias\nwhile allowing for statistical inference with frequentist guarantees,\npreviously unavailable for complex models like SEIR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the null paradox in epidemic models, where standard methods\nestimate a non-zero treatment effect despite the true effect being zero. This\noccurs when epidemic models mis-specify how causal effects propagate over time,\nespecially when covariates act as colliders between past interventions and\nlatent variables, leading to spurious correlations. Standard approaches like\nmaximum likelihood and Bayesian methods can misinterpret these biases,\ninferring false causal relationships. While semi-parametric models and inverse\npropensity weighting offer potential solutions, they often limit the ability of\ndomain experts to incorporate epidemic-specific knowledge. To resolve this, we\npropose an alternative estimating equation that corrects for collider bias\nwhile allowing for statistical inference with frequentist guarantees,\npreviously unavailable for complex models like SEIR."
                },
                "authors": [
                    {
                        "name": "Heejong Bong"
                    },
                    {
                        "name": "Valérie Ventura"
                    },
                    {
                        "name": "Larry Wasserman"
                    }
                ],
                "author_detail": {
                    "name": "Larry Wasserman"
                },
                "author": "Larry Wasserman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05874v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05874v2",
                "updated": "2024-10-15T16:18:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    18,
                    10,
                    1,
                    289,
                    0
                ],
                "published": "2024-08-11T22:59:32Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    22,
                    59,
                    32,
                    6,
                    224,
                    0
                ],
                "title": "LLM-Based Robust Product Classification in Commerce and Compliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Robust Product Classification in Commerce and Compliance"
                },
                "summary": "Product classification is a crucial task in international trade, as\ncompliance regulations are verified and taxes and duties are applied based on\nproduct categories. Manual classification of products is time-consuming and\nerror-prone, and the sheer volume of products imported and exported renders the\nmanual process infeasible. Consequently, e-commerce platforms and enterprises\ninvolved in international trade have turned to automatic product classification\nusing machine learning. However, current approaches do not consider the\nreal-world challenges associated with product classification, such as very\nabbreviated and incomplete product descriptions. In addition, recent\nadvancements in generative Large Language Models (LLMs) and their reasoning\ncapabilities are mainly untapped in product classification and e-commerce. In\nthis research, we explore the real-life challenges of industrial classification\nand we propose data perturbations that allow for realistic data simulation.\nFurthermore, we employ LLM-based product classification to improve the\nrobustness of the prediction in presence of incomplete data. Our research shows\nthat LLMs with in-context learning outperform the supervised approaches in the\nclean-data scenario. Additionally, we illustrate that LLMs are significantly\nmore robust than the supervised approaches when data attacks are present.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Product classification is a crucial task in international trade, as\ncompliance regulations are verified and taxes and duties are applied based on\nproduct categories. Manual classification of products is time-consuming and\nerror-prone, and the sheer volume of products imported and exported renders the\nmanual process infeasible. Consequently, e-commerce platforms and enterprises\ninvolved in international trade have turned to automatic product classification\nusing machine learning. However, current approaches do not consider the\nreal-world challenges associated with product classification, such as very\nabbreviated and incomplete product descriptions. In addition, recent\nadvancements in generative Large Language Models (LLMs) and their reasoning\ncapabilities are mainly untapped in product classification and e-commerce. In\nthis research, we explore the real-life challenges of industrial classification\nand we propose data perturbations that allow for realistic data simulation.\nFurthermore, we employ LLM-based product classification to improve the\nrobustness of the prediction in presence of incomplete data. Our research shows\nthat LLMs with in-context learning outperform the supervised approaches in the\nclean-data scenario. Additionally, we illustrate that LLMs are significantly\nmore robust than the supervised approaches when data attacks are present."
                },
                "authors": [
                    {
                        "name": "Sina Gholamian"
                    },
                    {
                        "name": "Gianfranco Romani"
                    },
                    {
                        "name": "Bartosz Rudnikowicz"
                    },
                    {
                        "name": "Stavroula Skylaki"
                    }
                ],
                "author_detail": {
                    "name": "Stavroula Skylaki"
                },
                "author": "Stavroula Skylaki",
                "arxiv_comment": "Camera-ready version for Customizable NLP Workshop at EMNLP 2024. 11\n  pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05874v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05874v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11735v1",
                "updated": "2024-10-15T16:11:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    11,
                    52,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T16:11:52Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    11,
                    52,
                    1,
                    289,
                    0
                ],
                "title": "Probabilistic Principles for Biophysics and Neuroscience: Entropy\n  Production, Bayesian Mechanics & the Free-Energy Principle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Principles for Biophysics and Neuroscience: Entropy\n  Production, Bayesian Mechanics & the Free-Energy Principle"
                },
                "summary": "This thesis focuses on three fundamental aspects of biological systems;\nnamely, entropy production, Bayesian mechanics, and the free-energy principle.\nThe contributions are threefold: 1) We compute the entropy production for a\ngreater class of systems than before, including almost any stationary diffusion\nprocess, such as degenerate diffusions where the driving noise does not act on\nall coordinates of the system. Importantly, this class of systems encompasses\nMarkovian approximations of stochastic differential equations driven by colored\nnoise, which is significant since biological systems at the macro- and\nmeso-scale are generally subject to colored fluctuations. 2) We develop a\nBayesian mechanics for biological and physical entities that interact with\ntheir environment in which we give sufficient and necessary conditions for the\ninternal states of something to infer its external states, consistently with\nvariational Bayesian inference in statistics and theoretical neuroscience. 3)\nWe refine the constraints on Bayesian mechanics to obtain a description that is\nmore specific to biological systems, called the free-energy principle. This\nsays that active and internal states of biological systems unfold as minimising\na quantity known as free energy. The mathematical foundation to the free-energy\nprinciple, presented here, unlocks a first principles approach to modeling and\nsimulating behavior in neurobiology and artificial intelligence, by minimising\nfree energy given a generative model of external and sensory states.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This thesis focuses on three fundamental aspects of biological systems;\nnamely, entropy production, Bayesian mechanics, and the free-energy principle.\nThe contributions are threefold: 1) We compute the entropy production for a\ngreater class of systems than before, including almost any stationary diffusion\nprocess, such as degenerate diffusions where the driving noise does not act on\nall coordinates of the system. Importantly, this class of systems encompasses\nMarkovian approximations of stochastic differential equations driven by colored\nnoise, which is significant since biological systems at the macro- and\nmeso-scale are generally subject to colored fluctuations. 2) We develop a\nBayesian mechanics for biological and physical entities that interact with\ntheir environment in which we give sufficient and necessary conditions for the\ninternal states of something to infer its external states, consistently with\nvariational Bayesian inference in statistics and theoretical neuroscience. 3)\nWe refine the constraints on Bayesian mechanics to obtain a description that is\nmore specific to biological systems, called the free-energy principle. This\nsays that active and internal states of biological systems unfold as minimising\na quantity known as free energy. The mathematical foundation to the free-energy\nprinciple, presented here, unlocks a first principles approach to modeling and\nsimulating behavior in neurobiology and artificial intelligence, by minimising\nfree energy given a generative model of external and sensory states."
                },
                "authors": [
                    {
                        "name": "Lancelot Da Costa"
                    }
                ],
                "author_detail": {
                    "name": "Lancelot Da Costa"
                },
                "author": "Lancelot Da Costa",
                "arxiv_comment": "117 pages, PhD thesis",
                "arxiv_journal_ref": "PhD Thesis Imperial College London 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09962v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09962v2",
                "updated": "2024-10-15T16:10:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    10,
                    26,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-13T18:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    18,
                    59,
                    58,
                    6,
                    287,
                    0
                ],
                "title": "LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large\n  Language Models"
                },
                "summary": "Hallucination, a phenomenon where multimodal large language models~(MLLMs)\ntend to generate textual responses that are plausible but unaligned with the\nimage, has become one major hurdle in various MLLM-related applications.\nSeveral benchmarks have been created to gauge the hallucination levels of\nMLLMs, by either raising discriminative questions about the existence of\nobjects or introducing LLM evaluators to score the generated text from MLLMs.\nHowever, the discriminative data largely involve simple questions that are not\naligned with real-world text, while the generative data involve LLM evaluators\nthat are computationally intensive and unstable due to their inherent\nrandomness. We propose LongHalQA, an LLM-free hallucination benchmark that\ncomprises 6K long and complex hallucination text. LongHalQA is featured by\nGPT4V-generated hallucinatory data that are well aligned with real-world\nscenarios, including object/image descriptions and multi-round conversations\nwith 14/130 words and 189 words, respectively, on average. It introduces two\nnew tasks, hallucination discrimination and hallucination completion, unifying\nboth discriminative and generative evaluations in a single\nmultiple-choice-question form and leading to more reliable and efficient\nevaluations without the need for LLM evaluators. Further, we propose an\nadvanced pipeline that greatly facilitates the construction of future\nhallucination benchmarks with long and complex questions and descriptions.\nExtensive experiments over multiple recent MLLMs reveal various new challenges\nwhen they are handling hallucinations with long and complex textual data.\nDataset and evaluation code are available at\nhttps://github.com/hanqiu-hq/LongHalQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination, a phenomenon where multimodal large language models~(MLLMs)\ntend to generate textual responses that are plausible but unaligned with the\nimage, has become one major hurdle in various MLLM-related applications.\nSeveral benchmarks have been created to gauge the hallucination levels of\nMLLMs, by either raising discriminative questions about the existence of\nobjects or introducing LLM evaluators to score the generated text from MLLMs.\nHowever, the discriminative data largely involve simple questions that are not\naligned with real-world text, while the generative data involve LLM evaluators\nthat are computationally intensive and unstable due to their inherent\nrandomness. We propose LongHalQA, an LLM-free hallucination benchmark that\ncomprises 6K long and complex hallucination text. LongHalQA is featured by\nGPT4V-generated hallucinatory data that are well aligned with real-world\nscenarios, including object/image descriptions and multi-round conversations\nwith 14/130 words and 189 words, respectively, on average. It introduces two\nnew tasks, hallucination discrimination and hallucination completion, unifying\nboth discriminative and generative evaluations in a single\nmultiple-choice-question form and leading to more reliable and efficient\nevaluations without the need for LLM evaluators. Further, we propose an\nadvanced pipeline that greatly facilitates the construction of future\nhallucination benchmarks with long and complex questions and descriptions.\nExtensive experiments over multiple recent MLLMs reveal various new challenges\nwhen they are handling hallucinations with long and complex textual data.\nDataset and evaluation code are available at\nhttps://github.com/hanqiu-hq/LongHalQA."
                },
                "authors": [
                    {
                        "name": "Han Qiu"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Qin Qi"
                    },
                    {
                        "name": "Xiaoqin Zhang"
                    },
                    {
                        "name": "Ling Shao"
                    },
                    {
                        "name": "Shijian Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shijian Lu"
                },
                "author": "Shijian Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09962v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11353v2",
                "updated": "2024-10-15T16:05:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    5,
                    27,
                    1,
                    289,
                    0
                ],
                "published": "2024-09-17T16:55:25Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    55,
                    25,
                    1,
                    261,
                    0
                ],
                "title": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation\n  in Large Language Models"
                },
                "summary": "Hallucination, the generation of factually incorrect content, is a growing\nchallenge in Large Language Models (LLMs). Existing detection and mitigation\nmethods are often isolated and insufficient for domain-specific needs, lacking\na standardized pipeline. This paper introduces THaMES (Tool for Hallucination\nMitigations and EvaluationS), an integrated framework and library addressing\nthis gap. THaMES offers an end-to-end solution for evaluating and mitigating\nhallucinations in LLMs, featuring automated test set generation, multifaceted\nbenchmarking, and adaptable mitigation strategies. It automates test set\ncreation from any corpus, ensuring high data quality, diversity, and\ncost-efficiency through techniques like batch processing, weighted sampling,\nand counterfactual validation. THaMES assesses a model's ability to detect and\nreduce hallucinations across various tasks, including text generation and\nbinary classification, applying optimal mitigation strategies like In-Context\nLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient\nFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base\nof academic papers, political news, and Wikipedia reveal that commercial models\nlike GPT-4o benefit more from RAG than ICL, while open-weight models like\nLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT\nsignificantly enhances the performance of Llama-3.1-8B-Instruct in both\nevaluation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination, the generation of factually incorrect content, is a growing\nchallenge in Large Language Models (LLMs). Existing detection and mitigation\nmethods are often isolated and insufficient for domain-specific needs, lacking\na standardized pipeline. This paper introduces THaMES (Tool for Hallucination\nMitigations and EvaluationS), an integrated framework and library addressing\nthis gap. THaMES offers an end-to-end solution for evaluating and mitigating\nhallucinations in LLMs, featuring automated test set generation, multifaceted\nbenchmarking, and adaptable mitigation strategies. It automates test set\ncreation from any corpus, ensuring high data quality, diversity, and\ncost-efficiency through techniques like batch processing, weighted sampling,\nand counterfactual validation. THaMES assesses a model's ability to detect and\nreduce hallucinations across various tasks, including text generation and\nbinary classification, applying optimal mitigation strategies like In-Context\nLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient\nFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base\nof academic papers, political news, and Wikipedia reveal that commercial models\nlike GPT-4o benefit more from RAG than ICL, while open-weight models like\nLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT\nsignificantly enhances the performance of Llama-3.1-8B-Instruct in both\nevaluation tasks."
                },
                "authors": [
                    {
                        "name": "Mengfei Liang"
                    },
                    {
                        "name": "Archish Arun"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Cristian Munoz"
                    },
                    {
                        "name": "Jonathan Lutch"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "Accepted in NeurIPS 2024 SoLaR (Socially Responsible Language\n  Modelling Research ) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.06361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.06361v2",
                "updated": "2024-10-15T16:02:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    2,
                    47,
                    1,
                    289,
                    0
                ],
                "published": "2023-02-13T13:48:08Z",
                "published_parsed": [
                    2023,
                    2,
                    13,
                    13,
                    48,
                    8,
                    0,
                    44,
                    0
                ],
                "title": "Dash: Accelerating Distributed Private Convolutional Neural Network\n  Inference with Arithmetic Garbled Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dash: Accelerating Distributed Private Convolutional Neural Network\n  Inference with Arithmetic Garbled Circuits"
                },
                "summary": "The adoption of machine learning solutions is rapidly increasing across all\nparts of society. As the models grow larger, both training and inference of\nmachine learning models is increasingly outsourced, e.g. to cloud service\nproviders. This means that potentially sensitive data is processed on untrusted\nplatforms, which bears inherent data security and privacy risks. In this work,\nwe investigate how to protect distributed machine learning systems, focusing on\ndeep convolutional neural networks. The most common and best-performing mixed\nMPC approaches are based on HE, secret sharing, and garbled circuits. They\ncommonly suffer from large performance overheads, big accuracy losses, and\ncommunication overheads that grow linearly in the depth of the neural network.\nTo improve on these problems, we present Dash, a fast and distributed private\nconvolutional neural network inference scheme secure against malicious\nattackers. Building on arithmetic garbling gadgets [BMR16] and fancy-garbling\n[BCM+19], Dash is based purely on arithmetic garbled circuits. We introduce\nLabelTensors that allow us to leverage the massive parallelity of modern GPUs.\nCombined with state-of-the-art garbling optimizations, Dash outperforms\nprevious garbling approaches up to a factor of about 100. Furthermore, we\nintroduce an efficient scaling operation over the residues of the Chinese\nremainder theorem representation to arithmetic garbled circuits, which allows\nus to garble larger networks and achieve much higher accuracy than previous\napproaches. Finally, Dash requires only a single communication round per\ninference step, regardless of the depth of the neural network, and a very small\nconstant online communication volume.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of machine learning solutions is rapidly increasing across all\nparts of society. As the models grow larger, both training and inference of\nmachine learning models is increasingly outsourced, e.g. to cloud service\nproviders. This means that potentially sensitive data is processed on untrusted\nplatforms, which bears inherent data security and privacy risks. In this work,\nwe investigate how to protect distributed machine learning systems, focusing on\ndeep convolutional neural networks. The most common and best-performing mixed\nMPC approaches are based on HE, secret sharing, and garbled circuits. They\ncommonly suffer from large performance overheads, big accuracy losses, and\ncommunication overheads that grow linearly in the depth of the neural network.\nTo improve on these problems, we present Dash, a fast and distributed private\nconvolutional neural network inference scheme secure against malicious\nattackers. Building on arithmetic garbling gadgets [BMR16] and fancy-garbling\n[BCM+19], Dash is based purely on arithmetic garbled circuits. We introduce\nLabelTensors that allow us to leverage the massive parallelity of modern GPUs.\nCombined with state-of-the-art garbling optimizations, Dash outperforms\nprevious garbling approaches up to a factor of about 100. Furthermore, we\nintroduce an efficient scaling operation over the residues of the Chinese\nremainder theorem representation to arithmetic garbled circuits, which allows\nus to garble larger networks and achieve much higher accuracy than previous\napproaches. Finally, Dash requires only a single communication round per\ninference step, regardless of the depth of the neural network, and a very small\nconstant online communication volume."
                },
                "authors": [
                    {
                        "name": "Jonas Sander"
                    },
                    {
                        "name": "Sebastian Berndt"
                    },
                    {
                        "name": "Ida Bruhns"
                    },
                    {
                        "name": "Thomas Eisenbarth"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Eisenbarth"
                },
                "author": "Thomas Eisenbarth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.06361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.06361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03764v2",
                "updated": "2024-10-15T16:01:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    1,
                    11,
                    1,
                    289,
                    0
                ],
                "published": "2024-05-06T18:02:00Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    18,
                    2,
                    0,
                    0,
                    127,
                    0
                ],
                "title": "GOVERN: Gradient Orientation Vote Ensemble for Multi-Teacher Reinforced\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GOVERN: Gradient Orientation Vote Ensemble for Multi-Teacher Reinforced\n  Distillation"
                },
                "summary": "Pre-trained language models have become an integral component of\nquestion-answering systems, achieving remarkable performance. However, for\npractical deployment, it is crucial to perform knowledge distillation to\nmaintain high performance while operating under computational constraints. In\nthis paper, we address a key question: given the importance of unsupervised\ndistillation for student model performance, how can knowledge from multiple\nteacher models be effectively ensemble during this stage without the guidance\nof labels? We propose a novel algorithm, GOVERN, to tackle this issue. GOVERN\nhas demonstrated significant improvements in both offline and online\nexperiments, enabling the student model to achieve results comparable to that\nof teacher ensembles. Our experiments show that GOVERN remarkably requires a\nmere 1\\% of the ensemble method's inference budget to achieve 99.5\\% of\nperformance. The proposed algorithm has been successfully deployed in a\nreal-world commercial question-answering system, demonstrating its real-world\napplicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained language models have become an integral component of\nquestion-answering systems, achieving remarkable performance. However, for\npractical deployment, it is crucial to perform knowledge distillation to\nmaintain high performance while operating under computational constraints. In\nthis paper, we address a key question: given the importance of unsupervised\ndistillation for student model performance, how can knowledge from multiple\nteacher models be effectively ensemble during this stage without the guidance\nof labels? We propose a novel algorithm, GOVERN, to tackle this issue. GOVERN\nhas demonstrated significant improvements in both offline and online\nexperiments, enabling the student model to achieve results comparable to that\nof teacher ensembles. Our experiments show that GOVERN remarkably requires a\nmere 1\\% of the ensemble method's inference budget to achieve 99.5\\% of\nperformance. The proposed algorithm has been successfully deployed in a\nreal-world commercial question-answering system, demonstrating its real-world\napplicability."
                },
                "authors": [
                    {
                        "name": "Wenjie Zhou"
                    },
                    {
                        "name": "Zhenxin Ding"
                    },
                    {
                        "name": "Xiaodong Zhang"
                    },
                    {
                        "name": "Haibo Shi"
                    },
                    {
                        "name": "Junfeng Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "arxiv_comment": "Accepted by EMNLP 2024 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v2",
                "updated": "2024-10-15T15:58:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    58,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03936v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03936v2",
                "updated": "2024-10-15T15:57:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    57,
                    10,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-04T21:31:02Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    21,
                    31,
                    2,
                    4,
                    278,
                    0
                ],
                "title": "Learning Truncated Causal History Model for Video Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Truncated Causal History Model for Video Restoration"
                },
                "summary": "One key challenge to video restoration is to model the transition dynamics of\nvideo frames governed by motion. In this work, we propose TURTLE to learn the\ntruncated causal history model for efficient and high-performing video\nrestoration. Unlike traditional methods that process a range of contextual\nframes in parallel, TURTLE enhances efficiency by storing and summarizing a\ntruncated history of the input frame latent representation into an evolving\nhistorical state. This is achieved through a sophisticated similarity-based\nretrieval mechanism that implicitly accounts for inter-frame motion and\nalignment. The causal design in TURTLE enables recurrence in inference through\nstate-memorized historical features while allowing parallel training by\nsampling truncated video clips. We report new state-of-the-art results on a\nmultitude of video restoration benchmark tasks, including video desnowing,\nnighttime video deraining, video raindrops and rain streak removal, video\nsuper-resolution, real-world and synthetic video deblurring, and blind video\ndenoising while reducing the computational cost compared to existing best\ncontextual methods on all these tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key challenge to video restoration is to model the transition dynamics of\nvideo frames governed by motion. In this work, we propose TURTLE to learn the\ntruncated causal history model for efficient and high-performing video\nrestoration. Unlike traditional methods that process a range of contextual\nframes in parallel, TURTLE enhances efficiency by storing and summarizing a\ntruncated history of the input frame latent representation into an evolving\nhistorical state. This is achieved through a sophisticated similarity-based\nretrieval mechanism that implicitly accounts for inter-frame motion and\nalignment. The causal design in TURTLE enables recurrence in inference through\nstate-memorized historical features while allowing parallel training by\nsampling truncated video clips. We report new state-of-the-art results on a\nmultitude of video restoration benchmark tasks, including video desnowing,\nnighttime video deraining, video raindrops and rain streak removal, video\nsuper-resolution, real-world and synthetic video deblurring, and blind video\ndenoising while reducing the computational cost compared to existing best\ncontextual methods on all these tasks."
                },
                "authors": [
                    {
                        "name": "Amirhosein Ghasemabadi"
                    },
                    {
                        "name": "Muhammad Kamran Janjua"
                    },
                    {
                        "name": "Mohammad Salameh"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "Accepted to NeurIPS 2024. 24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03936v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20485v2",
                "updated": "2024-10-15T15:56:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    56,
                    58,
                    1,
                    289,
                    0
                ],
                "published": "2024-05-30T21:19:24Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    21,
                    19,
                    24,
                    3,
                    151,
                    0
                ],
                "title": "Phantom: General Trigger Attacks on Retrieval Augmented Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phantom: General Trigger Attacks on Retrieval Augmented Language\n  Generation"
                },
                "summary": "Retrieval Augmented Generation (RAG) expands the capabilities of modern large\nlanguage models (LLMs), by anchoring, adapting, and personalizing their\nresponses to the most relevant knowledge sources. It is particularly useful in\nchatbot applications, allowing developers to customize LLM output without\nexpensive retraining. Despite their significant utility in various\napplications, RAG systems present new security risks. In this work, we propose\nnew attack vectors that allow an adversary to inject a single malicious\ndocument into a RAG system's knowledge base, and mount a backdoor poisoning\nattack. We design Phantom, a general two-stage optimization framework against\nRAG systems, that crafts a malicious poisoned document leading to an integrity\nviolation in the model's output. First, the document is constructed to be\nretrieved only when a specific trigger sequence of tokens appears in the\nvictim's queries. Second, the document is further optimized with crafted\nadversarial text that induces various adversarial objectives on the LLM output,\nincluding refusal to answer, reputation damage, privacy violations, and harmful\nbehaviors. We demonstrate our attacks on multiple LLM architectures, including\nGemma, Vicuna, and Llama, and show that they transfer to GPT-3.5 Turbo and\nGPT-4. Finally, we successfully conducted a Phantom attack on NVIDIA's\nblack-box production RAG system, \"Chat with RTX\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) expands the capabilities of modern large\nlanguage models (LLMs), by anchoring, adapting, and personalizing their\nresponses to the most relevant knowledge sources. It is particularly useful in\nchatbot applications, allowing developers to customize LLM output without\nexpensive retraining. Despite their significant utility in various\napplications, RAG systems present new security risks. In this work, we propose\nnew attack vectors that allow an adversary to inject a single malicious\ndocument into a RAG system's knowledge base, and mount a backdoor poisoning\nattack. We design Phantom, a general two-stage optimization framework against\nRAG systems, that crafts a malicious poisoned document leading to an integrity\nviolation in the model's output. First, the document is constructed to be\nretrieved only when a specific trigger sequence of tokens appears in the\nvictim's queries. Second, the document is further optimized with crafted\nadversarial text that induces various adversarial objectives on the LLM output,\nincluding refusal to answer, reputation damage, privacy violations, and harmful\nbehaviors. We demonstrate our attacks on multiple LLM architectures, including\nGemma, Vicuna, and Llama, and show that they transfer to GPT-3.5 Turbo and\nGPT-4. Finally, we successfully conducted a Phantom attack on NVIDIA's\nblack-box production RAG system, \"Chat with RTX\"."
                },
                "authors": [
                    {
                        "name": "Harsh Chaudhari"
                    },
                    {
                        "name": "Giorgio Severi"
                    },
                    {
                        "name": "John Abascal"
                    },
                    {
                        "name": "Matthew Jagielski"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "Milad Nasr"
                    },
                    {
                        "name": "Cristina Nita-Rotaru"
                    },
                    {
                        "name": "Alina Oprea"
                    }
                ],
                "author_detail": {
                    "name": "Alina Oprea"
                },
                "author": "Alina Oprea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18679v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18679v4",
                "updated": "2024-10-15T15:52:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    52,
                    57,
                    1,
                    289,
                    0
                ],
                "published": "2024-02-28T19:49:55Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    49,
                    55,
                    2,
                    59,
                    0
                ],
                "title": "Data Interpreter: An LLM Agent For Data Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Interpreter: An LLM Agent For Data Science"
                },
                "summary": "Large Language Model (LLM)-based agents have shown effectiveness across many\napplications. However, their use in data science scenarios requiring solving\nlong-term interconnected tasks, dynamic data adjustments and domain expertise\nremains challenging. Previous approaches primarily focus on individual tasks,\nmaking it difficult to assess the complete data science workflow. Moreover,\nthey struggle to handle real-time changes in intermediate data and fail to\nadapt dynamically to evolving task dependencies inherent to data science\nproblems. In this paper, we present Data Interpreter, an LLM-based agent\ndesigned to automatically solve various data science problems end-to-end. Our\nData Interpreter incorporates two key modules: 1) Hierarchical Graph Modeling,\nwhich breaks down complex problems into manageable subproblems, enabling\ndynamic node generation and graph optimization; and 2) Programmable Node\nGeneration, a technique that refines and verifies each subproblem to\niteratively improve code generation results and robustness. Extensive\nexperiments consistently demonstrate the superiority of Data Interpreter. On\nInfiAgent-DABench, it achieves a 25% performance boost, raising accuracy from\n75.9% to 94.9%. For machine learning and open-ended tasks, it improves\nperformance from 88% to 95%, and from 60% to 97%, respectively. Moreover, on\nthe MATH dataset, Data Interpreter achieves remarkable performance with a 26%\nimprovement compared to state-of-the-art baselines. The code is available at\nhttps://github.com/geekan/MetaGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have shown effectiveness across many\napplications. However, their use in data science scenarios requiring solving\nlong-term interconnected tasks, dynamic data adjustments and domain expertise\nremains challenging. Previous approaches primarily focus on individual tasks,\nmaking it difficult to assess the complete data science workflow. Moreover,\nthey struggle to handle real-time changes in intermediate data and fail to\nadapt dynamically to evolving task dependencies inherent to data science\nproblems. In this paper, we present Data Interpreter, an LLM-based agent\ndesigned to automatically solve various data science problems end-to-end. Our\nData Interpreter incorporates two key modules: 1) Hierarchical Graph Modeling,\nwhich breaks down complex problems into manageable subproblems, enabling\ndynamic node generation and graph optimization; and 2) Programmable Node\nGeneration, a technique that refines and verifies each subproblem to\niteratively improve code generation results and robustness. Extensive\nexperiments consistently demonstrate the superiority of Data Interpreter. On\nInfiAgent-DABench, it achieves a 25% performance boost, raising accuracy from\n75.9% to 94.9%. For machine learning and open-ended tasks, it improves\nperformance from 88% to 95%, and from 60% to 97%, respectively. Moreover, on\nthe MATH dataset, Data Interpreter achieves remarkable performance with a 26%\nimprovement compared to state-of-the-art baselines. The code is available at\nhttps://github.com/geekan/MetaGPT."
                },
                "authors": [
                    {
                        "name": "Sirui Hong"
                    },
                    {
                        "name": "Yizhang Lin"
                    },
                    {
                        "name": "Bang Liu"
                    },
                    {
                        "name": "Bangbang Liu"
                    },
                    {
                        "name": "Binhao Wu"
                    },
                    {
                        "name": "Ceyao Zhang"
                    },
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Danyang Li"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Jinlin Wang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Lingyao Zhang"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Mingchen Zhuge"
                    },
                    {
                        "name": "Taicheng Guo"
                    },
                    {
                        "name": "Tuo Zhou"
                    },
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Xiangtao Lu"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Xinbing Liang"
                    },
                    {
                        "name": "Yaying Fei"
                    },
                    {
                        "name": "Yuheng Cheng"
                    },
                    {
                        "name": "Zhibin Gou"
                    },
                    {
                        "name": "Zongze Xu"
                    },
                    {
                        "name": "Chenglin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chenglin Wu"
                },
                "author": "Chenglin Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18679v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18679v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11720v1",
                "updated": "2024-10-15T15:52:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    52,
                    45,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:52:45Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    52,
                    45,
                    1,
                    289,
                    0
                ],
                "title": "Light-Weight Fault Tolerant Attention for Large Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light-Weight Fault Tolerant Attention for Large Language Model Training"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks. However, the training of these\nmodels is computationally intensive and susceptible to faults, particularly in\nthe attention mechanism, which is a critical component of transformer-based\nLLMs. In this paper, we investigate the impact of faults on LLM training,\nfocusing on INF, NaN, and near-INF values in the computation results with\nsystematic fault injection experiments. We observe the propagation patterns of\nthese errors, which can trigger non-trainable states in the model and disrupt\ntraining, forcing the procedure to load from checkpoints.To mitigate the impact\nof these faults, we propose ATTNChecker, the first Algorithm-Based Fault\nTolerance (ABFT) technique tailored for the attention mechanism in LLMs.\nATTNChecker is designed based on fault propagation patterns of LLM and\nincorporates performance optimization to adapt to both system reliability and\nmodel vulnerability while providing lightweight protection for fast LLM\ntraining. Evaluations on four LLMs show that ATTNChecker on average incurs on\naverage 7% overhead on training while detecting and correcting all extreme\nerrors. Compared with the state-of-the-art checkpoint/restore approach,\nATTNChecker reduces recovery overhead by up to 49x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks. However, the training of these\nmodels is computationally intensive and susceptible to faults, particularly in\nthe attention mechanism, which is a critical component of transformer-based\nLLMs. In this paper, we investigate the impact of faults on LLM training,\nfocusing on INF, NaN, and near-INF values in the computation results with\nsystematic fault injection experiments. We observe the propagation patterns of\nthese errors, which can trigger non-trainable states in the model and disrupt\ntraining, forcing the procedure to load from checkpoints.To mitigate the impact\nof these faults, we propose ATTNChecker, the first Algorithm-Based Fault\nTolerance (ABFT) technique tailored for the attention mechanism in LLMs.\nATTNChecker is designed based on fault propagation patterns of LLM and\nincorporates performance optimization to adapt to both system reliability and\nmodel vulnerability while providing lightweight protection for fast LLM\ntraining. Evaluations on four LLMs show that ATTNChecker on average incurs on\naverage 7% overhead on training while detecting and correcting all extreme\nerrors. Compared with the state-of-the-art checkpoint/restore approach,\nATTNChecker reduces recovery overhead by up to 49x."
                },
                "authors": [
                    {
                        "name": "Yuhang Liang"
                    },
                    {
                        "name": "Xinyi Li"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Bo Fang"
                    },
                    {
                        "name": "Jieyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jieyang Chen"
                },
                "author": "Jieyang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; B.2.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11718v1",
                "updated": "2024-10-15T15:49:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    49,
                    15,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:49:15Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    49,
                    15,
                    1,
                    289,
                    0
                ],
                "title": "Converging to a Lingua Franca: Evolution of Linguistic Regions and\n  Semantics Alignment in Multilingual Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Converging to a Lingua Franca: Evolution of Linguistic Regions and\n  Semantics Alignment in Multilingual Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance,\nparticularly in multilingual contexts. While recent studies suggest that LLMs\ncan transfer skills learned in one language to others, the internal mechanisms\nbehind this ability remain unclear. We observed that the neuron activation\npatterns of LLMs exhibit similarities when processing the same language,\nrevealing the existence and location of key linguistic regions. Additionally,\nwe found that neuron activation patterns are similar when processing sentences\nwith the same semantic meaning in different languages. This indicates that LLMs\nmap semantically identical inputs from different languages into a \"Lingua\nFranca\", a common semantic latent space that allows for consistent processing\nacross languages. This semantic alignment becomes more pronounced with training\nand increased model size, resulting in a more language-agnostic activation\npattern. Moreover, we found that key linguistic neurons are concentrated in the\nfirst and last layers of LLMs, becoming denser in the first layers as training\nprogresses. Experiments on BLOOM and LLaMA2 support these findings,\nhighlighting the structural evolution of multilingual LLMs during training and\nscaling up. This paper provides insights into the internal workings of LLMs,\noffering a foundation for future improvements in their cross-lingual\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance,\nparticularly in multilingual contexts. While recent studies suggest that LLMs\ncan transfer skills learned in one language to others, the internal mechanisms\nbehind this ability remain unclear. We observed that the neuron activation\npatterns of LLMs exhibit similarities when processing the same language,\nrevealing the existence and location of key linguistic regions. Additionally,\nwe found that neuron activation patterns are similar when processing sentences\nwith the same semantic meaning in different languages. This indicates that LLMs\nmap semantically identical inputs from different languages into a \"Lingua\nFranca\", a common semantic latent space that allows for consistent processing\nacross languages. This semantic alignment becomes more pronounced with training\nand increased model size, resulting in a more language-agnostic activation\npattern. Moreover, we found that key linguistic neurons are concentrated in the\nfirst and last layers of LLMs, becoming denser in the first layers as training\nprogresses. Experiments on BLOOM and LLaMA2 support these findings,\nhighlighting the structural evolution of multilingual LLMs during training and\nscaling up. This paper provides insights into the internal workings of LLMs,\noffering a foundation for future improvements in their cross-lingual\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Hongchuan Zeng"
                    },
                    {
                        "name": "Senyu Han"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "16 pages, 11 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11716v1",
                "updated": "2024-10-15T15:48:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    48,
                    27,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:48:27Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    48,
                    27,
                    1,
                    289,
                    0
                ],
                "title": "Randomization-based Inference for MCP-Mod",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomization-based Inference for MCP-Mod"
                },
                "summary": "Dose selection is critical in pharmaceutical drug development, as it directly\nimpacts therapeutic efficacy and patient safety of a drug. The Generalized\nMultiple Comparison Procedures and Modeling (MCP-Mod) approach is commonly used\nin Phase II trials for testing and estimation of dose-response relationships.\nHowever, its effectiveness in small sample sizes, particularly with binary\nendpoints, is hindered by issues like complete separation in logistic\nregression, leading to non-existence of estimates. Motivated by an actual\nclinical trial using the MCP-Mod approach, this paper introduces penalized\nmaximum likelihood estimation (MLE) and randomization-based inference\ntechniques to address these challenges. Randomization-based inference allows\nfor exact finite sample inference, while population-based inference for MCP-Mod\ntypically relies on asymptotic approximations. Simulation studies demonstrate\nthat randomization-based tests can enhance statistical power in small to\nmedium-sized samples while maintaining control over type-I error rates, even in\nthe presence of time trends. Our results show that residual-based randomization\ntests using penalized MLEs not only improve computational efficiency but also\noutperform standard randomization-based methods, making them an adequate choice\nfor dose-finding analyses within the MCP-Mod framework. Additionally, we apply\nthese methods to pharmacometric settings, demonstrating their effectiveness in\nsuch scenarios. The results in this paper underscore the potential of\nrandomization-based inference for the analysis of dose-finding trials,\nparticularly in small sample contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dose selection is critical in pharmaceutical drug development, as it directly\nimpacts therapeutic efficacy and patient safety of a drug. The Generalized\nMultiple Comparison Procedures and Modeling (MCP-Mod) approach is commonly used\nin Phase II trials for testing and estimation of dose-response relationships.\nHowever, its effectiveness in small sample sizes, particularly with binary\nendpoints, is hindered by issues like complete separation in logistic\nregression, leading to non-existence of estimates. Motivated by an actual\nclinical trial using the MCP-Mod approach, this paper introduces penalized\nmaximum likelihood estimation (MLE) and randomization-based inference\ntechniques to address these challenges. Randomization-based inference allows\nfor exact finite sample inference, while population-based inference for MCP-Mod\ntypically relies on asymptotic approximations. Simulation studies demonstrate\nthat randomization-based tests can enhance statistical power in small to\nmedium-sized samples while maintaining control over type-I error rates, even in\nthe presence of time trends. Our results show that residual-based randomization\ntests using penalized MLEs not only improve computational efficiency but also\noutperform standard randomization-based methods, making them an adequate choice\nfor dose-finding analyses within the MCP-Mod framework. Additionally, we apply\nthese methods to pharmacometric settings, demonstrating their effectiveness in\nsuch scenarios. The results in this paper underscore the potential of\nrandomization-based inference for the analysis of dose-finding trials,\nparticularly in small sample contexts."
                },
                "authors": [
                    {
                        "name": "Lukas Pin"
                    },
                    {
                        "name": "Oleksandr Sverdlov"
                    },
                    {
                        "name": "Frank Bretz"
                    },
                    {
                        "name": "Björn Bornkamp"
                    }
                ],
                "author_detail": {
                    "name": "Björn Bornkamp"
                },
                "author": "Björn Bornkamp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17695v2",
                "updated": "2024-10-15T15:48:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    48,
                    4,
                    1,
                    289,
                    0
                ],
                "published": "2024-07-25T01:32:41Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    1,
                    32,
                    41,
                    3,
                    207,
                    0
                ],
                "title": "Enhancing Agent Learning through World Dynamics Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Agent Learning through World Dynamics Modeling"
                },
                "summary": "Large language models (LLMs) have been increasingly applied to tasks in\nlanguage understanding and interactive decision-making, with their impressive\nperformance largely attributed to the extensive domain knowledge embedded\nwithin them. However, the depth and breadth of this knowledge can vary across\ndomains. Many existing approaches assume that LLMs possess a comprehensive\nunderstanding of their environment, often overlooking potential gaps in their\ngrasp of actual world dynamics. To address this, we introduce Discover, Verify,\nand Evolve (DiVE), a framework that discovers world dynamics from a small\nnumber of demonstrations, verifies the accuracy of these dynamics, and evolves\nnew, advanced dynamics tailored to the current situation. Through extensive\nevaluations, we assess the impact of each component on performance and compare\nthe dynamics generated by DiVE to human-annotated dynamics. Our results show\nthat LLMs guided by DiVE make more informed decisions, achieving rewards\ncomparable to human players in the Crafter environment and surpassing methods\nthat require prior task-specific training in the MiniHack environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly applied to tasks in\nlanguage understanding and interactive decision-making, with their impressive\nperformance largely attributed to the extensive domain knowledge embedded\nwithin them. However, the depth and breadth of this knowledge can vary across\ndomains. Many existing approaches assume that LLMs possess a comprehensive\nunderstanding of their environment, often overlooking potential gaps in their\ngrasp of actual world dynamics. To address this, we introduce Discover, Verify,\nand Evolve (DiVE), a framework that discovers world dynamics from a small\nnumber of demonstrations, verifies the accuracy of these dynamics, and evolves\nnew, advanced dynamics tailored to the current situation. Through extensive\nevaluations, we assess the impact of each component on performance and compare\nthe dynamics generated by DiVE to human-annotated dynamics. Our results show\nthat LLMs guided by DiVE make more informed decisions, achieving rewards\ncomparable to human players in the Crafter environment and surpassing methods\nthat require prior task-specific training in the MiniHack environment."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Sun"
                    },
                    {
                        "name": "Haochen Shi"
                    },
                    {
                        "name": "Marc-Alexandre Côté"
                    },
                    {
                        "name": "Glen Berseth"
                    },
                    {
                        "name": "Xingdi Yuan"
                    },
                    {
                        "name": "Bang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bang Liu"
                },
                "author": "Bang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11713v1",
                "updated": "2024-10-15T15:47:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    47,
                    10,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:47:10Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    47,
                    10,
                    1,
                    289,
                    0
                ],
                "title": "Enhancing Statistical Validity and Power in Hybrid Controlled Trials: A\n  Randomization Inference Approach with Conformal Selective Borrowing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Statistical Validity and Power in Hybrid Controlled Trials: A\n  Randomization Inference Approach with Conformal Selective Borrowing"
                },
                "summary": "Randomized controlled trials (RCTs) are the gold standard for causal\ninference on treatment effects. However, they can be underpowered due to small\npopulation sizes in rare diseases and limited number of patients willing to\nparticipate due to questions regarding equipoise among treatment groups in\ncommon diseases. Hybrid controlled trials use external controls (ECs) from\nhistorical studies or large observational databases to enhance statistical\nefficiency, and as such, are considered for drug evaluation in rare diseases or\nindications associated with low trial participation. However, non-randomized\nECs can introduce biases that compromise validity and inflate type I errors for\ntreatment discovery, particularly in small samples. To address this, we extend\nthe Fisher randomization test (FRT) to hybrid controlled trials. Our approach\ninvolves a test statistic combining RCT and EC data and is based solely on\nrandomization in the RCT. This method strictly controls the type I error rate,\neven with biased ECs, and improves power by incorporating unbiased ECs. To\nmitigate the power loss caused by biased ECs, we introduce conformal selective\nborrowing, which uses conformal p-values to individually select unbiased ECs,\noffering the flexibility to use either computationally efficient parametric\nmodels or off-the-shelf machine learning models to construct the conformal\nscore function, along with model-agnostic reliability. We identify a\nrisk-benefit trade-off in the power of FRT associated with different selection\nthresholds for conformal p-values. We offer both fixed and adaptive selection\nthreshold options, enabling robust performance across varying levels of hidden\nbias. The advantages of our method are demonstrated through simulations and an\napplication to a small lung cancer RCT with ECs from the National Cancer\nDatabase. Our method is available in the R package intFRT on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomized controlled trials (RCTs) are the gold standard for causal\ninference on treatment effects. However, they can be underpowered due to small\npopulation sizes in rare diseases and limited number of patients willing to\nparticipate due to questions regarding equipoise among treatment groups in\ncommon diseases. Hybrid controlled trials use external controls (ECs) from\nhistorical studies or large observational databases to enhance statistical\nefficiency, and as such, are considered for drug evaluation in rare diseases or\nindications associated with low trial participation. However, non-randomized\nECs can introduce biases that compromise validity and inflate type I errors for\ntreatment discovery, particularly in small samples. To address this, we extend\nthe Fisher randomization test (FRT) to hybrid controlled trials. Our approach\ninvolves a test statistic combining RCT and EC data and is based solely on\nrandomization in the RCT. This method strictly controls the type I error rate,\neven with biased ECs, and improves power by incorporating unbiased ECs. To\nmitigate the power loss caused by biased ECs, we introduce conformal selective\nborrowing, which uses conformal p-values to individually select unbiased ECs,\noffering the flexibility to use either computationally efficient parametric\nmodels or off-the-shelf machine learning models to construct the conformal\nscore function, along with model-agnostic reliability. We identify a\nrisk-benefit trade-off in the power of FRT associated with different selection\nthresholds for conformal p-values. We offer both fixed and adaptive selection\nthreshold options, enabling robust performance across varying levels of hidden\nbias. The advantages of our method are demonstrated through simulations and an\napplication to a small lung cancer RCT with ECs from the National Cancer\nDatabase. Our method is available in the R package intFRT on GitHub."
                },
                "authors": [
                    {
                        "name": "Ke Zhu"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Xiaofei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofei Wang"
                },
                "author": "Xiaofei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11712v1",
                "updated": "2024-10-15T15:47:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    47,
                    1,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:47:01Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    47,
                    1,
                    1,
                    289,
                    0
                ],
                "title": "Parameter estimation of structural dynamics with neural operators\n  enabled surrogate modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter estimation of structural dynamics with neural operators\n  enabled surrogate modeling"
                },
                "summary": "Parameter estimation generally involves inferring the values of mathematical\nmodels derived from first principles or expert knowledge, which is challenging\nfor complex structural systems. In this work, we present a unified deep\nlearning-based framework for parameterization, forward modeling, and inverse\nmodeling of structural dynamics. The parameterization is flexible and can be\nuser-defined, including physical and/or non-physical (customized) parameters.\nIn forward modeling, we train a neural operator for response prediction --\nforming a surrogate model, which leverages the defined system parameters and\nexcitation forces as inputs. The inverse modeling focuses on estimating system\nparameters. In particular, the learned forward surrogate model (which is\ndifferentiable) is utilized for preliminary parameter estimation via\ngradient-based optimization; to further boost the parameter estimation, we\nintroduce a neural refinement method to mitigate ill-posed problems, which\noften occur in the former. The framework's effectiveness is verified\nnumerically and experimentally, in both interpolation and extrapolation cases,\nindicating its capability to capture intrinsic dynamics of structural systems\nfrom both forward and inverse perspectives. Moreover, the framework's\nflexibility is expected to support a wide range of applications, including\nsurrogate modeling, structural identification, damage detection, and inverse\ndesign of structural systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter estimation generally involves inferring the values of mathematical\nmodels derived from first principles or expert knowledge, which is challenging\nfor complex structural systems. In this work, we present a unified deep\nlearning-based framework for parameterization, forward modeling, and inverse\nmodeling of structural dynamics. The parameterization is flexible and can be\nuser-defined, including physical and/or non-physical (customized) parameters.\nIn forward modeling, we train a neural operator for response prediction --\nforming a surrogate model, which leverages the defined system parameters and\nexcitation forces as inputs. The inverse modeling focuses on estimating system\nparameters. In particular, the learned forward surrogate model (which is\ndifferentiable) is utilized for preliminary parameter estimation via\ngradient-based optimization; to further boost the parameter estimation, we\nintroduce a neural refinement method to mitigate ill-posed problems, which\noften occur in the former. The framework's effectiveness is verified\nnumerically and experimentally, in both interpolation and extrapolation cases,\nindicating its capability to capture intrinsic dynamics of structural systems\nfrom both forward and inverse perspectives. Moreover, the framework's\nflexibility is expected to support a wide range of applications, including\nsurrogate modeling, structural identification, damage detection, and inverse\ndesign of structural systems."
                },
                "authors": [
                    {
                        "name": "Mingyuan Zhou"
                    },
                    {
                        "name": "Haoze Song"
                    },
                    {
                        "name": "Wenjing Ye"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Zhilu Lai"
                    }
                ],
                "author_detail": {
                    "name": "Zhilu Lai"
                },
                "author": "Zhilu Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11711v1",
                "updated": "2024-10-15T15:46:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    46,
                    53,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:46:53Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    46,
                    53,
                    1,
                    289,
                    0
                ],
                "title": "Zero-shot Model-based Reinforcement Learning using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Model-based Reinforcement Learning using Large Language Models"
                },
                "summary": "The emerging zero-shot capabilities of Large Language Models (LLMs) have led\nto their applications in areas extending well beyond natural language\nprocessing tasks. In reinforcement learning, while LLMs have been extensively\nused in text-based environments, their integration with continuous state spaces\nremains understudied. In this paper, we investigate how pre-trained LLMs can be\nleveraged to predict in context the dynamics of continuous Markov decision\nprocesses. We identify handling multivariate data and incorporating the control\nsignal as key challenges that limit the potential of LLMs' deployment in this\nsetup and propose Disentangled In-Context Learning (DICL) to address them. We\npresent proof-of-concept applications in two reinforcement learning settings:\nmodel-based policy evaluation and data-augmented off-policy reinforcement\nlearning, supported by theoretical analysis of the proposed methods. Our\nexperiments further demonstrate that our approach produces well-calibrated\nuncertainty estimates. We release the code at\nhttps://github.com/abenechehab/dicl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emerging zero-shot capabilities of Large Language Models (LLMs) have led\nto their applications in areas extending well beyond natural language\nprocessing tasks. In reinforcement learning, while LLMs have been extensively\nused in text-based environments, their integration with continuous state spaces\nremains understudied. In this paper, we investigate how pre-trained LLMs can be\nleveraged to predict in context the dynamics of continuous Markov decision\nprocesses. We identify handling multivariate data and incorporating the control\nsignal as key challenges that limit the potential of LLMs' deployment in this\nsetup and propose Disentangled In-Context Learning (DICL) to address them. We\npresent proof-of-concept applications in two reinforcement learning settings:\nmodel-based policy evaluation and data-augmented off-policy reinforcement\nlearning, supported by theoretical analysis of the proposed methods. Our\nexperiments further demonstrate that our approach produces well-calibrated\nuncertainty estimates. We release the code at\nhttps://github.com/abenechehab/dicl."
                },
                "authors": [
                    {
                        "name": "Abdelhakim Benechehab"
                    },
                    {
                        "name": "Youssef Attia El Hili"
                    },
                    {
                        "name": "Ambroise Odonnat"
                    },
                    {
                        "name": "Oussama Zekri"
                    },
                    {
                        "name": "Albert Thomas"
                    },
                    {
                        "name": "Giuseppe Paolo"
                    },
                    {
                        "name": "Maurizio Filippone"
                    },
                    {
                        "name": "Ievgen Redko"
                    },
                    {
                        "name": "Balázs Kégl"
                    }
                ],
                "author_detail": {
                    "name": "Balázs Kégl"
                },
                "author": "Balázs Kégl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11710v1",
                "updated": "2024-10-15T15:46:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    46,
                    17,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:46:17Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    46,
                    17,
                    1,
                    289,
                    0
                ],
                "title": "MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have displayed massive improvements in reasoning\nand decision-making skills and can hold natural conversations with users.\nRecently, many tool-use benchmark datasets have been proposed. However,\nexisting datasets have the following limitations: (1). Insufficient evaluation\nscenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation\ncosts (e.g., GPT API costs). To address these limitations, in this work, we\npropose a multi-granularity tool-use benchmark for large language models called\nMTU-Bench. For the \"multi-granularity\" property, our MTU-Bench covers five tool\nusage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool,\nmultiple-turn and single-tool, multiple-turn and multiple-tool, and\nout-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench\nare based on the prediction results and the ground truth without using any GPT\nor human evaluation metrics. Moreover, our MTU-Bench is collected by\ntransforming existing high-quality datasets to simulate real-world tool usage\nscenarios, and we also propose an instruction dataset called MTU-Instruct data\nto enhance the tool-use abilities of existing LLMs. Comprehensive experimental\nresults demonstrate the effectiveness of our MTU-Bench. Code and data will be\nreleased at https: //github.com/MTU-Bench-Team/MTU-Bench.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have displayed massive improvements in reasoning\nand decision-making skills and can hold natural conversations with users.\nRecently, many tool-use benchmark datasets have been proposed. However,\nexisting datasets have the following limitations: (1). Insufficient evaluation\nscenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation\ncosts (e.g., GPT API costs). To address these limitations, in this work, we\npropose a multi-granularity tool-use benchmark for large language models called\nMTU-Bench. For the \"multi-granularity\" property, our MTU-Bench covers five tool\nusage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool,\nmultiple-turn and single-tool, multiple-turn and multiple-tool, and\nout-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench\nare based on the prediction results and the ground truth without using any GPT\nor human evaluation metrics. Moreover, our MTU-Bench is collected by\ntransforming existing high-quality datasets to simulate real-world tool usage\nscenarios, and we also propose an instruction dataset called MTU-Instruct data\nto enhance the tool-use abilities of existing LLMs. Comprehensive experimental\nresults demonstrate the effectiveness of our MTU-Bench. Code and data will be\nreleased at https: //github.com/MTU-Bench-Team/MTU-Bench.git."
                },
                "authors": [
                    {
                        "name": "Pei Wang"
                    },
                    {
                        "name": "Yanan Wu"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xiaoshuai Song"
                    },
                    {
                        "name": "Zhongyuan Peng"
                    },
                    {
                        "name": "Ken Deng"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Jiakai Wang"
                    },
                    {
                        "name": "Junran Peng"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16112v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16112v2",
                "updated": "2024-10-15T15:30:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    30,
                    46,
                    1,
                    289,
                    0
                ],
                "published": "2024-05-25T07:52:26Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    7,
                    52,
                    26,
                    5,
                    146,
                    0
                ],
                "title": "Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Backdoor Attack by Injecting Proactive Defensive Backdoor"
                },
                "summary": "Data-poisoning backdoor attacks are serious security threats to machine\nlearning models, where an adversary can manipulate the training dataset to\ninject backdoors into models. In this paper, we focus on in-training backdoor\ndefense, aiming to train a clean model even when the dataset may be potentially\npoisoned. Unlike most existing methods that primarily detect and remove/unlearn\nsuspicious samples to mitigate malicious backdoor attacks, we propose a novel\ndefense approach called PDB (Proactive Defensive Backdoor). Specifically, PDB\nleverages the home-field advantage of defenders by proactively injecting a\ndefensive backdoor into the model during training. Taking advantage of\ncontrolling the training process, the defensive backdoor is designed to\nsuppress the malicious backdoor effectively while remaining secret to\nattackers. In addition, we introduce a reversible mapping to determine the\ndefensive target label. During inference, PDB embeds a defensive trigger in the\ninputs and reverses the model's prediction, suppressing malicious backdoor and\nensuring the model's utility on the original task. Experimental results across\nvarious datasets and models demonstrate that our approach achieves\nstate-of-the-art defense performance against a wide range of backdoor attacks.\nThe code is available at\nhttps://github.com/shawkui/Proactive_Defensive_Backdoor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-poisoning backdoor attacks are serious security threats to machine\nlearning models, where an adversary can manipulate the training dataset to\ninject backdoors into models. In this paper, we focus on in-training backdoor\ndefense, aiming to train a clean model even when the dataset may be potentially\npoisoned. Unlike most existing methods that primarily detect and remove/unlearn\nsuspicious samples to mitigate malicious backdoor attacks, we propose a novel\ndefense approach called PDB (Proactive Defensive Backdoor). Specifically, PDB\nleverages the home-field advantage of defenders by proactively injecting a\ndefensive backdoor into the model during training. Taking advantage of\ncontrolling the training process, the defensive backdoor is designed to\nsuppress the malicious backdoor effectively while remaining secret to\nattackers. In addition, we introduce a reversible mapping to determine the\ndefensive target label. During inference, PDB embeds a defensive trigger in the\ninputs and reverses the model's prediction, suppressing malicious backdoor and\nensuring the model's utility on the original task. Experimental results across\nvarious datasets and models demonstrate that our approach achieves\nstate-of-the-art defense performance against a wide range of backdoor attacks.\nThe code is available at\nhttps://github.com/shawkui/Proactive_Defensive_Backdoor."
                },
                "authors": [
                    {
                        "name": "Shaokui Wei"
                    },
                    {
                        "name": "Hongyuan Zha"
                    },
                    {
                        "name": "Baoyuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Baoyuan Wu"
                },
                "author": "Baoyuan Wu",
                "arxiv_comment": "Accepted by NeurIPS 2024. 32 pages, 7 figures, 28 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16112v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11693v2",
                "updated": "2024-10-16T01:45:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    1,
                    45,
                    28,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-15T15:26:28Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    26,
                    28,
                    1,
                    289,
                    0
                ],
                "title": "IntGrad MT: Eliciting LLMs' Machine Translation Capabilities with\n  Sentence Interpolation and Gradual MT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntGrad MT: Eliciting LLMs' Machine Translation Capabilities with\n  Sentence Interpolation and Gradual MT"
                },
                "summary": "Recent Large Language Models (LLMs) have demonstrated strong performance in\ntranslation without needing to be finetuned on additional parallel corpora.\nHowever, they still underperform for low-resource language pairs. Previous\nworks have focused on mitigating this issue by leveraging relevant few-shot\nexamples or external resources such as dictionaries or grammar books, making\nmodels heavily reliant on these nonparametric sources of information. In this\npaper, we propose a novel method named IntGrad MT that focuses on fully\nexploiting an LLM's inherent translation capability. IntGrad MT achieves this\nby constructing a chain of few-shot examples, each consisting of a source\nsentence and the model's own translation, that rise incrementally in\ndifficulty. IntGrad MT employs two techniques: Sentence Interpolation, which\ngenerates a sequence of sentences that gradually change from an easy sentence\nto translate to a difficult one, and Gradual MT, which sequentially translates\nthis chain using translations of earlier sentences as few-shot examples for the\ntranslation of subsequent ones. With this approach, we observe a substantial\nenhancement in the xCOMET scores of various LLMs for multiple languages,\nespecially in low-resource languages such as Hindi(8.26), Swahili(7.10),\nBengali(6.97) and Marathi(13.03). Our approach presents a practical way of\nenhancing LLMs' performance without extra training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Language Models (LLMs) have demonstrated strong performance in\ntranslation without needing to be finetuned on additional parallel corpora.\nHowever, they still underperform for low-resource language pairs. Previous\nworks have focused on mitigating this issue by leveraging relevant few-shot\nexamples or external resources such as dictionaries or grammar books, making\nmodels heavily reliant on these nonparametric sources of information. In this\npaper, we propose a novel method named IntGrad MT that focuses on fully\nexploiting an LLM's inherent translation capability. IntGrad MT achieves this\nby constructing a chain of few-shot examples, each consisting of a source\nsentence and the model's own translation, that rise incrementally in\ndifficulty. IntGrad MT employs two techniques: Sentence Interpolation, which\ngenerates a sequence of sentences that gradually change from an easy sentence\nto translate to a difficult one, and Gradual MT, which sequentially translates\nthis chain using translations of earlier sentences as few-shot examples for the\ntranslation of subsequent ones. With this approach, we observe a substantial\nenhancement in the xCOMET scores of various LLMs for multiple languages,\nespecially in low-resource languages such as Hindi(8.26), Swahili(7.10),\nBengali(6.97) and Marathi(13.03). Our approach presents a practical way of\nenhancing LLMs' performance without extra training."
                },
                "authors": [
                    {
                        "name": "Seung-Woo Choi"
                    },
                    {
                        "name": "Ga-Hyun Yoo"
                    },
                    {
                        "name": "Jay-Yoon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jay-Yoon Lee"
                },
                "author": "Jay-Yoon Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.05794v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.05794v4",
                "updated": "2024-10-15T15:25:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    25,
                    3,
                    1,
                    289,
                    0
                ],
                "published": "2023-11-09T23:57:32Z",
                "published_parsed": [
                    2023,
                    11,
                    9,
                    23,
                    57,
                    32,
                    3,
                    313,
                    0
                ],
                "title": "An Experimental Design for Anytime-Valid Causal Inference on Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Experimental Design for Anytime-Valid Causal Inference on Multi-Armed\n  Bandits"
                },
                "summary": "Experimentation is crucial for managers to rigorously quantify the value of a\nchange and determine if it leads to a statistically significant improvement\nover the status quo. As companies increasingly mandate that all changes undergo\nexperimentation before widespread release, two challenges arise: (1) minimizing\nthe proportion of customers assigned to the inferior treatment and (2)\nincreasing experimentation velocity by enabling data-dependent stopping. This\npaper addresses both challenges by introducing the Mixture Adaptive Design\n(MAD), a new experimental design for multi-armed bandit (MAB) algorithms that\nenables anytime-valid inference on the Average Treatment Effect (ATE) for\n\\emph{any} MAB algorithm. Intuitively, MAD \"mixes\" any bandit algorithm with a\nBernoulli design, where at each time step, the probability of assigning a unit\nvia the Bernoulli design is determined by a user-specified deterministic\nsequence that can converge to zero. This sequence lets managers directly\ncontrol the trade-off between regret minimization and inferential precision.\nUnder mild conditions on the rate the sequence converges to zero, we provide a\nconfidence sequence that is asymptotically anytime-valid and guaranteed to\nshrink around the true ATE. Hence, when the true ATE converges to a non-zero\nvalue, the MAD confidence sequence is guaranteed to exclude zero in finite\ntime. Therefore, the MAD enables managers to stop experiments early while\nensuring valid inference, enhancing both the efficiency and reliability of\nadaptive experiments. Empirically, we demonstrate that the MAD achieves\nfinite-sample anytime-validity while accurately and precisely estimating the\nATE, all without incurring significant losses in reward compared to standard\nbandit designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimentation is crucial for managers to rigorously quantify the value of a\nchange and determine if it leads to a statistically significant improvement\nover the status quo. As companies increasingly mandate that all changes undergo\nexperimentation before widespread release, two challenges arise: (1) minimizing\nthe proportion of customers assigned to the inferior treatment and (2)\nincreasing experimentation velocity by enabling data-dependent stopping. This\npaper addresses both challenges by introducing the Mixture Adaptive Design\n(MAD), a new experimental design for multi-armed bandit (MAB) algorithms that\nenables anytime-valid inference on the Average Treatment Effect (ATE) for\n\\emph{any} MAB algorithm. Intuitively, MAD \"mixes\" any bandit algorithm with a\nBernoulli design, where at each time step, the probability of assigning a unit\nvia the Bernoulli design is determined by a user-specified deterministic\nsequence that can converge to zero. This sequence lets managers directly\ncontrol the trade-off between regret minimization and inferential precision.\nUnder mild conditions on the rate the sequence converges to zero, we provide a\nconfidence sequence that is asymptotically anytime-valid and guaranteed to\nshrink around the true ATE. Hence, when the true ATE converges to a non-zero\nvalue, the MAD confidence sequence is guaranteed to exclude zero in finite\ntime. Therefore, the MAD enables managers to stop experiments early while\nensuring valid inference, enhancing both the efficiency and reliability of\nadaptive experiments. Empirically, we demonstrate that the MAD achieves\nfinite-sample anytime-validity while accurately and precisely estimating the\nATE, all without incurring significant losses in reward compared to standard\nbandit designs."
                },
                "authors": [
                    {
                        "name": "Biyonka Liang"
                    },
                    {
                        "name": "Iavor Bojinov"
                    }
                ],
                "author_detail": {
                    "name": "Iavor Bojinov"
                },
                "author": "Iavor Bojinov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.05794v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.05794v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11685v1",
                "updated": "2024-10-15T15:21:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    21,
                    3,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:21:03Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    21,
                    3,
                    1,
                    289,
                    0
                ],
                "title": "Polarization-encoded photonic quantum-to-quantum Bernoulli factory based\n  on a quantum dot source",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polarization-encoded photonic quantum-to-quantum Bernoulli factory based\n  on a quantum dot source"
                },
                "summary": "A Bernoulli factory is a randomness manipulation routine that takes as input\na Bernoulli random variable, outputting another Bernoulli variable whose bias\nis a function of the input bias. Recently proposed quantum-to-quantum Bernoulli\nfactory schemes encode both input and output variables in qubit amplitudes.\nThis primitive could be used as a sub-routine for more complex quantum\nalgorithms involving Bayesian inference and Monte Carlo methods. Here, we\nreport an experimental implementation of a polarization-encoded photonic\nquantum-to-quantum Bernoulli factory. We present and test three interferometric\nset-ups implementing the basic operations of an algebraic field (inversion,\nmultiplication, and addition) which, chained together, allow for the\nimplementation of a generic quantum-to-quantum Bernoulli factory. These in-bulk\nschemes are validated using a quantum dot-based single-photon source featuring\nhigh brightness and indistinguishability, paired with a time-to-spatial\ndemultiplexing setup to prepare input resources of up to three single-photon\nstates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bernoulli factory is a randomness manipulation routine that takes as input\na Bernoulli random variable, outputting another Bernoulli variable whose bias\nis a function of the input bias. Recently proposed quantum-to-quantum Bernoulli\nfactory schemes encode both input and output variables in qubit amplitudes.\nThis primitive could be used as a sub-routine for more complex quantum\nalgorithms involving Bayesian inference and Monte Carlo methods. Here, we\nreport an experimental implementation of a polarization-encoded photonic\nquantum-to-quantum Bernoulli factory. We present and test three interferometric\nset-ups implementing the basic operations of an algebraic field (inversion,\nmultiplication, and addition) which, chained together, allow for the\nimplementation of a generic quantum-to-quantum Bernoulli factory. These in-bulk\nschemes are validated using a quantum dot-based single-photon source featuring\nhigh brightness and indistinguishability, paired with a time-to-spatial\ndemultiplexing setup to prepare input resources of up to three single-photon\nstates."
                },
                "authors": [
                    {
                        "name": "Giovanni Rodari"
                    },
                    {
                        "name": "Francesco Hoch"
                    },
                    {
                        "name": "Alessia Suprano"
                    },
                    {
                        "name": "Taira Giordani"
                    },
                    {
                        "name": "Elena Negro"
                    },
                    {
                        "name": "Gonzalo Carvacho"
                    },
                    {
                        "name": "Nicolò Spagnolo"
                    },
                    {
                        "name": "Ernesto F. Galvão"
                    },
                    {
                        "name": "Fabio Sciarrino"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Sciarrino"
                },
                "author": "Fabio Sciarrino",
                "arxiv_doi": "10.1126/sciadv.ado6244",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1126/sciadv.ado6244",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Science Advances 10, eado6244 (2024)",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11680v1",
                "updated": "2024-10-15T15:18:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    18,
                    46,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:18:46Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    18,
                    46,
                    1,
                    289,
                    0
                ],
                "title": "Population synthesis and astrophysical inference for high-$z$ JWST\n  galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Population synthesis and astrophysical inference for high-$z$ JWST\n  galaxies"
                },
                "summary": "Observations of the high-$z$ Universe from JWST have revealed a new\npopulation of bright, early galaxies. A robust statistical interpretation of\nthis data requires fast forward models that account for uncertainties in galaxy\nevolution and incorporate observational systematic effects. We present a\nprobabilistic framework for population synthesis of high-$z$ galaxies and\ninference of their properties. Our framework is based on the semi-analytic\ngalaxy-formation model Galacticus. To infer the astrophysical parameters\ngoverning high-$z$ galaxy evolution, we analyze JWST data from the CEERS and\nNGDEEP surveys and calculate the likelihood of observing individual objects in\napparent magnitude--redshift space, for $z\\geq8.5$ galaxy candidates. We\ninclude observational selection effects due to limited survey volume and depth,\nas well as photometric redshift uncertainties. We recover the posterior\nprobability distribution for parameters describing star formation and outflow\nrates. We place an upper limit on the star formation timescale of\n$500~\\mathrm{Myr}$ at a disk velocity of $50~\\mathrm{km\\ s}^{-1}$, and we infer\na characteristic velocity at which the outflow mass-loading factor is $\\sim 1$\nof $150^{+280}_{-60}~\\mathrm{km\\ s}^{-1}$, both at $95\\%$ confidence.\nMarginalizing over our astrophysical model, we find that galaxies in CEERS and\nNGDEEP data occupy halos with virial masses $10^{10\\pm 0.5}~M_{\\mathrm{\\odot}}$\nat $8.5\\leq z\\leq 12$, at $95\\%$ confidence. The star formation timescale\npreferred by our fit is relatively short compared to typical values at lower\nredshifts, consistent with previous findings. The modeling and analysis\nframework presented here can enable systematic tests of high-$z$ galaxies' dust\ncontent, initial mass functions, and star formation burstiness in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observations of the high-$z$ Universe from JWST have revealed a new\npopulation of bright, early galaxies. A robust statistical interpretation of\nthis data requires fast forward models that account for uncertainties in galaxy\nevolution and incorporate observational systematic effects. We present a\nprobabilistic framework for population synthesis of high-$z$ galaxies and\ninference of their properties. Our framework is based on the semi-analytic\ngalaxy-formation model Galacticus. To infer the astrophysical parameters\ngoverning high-$z$ galaxy evolution, we analyze JWST data from the CEERS and\nNGDEEP surveys and calculate the likelihood of observing individual objects in\napparent magnitude--redshift space, for $z\\geq8.5$ galaxy candidates. We\ninclude observational selection effects due to limited survey volume and depth,\nas well as photometric redshift uncertainties. We recover the posterior\nprobability distribution for parameters describing star formation and outflow\nrates. We place an upper limit on the star formation timescale of\n$500~\\mathrm{Myr}$ at a disk velocity of $50~\\mathrm{km\\ s}^{-1}$, and we infer\na characteristic velocity at which the outflow mass-loading factor is $\\sim 1$\nof $150^{+280}_{-60}~\\mathrm{km\\ s}^{-1}$, both at $95\\%$ confidence.\nMarginalizing over our astrophysical model, we find that galaxies in CEERS and\nNGDEEP data occupy halos with virial masses $10^{10\\pm 0.5}~M_{\\mathrm{\\odot}}$\nat $8.5\\leq z\\leq 12$, at $95\\%$ confidence. The star formation timescale\npreferred by our fit is relatively short compared to typical values at lower\nredshifts, consistent with previous findings. The modeling and analysis\nframework presented here can enable systematic tests of high-$z$ galaxies' dust\ncontent, initial mass functions, and star formation burstiness in the future."
                },
                "authors": [
                    {
                        "name": "Trey Driskell"
                    },
                    {
                        "name": "Ethan O. Nadler"
                    },
                    {
                        "name": "Andrew Benson"
                    },
                    {
                        "name": "Vera Gluscevic"
                    }
                ],
                "author_detail": {
                    "name": "Vera Gluscevic"
                },
                "author": "Vera Gluscevic",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.15117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.15117v2",
                "updated": "2024-10-15T15:09:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    9,
                    41,
                    1,
                    289,
                    0
                ],
                "published": "2022-09-29T22:10:42Z",
                "published_parsed": [
                    2022,
                    9,
                    29,
                    22,
                    10,
                    42,
                    3,
                    272,
                    0
                ],
                "title": "Structured Optimal Variational Inference for Dynamic Latent Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured Optimal Variational Inference for Dynamic Latent Space Models"
                },
                "summary": "We consider a latent space model for dynamic networks, where our objective is\nto estimate the pairwise inner products plus the intercept of the latent\npositions. To balance posterior inference and computational scalability, we\nconsider a structured mean-field variational inference framework, where the\ntime-dependent properties of the dynamic networks are exploited to facilitate\ncomputation and inference. Additionally, an easy-to-implement block coordinate\nascent algorithm is developed with message-passing type updates in each block,\nwhereas the complexity per iteration is linear with the number of nodes and\ntime points. To certify the optimality, we demonstrate that the variational\nrisk of the proposed variational inference approach attains the minimax optimal\nrate with only a logarithm factor under certain conditions. To this end, we\nfirst derive the minimax lower bound, which might be of independent interest.\nIn addition, we show that the posterior under commonly adopted Gaussian random\nwalk priors can achieve the minimax lower bound with only a logarithm factor.\nTo the best of our knowledge, this is the first such a throughout theoretical\nanalysis of Bayesian dynamic latent space models. Simulations and real data\nanalysis demonstrate the efficacy of our methodology and the efficiency of our\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a latent space model for dynamic networks, where our objective is\nto estimate the pairwise inner products plus the intercept of the latent\npositions. To balance posterior inference and computational scalability, we\nconsider a structured mean-field variational inference framework, where the\ntime-dependent properties of the dynamic networks are exploited to facilitate\ncomputation and inference. Additionally, an easy-to-implement block coordinate\nascent algorithm is developed with message-passing type updates in each block,\nwhereas the complexity per iteration is linear with the number of nodes and\ntime points. To certify the optimality, we demonstrate that the variational\nrisk of the proposed variational inference approach attains the minimax optimal\nrate with only a logarithm factor under certain conditions. To this end, we\nfirst derive the minimax lower bound, which might be of independent interest.\nIn addition, we show that the posterior under commonly adopted Gaussian random\nwalk priors can achieve the minimax lower bound with only a logarithm factor.\nTo the best of our knowledge, this is the first such a throughout theoretical\nanalysis of Bayesian dynamic latent space models. Simulations and real data\nanalysis demonstrate the efficacy of our methodology and the efficiency of our\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Peng Zhao"
                    },
                    {
                        "name": "Anirban Bhattacharya"
                    },
                    {
                        "name": "Debdeep Pati"
                    },
                    {
                        "name": "Bani K. Mallick"
                    }
                ],
                "author_detail": {
                    "name": "Bani K. Mallick"
                },
                "author": "Bani K. Mallick",
                "arxiv_comment": "Accepted by the Journal of Machine Learning Research\n  http://jmlr.org/papers/v25/22-0514.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.15117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.15117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11674v1",
                "updated": "2024-10-15T15:08:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    8,
                    57,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:08:57Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    8,
                    57,
                    1,
                    289,
                    0
                ],
                "title": "LLM-Mixer: Multiscale Mixing in LLMs for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Mixer: Multiscale Mixing in LLMs for Time Series Forecasting"
                },
                "summary": "Time series forecasting remains a challenging task, particularly in the\ncontext of complex multiscale temporal patterns. This study presents LLM-Mixer,\na framework that improves forecasting accuracy through the combination of\nmultiscale time-series decomposition with pre-trained LLMs (Large Language\nModels). LLM-Mixer captures both short-term fluctuations and long-term trends\nby decomposing the data into multiple temporal resolutions and processing them\nwith a frozen LLM, guided by a textual prompt specifically designed for\ntime-series data. Extensive experiments conducted on multivariate and\nunivariate datasets demonstrate that LLM-Mixer achieves competitive\nperformance, outperforming recent state-of-the-art models across various\nforecasting horizons. This work highlights the potential of combining\nmultiscale analysis and LLMs for effective and scalable time-series\nforecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting remains a challenging task, particularly in the\ncontext of complex multiscale temporal patterns. This study presents LLM-Mixer,\na framework that improves forecasting accuracy through the combination of\nmultiscale time-series decomposition with pre-trained LLMs (Large Language\nModels). LLM-Mixer captures both short-term fluctuations and long-term trends\nby decomposing the data into multiple temporal resolutions and processing them\nwith a frozen LLM, guided by a textual prompt specifically designed for\ntime-series data. Extensive experiments conducted on multivariate and\nunivariate datasets demonstrate that LLM-Mixer achieves competitive\nperformance, outperforming recent state-of-the-art models across various\nforecasting horizons. This work highlights the potential of combining\nmultiscale analysis and LLMs for effective and scalable time-series\nforecasting."
                },
                "authors": [
                    {
                        "name": "Md Kowsher"
                    },
                    {
                        "name": "Md. Shohanur Islam Sobuj"
                    },
                    {
                        "name": "Nusrat Jahan Prottasha"
                    },
                    {
                        "name": "E. Alejandro Alanis"
                    },
                    {
                        "name": "Ozlem Ozmen Garibay"
                    },
                    {
                        "name": "Niloofar Yousefi"
                    }
                ],
                "author_detail": {
                    "name": "Niloofar Yousefi"
                },
                "author": "Niloofar Yousefi",
                "arxiv_comment": "Time series forecasting using LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02953v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02953v2",
                "updated": "2024-10-15T15:08:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    8,
                    32,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-03T19:53:47Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    19,
                    53,
                    47,
                    3,
                    277,
                    0
                ],
                "title": "Unlocking Structured Thinking in Language Models with Cognitive\n  Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Structured Thinking in Language Models with Cognitive\n  Prompting"
                },
                "summary": "We propose cognitive prompting as a novel approach to guide problem-solving\nin large language models (LLMs) through structured, human-like cognitive\noperations such as goal clarification, decomposition, filtering, abstraction,\nand pattern recognition. By employing systematic, step-by-step reasoning,\ncognitive prompting enables LLMs to efficiently tackle complex, multi-step\ntasks. We evaluate the effectiveness of cognitive prompting on Meta's LLaMA\nmodels, comparing performance on arithmetic reasoning tasks using the GSM8K\ndataset and on commonsense reasoning benchmarks. Our analysis includes\ncomparisons between models without cognitive prompting, models with a static\nsequence of cognitive operations, and models using reflective cognitive\nprompting, where the LLM dynamically self-selects the sequence of cognitive\noperations. The results show that cognitive prompting, particularly when\ndynamically adapted, significantly improves the performance of larger models,\nsuch as LLaMA3.1 70B, and enhances their ability to handle multi-step reasoning\ntasks. This approach also improves interpretability and flexibility,\nhighlighting cognitive prompting as a promising strategy for general-purpose AI\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cognitive prompting as a novel approach to guide problem-solving\nin large language models (LLMs) through structured, human-like cognitive\noperations such as goal clarification, decomposition, filtering, abstraction,\nand pattern recognition. By employing systematic, step-by-step reasoning,\ncognitive prompting enables LLMs to efficiently tackle complex, multi-step\ntasks. We evaluate the effectiveness of cognitive prompting on Meta's LLaMA\nmodels, comparing performance on arithmetic reasoning tasks using the GSM8K\ndataset and on commonsense reasoning benchmarks. Our analysis includes\ncomparisons between models without cognitive prompting, models with a static\nsequence of cognitive operations, and models using reflective cognitive\nprompting, where the LLM dynamically self-selects the sequence of cognitive\noperations. The results show that cognitive prompting, particularly when\ndynamically adapted, significantly improves the performance of larger models,\nsuch as LLaMA3.1 70B, and enhances their ability to handle multi-step reasoning\ntasks. This approach also improves interpretability and flexibility,\nhighlighting cognitive prompting as a promising strategy for general-purpose AI\nreasoning."
                },
                "authors": [
                    {
                        "name": "Oliver Kramer"
                    },
                    {
                        "name": "Jill Baumann"
                    }
                ],
                "author_detail": {
                    "name": "Jill Baumann"
                },
                "author": "Jill Baumann",
                "arxiv_comment": "11 pages, submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02953v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02953v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11672v1",
                "updated": "2024-10-15T15:05:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    5,
                    41,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:05:41Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    5,
                    41,
                    1,
                    289,
                    0
                ],
                "title": "Leaving the barn door open for Clever Hans: Simple features predict LLM\n  benchmark answers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leaving the barn door open for Clever Hans: Simple features predict LLM\n  benchmark answers"
                },
                "summary": "The integrity of AI benchmarks is fundamental to accurately assess the\ncapabilities of AI systems. The internal validity of these benchmarks - i.e.,\nmaking sure they are free from confounding factors - is crucial for ensuring\nthat they are measuring what they are designed to measure. In this paper, we\nexplore a key issue related to internal validity: the possibility that AI\nsystems can solve benchmarks in unintended ways, bypassing the capability being\ntested. This phenomenon, widely known in human and animal experiments, is often\nreferred to as the 'Clever Hans' effect, where tasks are solved using spurious\ncues, often involving much simpler processes than those putatively assessed.\nPrevious research suggests that language models can exhibit this behaviour as\nwell. In several older Natural Language Processing (NLP) benchmarks, individual\n$n$-grams like \"not\" have been found to be highly predictive of the correct\nlabels, and supervised NLP models have been shown to exploit these patterns. In\nthis work, we investigate the extent to which simple $n$-grams extracted from\nbenchmark instances can be combined to predict labels in modern multiple-choice\nbenchmarks designed for LLMs, and whether LLMs might be using such $n$-gram\npatterns to solve these benchmarks. We show how simple classifiers trained on\nthese $n$-grams can achieve high scores on several benchmarks, despite lacking\nthe capabilities being tested. Additionally, we provide evidence that modern\nLLMs might be using these superficial patterns to solve benchmarks. This\nsuggests that the internal validity of these benchmarks may be compromised and\ncaution should be exercised when interpreting LLM performance results on them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integrity of AI benchmarks is fundamental to accurately assess the\ncapabilities of AI systems. The internal validity of these benchmarks - i.e.,\nmaking sure they are free from confounding factors - is crucial for ensuring\nthat they are measuring what they are designed to measure. In this paper, we\nexplore a key issue related to internal validity: the possibility that AI\nsystems can solve benchmarks in unintended ways, bypassing the capability being\ntested. This phenomenon, widely known in human and animal experiments, is often\nreferred to as the 'Clever Hans' effect, where tasks are solved using spurious\ncues, often involving much simpler processes than those putatively assessed.\nPrevious research suggests that language models can exhibit this behaviour as\nwell. In several older Natural Language Processing (NLP) benchmarks, individual\n$n$-grams like \"not\" have been found to be highly predictive of the correct\nlabels, and supervised NLP models have been shown to exploit these patterns. In\nthis work, we investigate the extent to which simple $n$-grams extracted from\nbenchmark instances can be combined to predict labels in modern multiple-choice\nbenchmarks designed for LLMs, and whether LLMs might be using such $n$-gram\npatterns to solve these benchmarks. We show how simple classifiers trained on\nthese $n$-grams can achieve high scores on several benchmarks, despite lacking\nthe capabilities being tested. Additionally, we provide evidence that modern\nLLMs might be using these superficial patterns to solve benchmarks. This\nsuggests that the internal validity of these benchmarks may be compromised and\ncaution should be exercised when interpreting LLM performance results on them."
                },
                "authors": [
                    {
                        "name": "Lorenzo Pacchiardi"
                    },
                    {
                        "name": "Marko Tesic"
                    },
                    {
                        "name": "Lucy G. Cheke"
                    },
                    {
                        "name": "José Hernández-Orallo"
                    }
                ],
                "author_detail": {
                    "name": "José Hernández-Orallo"
                },
                "author": "José Hernández-Orallo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02052v2",
                "updated": "2024-10-15T14:59:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    59,
                    46,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-02T21:42:35Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    21,
                    42,
                    35,
                    2,
                    276,
                    0
                ],
                "title": "Teaching AI Agents to Search with Reflective-MCTS and Exploratory\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching AI Agents to Search with Reflective-MCTS and Exploratory\n  Learning"
                },
                "summary": "Autonomous agents have demonstrated significant potential in automating\ncomplex multistep decision-making tasks. However, even state-of-the-art\nvision-language models (VLMs), such as GPT-4o, still fall short of human-level\nperformance, particularly in intricate web environments and long-horizon\nplanning tasks. To address these limitations, we present Reflective Monte Carlo\nTree Search (R-MCTS) and Exploratory Learning to build o1-like models for\nagentic applications. We first introduce R-MCTS, a novel test-time algorithm\ndesigned to enhance the ability of AI agents to explore decision space on the\nfly. R-MCTS extends traditional MCTS by 1) incorporating contrastive\nreflection, allowing agents to learn from past interactions and dynamically\nimprove their search efficiency; and 2) using multi-agent debate to provide\nreliable state evaluation. Next, we introduce Exploratory Learning, a novel\nlearning strategy to teach agents to search at inference time without relying\non any external search algorithms. On the challenging VisualWebArena benchmark,\nour GPT-4o-based R-MCTS agent achieves a 6% to 30% relative improvement across\nvarious tasks compared to the previous state-of-the-art. Additionally, we show\nthat the experience gained from test-time search can be effectively transferred\nback to GPT-4o via fine-tuning. After Exploratory Learning, GPT-4o 1)\ndemonstrates the ability to explore the environment, evaluate a state, and\nbacktrack to viable ones when it detects that the current state cannot lead to\nsuccess, and 2) matches 87% of R-MCTS's performance while using significantly\nless compute. Notably, our work demonstrates the compute scaling properties in\nboth training - data collection with R-MCTS - and testing time. These results\nsuggest a promising research direction to enhance VLMs' reasoning and planning\ncapabilities for agentic applications via test-time search and self-learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous agents have demonstrated significant potential in automating\ncomplex multistep decision-making tasks. However, even state-of-the-art\nvision-language models (VLMs), such as GPT-4o, still fall short of human-level\nperformance, particularly in intricate web environments and long-horizon\nplanning tasks. To address these limitations, we present Reflective Monte Carlo\nTree Search (R-MCTS) and Exploratory Learning to build o1-like models for\nagentic applications. We first introduce R-MCTS, a novel test-time algorithm\ndesigned to enhance the ability of AI agents to explore decision space on the\nfly. R-MCTS extends traditional MCTS by 1) incorporating contrastive\nreflection, allowing agents to learn from past interactions and dynamically\nimprove their search efficiency; and 2) using multi-agent debate to provide\nreliable state evaluation. Next, we introduce Exploratory Learning, a novel\nlearning strategy to teach agents to search at inference time without relying\non any external search algorithms. On the challenging VisualWebArena benchmark,\nour GPT-4o-based R-MCTS agent achieves a 6% to 30% relative improvement across\nvarious tasks compared to the previous state-of-the-art. Additionally, we show\nthat the experience gained from test-time search can be effectively transferred\nback to GPT-4o via fine-tuning. After Exploratory Learning, GPT-4o 1)\ndemonstrates the ability to explore the environment, evaluate a state, and\nbacktrack to viable ones when it detects that the current state cannot lead to\nsuccess, and 2) matches 87% of R-MCTS's performance while using significantly\nless compute. Notably, our work demonstrates the compute scaling properties in\nboth training - data collection with R-MCTS - and testing time. These results\nsuggest a promising research direction to enhance VLMs' reasoning and planning\ncapabilities for agentic applications via test-time search and self-learning."
                },
                "authors": [
                    {
                        "name": "Xiao Yu"
                    },
                    {
                        "name": "Baolin Peng"
                    },
                    {
                        "name": "Vineeth Vajipey"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Michel Galley"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Zhou Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Yu"
                },
                "author": "Zhou Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11668v1",
                "updated": "2024-10-15T14:54:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    54,
                    53,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T14:54:53Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    54,
                    53,
                    1,
                    289,
                    0
                ],
                "title": "Exploiting the high-resolution NIKA2 data to study the intracluster\n  medium and dynamical state of ACT-CL J0240.0+0116",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting the high-resolution NIKA2 data to study the intracluster\n  medium and dynamical state of ACT-CL J0240.0+0116"
                },
                "summary": "Having a detailed knowledge of the intracluster medium (ICM) to infer the\nexact cluster physics such as the cluster dynamical state is crucial for\ncluster-based cosmological studies. This knowledge limits the accuracy and\nprecision of mass estimation, a key parameter for such studies. In this paper,\nwe conduct an in-depth analysis of cluster ACT-CL J0240.0+0116 using a\nmulti-wavelength approach, with a primary focus on high angular resolution\nSunyaev-Zeldovich (SZ) thermal component observations obtained under the NIKA2\nSunyaev-Zeldovich Large Programme (LPSZ). We create composite images using\nNIKA2, X-ray, and optical galaxy number density maps. The results reveal\ndistinct signs of disturbance within the cluster with the distributions of gas\nand member galaxies that do not overlap. We also find suggestions of an inflow\nof matter onto the cluster from the southwestern direction. Ultimately, we\nclassify the cluster as disturbed, using morphological indicators derived from\nits SZ, X-ray, and optical image. The cluster SZ signal is also contaminated by\na strong central point source. We adopt different approaches to handling this\ncontaminant and find the estimates of our pressure and hydrostatic mass\nprofiles robust to the point source mitigation model. The cluster hydrostatic\nmass is estimated at $4.25^{+0.50}_{-0.45\\, } \\times 10^{14}\n\\,\\mathrm{M}_{\\odot}$ for the case where the point source was masked. These\nvalues are consistent with the mass estimated using only X-ray data and with\nthose from previous SZ studies of the Atacama cosmology telescope (ACT) survey,\nwith improved precision on the mass estimate. Our findings strongly suggest\nthat ACT-CL J0240.0+0116 is a disturbed cluster system, and the detailed\nobservations and derived values serve as a compelling case study for the\ncapabilities of the LPSZ in mapping the cluster ICM with high precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Having a detailed knowledge of the intracluster medium (ICM) to infer the\nexact cluster physics such as the cluster dynamical state is crucial for\ncluster-based cosmological studies. This knowledge limits the accuracy and\nprecision of mass estimation, a key parameter for such studies. In this paper,\nwe conduct an in-depth analysis of cluster ACT-CL J0240.0+0116 using a\nmulti-wavelength approach, with a primary focus on high angular resolution\nSunyaev-Zeldovich (SZ) thermal component observations obtained under the NIKA2\nSunyaev-Zeldovich Large Programme (LPSZ). We create composite images using\nNIKA2, X-ray, and optical galaxy number density maps. The results reveal\ndistinct signs of disturbance within the cluster with the distributions of gas\nand member galaxies that do not overlap. We also find suggestions of an inflow\nof matter onto the cluster from the southwestern direction. Ultimately, we\nclassify the cluster as disturbed, using morphological indicators derived from\nits SZ, X-ray, and optical image. The cluster SZ signal is also contaminated by\na strong central point source. We adopt different approaches to handling this\ncontaminant and find the estimates of our pressure and hydrostatic mass\nprofiles robust to the point source mitigation model. The cluster hydrostatic\nmass is estimated at $4.25^{+0.50}_{-0.45\\, } \\times 10^{14}\n\\,\\mathrm{M}_{\\odot}$ for the case where the point source was masked. These\nvalues are consistent with the mass estimated using only X-ray data and with\nthose from previous SZ studies of the Atacama cosmology telescope (ACT) survey,\nwith improved precision on the mass estimate. Our findings strongly suggest\nthat ACT-CL J0240.0+0116 is a disturbed cluster system, and the detailed\nobservations and derived values serve as a compelling case study for the\ncapabilities of the LPSZ in mapping the cluster ICM with high precision."
                },
                "authors": [
                    {
                        "name": "A. Paliwal"
                    },
                    {
                        "name": "M. De Petris"
                    },
                    {
                        "name": "A. Ferragamo"
                    },
                    {
                        "name": "R. Adam"
                    },
                    {
                        "name": "P. Ade"
                    },
                    {
                        "name": "H. Ajeddig"
                    },
                    {
                        "name": "P. André"
                    },
                    {
                        "name": "E. Artis"
                    },
                    {
                        "name": "H. Aussel"
                    },
                    {
                        "name": "I. Bartalucci"
                    },
                    {
                        "name": "A. Beelen"
                    },
                    {
                        "name": "A. Benoît"
                    },
                    {
                        "name": "S. Berta"
                    },
                    {
                        "name": "L. Bing"
                    },
                    {
                        "name": "O. Bourrion"
                    },
                    {
                        "name": "M. Calvo"
                    },
                    {
                        "name": "A. Catalano"
                    },
                    {
                        "name": "F. De Luca"
                    },
                    {
                        "name": "F. -X. Désert"
                    },
                    {
                        "name": "S. Doyle"
                    },
                    {
                        "name": "E. F. C. Driessen"
                    },
                    {
                        "name": "G. Ejlali"
                    },
                    {
                        "name": "A. Gomez"
                    },
                    {
                        "name": "J. Goupy"
                    },
                    {
                        "name": "C. Hanser"
                    },
                    {
                        "name": "S. Katsioli"
                    },
                    {
                        "name": "F. Kéruzoré"
                    },
                    {
                        "name": "C. Kramer"
                    },
                    {
                        "name": "B. Ladjelate"
                    },
                    {
                        "name": "G. Lagache"
                    },
                    {
                        "name": "S. Leclercq"
                    },
                    {
                        "name": "J. -F. Lestrade"
                    },
                    {
                        "name": "J. F. Macías-Pérez"
                    },
                    {
                        "name": "S. C. Madden"
                    },
                    {
                        "name": "A. Maury"
                    },
                    {
                        "name": "P. Mauskopf"
                    },
                    {
                        "name": "F. Mayet"
                    },
                    {
                        "name": "J. -B. Melin"
                    },
                    {
                        "name": "A. Monfardini"
                    },
                    {
                        "name": "A. Moyer-Anin"
                    },
                    {
                        "name": "M. Muñoz-Echeverría"
                    },
                    {
                        "name": "L. Perotto"
                    },
                    {
                        "name": "G. Pisano"
                    },
                    {
                        "name": "E. Pointecouteau"
                    },
                    {
                        "name": "N. Ponthieu"
                    },
                    {
                        "name": "G. W. Pratt"
                    },
                    {
                        "name": "V. Revéret"
                    },
                    {
                        "name": "A. J. Rigby"
                    },
                    {
                        "name": "A. Ritacco"
                    },
                    {
                        "name": "C. Romero"
                    },
                    {
                        "name": "H. Roussel"
                    },
                    {
                        "name": "F. Ruppin"
                    },
                    {
                        "name": "K. Schuster"
                    },
                    {
                        "name": "A. Sievers"
                    },
                    {
                        "name": "C. Tucker"
                    },
                    {
                        "name": "R. Wicker"
                    },
                    {
                        "name": "R. Zylka"
                    }
                ],
                "author_detail": {
                    "name": "R. Zylka"
                },
                "author": "R. Zylka",
                "arxiv_comment": "13 pages, 7 figures, submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04148v2",
                "updated": "2024-10-15T14:53:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    53,
                    4,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-05T14:51:03Z",
                "published_parsed": [
                    2024,
                    4,
                    5,
                    14,
                    51,
                    3,
                    4,
                    96,
                    0
                ],
                "title": "GA-NIFS: An extremely nitrogen-loud and chemically stratified galaxy at\n  $z\\sim 5.55$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GA-NIFS: An extremely nitrogen-loud and chemically stratified galaxy at\n  $z\\sim 5.55$"
                },
                "summary": "We report the chemical abundance pattern of GS\\_3073, a galaxy at $z=5.55$\nwhich was previously confirmed to host an overmassive active black hole, by\nleveraging the detection of about 40 emission lines, combining JWST/NIRSpec\nobservations and ground-based (VLT/VIMOS) data. Based on the rest-frame UV\nemission lines, which trace high-density ($\\sim 10^5~{\\rm cm}^{-3}$) and highly\nionized gas, we derived an abundance ratio of $\\rm log(N/O) =\n0.42^{+0.13}_{-0.10}$. At an estimated metallicity of $0.2~Z_{\\odot}$, this is\nthe most extreme nitrogen-rich object found by JWST thus far. In comparison,\nthe relative carbon abundance derived from the rest-frame UV emission lines is\n$\\rm log(C/O) = -0.38^{+0.13}_{-0.11}$, which is not significantly higher than\nthose in local galaxies and stars with similar metallicities. We also found\npotential detection of coronal lines including [FeVII]$\\lambda 6087$ and\n[FeXIV]$\\lambda 5303$, both blended with [CaV]. We inferred a range of Fe\nabundances compatible with those in local stars and galaxies. Overall, the\nchemical abundance pattern of GS\\_3073 is compatible with enrichment by\nsuper-massive stars with $M_* \\gtrsim 1000~M_\\odot$, asymptotic giant branch\n(AGB) stars, or Wolf-Rayet (WR) stars. Interestingly, when using optical\nemission lines which trace lower density ($\\sim 10^3~{\\rm cm}^{-3}$) and lower\nionization gas, we found a sub-solar N/O ratio. We interpret the difference in\nN/O derived from UV lines and optical lines as evidence for a stratified\nsystem, where the inner and denser region is both more chemically enriched and\nmore ionized. Taking this luminous, well-studied system as a benchmark, our\nresults suggest that nitrogen loudness in high-$z$ galaxies is confined to the\ncentral, dense, and highly ionized region of the galaxies, while the bulk of\nthe galaxies evolves more normally.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report the chemical abundance pattern of GS\\_3073, a galaxy at $z=5.55$\nwhich was previously confirmed to host an overmassive active black hole, by\nleveraging the detection of about 40 emission lines, combining JWST/NIRSpec\nobservations and ground-based (VLT/VIMOS) data. Based on the rest-frame UV\nemission lines, which trace high-density ($\\sim 10^5~{\\rm cm}^{-3}$) and highly\nionized gas, we derived an abundance ratio of $\\rm log(N/O) =\n0.42^{+0.13}_{-0.10}$. At an estimated metallicity of $0.2~Z_{\\odot}$, this is\nthe most extreme nitrogen-rich object found by JWST thus far. In comparison,\nthe relative carbon abundance derived from the rest-frame UV emission lines is\n$\\rm log(C/O) = -0.38^{+0.13}_{-0.11}$, which is not significantly higher than\nthose in local galaxies and stars with similar metallicities. We also found\npotential detection of coronal lines including [FeVII]$\\lambda 6087$ and\n[FeXIV]$\\lambda 5303$, both blended with [CaV]. We inferred a range of Fe\nabundances compatible with those in local stars and galaxies. Overall, the\nchemical abundance pattern of GS\\_3073 is compatible with enrichment by\nsuper-massive stars with $M_* \\gtrsim 1000~M_\\odot$, asymptotic giant branch\n(AGB) stars, or Wolf-Rayet (WR) stars. Interestingly, when using optical\nemission lines which trace lower density ($\\sim 10^3~{\\rm cm}^{-3}$) and lower\nionization gas, we found a sub-solar N/O ratio. We interpret the difference in\nN/O derived from UV lines and optical lines as evidence for a stratified\nsystem, where the inner and denser region is both more chemically enriched and\nmore ionized. Taking this luminous, well-studied system as a benchmark, our\nresults suggest that nitrogen loudness in high-$z$ galaxies is confined to the\ncentral, dense, and highly ionized region of the galaxies, while the bulk of\nthe galaxies evolves more normally."
                },
                "authors": [
                    {
                        "name": "Xihan Ji"
                    },
                    {
                        "name": "Hannah Übler"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Santiago Arribas"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Stéphane Charlot"
                    },
                    {
                        "name": "Michele Perna"
                    },
                    {
                        "name": "Bruno Rodríguez Del Pino"
                    },
                    {
                        "name": "Torsten Böker"
                    },
                    {
                        "name": "Giovanni Cresci"
                    },
                    {
                        "name": "Mirko Curti"
                    },
                    {
                        "name": "Nimisha Kumari"
                    },
                    {
                        "name": "Isabella Lamperti"
                    }
                ],
                "author_detail": {
                    "name": "Isabella Lamperti"
                },
                "author": "Isabella Lamperti",
                "arxiv_comment": "28 pages, 15 figures, accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11919v2",
                "updated": "2024-10-15T14:52:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    52,
                    55,
                    1,
                    289,
                    0
                ],
                "published": "2024-09-18T12:32:25Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    12,
                    32,
                    25,
                    2,
                    262,
                    0
                ],
                "title": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language\n  Models for Referring Expression Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language\n  Models for Referring Expression Comprehension"
                },
                "summary": "Vision Language Models (VLMs) have demonstrated remarkable capabilities in\nvarious open-vocabulary tasks, yet their zero-shot performance lags behind\ntask-specific finetuned models, particularly in complex tasks like Referring\nExpression Comprehension (REC). Fine-tuning usually requires 'white-box' access\nto the model's architecture and weights, which is not always feasible due to\nproprietary or privacy concerns. In this work, we propose LLM-wrapper, a method\nfor 'black-box' adaptation of VLMs for the REC task using Large Language Models\n(LLMs). LLM-wrapper capitalizes on the reasoning abilities of LLMs, improved\nwith a light fine-tuning, to select the most relevant bounding box matching the\nreferring expression, from candidates generated by a zero-shot black-box VLM.\nOur approach offers several advantages: it enables the adaptation of\nclosed-source models without needing access to their internal workings, it is\nversatile as it works with any VLM, it transfers to new VLMs, and it allows for\nthe adaptation of an ensemble of VLMs. We evaluate LLM-wrapper on multiple\ndatasets using different VLMs and LLMs, demonstrating significant performance\nimprovements and highlighting the versatility of our method. While LLM-wrapper\nis not meant to directly compete with standard white-box fine-tuning, it offers\na practical and effective alternative for black-box VLM adaptation. The code\nwill be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have demonstrated remarkable capabilities in\nvarious open-vocabulary tasks, yet their zero-shot performance lags behind\ntask-specific finetuned models, particularly in complex tasks like Referring\nExpression Comprehension (REC). Fine-tuning usually requires 'white-box' access\nto the model's architecture and weights, which is not always feasible due to\nproprietary or privacy concerns. In this work, we propose LLM-wrapper, a method\nfor 'black-box' adaptation of VLMs for the REC task using Large Language Models\n(LLMs). LLM-wrapper capitalizes on the reasoning abilities of LLMs, improved\nwith a light fine-tuning, to select the most relevant bounding box matching the\nreferring expression, from candidates generated by a zero-shot black-box VLM.\nOur approach offers several advantages: it enables the adaptation of\nclosed-source models without needing access to their internal workings, it is\nversatile as it works with any VLM, it transfers to new VLMs, and it allows for\nthe adaptation of an ensemble of VLMs. We evaluate LLM-wrapper on multiple\ndatasets using different VLMs and LLMs, demonstrating significant performance\nimprovements and highlighting the versatility of our method. While LLM-wrapper\nis not meant to directly compete with standard white-box fine-tuning, it offers\na practical and effective alternative for black-box VLM adaptation. The code\nwill be open-sourced."
                },
                "authors": [
                    {
                        "name": "Amaia Cardiel"
                    },
                    {
                        "name": "Eloi Zablocki"
                    },
                    {
                        "name": "Elias Ramzi"
                    },
                    {
                        "name": "Oriane Siméoni"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "v1 at EVAL-FoMo workshop, ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11660v1",
                "updated": "2024-10-15T14:46:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    46,
                    11,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T14:46:11Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    46,
                    11,
                    1,
                    289,
                    0
                ],
                "title": "Eliciting Textual Descriptions from Representations of Continuous\n  Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting Textual Descriptions from Representations of Continuous\n  Prompts"
                },
                "summary": "Continuous prompts, or \"soft prompts\", are a widely-adopted\nparameter-efficient tuning strategy for large language models, but are often\nless favorable due to their opaque nature. Prior attempts to interpret\ncontinuous prompts relied on projecting individual prompt tokens onto the\nvocabulary space. However, this approach is problematic as performant prompts\ncan yield arbitrary or contradictory text, and it interprets prompt tokens\nindividually. In this work, we propose a new approach to interpret continuous\nprompts that elicits textual descriptions from their representations during\nmodel inference. Using a Patchscopes variant (Ghandeharioun et al., 2024)\ncalled InSPEcT over various tasks, we show our method often yields accurate\ntask descriptions which become more faithful as task performance increases.\nMoreover, an elaborated version of InSPEcT reveals biased features in\ncontinuous prompts, whose presence correlates with biased model predictions.\nProviding an effective interpretability solution, InSPEcT can be leveraged to\ndebug unwanted properties in continuous prompts and inform developers on ways\nto mitigate them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous prompts, or \"soft prompts\", are a widely-adopted\nparameter-efficient tuning strategy for large language models, but are often\nless favorable due to their opaque nature. Prior attempts to interpret\ncontinuous prompts relied on projecting individual prompt tokens onto the\nvocabulary space. However, this approach is problematic as performant prompts\ncan yield arbitrary or contradictory text, and it interprets prompt tokens\nindividually. In this work, we propose a new approach to interpret continuous\nprompts that elicits textual descriptions from their representations during\nmodel inference. Using a Patchscopes variant (Ghandeharioun et al., 2024)\ncalled InSPEcT over various tasks, we show our method often yields accurate\ntask descriptions which become more faithful as task performance increases.\nMoreover, an elaborated version of InSPEcT reveals biased features in\ncontinuous prompts, whose presence correlates with biased model predictions.\nProviding an effective interpretability solution, InSPEcT can be leveraged to\ndebug unwanted properties in continuous prompts and inform developers on ways\nto mitigate them."
                },
                "authors": [
                    {
                        "name": "Dana Ramati"
                    },
                    {
                        "name": "Daniela Gottesman"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11655v1",
                "updated": "2024-10-15T14:42:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    42,
                    18,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T14:42:18Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    42,
                    18,
                    1,
                    289,
                    0
                ],
                "title": "Retrieval Augmented Spelling Correction for E-Commerce Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Spelling Correction for E-Commerce Applications"
                },
                "summary": "The rapid introduction of new brand names into everyday language poses a\nunique challenge for e-commerce spelling correction services, which must\ndistinguish genuine misspellings from novel brand names that use unconventional\nspelling. We seek to address this challenge via Retrieval Augmented Generation\n(RAG). On this approach, product names are retrieved from a catalog and\nincorporated into the context used by a large language model (LLM) that has\nbeen fine-tuned to do contextual spelling correction. Through quantitative\nevaluation and qualitative error analyses, we find improvements in spelling\ncorrection utilizing the RAG framework beyond a stand-alone LLM. We also\ndemonstrate the value of additional finetuning of the LLM to incorporate\nretrieved context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid introduction of new brand names into everyday language poses a\nunique challenge for e-commerce spelling correction services, which must\ndistinguish genuine misspellings from novel brand names that use unconventional\nspelling. We seek to address this challenge via Retrieval Augmented Generation\n(RAG). On this approach, product names are retrieved from a catalog and\nincorporated into the context used by a large language model (LLM) that has\nbeen fine-tuned to do contextual spelling correction. Through quantitative\nevaluation and qualitative error analyses, we find improvements in spelling\ncorrection utilizing the RAG framework beyond a stand-alone LLM. We also\ndemonstrate the value of additional finetuning of the LLM to incorporate\nretrieved context."
                },
                "authors": [
                    {
                        "name": "Xuan Guo"
                    },
                    {
                        "name": "Rohit Patki"
                    },
                    {
                        "name": "Dante Everaert"
                    },
                    {
                        "name": "Christopher Potts"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Potts"
                },
                "author": "Christopher Potts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11654v1",
                "updated": "2024-10-15T14:41:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    41,
                    44,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T14:41:44Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    41,
                    44,
                    1,
                    289,
                    0
                ],
                "title": "Transformer Layer Injection: A Novel Approach for Efficient Upscaling of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer Layer Injection: A Novel Approach for Efficient Upscaling of\n  Large Language Models"
                },
                "summary": "In this paper, we propose Transformer Layer Injection (TLI), a novel method\nfor efficiently upscaling large language models (LLMs) while minimizing\ncomputational costs and maintaining model performance. Model scale is a key\nfactor in enhancing the quality of machine learning models, and TLI addresses\nthe challenge of scaling by reducing initial loss, minimizing fine-tuning\nrequirements, and preserving model complexity. Our approach improves upon the\nconventional Depth Up-Scaling (DUS) technique by injecting new layers into\nevery set of K layers, enabling hidden representations to pass through\ntransformer blocks with minimal disruption. We compare TLI with existing\napproaches, including Mixture of Experts (MoE) and DUS, and validate its\nefficiency through experiments on small LLMs (LLama3 1B, 3B, and 8B). Results\nshow that TLI achieves better initialization, requires fewer training steps,\nand delivers superior accuracy on tasks such as KoBEST and KMCQA, with models\nperforming effectively even without additional training. TLI is demonstrated to\nbe both data-efficient and cost-effective, significantly outperforming existing\nmethods. Its scalability and simplicity make it a promising solution for\nupscaling transformer-based models, with potential applications in scaling\nmodels from 10B to 405B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Transformer Layer Injection (TLI), a novel method\nfor efficiently upscaling large language models (LLMs) while minimizing\ncomputational costs and maintaining model performance. Model scale is a key\nfactor in enhancing the quality of machine learning models, and TLI addresses\nthe challenge of scaling by reducing initial loss, minimizing fine-tuning\nrequirements, and preserving model complexity. Our approach improves upon the\nconventional Depth Up-Scaling (DUS) technique by injecting new layers into\nevery set of K layers, enabling hidden representations to pass through\ntransformer blocks with minimal disruption. We compare TLI with existing\napproaches, including Mixture of Experts (MoE) and DUS, and validate its\nefficiency through experiments on small LLMs (LLama3 1B, 3B, and 8B). Results\nshow that TLI achieves better initialization, requires fewer training steps,\nand delivers superior accuracy on tasks such as KoBEST and KMCQA, with models\nperforming effectively even without additional training. TLI is demonstrated to\nbe both data-efficient and cost-effective, significantly outperforming existing\nmethods. Its scalability and simplicity make it a promising solution for\nupscaling transformer-based models, with potential applications in scaling\nmodels from 10B to 405B parameters."
                },
                "authors": [
                    {
                        "name": "James Vo"
                    }
                ],
                "author_detail": {
                    "name": "James Vo"
                },
                "author": "James Vo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11650v1",
                "updated": "2024-10-15T14:38:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    38,
                    14,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T14:38:14Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    38,
                    14,
                    1,
                    289,
                    0
                ],
                "title": "ED-ViT: Splitting Vision Transformer for Distributed Inference on Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ED-ViT: Splitting Vision Transformer for Distributed Inference on Edge\n  Devices"
                },
                "summary": "Deep learning models are increasingly deployed on resource-constrained edge\ndevices for real-time data analytics. In recent years, Vision Transformer\nmodels and their variants have demonstrated outstanding performance across\nvarious computer vision tasks. However, their high computational demands and\ninference latency pose significant challenges for model deployment on\nresource-constraint edge devices. To address this issue, we propose a novel\nVision Transformer splitting framework, ED-ViT, designed to execute complex\nmodels across multiple edge devices efficiently. Specifically, we partition\nVision Transformer models into several sub-models, where each sub-model is\ntailored to handle a specific subset of data classes. To further minimize\ncomputation overhead and inference latency, we introduce a class-wise pruning\ntechnique that reduces the size of each sub-model. We conduct extensive\nexperiments on five datasets with three model structures, demonstrating that\nour approach significantly reduces inference latency on edge devices and\nachieves a model size reduction of up to 28.9 times and 34.1 times,\nrespectively, while maintaining test accuracy comparable to the original Vision\nTransformer. Additionally, we compare ED-ViT with two state-of-the-art methods\nthat deploy CNN and SNN models on edge devices, evaluating accuracy, inference\ntime, and overall model size. Our comprehensive evaluation underscores the\neffectiveness of the proposed ED-ViT framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models are increasingly deployed on resource-constrained edge\ndevices for real-time data analytics. In recent years, Vision Transformer\nmodels and their variants have demonstrated outstanding performance across\nvarious computer vision tasks. However, their high computational demands and\ninference latency pose significant challenges for model deployment on\nresource-constraint edge devices. To address this issue, we propose a novel\nVision Transformer splitting framework, ED-ViT, designed to execute complex\nmodels across multiple edge devices efficiently. Specifically, we partition\nVision Transformer models into several sub-models, where each sub-model is\ntailored to handle a specific subset of data classes. To further minimize\ncomputation overhead and inference latency, we introduce a class-wise pruning\ntechnique that reduces the size of each sub-model. We conduct extensive\nexperiments on five datasets with three model structures, demonstrating that\nour approach significantly reduces inference latency on edge devices and\nachieves a model size reduction of up to 28.9 times and 34.1 times,\nrespectively, while maintaining test accuracy comparable to the original Vision\nTransformer. Additionally, we compare ED-ViT with two state-of-the-art methods\nthat deploy CNN and SNN models on edge devices, evaluating accuracy, inference\ntime, and overall model size. Our comprehensive evaluation underscores the\neffectiveness of the proposed ED-ViT framework."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Yijun Song"
                    },
                    {
                        "name": "Xia Li"
                    },
                    {
                        "name": "Yifei Sun"
                    },
                    {
                        "name": "Huiying Lan"
                    },
                    {
                        "name": "Zemin Liu"
                    },
                    {
                        "name": "Linshan Jiang"
                    },
                    {
                        "name": "Jialin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Li"
                },
                "author": "Jialin Li",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11647v1",
                "updated": "2024-10-15T14:33:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    33,
                    23,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T14:33:23Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    33,
                    23,
                    1,
                    289,
                    0
                ],
                "title": "Measuring Spiritual Values and Bias of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Spiritual Values and Bias of Large Language Models"
                },
                "summary": "Large language models (LLMs) have become integral tool for users from various\nbackgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural\nnuances embedded in their pre-training data. However, the values and\nperspectives inherent in this data can influence the behavior of LLMs, leading\nto potential biases. As a result, the use of LLMs in contexts involving\nspiritual or moral values necessitates careful consideration of these\nunderlying biases. Our work starts with verification of our hypothesis by\ntesting the spiritual values of popular LLMs. Experimental results show that\nLLMs' spiritual values are quite diverse, as opposed to the stereotype of\natheists or secularists. We then investigate how different spiritual values\naffect LLMs in social-fairness scenarios e.g., hate speech identification). Our\nfindings reveal that different spiritual values indeed lead to different\nsensitivity to different hate target groups. Furthermore, we propose to\ncontinue pre-training LLMs on spiritual texts, and empirical results\ndemonstrate the effectiveness of this approach in mitigating spiritual bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become integral tool for users from various\nbackgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural\nnuances embedded in their pre-training data. However, the values and\nperspectives inherent in this data can influence the behavior of LLMs, leading\nto potential biases. As a result, the use of LLMs in contexts involving\nspiritual or moral values necessitates careful consideration of these\nunderlying biases. Our work starts with verification of our hypothesis by\ntesting the spiritual values of popular LLMs. Experimental results show that\nLLMs' spiritual values are quite diverse, as opposed to the stereotype of\natheists or secularists. We then investigate how different spiritual values\naffect LLMs in social-fairness scenarios e.g., hate speech identification). Our\nfindings reveal that different spiritual values indeed lead to different\nsensitivity to different hate target groups. Furthermore, we propose to\ncontinue pre-training LLMs on spiritual texts, and empirical results\ndemonstrate the effectiveness of this approach in mitigating spiritual bias."
                },
                "authors": [
                    {
                        "name": "Songyuan Liu"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Runze Yan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Carl Yang"
                    },
                    {
                        "name": "Jiaying Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaying Lu"
                },
                "author": "Jiaying Lu",
                "arxiv_comment": "9 pages including appendix; 5 figures; 5 tables; submitted to ARR -\n  Octobor 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11637v1",
                "updated": "2024-10-15T14:28:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    28,
                    15,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T14:28:15Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    28,
                    15,
                    1,
                    289,
                    0
                ],
                "title": "Prediction-Centric Uncertainty Quantification via MMD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction-Centric Uncertainty Quantification via MMD"
                },
                "summary": "Deterministic mathematical models, such as those specified via differential\nequations, are a powerful tool to communicate scientific insight. However, such\nmodels are necessarily simplified descriptions of the real world. Generalised\nBayesian methodologies have been proposed for inference with misspecified\nmodels, but these are typically associated with vanishing parameter uncertainty\nas more data are observed. In the context of a misspecified deterministic\nmathematical model, this has the undesirable consequence that posterior\npredictions become deterministic and certain, while being incorrect. Taking\nthis observation as a starting point, we propose Prediction-Centric Uncertainty\nQuantification, where a mixture distribution based on the deterministic model\nconfers improved uncertainty quantification in the predictive context.\nComputation of the mixing distribution is cast as a (regularised) gradient flow\nof the maximum mean discrepancy (MMD), enabling consistent numerical\napproximations to be obtained. Results are reported on both a toy model from\npopulation ecology and a real model of protein signalling in cell biology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deterministic mathematical models, such as those specified via differential\nequations, are a powerful tool to communicate scientific insight. However, such\nmodels are necessarily simplified descriptions of the real world. Generalised\nBayesian methodologies have been proposed for inference with misspecified\nmodels, but these are typically associated with vanishing parameter uncertainty\nas more data are observed. In the context of a misspecified deterministic\nmathematical model, this has the undesirable consequence that posterior\npredictions become deterministic and certain, while being incorrect. Taking\nthis observation as a starting point, we propose Prediction-Centric Uncertainty\nQuantification, where a mixture distribution based on the deterministic model\nconfers improved uncertainty quantification in the predictive context.\nComputation of the mixing distribution is cast as a (regularised) gradient flow\nof the maximum mean discrepancy (MMD), enabling consistent numerical\napproximations to be obtained. Results are reported on both a toy model from\npopulation ecology and a real model of protein signalling in cell biology."
                },
                "authors": [
                    {
                        "name": "Zheyang Shen"
                    },
                    {
                        "name": "Jeremias Knoblauch"
                    },
                    {
                        "name": "Sam Power"
                    },
                    {
                        "name": "Chris. J. Oates"
                    }
                ],
                "author_detail": {
                    "name": "Chris. J. Oates"
                },
                "author": "Chris. J. Oates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18921v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18921v3",
                "updated": "2024-10-15T14:24:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    24,
                    56,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-27T06:24:00Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    6,
                    24,
                    0,
                    3,
                    179,
                    0
                ],
                "title": "Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models\n  with Personality-Indicative Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models\n  with Personality-Indicative Data"
                },
                "summary": "Role-playing agents (RPA) have been a popular application area for large\nlanguage models (LLMs), attracting significant interest from both industry and\nacademia.While existing RPAs well portray the characters' knowledge and tones,\nthey face challenges in capturing their minds, especially for small\nrole-playing language models (RPLMs). In this paper, we propose to enhance\nRPLMs via personality-indicative data. Specifically, we leverage questions from\npsychological scales and distill advanced RPAs to generate dialogues that grasp\nthe minds of characters. Experimental results validate that RPLMs trained with\nour dataset exhibit advanced role-playing capabilities for both general and\npersonality-related evaluations. Code and data are available at\n\\href{https://github.com/alienet1109/RolePersonality}{this URL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-playing agents (RPA) have been a popular application area for large\nlanguage models (LLMs), attracting significant interest from both industry and\nacademia.While existing RPAs well portray the characters' knowledge and tones,\nthey face challenges in capturing their minds, especially for small\nrole-playing language models (RPLMs). In this paper, we propose to enhance\nRPLMs via personality-indicative data. Specifically, we leverage questions from\npsychological scales and distill advanced RPAs to generate dialogues that grasp\nthe minds of characters. Experimental results validate that RPLMs trained with\nour dataset exhibit advanced role-playing capabilities for both general and\npersonality-related evaluations. Code and data are available at\n\\href{https://github.com/alienet1109/RolePersonality}{this URL}."
                },
                "authors": [
                    {
                        "name": "Yiting Ran"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Xinfeng Yuan"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Deqing Yang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "11 pages, 1 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18921v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18921v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19759v2",
                "updated": "2024-10-15T14:22:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    22,
                    49,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-30T17:59:47Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    17,
                    59,
                    47,
                    1,
                    121,
                    0
                ],
                "title": "MotionLCM: Real-time Controllable Motion Generation via Latent\n  Consistency Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionLCM: Real-time Controllable Motion Generation via Latent\n  Consistency Model"
                },
                "summary": "This work introduces MotionLCM, extending controllable motion generation to a\nreal-time level. Existing methods for spatial-temporal control in\ntext-conditioned motion generation suffer from significant runtime\ninefficiency. To address this issue, we first propose the motion latent\nconsistency model (MotionLCM) for motion generation, building upon the latent\ndiffusion model. By adopting one-step (or few-step) inference, we further\nimprove the runtime efficiency of the motion latent diffusion model for motion\ngeneration. To ensure effective controllability, we incorporate a motion\nControlNet within the latent space of MotionLCM and enable explicit control\nsignals (e.g., initial poses) in the vanilla motion space to control the\ngeneration process directly, similar to controlling other latent-free diffusion\nmodels for motion generation. By employing these techniques, our approach can\ngenerate human motions with text and control signals in real-time. Experimental\nresults demonstrate the remarkable generation and controlling capabilities of\nMotionLCM while maintaining real-time runtime efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces MotionLCM, extending controllable motion generation to a\nreal-time level. Existing methods for spatial-temporal control in\ntext-conditioned motion generation suffer from significant runtime\ninefficiency. To address this issue, we first propose the motion latent\nconsistency model (MotionLCM) for motion generation, building upon the latent\ndiffusion model. By adopting one-step (or few-step) inference, we further\nimprove the runtime efficiency of the motion latent diffusion model for motion\ngeneration. To ensure effective controllability, we incorporate a motion\nControlNet within the latent space of MotionLCM and enable explicit control\nsignals (e.g., initial poses) in the vanilla motion space to control the\ngeneration process directly, similar to controlling other latent-free diffusion\nmodels for motion generation. By employing these techniques, our approach can\ngenerate human motions with text and control signals in real-time. Experimental\nresults demonstrate the remarkable generation and controlling capabilities of\nMotionLCM while maintaining real-time runtime efficiency."
                },
                "authors": [
                    {
                        "name": "Wenxun Dai"
                    },
                    {
                        "name": "Ling-Hao Chen"
                    },
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Jinpeng Liu"
                    },
                    {
                        "name": "Bo Dai"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "MotionLCM project version 1.0 (ECCV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12701v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12701v3",
                "updated": "2024-10-15T14:21:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    21,
                    54,
                    1,
                    289,
                    0
                ],
                "published": "2024-05-21T11:50:16Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    11,
                    50,
                    16,
                    1,
                    142,
                    0
                ],
                "title": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering"
                },
                "summary": "In the medical domain, numerous scenarios necessitate the long-form\ngeneration ability of large language models (LLMs). Specifically, when\naddressing patients' questions, it is essential that the model's response\nconveys factual claims, highlighting the need for an automated method to\nevaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset\nreconstructed using long-form question-answering datasets related to the\nbiomedical domain. We use MedLFQA to facilitate a cost-effective automatic\nevaluations of factuality. We also propose OLAPH, a simple and novel framework\nthat utilizes cost-effective and multifaceted automatic evaluation to construct\na synthetic preference set and answers questions in our preferred manner. Our\nframework leads us to train LLMs step-by-step to reduce hallucinations and\ninclude crucial medical claims. We highlight that, even on evaluation metrics\nnot used during training, LLMs trained with our OLAPH framework demonstrate\nsignificant performance improvement in factuality. Our findings reveal that a\n7B LLM trained with our OLAPH framework can provide long answers comparable to\nthe medical experts' answers in terms of factuality. We believe that our work\ncould shed light on gauging the long-text generation ability of LLMs in the\nmedical domain. Our code and datasets are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the medical domain, numerous scenarios necessitate the long-form\ngeneration ability of large language models (LLMs). Specifically, when\naddressing patients' questions, it is essential that the model's response\nconveys factual claims, highlighting the need for an automated method to\nevaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset\nreconstructed using long-form question-answering datasets related to the\nbiomedical domain. We use MedLFQA to facilitate a cost-effective automatic\nevaluations of factuality. We also propose OLAPH, a simple and novel framework\nthat utilizes cost-effective and multifaceted automatic evaluation to construct\na synthetic preference set and answers questions in our preferred manner. Our\nframework leads us to train LLMs step-by-step to reduce hallucinations and\ninclude crucial medical claims. We highlight that, even on evaluation metrics\nnot used during training, LLMs trained with our OLAPH framework demonstrate\nsignificant performance improvement in factuality. Our findings reveal that a\n7B LLM trained with our OLAPH framework can provide long answers comparable to\nthe medical experts' answers in terms of factuality. We believe that our work\ncould shed light on gauging the long-text generation ability of LLMs in the\nmedical domain. Our code and datasets are available."
                },
                "authors": [
                    {
                        "name": "Minbyul Jeong"
                    },
                    {
                        "name": "Hyeon Hwang"
                    },
                    {
                        "name": "Chanwoong Yoon"
                    },
                    {
                        "name": "Taewhoo Lee"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12701v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12701v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.11111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.11111v2",
                "updated": "2024-10-15T14:20:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    20,
                    51,
                    1,
                    289,
                    0
                ],
                "published": "2023-04-21T16:29:43Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    16,
                    29,
                    43,
                    4,
                    111,
                    0
                ],
                "title": "Inducing anxiety in large language models can induce bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inducing anxiety in large language models can induce bias"
                },
                "summary": "Large language models (LLMs) are transforming research on machine learning\nwhile galvanizing public debates. Understanding not only when these models work\nwell and succeed but also why they fail and misbehave is of great societal\nrelevance. We propose to turn the lens of psychiatry, a framework used to\ndescribe and modify maladaptive behavior, to the outputs produced by these\nmodels. We focus on twelve established LLMs and subject them to a questionnaire\ncommonly used in psychiatry. Our results show that six of the latest LLMs\nrespond robustly to the anxiety questionnaire, producing comparable anxiety\nscores to humans. Moreover, the LLMs' responses can be predictably changed by\nusing anxiety-inducing prompts. Anxiety-induction not only influences LLMs'\nscores on an anxiety questionnaire but also influences their behavior in a\npreviously-established benchmark measuring biases such as racism and ageism.\nImportantly, greater anxiety-inducing text leads to stronger increases in\nbiases, suggesting that how anxiously a prompt is communicated to large\nlanguage models has a strong influence on their behavior in applied settings.\nThese results demonstrate the usefulness of methods taken from psychiatry for\nstudying the capable algorithms to which we increasingly delegate authority and\nautonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are transforming research on machine learning\nwhile galvanizing public debates. Understanding not only when these models work\nwell and succeed but also why they fail and misbehave is of great societal\nrelevance. We propose to turn the lens of psychiatry, a framework used to\ndescribe and modify maladaptive behavior, to the outputs produced by these\nmodels. We focus on twelve established LLMs and subject them to a questionnaire\ncommonly used in psychiatry. Our results show that six of the latest LLMs\nrespond robustly to the anxiety questionnaire, producing comparable anxiety\nscores to humans. Moreover, the LLMs' responses can be predictably changed by\nusing anxiety-inducing prompts. Anxiety-induction not only influences LLMs'\nscores on an anxiety questionnaire but also influences their behavior in a\npreviously-established benchmark measuring biases such as racism and ageism.\nImportantly, greater anxiety-inducing text leads to stronger increases in\nbiases, suggesting that how anxiously a prompt is communicated to large\nlanguage models has a strong influence on their behavior in applied settings.\nThese results demonstrate the usefulness of methods taken from psychiatry for\nstudying the capable algorithms to which we increasingly delegate authority and\nautonomy."
                },
                "authors": [
                    {
                        "name": "Julian Coda-Forno"
                    },
                    {
                        "name": "Kristin Witte"
                    },
                    {
                        "name": "Akshay K. Jagadish"
                    },
                    {
                        "name": "Marcel Binz"
                    },
                    {
                        "name": "Zeynep Akata"
                    },
                    {
                        "name": "Eric Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Eric Schulz"
                },
                "author": "Eric Schulz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.11111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.11111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11620v1",
                "updated": "2024-10-15T13:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    58,
                    39,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T13:58:39Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    58,
                    39,
                    1,
                    289,
                    0
                ],
                "title": "Exploring the effect of different cosmologies on the Epoch of\n  Reionization 21-cm signal with POLAR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the effect of different cosmologies on the Epoch of\n  Reionization 21-cm signal with POLAR"
                },
                "summary": "A detection of the 21-cm signal power spectrum from the Epoch of Reionization\nis imminent, thanks to consistent advancements from telescopes such as LOFAR,\nMWA, and HERA, along with the development of SKA. In light of this progress, it\nis crucial to expand the parameter space of simulations used to infer\nastrophysical properties from this signal. In this work, we explore the role of\ncosmological parameters such as the Hubble constant $H_0$ and the matter\nclustering amplitude $\\sigma_8$, whose values as provided by measurements at\ndifferent redshifts are in tension. We run $N$-body simulations using GADGET-4,\nand post-process them with the reionization simulation code POLAR, that uses\nL-GALAXIES to include galaxy formation and evolution properties and GRIZZLY to\nexecute 1-D radiative transfer of ionizing photons in the intergalactic medium\n(IGM). We compare our results with the latest JWST observations and explore\nwhich astrophysical properties for different cosmologies are necessary to match\nthe observed UV luminosity functions at redshifts $z = 10$ and 9. Additionally,\nwe explore the impact of these parameters on the observed 21-cm signal power\nspectrum, focusing on the redshifts within the range of LOFAR 21-cm signal\nobservations ($z \\approx 8.5-10$). Despite differences in cosmological and\nastrophysical parameters, the 21-cm power spectrum at these redshifts agrees\nwith presently observed upper limits. This suggests the need for broader\nphysical parameter spaces for inference modeling to account for all models that\nagree with observations. However, we also propose stronger constraining power\nby using a combination of galactic and IGM observables.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A detection of the 21-cm signal power spectrum from the Epoch of Reionization\nis imminent, thanks to consistent advancements from telescopes such as LOFAR,\nMWA, and HERA, along with the development of SKA. In light of this progress, it\nis crucial to expand the parameter space of simulations used to infer\nastrophysical properties from this signal. In this work, we explore the role of\ncosmological parameters such as the Hubble constant $H_0$ and the matter\nclustering amplitude $\\sigma_8$, whose values as provided by measurements at\ndifferent redshifts are in tension. We run $N$-body simulations using GADGET-4,\nand post-process them with the reionization simulation code POLAR, that uses\nL-GALAXIES to include galaxy formation and evolution properties and GRIZZLY to\nexecute 1-D radiative transfer of ionizing photons in the intergalactic medium\n(IGM). We compare our results with the latest JWST observations and explore\nwhich astrophysical properties for different cosmologies are necessary to match\nthe observed UV luminosity functions at redshifts $z = 10$ and 9. Additionally,\nwe explore the impact of these parameters on the observed 21-cm signal power\nspectrum, focusing on the redshifts within the range of LOFAR 21-cm signal\nobservations ($z \\approx 8.5-10$). Despite differences in cosmological and\nastrophysical parameters, the 21-cm power spectrum at these redshifts agrees\nwith presently observed upper limits. This suggests the need for broader\nphysical parameter spaces for inference modeling to account for all models that\nagree with observations. However, we also propose stronger constraining power\nby using a combination of galactic and IGM observables."
                },
                "authors": [
                    {
                        "name": "Anshuman Acharya"
                    },
                    {
                        "name": "Qing-bo Ma"
                    },
                    {
                        "name": "Sambit K. Giri"
                    },
                    {
                        "name": "Benedetta Ciardi"
                    },
                    {
                        "name": "Raghunath Ghara"
                    },
                    {
                        "name": "Garrelt Mellema"
                    },
                    {
                        "name": "Saleem Zaroubi"
                    },
                    {
                        "name": "Ian Hothi"
                    },
                    {
                        "name": "Ilian T. Iliev"
                    },
                    {
                        "name": "Léon V. E. Koopmans"
                    },
                    {
                        "name": "Michele Bianco"
                    }
                ],
                "author_detail": {
                    "name": "Michele Bianco"
                },
                "author": "Michele Bianco",
                "arxiv_comment": "16 pages, 8 figures, 2 tables. Submitted to the Monthly Notices of\n  the Royal Astronomical Society (MNRAS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14205v2",
                "updated": "2024-10-15T13:58:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    58,
                    17,
                    1,
                    289,
                    0
                ],
                "published": "2024-05-23T06:03:19Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    6,
                    3,
                    19,
                    3,
                    144,
                    0
                ],
                "title": "Agent Planning with World Knowledge Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Planning with World Knowledge Model"
                },
                "summary": "Recent endeavors towards directly using large language models (LLMs) as agent\nmodels to execute interactive planning tasks have shown commendable results.\nDespite their achievements, however, they still struggle with brainless\ntrial-and-error in global planning and generating hallucinatory actions in\nlocal planning due to their poor understanding of the ``real'' physical world.\nImitating humans' mental world knowledge model which provides global prior\nknowledge before the task and maintains local dynamic knowledge during the\ntask, in this paper, we introduce parametric World Knowledge Model (WKM) to\nfacilitate agent planning. Concretely, we steer the agent model to\nself-synthesize knowledge from both expert and sampled trajectories. Then we\ndevelop WKM, providing prior task knowledge to guide the global planning and\ndynamic state knowledge to assist the local planning. Experimental results on\nthree complex real-world simulated datasets with three state-of-the-art\nopen-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our\nmethod can achieve superior performance compared to various strong baselines.\nBesides, we analyze to illustrate that our WKM can effectively alleviate the\nblind trial-and-error and hallucinatory action issues, providing strong support\nfor the agent's understanding of the world. Other interesting findings include:\n1) our instance-level task knowledge can generalize better to unseen tasks, 2)\nweak WKM can guide strong agent model planning, and 3) unified WKM training has\npromising potential for further development. The code is available at\nhttps://github.com/zjunlp/WKM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent endeavors towards directly using large language models (LLMs) as agent\nmodels to execute interactive planning tasks have shown commendable results.\nDespite their achievements, however, they still struggle with brainless\ntrial-and-error in global planning and generating hallucinatory actions in\nlocal planning due to their poor understanding of the ``real'' physical world.\nImitating humans' mental world knowledge model which provides global prior\nknowledge before the task and maintains local dynamic knowledge during the\ntask, in this paper, we introduce parametric World Knowledge Model (WKM) to\nfacilitate agent planning. Concretely, we steer the agent model to\nself-synthesize knowledge from both expert and sampled trajectories. Then we\ndevelop WKM, providing prior task knowledge to guide the global planning and\ndynamic state knowledge to assist the local planning. Experimental results on\nthree complex real-world simulated datasets with three state-of-the-art\nopen-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our\nmethod can achieve superior performance compared to various strong baselines.\nBesides, we analyze to illustrate that our WKM can effectively alleviate the\nblind trial-and-error and hallucinatory action issues, providing strong support\nfor the agent's understanding of the world. Other interesting findings include:\n1) our instance-level task knowledge can generalize better to unseen tasks, 2)\nweak WKM can guide strong agent model planning, and 3) unified WKM training has\npromising potential for further development. The code is available at\nhttps://github.com/zjunlp/WKM."
                },
                "authors": [
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11594v1",
                "updated": "2024-10-15T13:29:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    29,
                    22,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T13:29:22Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    29,
                    22,
                    1,
                    289,
                    0
                ],
                "title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge"
                },
                "summary": "LLM-as-a-Judge is a widely used method for evaluating the performance of\nLarge Language Models (LLMs) across various tasks. We address the challenge of\nquantifying the uncertainty of LLM-as-a-Judge evaluations. While uncertainty\nquantification has been well-studied in other domains, applying it effectively\nto LLMs poses unique challenges due to their complex decision-making\ncapabilities and computational demands. In this paper, we introduce a novel\nmethod for quantifying uncertainty designed to enhance the trustworthiness of\nLLM-as-a-Judge evaluations. The method quantifies uncertainty by analyzing the\nrelationships between generated assessments and possible ratings. By\ncross-evaluating these relationships and constructing a confusion matrix based\non token probabilities, the method derives labels of high or low uncertainty.\nWe evaluate our method across multiple benchmarks, demonstrating a strong\ncorrelation between the accuracy of LLM evaluations and the derived uncertainty\nscores. Our findings suggest that this method can significantly improve the\nreliability and consistency of LLM-as-a-Judge evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge is a widely used method for evaluating the performance of\nLarge Language Models (LLMs) across various tasks. We address the challenge of\nquantifying the uncertainty of LLM-as-a-Judge evaluations. While uncertainty\nquantification has been well-studied in other domains, applying it effectively\nto LLMs poses unique challenges due to their complex decision-making\ncapabilities and computational demands. In this paper, we introduce a novel\nmethod for quantifying uncertainty designed to enhance the trustworthiness of\nLLM-as-a-Judge evaluations. The method quantifies uncertainty by analyzing the\nrelationships between generated assessments and possible ratings. By\ncross-evaluating these relationships and constructing a confusion matrix based\non token probabilities, the method derives labels of high or low uncertainty.\nWe evaluate our method across multiple benchmarks, demonstrating a strong\ncorrelation between the accuracy of LLM evaluations and the derived uncertainty\nscores. Our findings suggest that this method can significantly improve the\nreliability and consistency of LLM-as-a-Judge evaluations."
                },
                "authors": [
                    {
                        "name": "Nico Wagner"
                    },
                    {
                        "name": "Michael Desmond"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Zahra Ashktorab"
                    },
                    {
                        "name": "Elizabeth M. Daly"
                    },
                    {
                        "name": "Qian Pan"
                    },
                    {
                        "name": "Martín Santillán Cooper"
                    },
                    {
                        "name": "James M. Johnson"
                    },
                    {
                        "name": "Werner Geyer"
                    }
                ],
                "author_detail": {
                    "name": "Werner Geyer"
                },
                "author": "Werner Geyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11591v1",
                "updated": "2024-10-15T13:25:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    25,
                    43,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T13:25:43Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    25,
                    43,
                    1,
                    289,
                    0
                ],
                "title": "PaSTe: Improving the Efficiency of Visual Anomaly Detection at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaSTe: Improving the Efficiency of Visual Anomaly Detection at the Edge"
                },
                "summary": "Visual Anomaly Detection (VAD) has gained significant research attention for\nits ability to identify anomalous images and pinpoint the specific areas\nresponsible for the anomaly. A key advantage of VAD is its unsupervised nature,\nwhich eliminates the need for costly and time-consuming labeled data\ncollection. However, despite its potential for real-world applications, the\nliterature has given limited focus to resource-efficient VAD, particularly for\ndeployment on edge devices. This work addresses this gap by leveraging\nlightweight neural networks to reduce memory and computation requirements,\nenabling VAD deployment on resource-constrained edge devices. We benchmark the\nmajor VAD algorithms within this framework and demonstrate the feasibility of\nedge-based VAD using the well-known MVTec dataset. Furthermore, we introduce a\nnovel algorithm, Partially Shared Teacher-student (PaSTe), designed to address\nthe high resource demands of the existing Student Teacher Feature Pyramid\nMatching (STFPM) approach. Our results show that PaSTe decreases the inference\ntime by 25%, while reducing the training time by 33% and peak RAM usage during\ntraining by 76%. These improvements make the VAD process significantly more\nefficient, laying a solid foundation for real-world deployment on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Anomaly Detection (VAD) has gained significant research attention for\nits ability to identify anomalous images and pinpoint the specific areas\nresponsible for the anomaly. A key advantage of VAD is its unsupervised nature,\nwhich eliminates the need for costly and time-consuming labeled data\ncollection. However, despite its potential for real-world applications, the\nliterature has given limited focus to resource-efficient VAD, particularly for\ndeployment on edge devices. This work addresses this gap by leveraging\nlightweight neural networks to reduce memory and computation requirements,\nenabling VAD deployment on resource-constrained edge devices. We benchmark the\nmajor VAD algorithms within this framework and demonstrate the feasibility of\nedge-based VAD using the well-known MVTec dataset. Furthermore, we introduce a\nnovel algorithm, Partially Shared Teacher-student (PaSTe), designed to address\nthe high resource demands of the existing Student Teacher Feature Pyramid\nMatching (STFPM) approach. Our results show that PaSTe decreases the inference\ntime by 25%, while reducing the training time by 33% and peak RAM usage during\ntraining by 76%. These improvements make the VAD process significantly more\nefficient, laying a solid foundation for real-world deployment on edge devices."
                },
                "authors": [
                    {
                        "name": "Manuel Barusco"
                    },
                    {
                        "name": "Francesco Borsatti"
                    },
                    {
                        "name": "Davide Dalle Pezze"
                    },
                    {
                        "name": "Francesco Paissan"
                    },
                    {
                        "name": "Elisabetta Farella"
                    },
                    {
                        "name": "Gian Antonio Susto"
                    }
                ],
                "author_detail": {
                    "name": "Gian Antonio Susto"
                },
                "author": "Gian Antonio Susto",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11588v1",
                "updated": "2024-10-15T13:24:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    24,
                    44,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T13:24:44Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    24,
                    44,
                    1,
                    289,
                    0
                ],
                "title": "Causal Reasoning in Large Language Models: A Knowledge Graph Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Reasoning in Large Language Models: A Knowledge Graph Approach"
                },
                "summary": "Large language models (LLMs) typically improve performance by either\nretrieving semantically similar information, or enhancing reasoning abilities\nthrough structured prompts like chain-of-thought. While both strategies are\nconsidered crucial, it remains unclear which has a greater impact on model\nperformance or whether a combination of both is necessary. This paper answers\nthis question by proposing a knowledge graph (KG)-based random-walk reasoning\napproach that leverages causal relationships. We conduct experiments on the\ncommonsense question answering task that is based on a KG. The KG inherently\nprovides both relevant information, such as related entity keywords, and a\nreasoning structure through the connections between nodes. Experimental results\nshow that the proposed KG-based random-walk reasoning method improves the\nreasoning ability and performance of LLMs. Interestingly, incorporating three\nseemingly irrelevant sentences into the query using KG-based random-walk\nreasoning enhances LLM performance, contrary to conventional wisdom. These\nfindings suggest that integrating causal structures into prompts can\nsignificantly improve reasoning capabilities, providing new insights into the\nrole of causality in optimizing LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) typically improve performance by either\nretrieving semantically similar information, or enhancing reasoning abilities\nthrough structured prompts like chain-of-thought. While both strategies are\nconsidered crucial, it remains unclear which has a greater impact on model\nperformance or whether a combination of both is necessary. This paper answers\nthis question by proposing a knowledge graph (KG)-based random-walk reasoning\napproach that leverages causal relationships. We conduct experiments on the\ncommonsense question answering task that is based on a KG. The KG inherently\nprovides both relevant information, such as related entity keywords, and a\nreasoning structure through the connections between nodes. Experimental results\nshow that the proposed KG-based random-walk reasoning method improves the\nreasoning ability and performance of LLMs. Interestingly, incorporating three\nseemingly irrelevant sentences into the query using KG-based random-walk\nreasoning enhances LLM performance, contrary to conventional wisdom. These\nfindings suggest that integrating causal structures into prompts can\nsignificantly improve reasoning capabilities, providing new insights into the\nrole of causality in optimizing LLM performance."
                },
                "authors": [
                    {
                        "name": "Yejin Kim"
                    },
                    {
                        "name": "Eojin Kang"
                    },
                    {
                        "name": "Juae Kim"
                    },
                    {
                        "name": "H. Howie Huang"
                    }
                ],
                "author_detail": {
                    "name": "H. Howie Huang"
                },
                "author": "H. Howie Huang",
                "arxiv_comment": "Accepted at NeurIPS 2024 Workshop on Causality and Large Models\n  (CaLM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10210v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10210v2",
                "updated": "2024-10-15T13:21:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    21,
                    19,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-14T07:09:02Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    9,
                    2,
                    0,
                    288,
                    0
                ],
                "title": "Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as\n  the Key",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as\n  the Key"
                },
                "summary": "As large language models rapidly evolve to support longer context, there is a\nnotable disparity in their capability to generate output at greater lengths.\nRecent study suggests that the primary cause for this imbalance may arise from\nthe lack of data with long-output during alignment training. In light of this\nobservation, attempts are made to re-align foundation models with data that\nfills the gap, which result in models capable of generating lengthy output when\ninstructed. In this paper, we explore the impact of data-quality in tuning a\nmodel for long output, and the possibility of doing so from the starting points\nof human-aligned (instruct or chat) models. With careful data curation, we show\nthat it possible to achieve similar performance improvement in our tuned\nmodels, with only a small fraction of training data instances and compute. In\naddition, we assess the generalizability of such approaches by applying our\ntuning-recipes to several models. our findings suggest that, while capacities\nfor generating long output vary across different models out-of-the-box, our\napproach to tune them with high-quality data using lite compute, consistently\nyields notable improvement across all models we experimented on. We have made\npublic our curated dataset for tuning long-writing capability, the\nimplementations of model tuning and evaluation, as well as the fine-tuned\nmodels, all of which can be openly-accessed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models rapidly evolve to support longer context, there is a\nnotable disparity in their capability to generate output at greater lengths.\nRecent study suggests that the primary cause for this imbalance may arise from\nthe lack of data with long-output during alignment training. In light of this\nobservation, attempts are made to re-align foundation models with data that\nfills the gap, which result in models capable of generating lengthy output when\ninstructed. In this paper, we explore the impact of data-quality in tuning a\nmodel for long output, and the possibility of doing so from the starting points\nof human-aligned (instruct or chat) models. With careful data curation, we show\nthat it possible to achieve similar performance improvement in our tuned\nmodels, with only a small fraction of training data instances and compute. In\naddition, we assess the generalizability of such approaches by applying our\ntuning-recipes to several models. our findings suggest that, while capacities\nfor generating long output vary across different models out-of-the-box, our\napproach to tune them with high-quality data using lite compute, consistently\nyields notable improvement across all models we experimented on. We have made\npublic our curated dataset for tuning long-writing capability, the\nimplementations of model tuning and evaluation, as well as the fine-tuned\nmodels, all of which can be openly-accessed."
                },
                "authors": [
                    {
                        "name": "Yingda Chen"
                    },
                    {
                        "name": "Xingjun Wang"
                    },
                    {
                        "name": "Jintao Huang"
                    },
                    {
                        "name": "Yunlin Mao"
                    },
                    {
                        "name": "Daoze Zhang"
                    },
                    {
                        "name": "Yuze Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yuze Zhao"
                },
                "author": "Yuze Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10210v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10210v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11584v1",
                "updated": "2024-10-15T13:19:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    19,
                    16,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T13:19:16Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    19,
                    16,
                    1,
                    289,
                    0
                ],
                "title": "DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object\n  Manipulation via Preference-based Action Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeformPAM: Data-Efficient Learning for Long-horizon Deformable Object\n  Manipulation via Preference-based Action Alignment"
                },
                "summary": "In recent years, imitation learning has made progress in the field of robotic\nmanipulation. However, it still faces challenges when dealing with complex\nlong-horizon deformable object tasks, such as high-dimensional state spaces,\ncomplex dynamics, and multimodal action distributions. Traditional imitation\nlearning methods often require a large amount of data and encounter\ndistributional shifts and accumulative errors in these tasks. To address these\nissues, we propose a data-efficient general learning framework (DeformPAM)\nbased on preference learning and reward-guided action selection. DeformPAM\ndecomposes long-horizon tasks into multiple action primitives, utilizes 3D\npoint cloud inputs and diffusion models to model action distributions, and\ntrains an implicit reward model using human preference data. During the\ninference phase, the reward model scores multiple candidate actions, selecting\nthe optimal action for execution, thereby reducing the occurrence of anomalous\nactions and improving task completion quality. Experiments conducted on three\nchallenging real-world long-horizon deformable object manipulation tasks\ndemonstrate the effectiveness of this method. Results show that DeformPAM\nimproves both task completion quality and efficiency compared to baseline\nmethods even with limited data. Code and data will be available at\nhttps://deform-pam.robotflow.ai.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, imitation learning has made progress in the field of robotic\nmanipulation. However, it still faces challenges when dealing with complex\nlong-horizon deformable object tasks, such as high-dimensional state spaces,\ncomplex dynamics, and multimodal action distributions. Traditional imitation\nlearning methods often require a large amount of data and encounter\ndistributional shifts and accumulative errors in these tasks. To address these\nissues, we propose a data-efficient general learning framework (DeformPAM)\nbased on preference learning and reward-guided action selection. DeformPAM\ndecomposes long-horizon tasks into multiple action primitives, utilizes 3D\npoint cloud inputs and diffusion models to model action distributions, and\ntrains an implicit reward model using human preference data. During the\ninference phase, the reward model scores multiple candidate actions, selecting\nthe optimal action for execution, thereby reducing the occurrence of anomalous\nactions and improving task completion quality. Experiments conducted on three\nchallenging real-world long-horizon deformable object manipulation tasks\ndemonstrate the effectiveness of this method. Results show that DeformPAM\nimproves both task completion quality and efficiency compared to baseline\nmethods even with limited data. Code and data will be available at\nhttps://deform-pam.robotflow.ai."
                },
                "authors": [
                    {
                        "name": "Wendi Chen"
                    },
                    {
                        "name": "Han Xue"
                    },
                    {
                        "name": "Fangyuan Zhou"
                    },
                    {
                        "name": "Yuan Fang"
                    },
                    {
                        "name": "Cewu Lu"
                    }
                ],
                "author_detail": {
                    "name": "Cewu Lu"
                },
                "author": "Cewu Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01166v2",
                "updated": "2024-10-15T12:58:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    58,
                    0,
                    1,
                    289,
                    0
                ],
                "published": "2023-10-02T12:50:43Z",
                "published_parsed": [
                    2023,
                    10,
                    2,
                    12,
                    50,
                    43,
                    0,
                    275,
                    0
                ],
                "title": "Gotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in\n  Code Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gotcha! This Model Uses My Code! Evaluating Membership Leakage Risks in\n  Code Models"
                },
                "summary": "Given large-scale source code datasets available in open-source projects and\nadvanced large language models, recent code models have been proposed to\naddress a series of critical software engineering tasks, such as program repair\nand code completion. The training data of the code models come from various\nsources, not only the publicly available source code, e.g., open-source\nprojects on GitHub but also the private data such as the confidential source\ncode from companies, which may contain sensitive information (for example, SSH\nkeys and personal information). As a result, the use of these code models may\nraise new privacy concerns.\n  In this paper, we focus on a critical yet not well-explored question on using\ncode models: what is the risk of membership information leakage in code models?\nMembership information leakage refers to the risk that an attacker can infer\nwhether a given data point is included in (i.e., a member of) the training\ndata. To answer this question, we propose Gotcha, a novel membership inference\nattack method specifically for code models. We investigate the membership\nleakage risk of code models. Our results reveal a worrying fact that the risk\nof membership leakage is high: although the previous attack methods are close\nto random guessing, Gotcha can predict the data membership with a high true\npositive rate of 0.95 and a low false positive rate of 0.10. We also show that\nthe attacker's knowledge of the victim model (e.g., the model architecture and\nthe pre-training data) impacts the success rate of attacks. Further analysis\ndemonstrates that changing the decoding strategy can mitigate the risk of\nmembership leakage. This study calls for more attention to understanding the\nprivacy of code models and developing more effective countermeasures against\nsuch attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given large-scale source code datasets available in open-source projects and\nadvanced large language models, recent code models have been proposed to\naddress a series of critical software engineering tasks, such as program repair\nand code completion. The training data of the code models come from various\nsources, not only the publicly available source code, e.g., open-source\nprojects on GitHub but also the private data such as the confidential source\ncode from companies, which may contain sensitive information (for example, SSH\nkeys and personal information). As a result, the use of these code models may\nraise new privacy concerns.\n  In this paper, we focus on a critical yet not well-explored question on using\ncode models: what is the risk of membership information leakage in code models?\nMembership information leakage refers to the risk that an attacker can infer\nwhether a given data point is included in (i.e., a member of) the training\ndata. To answer this question, we propose Gotcha, a novel membership inference\nattack method specifically for code models. We investigate the membership\nleakage risk of code models. Our results reveal a worrying fact that the risk\nof membership leakage is high: although the previous attack methods are close\nto random guessing, Gotcha can predict the data membership with a high true\npositive rate of 0.95 and a low false positive rate of 0.10. We also show that\nthe attacker's knowledge of the victim model (e.g., the model architecture and\nthe pre-training data) impacts the success rate of attacks. Further analysis\ndemonstrates that changing the decoding strategy can mitigate the risk of\nmembership leakage. This study calls for more attention to understanding the\nprivacy of code models and developing more effective countermeasures against\nsuch attacks."
                },
                "authors": [
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "Zhipeng Zhao"
                    },
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Jieke Shi"
                    },
                    {
                        "name": "Dongsum Kim"
                    },
                    {
                        "name": "Donggyun Han"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "arxiv_comment": "Accepted by IEEE Transactions on Software Engineering, Camera-Ready\n  Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19020v2",
                "updated": "2024-10-15T12:55:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    55,
                    27,
                    1,
                    289,
                    0
                ],
                "published": "2024-09-25T07:03:31Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    3,
                    31,
                    2,
                    269,
                    0
                ],
                "title": "DiaSynth: Synthetic Dialogue Generation Framework for Low Resource\n  Dialogue Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiaSynth: Synthetic Dialogue Generation Framework for Low Resource\n  Dialogue Applications"
                },
                "summary": "The scarcity of domain-specific dialogue datasets limits the development of\ndialogue systems across applications. Existing research is constrained by\ngeneral or niche datasets that lack sufficient scale for training dialogue\nsystems. To address this gap, we introduce DiaSynth - a synthetic dialogue\ngeneration framework capable of generating high-quality, contextually rich\ndialogues across a wide range of domains. Unlike existing frameworks, DiaSynth\nuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to\ngenerate dynamic, domain-specific dialogues with simulated personas and diverse\nconversational features. We perform our experiments by generating synthetic\ndata using different LLMs and few-shot examples from DialogSum and SAMSum. The\npretrained language models fine-tuned on the synthetic data outperform the base\nmodels by 16.47% on dialogue summarization, while the comparison between models\nfine-tuned on in-domain data and synthetic data shows that the synthetic data\nis able to capture 90.48% of the performance distribution of the in-domain data\non dialogue summarization. The quality of the data generated also increases as\nwe increase the size of LLM from 3B to 8B. These results validate DiaSynth's\npotential as a robust alternative to traditional data collection methods. We\nopen source the code and data generated for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of domain-specific dialogue datasets limits the development of\ndialogue systems across applications. Existing research is constrained by\ngeneral or niche datasets that lack sufficient scale for training dialogue\nsystems. To address this gap, we introduce DiaSynth - a synthetic dialogue\ngeneration framework capable of generating high-quality, contextually rich\ndialogues across a wide range of domains. Unlike existing frameworks, DiaSynth\nuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to\ngenerate dynamic, domain-specific dialogues with simulated personas and diverse\nconversational features. We perform our experiments by generating synthetic\ndata using different LLMs and few-shot examples from DialogSum and SAMSum. The\npretrained language models fine-tuned on the synthetic data outperform the base\nmodels by 16.47% on dialogue summarization, while the comparison between models\nfine-tuned on in-domain data and synthetic data shows that the synthetic data\nis able to capture 90.48% of the performance distribution of the in-domain data\non dialogue summarization. The quality of the data generated also increases as\nwe increase the size of LLM from 3B to 8B. These results validate DiaSynth's\npotential as a robust alternative to traditional data collection methods. We\nopen source the code and data generated for future research."
                },
                "authors": [
                    {
                        "name": "Sathya Krishnan Suresh"
                    },
                    {
                        "name": "Wu Mengjun"
                    },
                    {
                        "name": "Tushar Pranav"
                    },
                    {
                        "name": "Eng Siong Chng"
                    }
                ],
                "author_detail": {
                    "name": "Eng Siong Chng"
                },
                "author": "Eng Siong Chng",
                "arxiv_comment": "13 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11565v1",
                "updated": "2024-10-15T12:54:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    54,
                    13,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T12:54:13Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    54,
                    13,
                    1,
                    289,
                    0
                ],
                "title": "Demo: Testing AI-driven MAC Learning in Autonomic Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: Testing AI-driven MAC Learning in Autonomic Networks"
                },
                "summary": "6G networks will be highly dynamic, re-configurable, and resilient. To enable\nand support such features, employing AI has been suggested. Integrating AIin\nnetworks will likely require distributed AI deployments with resilient\nconnectivity, e.g., for communication between RL agents and environment. Such\napproaches need to be validated in realistic network environments. In this\ndemo, we use ContainerNet to emulate AI-capable and autonomic networks that\nemploy the routing protocol KIRA to provide resilient connectivity and service\ndiscovery. As an example AI application, we train and infer deep RL agents\nlearning medium access control (MAC) policies for a wireless network\nenvironment in the emulated network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G networks will be highly dynamic, re-configurable, and resilient. To enable\nand support such features, employing AI has been suggested. Integrating AIin\nnetworks will likely require distributed AI deployments with resilient\nconnectivity, e.g., for communication between RL agents and environment. Such\napproaches need to be validated in realistic network environments. In this\ndemo, we use ContainerNet to emulate AI-capable and autonomic networks that\nemploy the routing protocol KIRA to provide resilient connectivity and service\ndiscovery. As an example AI application, we train and infer deep RL agents\nlearning medium access control (MAC) policies for a wireless network\nenvironment in the emulated network."
                },
                "authors": [
                    {
                        "name": "Leonard Paeleke"
                    },
                    {
                        "name": "Navid Keshtiarast"
                    },
                    {
                        "name": "Paul Seehofer"
                    },
                    {
                        "name": "Roland Bless"
                    },
                    {
                        "name": "Holger Karl"
                    },
                    {
                        "name": "Marina Petrova"
                    },
                    {
                        "name": "Martina Zitterbart"
                    }
                ],
                "author_detail": {
                    "name": "Martina Zitterbart"
                },
                "author": "Martina Zitterbart",
                "arxiv_comment": "Accepted for presentation in the Demo Session at the IEEE\n  International Conference on Network Protocols (ICNP), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20231v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20231v3",
                "updated": "2024-10-15T12:53:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    53,
                    48,
                    1,
                    289,
                    0
                ],
                "published": "2024-05-30T16:32:31Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    16,
                    32,
                    31,
                    3,
                    151,
                    0
                ],
                "title": "The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof"
                },
                "summary": "Many algorithms and observed phenomena in deep learning appear to be affected\nby parameter symmetries -- transformations of neural network parameters that do\nnot change the underlying neural network function. These include linear mode\nconnectivity, model merging, Bayesian neural network inference, metanetworks,\nand several other characteristics of optimization or loss-landscapes. However,\ntheoretical analysis of the relationship between parameter space symmetries and\nthese phenomena is difficult. In this work, we empirically investigate the\nimpact of neural parameter symmetries by introducing new neural network\narchitectures that have reduced parameter space symmetries. We develop two\nmethods, with some provable guarantees, of modifying standard neural networks\nto reduce parameter space symmetries. With these new methods, we conduct a\ncomprehensive experimental study consisting of multiple tasks aimed at\nassessing the effect of removing parameter symmetries. Our experiments reveal\nseveral interesting observations on the empirical impact of parameter\nsymmetries; for instance, we observe linear mode connectivity between our\nnetworks without alignment of weight spaces, and we find that our networks\nallow for faster and more effective Bayesian neural network training. Our code\nis available at https://github.com/cptq/asymmetric-networks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many algorithms and observed phenomena in deep learning appear to be affected\nby parameter symmetries -- transformations of neural network parameters that do\nnot change the underlying neural network function. These include linear mode\nconnectivity, model merging, Bayesian neural network inference, metanetworks,\nand several other characteristics of optimization or loss-landscapes. However,\ntheoretical analysis of the relationship between parameter space symmetries and\nthese phenomena is difficult. In this work, we empirically investigate the\nimpact of neural parameter symmetries by introducing new neural network\narchitectures that have reduced parameter space symmetries. We develop two\nmethods, with some provable guarantees, of modifying standard neural networks\nto reduce parameter space symmetries. With these new methods, we conduct a\ncomprehensive experimental study consisting of multiple tasks aimed at\nassessing the effect of removing parameter symmetries. Our experiments reveal\nseveral interesting observations on the empirical impact of parameter\nsymmetries; for instance, we observe linear mode connectivity between our\nnetworks without alignment of weight spaces, and we find that our networks\nallow for faster and more effective Bayesian neural network training. Our code\nis available at https://github.com/cptq/asymmetric-networks"
                },
                "authors": [
                    {
                        "name": "Derek Lim"
                    },
                    {
                        "name": "Theo Moe Putterman"
                    },
                    {
                        "name": "Robin Walters"
                    },
                    {
                        "name": "Haggai Maron"
                    },
                    {
                        "name": "Stefanie Jegelka"
                    }
                ],
                "author_detail": {
                    "name": "Stefanie Jegelka"
                },
                "author": "Stefanie Jegelka",
                "arxiv_comment": "NeurIPS 2024. v2: added / updated some citations. v3 added link to\n  code, and some additional ablations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20231v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20231v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11564v1",
                "updated": "2024-10-15T12:53:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    53,
                    42,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T12:53:42Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    53,
                    42,
                    1,
                    289,
                    0
                ],
                "title": "PAVLM: Advancing Point Cloud based Affordance Understanding Via\n  Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAVLM: Advancing Point Cloud based Affordance Understanding Via\n  Vision-Language Model"
                },
                "summary": "Affordance understanding, the task of identifying actionable regions on 3D\nobjects, plays a vital role in allowing robotic systems to engage with and\noperate within the physical world. Although Visual Language Models (VLMs) have\nexcelled in high-level reasoning and long-horizon planning for robotic\nmanipulation, they still fall short in grasping the nuanced physical properties\nrequired for effective human-robot interaction. In this paper, we introduce\nPAVLM (Point cloud Affordance Vision-Language Model), an innovative framework\nthat utilizes the extensive multimodal knowledge embedded in pre-trained\nlanguage models to enhance 3D affordance understanding of point cloud. PAVLM\nintegrates a geometric-guided propagation module with hidden embeddings from\nlarge language models (LLMs) to enrich visual semantics. On the language side,\nwe prompt Llama-3.1 models to generate refined context-aware text, augmenting\nthe instructional input with deeper semantic cues. Experimental results on the\n3D-AffordanceNet benchmark demonstrate that PAVLM outperforms baseline methods\nfor both full and partial point clouds, particularly excelling in its\ngeneralization to novel open-world affordance tasks of 3D objects. For more\ninformation, visit our project site: pavlm-source.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordance understanding, the task of identifying actionable regions on 3D\nobjects, plays a vital role in allowing robotic systems to engage with and\noperate within the physical world. Although Visual Language Models (VLMs) have\nexcelled in high-level reasoning and long-horizon planning for robotic\nmanipulation, they still fall short in grasping the nuanced physical properties\nrequired for effective human-robot interaction. In this paper, we introduce\nPAVLM (Point cloud Affordance Vision-Language Model), an innovative framework\nthat utilizes the extensive multimodal knowledge embedded in pre-trained\nlanguage models to enhance 3D affordance understanding of point cloud. PAVLM\nintegrates a geometric-guided propagation module with hidden embeddings from\nlarge language models (LLMs) to enrich visual semantics. On the language side,\nwe prompt Llama-3.1 models to generate refined context-aware text, augmenting\nthe instructional input with deeper semantic cues. Experimental results on the\n3D-AffordanceNet benchmark demonstrate that PAVLM outperforms baseline methods\nfor both full and partial point clouds, particularly excelling in its\ngeneralization to novel open-world affordance tasks of 3D objects. For more\ninformation, visit our project site: pavlm-source.github.io."
                },
                "authors": [
                    {
                        "name": "Shang-Ching Liu"
                    },
                    {
                        "name": "Van Nhiem Tran"
                    },
                    {
                        "name": "Wenkai Chen"
                    },
                    {
                        "name": "Wei-Lun Cheng"
                    },
                    {
                        "name": "Yen-Lin Huang"
                    },
                    {
                        "name": "I-Bin Liao"
                    },
                    {
                        "name": "Yung-Hui Li"
                    },
                    {
                        "name": "Jianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Zhang"
                },
                "author": "Jianwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11553v1",
                "updated": "2024-10-15T12:43:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    43,
                    28,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T12:43:28Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    43,
                    28,
                    1,
                    289,
                    0
                ],
                "title": "Efficiera Residual Networks: Hardware-Friendly Fully Binary Weight with\n  2-bit Activation Model Achieves Practical ImageNet Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiera Residual Networks: Hardware-Friendly Fully Binary Weight with\n  2-bit Activation Model Achieves Practical ImageNet Accuracy"
                },
                "summary": "The edge-device environment imposes severe resource limitations, encompassing\ncomputation costs, hardware resource usage, and energy consumption for\ndeploying deep neural network models. Ultra-low-bit quantization and hardware\naccelerators have been explored as promising approaches to address these\nchallenges. Ultra-low-bit quantization significantly reduces the model size and\nthe computational cost. Despite progress so far, many competitive ultra-low-bit\nmodels still partially rely on float or non-ultra-low-bit quantized computation\nsuch as the input and output layer. We introduce Efficiera Residual Networks\n(ERNs), a model optimized for low-resource edge devices. ERNs achieve full\nultra-low-bit quantization, with all weights, including the initial and output\nlayers, being binary, and activations set at 2 bits. We introduce the shared\nconstant scaling factor technique to enable integer-valued computation in\nresidual connections, allowing our model to operate without float values until\nthe final convolution layer. Demonstrating competitiveness, ERNs achieve an\nImageNet top-1 accuracy of 72.5pt with a ResNet50-compatible architecture and\n63.6pt with a model size less than 1MB. Moreover, ERNs exhibit impressive\ninference times, reaching 300FPS with the smallest model and 60FPS with the\nlargest model on a cost-efficient FPGA device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The edge-device environment imposes severe resource limitations, encompassing\ncomputation costs, hardware resource usage, and energy consumption for\ndeploying deep neural network models. Ultra-low-bit quantization and hardware\naccelerators have been explored as promising approaches to address these\nchallenges. Ultra-low-bit quantization significantly reduces the model size and\nthe computational cost. Despite progress so far, many competitive ultra-low-bit\nmodels still partially rely on float or non-ultra-low-bit quantized computation\nsuch as the input and output layer. We introduce Efficiera Residual Networks\n(ERNs), a model optimized for low-resource edge devices. ERNs achieve full\nultra-low-bit quantization, with all weights, including the initial and output\nlayers, being binary, and activations set at 2 bits. We introduce the shared\nconstant scaling factor technique to enable integer-valued computation in\nresidual connections, allowing our model to operate without float values until\nthe final convolution layer. Demonstrating competitiveness, ERNs achieve an\nImageNet top-1 accuracy of 72.5pt with a ResNet50-compatible architecture and\n63.6pt with a model size less than 1MB. Moreover, ERNs exhibit impressive\ninference times, reaching 300FPS with the smallest model and 60FPS with the\nlargest model on a cost-efficient FPGA device."
                },
                "authors": [
                    {
                        "name": "Shuntaro Takahashi"
                    },
                    {
                        "name": "Takuya Wakisaka"
                    },
                    {
                        "name": "Hiroyuki Tokunaga"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Tokunaga"
                },
                "author": "Hiroyuki Tokunaga",
                "arxiv_comment": "11pages, 2 figures, the model implementation is available at\n  https://github.com/LeapMind/ERN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11550v1",
                "updated": "2024-10-15T12:39:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    39,
                    20,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T12:39:20Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    39,
                    20,
                    1,
                    289,
                    0
                ],
                "title": "Y-Mol: A Multiscale Biomedical Knowledge-Guided Large Language Model for\n  Drug Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Y-Mol: A Multiscale Biomedical Knowledge-Guided Large Language Model for\n  Drug Development"
                },
                "summary": "Large Language Models (LLMs) have recently demonstrated remarkable\nperformance in general tasks across various fields. However, their\neffectiveness within specific domains such as drug development remains\nchallenges. To solve these challenges, we introduce \\textbf{Y-Mol}, forming a\nwell-established LLM paradigm for the flow of drug development. Y-Mol is a\nmultiscale biomedical knowledge-guided LLM designed to accomplish tasks across\nlead compound discovery, pre-clinic, and clinic prediction. By integrating\nmillions of multiscale biomedical knowledge and using LLaMA2 as the base LLM,\nY-Mol augments the reasoning capability in the biomedical domain by learning\nfrom a corpus of publications, knowledge graphs, and expert-designed synthetic\ndata. The capability is further enriched with three types of drug-oriented\ninstructions: description-based prompts from processed publications,\nsemantic-based prompts for extracting associations from knowledge graphs, and\ntemplate-based prompts for understanding expert knowledge from biomedical\ntools. Besides, Y-Mol offers a set of LLM paradigms that can autonomously\nexecute the downstream tasks across the entire process of drug development,\nincluding virtual screening, drug design, pharmacological properties\nprediction, and drug-related interaction prediction. Our extensive evaluations\nof various biomedical sources demonstrate that Y-Mol significantly outperforms\ngeneral-purpose LLMs in discovering lead compounds, predicting molecular\nproperties, and identifying drug interaction events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently demonstrated remarkable\nperformance in general tasks across various fields. However, their\neffectiveness within specific domains such as drug development remains\nchallenges. To solve these challenges, we introduce \\textbf{Y-Mol}, forming a\nwell-established LLM paradigm for the flow of drug development. Y-Mol is a\nmultiscale biomedical knowledge-guided LLM designed to accomplish tasks across\nlead compound discovery, pre-clinic, and clinic prediction. By integrating\nmillions of multiscale biomedical knowledge and using LLaMA2 as the base LLM,\nY-Mol augments the reasoning capability in the biomedical domain by learning\nfrom a corpus of publications, knowledge graphs, and expert-designed synthetic\ndata. The capability is further enriched with three types of drug-oriented\ninstructions: description-based prompts from processed publications,\nsemantic-based prompts for extracting associations from knowledge graphs, and\ntemplate-based prompts for understanding expert knowledge from biomedical\ntools. Besides, Y-Mol offers a set of LLM paradigms that can autonomously\nexecute the downstream tasks across the entire process of drug development,\nincluding virtual screening, drug design, pharmacological properties\nprediction, and drug-related interaction prediction. Our extensive evaluations\nof various biomedical sources demonstrate that Y-Mol significantly outperforms\ngeneral-purpose LLMs in discovering lead compounds, predicting molecular\nproperties, and identifying drug interaction events."
                },
                "authors": [
                    {
                        "name": "Tengfei Ma"
                    },
                    {
                        "name": "Xuan Lin"
                    },
                    {
                        "name": "Tianle Li"
                    },
                    {
                        "name": "Chaoyi Li"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Xibao Cai"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Daojian Zeng"
                    },
                    {
                        "name": "Dongsheng Cao"
                    },
                    {
                        "name": "Xiangxiang Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Xiangxiang Zeng"
                },
                "author": "Xiangxiang Zeng",
                "arxiv_comment": "12 pages, Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11548v1",
                "updated": "2024-10-15T12:32:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    32,
                    49,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T12:32:49Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    32,
                    49,
                    1,
                    289,
                    0
                ],
                "title": "Bayesian inference of mixed Gaussian phylogenetic models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference of mixed Gaussian phylogenetic models"
                },
                "summary": "Background: Continuous traits evolution of a group of taxa that are\ncorrelated through a phylogenetic tree is commonly modelled using parametric\nstochastic differential equations to represent deterministic change of trait\nthrough time, while incorporating noises that represent different unobservable\nevolutionary pressures. Often times, a heterogeneous Gaussian process that\nconsists of multiple parametric sub-processes is often used when the observed\ndata come from a very diverse set of taxa. In the maximum likelihood setting,\nchallenges can be found when exploring the involved likelihood surface and when\ninterpreting the uncertainty around the parameters.\n  Results: We extend the methods to tackle inference problems for mixed\nGaussian phylogenetic models (MGPMs) by implementing a Bayesian scheme that can\ntake into account biologically relevant priors. The posterior inference method\nis based on the Population Monte Carlo (PMC) algorithm that are easily\nparallelized, and using an efficient algorithm to calculate the likelihood of\nphylogenetically correlated observations. A model evaluation method that is\nbased on the proximity of the posterior predictive distribution to the observed\ndata is also implemented. Simulation study is done to test the inference and\nevaluation capability of the method. Finally, we test our method on a\nreal-world dataset.\n  Conclusion: We implement the method in the R package bgphy, available at\ngithub.com/bayubeta/bgphy. Simulation study demonstrates that the method is\nable to infer parameters and evaluate models properly, while its implementation\non the real-world dataset indicates that a carefully selected model of\nevolution based on naturally occurring classifications results in a better fit\nto the observed data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Continuous traits evolution of a group of taxa that are\ncorrelated through a phylogenetic tree is commonly modelled using parametric\nstochastic differential equations to represent deterministic change of trait\nthrough time, while incorporating noises that represent different unobservable\nevolutionary pressures. Often times, a heterogeneous Gaussian process that\nconsists of multiple parametric sub-processes is often used when the observed\ndata come from a very diverse set of taxa. In the maximum likelihood setting,\nchallenges can be found when exploring the involved likelihood surface and when\ninterpreting the uncertainty around the parameters.\n  Results: We extend the methods to tackle inference problems for mixed\nGaussian phylogenetic models (MGPMs) by implementing a Bayesian scheme that can\ntake into account biologically relevant priors. The posterior inference method\nis based on the Population Monte Carlo (PMC) algorithm that are easily\nparallelized, and using an efficient algorithm to calculate the likelihood of\nphylogenetically correlated observations. A model evaluation method that is\nbased on the proximity of the posterior predictive distribution to the observed\ndata is also implemented. Simulation study is done to test the inference and\nevaluation capability of the method. Finally, we test our method on a\nreal-world dataset.\n  Conclusion: We implement the method in the R package bgphy, available at\ngithub.com/bayubeta/bgphy. Simulation study demonstrates that the method is\nable to infer parameters and evaluate models properly, while its implementation\non the real-world dataset indicates that a carefully selected model of\nevolution based on naturally occurring classifications results in a better fit\nto the observed data."
                },
                "authors": [
                    {
                        "name": "Bayu Brahmantio"
                    },
                    {
                        "name": "Krzysztof Bartoszek"
                    },
                    {
                        "name": "Etka Yapar"
                    }
                ],
                "author_detail": {
                    "name": "Etka Yapar"
                },
                "author": "Etka Yapar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11545v1",
                "updated": "2024-10-15T12:24:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    24,
                    55,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T12:24:55Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    24,
                    55,
                    1,
                    289,
                    0
                ],
                "title": "A model learning framework for inferring the dynamics of transmission\n  rate depending on exogenous variables for epidemic forecasts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A model learning framework for inferring the dynamics of transmission\n  rate depending on exogenous variables for epidemic forecasts"
                },
                "summary": "In this work, we aim to formalize a novel scientific machine learning\nframework to reconstruct the hidden dynamics of the transmission rate, whose\ninaccurate extrapolation can significantly impair the quality of the epidemic\nforecasts, by incorporating the influence of exogenous variables (such as\nenvironmental conditions and strain-specific characteristics). We propose an\nhybrid model that blends a data-driven layer with a physics-based one. The\ndata-driven layer is based on a neural ordinary differential equation that\nlearns the dynamics of the transmission rate, conditioned on the meteorological\ndata and wave-specific latent parameters. The physics-based layer, instead,\nconsists of a standard SEIR compartmental model, wherein the transmission rate\nrepresents an input. The learning strategy follows an end-to-end approach: the\nloss function quantifies the mismatch between the actual numbers of infections\nand its numerical prediction obtained from the SEIR model incorporating as an\ninput the transmission rate predicted by the neural ordinary differential\nequation. We validate this original approach using both a synthetic test case\nand a realistic test case based on meteorological data (temperature and\nhumidity) and influenza data from Italy between 2010 and 2020. In both\nscenarios, we achieve low generalization error on the test set and observe\nstrong alignment between the reconstructed model and established findings on\nthe influence of meteorological factors on epidemic spread. Finally, we\nimplement a data assimilation strategy to adapt the neural equation to the\nspecific characteristics of an epidemic wave under investigation, and we\nconduct sensitivity tests on the network hyperparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we aim to formalize a novel scientific machine learning\nframework to reconstruct the hidden dynamics of the transmission rate, whose\ninaccurate extrapolation can significantly impair the quality of the epidemic\nforecasts, by incorporating the influence of exogenous variables (such as\nenvironmental conditions and strain-specific characteristics). We propose an\nhybrid model that blends a data-driven layer with a physics-based one. The\ndata-driven layer is based on a neural ordinary differential equation that\nlearns the dynamics of the transmission rate, conditioned on the meteorological\ndata and wave-specific latent parameters. The physics-based layer, instead,\nconsists of a standard SEIR compartmental model, wherein the transmission rate\nrepresents an input. The learning strategy follows an end-to-end approach: the\nloss function quantifies the mismatch between the actual numbers of infections\nand its numerical prediction obtained from the SEIR model incorporating as an\ninput the transmission rate predicted by the neural ordinary differential\nequation. We validate this original approach using both a synthetic test case\nand a realistic test case based on meteorological data (temperature and\nhumidity) and influenza data from Italy between 2010 and 2020. In both\nscenarios, we achieve low generalization error on the test set and observe\nstrong alignment between the reconstructed model and established findings on\nthe influence of meteorological factors on epidemic spread. Finally, we\nimplement a data assimilation strategy to adapt the neural equation to the\nspecific characteristics of an epidemic wave under investigation, and we\nconduct sensitivity tests on the network hyperparameters."
                },
                "authors": [
                    {
                        "name": "Giovanni Ziarelli"
                    },
                    {
                        "name": "Stefano Pagani"
                    },
                    {
                        "name": "Nicola Parolini"
                    },
                    {
                        "name": "Francesco Regazzoni"
                    },
                    {
                        "name": "Marco Verani"
                    }
                ],
                "author_detail": {
                    "name": "Marco Verani"
                },
                "author": "Marco Verani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11540v1",
                "updated": "2024-10-15T12:14:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    14,
                    57,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T12:14:57Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    14,
                    57,
                    1,
                    289,
                    0
                ],
                "title": "Data Quality Control in Federated Instruction-tuning of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Quality Control in Federated Instruction-tuning of Large Language\n  Models"
                },
                "summary": "By leveraging massively distributed data, federated learning (FL) enables\ncollaborative instruction tuning of large language models (LLMs) in a\nprivacy-preserving way. While FL effectively expands the data quantity, the\nissue of data quality remains under-explored in the current literature on FL\nfor LLMs. To address this gap, we propose a new framework of federated\ninstruction tuning of LLMs with data quality control (FedDQC), which measures\ndata quality to facilitate the subsequent filtering and hierarchical training\nprocesses. Our approach introduces an efficient metric to assess each client's\ninstruction-response alignment (IRA), identifying potentially noisy data\nthrough single-shot inference. Low-IRA samples are potentially noisy and\nfiltered to mitigate their negative impacts. To further utilize this IRA value,\nwe propose a quality-aware hierarchical training paradigm, where LLM is\nprogressively fine-tuned from high-IRA to low-IRA data, mirroring the\neasy-to-hard learning process. We conduct extensive experiments on 4 synthetic\nand a real-world dataset, and compare our method with baselines adapted from\ncentralized setting. Results show that our method consistently and\nsignificantly improves the performance of LLMs trained on mix-quality data in\nFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By leveraging massively distributed data, federated learning (FL) enables\ncollaborative instruction tuning of large language models (LLMs) in a\nprivacy-preserving way. While FL effectively expands the data quantity, the\nissue of data quality remains under-explored in the current literature on FL\nfor LLMs. To address this gap, we propose a new framework of federated\ninstruction tuning of LLMs with data quality control (FedDQC), which measures\ndata quality to facilitate the subsequent filtering and hierarchical training\nprocesses. Our approach introduces an efficient metric to assess each client's\ninstruction-response alignment (IRA), identifying potentially noisy data\nthrough single-shot inference. Low-IRA samples are potentially noisy and\nfiltered to mitigate their negative impacts. To further utilize this IRA value,\nwe propose a quality-aware hierarchical training paradigm, where LLM is\nprogressively fine-tuned from high-IRA to low-IRA data, mirroring the\neasy-to-hard learning process. We conduct extensive experiments on 4 synthetic\nand a real-world dataset, and compare our method with baselines adapted from\ncentralized setting. Results show that our method consistently and\nsignificantly improves the performance of LLMs trained on mix-quality data in\nFL."
                },
                "authors": [
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Fengting Yuchi"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "Jingjing Qu"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11533v1",
                "updated": "2024-10-15T12:08:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    8,
                    14,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T12:08:14Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    8,
                    14,
                    1,
                    289,
                    0
                ],
                "title": "Multi-round jailbreak attack on large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-round jailbreak attack on large language models"
                },
                "summary": "Ensuring the safety and alignment of large language models (LLMs) with human\nvalues is crucial for generating responses that are beneficial to humanity.\nWhile LLMs have the capability to identify and avoid harmful queries, they\nremain vulnerable to \"jailbreak\" attacks, where carefully crafted prompts can\ninduce the generation of toxic content. Traditional single-round jailbreak\nattacks, such as GCG and AutoDAN, do not alter the sensitive words in the\ndangerous prompts. Although they can temporarily bypass the model's safeguards\nthrough prompt engineering, their success rate drops significantly as the LLM\nis further fine-tuned, and they cannot effectively circumvent static rule-based\nfilters that remove the hazardous vocabulary.\n  In this study, to better understand jailbreak attacks, we introduce a\nmulti-round jailbreak approach. This method can rewrite the dangerous prompts,\ndecomposing them into a series of less harmful sub-questions to bypass the\nLLM's safety checks. We first use the LLM to perform a decomposition task,\nbreaking down a set of natural language questions into a sequence of\nprogressive sub-questions, which are then used to fine-tune the Llama3-8B\nmodel, enabling it to decompose hazardous prompts. The fine-tuned model is then\nused to break down the problematic prompt, and the resulting sub-questions are\nsequentially asked to the victim model. If the victim model rejects a\nsub-question, a new decomposition is generated, and the process is repeated\nuntil the final objective is achieved. Our experimental results show a 94\\%\nsuccess rate on the llama2-7B and demonstrate the effectiveness of this\napproach in circumventing static rule-based filters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safety and alignment of large language models (LLMs) with human\nvalues is crucial for generating responses that are beneficial to humanity.\nWhile LLMs have the capability to identify and avoid harmful queries, they\nremain vulnerable to \"jailbreak\" attacks, where carefully crafted prompts can\ninduce the generation of toxic content. Traditional single-round jailbreak\nattacks, such as GCG and AutoDAN, do not alter the sensitive words in the\ndangerous prompts. Although they can temporarily bypass the model's safeguards\nthrough prompt engineering, their success rate drops significantly as the LLM\nis further fine-tuned, and they cannot effectively circumvent static rule-based\nfilters that remove the hazardous vocabulary.\n  In this study, to better understand jailbreak attacks, we introduce a\nmulti-round jailbreak approach. This method can rewrite the dangerous prompts,\ndecomposing them into a series of less harmful sub-questions to bypass the\nLLM's safety checks. We first use the LLM to perform a decomposition task,\nbreaking down a set of natural language questions into a sequence of\nprogressive sub-questions, which are then used to fine-tune the Llama3-8B\nmodel, enabling it to decompose hazardous prompts. The fine-tuned model is then\nused to break down the problematic prompt, and the resulting sub-questions are\nsequentially asked to the victim model. If the victim model rejects a\nsub-question, a new decomposition is generated, and the process is repeated\nuntil the final objective is achieved. Our experimental results show a 94\\%\nsuccess rate on the llama2-7B and demonstrate the effectiveness of this\napproach in circumventing static rule-based filters."
                },
                "authors": [
                    {
                        "name": "Yihua Zhou"
                    },
                    {
                        "name": "Xiaochuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Xiaochuan Shi"
                },
                "author": "Xiaochuan Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11531v1",
                "updated": "2024-10-15T12:05:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    5,
                    58,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T12:05:58Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    5,
                    58,
                    1,
                    289,
                    0
                ],
                "title": "AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based\n  Chatbots Utilizing Private Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based\n  Chatbots Utilizing Private Data"
                },
                "summary": "Large Language Models~(LLMs) have demonstrated capabilities across various\napplications but face challenges such as hallucination, limited reasoning\nabilities, and factual inconsistencies, especially when tackling complex,\ndomain-specific tasks like question answering~(QA). While Knowledge\nGraphs~(KGs) have been shown to help mitigate these issues, research on the\nintegration of LLMs with background KGs remains limited. In particular, user\naccessibility and the flexibility of the underlying KG have not been thoroughly\nexplored. We introduce AGENTiGraph (Adaptive Generative ENgine for Task-based\nInteraction and Graphical Representation), a platform for knowledge management\nthrough natural language interaction. It integrates knowledge extraction,\nintegration, and real-time visualization. AGENTiGraph employs a multi-agent\narchitecture to dynamically interpret user intents, manage tasks, and integrate\nnew knowledge, ensuring adaptability to evolving user requirements and data\ncontexts. Our approach demonstrates superior performance in knowledge graph\ninteractions, particularly for complex domain-specific tasks. Experimental\nresults on a dataset of 3,500 test cases show AGENTiGraph significantly\noutperforms state-of-the-art zero-shot baselines, achieving 95.12\\% accuracy in\ntask classification and 90.45\\% success rate in task execution. User studies\ncorroborate its effectiveness in real-world scenarios. To showcase versatility,\nwe extended AGENTiGraph to legislation and healthcare domains, constructing\nspecialized KGs capable of answering complex queries in legal and medical\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models~(LLMs) have demonstrated capabilities across various\napplications but face challenges such as hallucination, limited reasoning\nabilities, and factual inconsistencies, especially when tackling complex,\ndomain-specific tasks like question answering~(QA). While Knowledge\nGraphs~(KGs) have been shown to help mitigate these issues, research on the\nintegration of LLMs with background KGs remains limited. In particular, user\naccessibility and the flexibility of the underlying KG have not been thoroughly\nexplored. We introduce AGENTiGraph (Adaptive Generative ENgine for Task-based\nInteraction and Graphical Representation), a platform for knowledge management\nthrough natural language interaction. It integrates knowledge extraction,\nintegration, and real-time visualization. AGENTiGraph employs a multi-agent\narchitecture to dynamically interpret user intents, manage tasks, and integrate\nnew knowledge, ensuring adaptability to evolving user requirements and data\ncontexts. Our approach demonstrates superior performance in knowledge graph\ninteractions, particularly for complex domain-specific tasks. Experimental\nresults on a dataset of 3,500 test cases show AGENTiGraph significantly\noutperforms state-of-the-art zero-shot baselines, achieving 95.12\\% accuracy in\ntask classification and 90.45\\% success rate in task execution. User studies\ncorroborate its effectiveness in real-world scenarios. To showcase versatility,\nwe extended AGENTiGraph to legislation and healthcare domains, constructing\nspecialized KGs capable of answering complex queries in legal and medical\ncontexts."
                },
                "authors": [
                    {
                        "name": "Xinjie Zhao"
                    },
                    {
                        "name": "Moritz Blum"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Boming Yang"
                    },
                    {
                        "name": "Luis Márquez Carpintero"
                    },
                    {
                        "name": "Mónica Pina-Navarro"
                    },
                    {
                        "name": "Tony Wang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Huitao Li"
                    },
                    {
                        "name": "Yanran Fu"
                    },
                    {
                        "name": "Rongrong Wang"
                    },
                    {
                        "name": "Juntao Zhang"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "arxiv_comment": "30 pages, 7 figures; Submitted to COLING 2025 System Demonstrations\n  Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11526v1",
                "updated": "2024-10-15T11:57:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    57,
                    34,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T11:57:34Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    57,
                    34,
                    1,
                    289,
                    0
                ],
                "title": "Human-LLM Collaborative Construction of a Cantonese Emotion Lexicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-LLM Collaborative Construction of a Cantonese Emotion Lexicon"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nlanguage understanding and generation. Advanced utilization of the knowledge\nembedded in LLMs for automated annotation has consistently been explored. This\nstudy proposed to develop an emotion lexicon for Cantonese, a low-resource\nlanguage, through collaborative efforts between LLM and human annotators. By\nintegrating emotion labels provided by LLM and human annotators, the study\nleveraged existing linguistic resources including lexicons in other languages\nand local forums to construct a Cantonese emotion lexicon enriched with\ncolloquial expressions. The consistency of the proposed emotion lexicon in\nemotion extraction was assessed through modification and utilization of three\ndistinct emotion text datasets. This study not only validates the efficacy of\nthe constructed lexicon but also emphasizes that collaborative annotation\nbetween human and artificial intelligence can significantly enhance the quality\nof emotion labels, highlighting the potential of such partnerships in\nfacilitating natural language processing tasks for low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nlanguage understanding and generation. Advanced utilization of the knowledge\nembedded in LLMs for automated annotation has consistently been explored. This\nstudy proposed to develop an emotion lexicon for Cantonese, a low-resource\nlanguage, through collaborative efforts between LLM and human annotators. By\nintegrating emotion labels provided by LLM and human annotators, the study\nleveraged existing linguistic resources including lexicons in other languages\nand local forums to construct a Cantonese emotion lexicon enriched with\ncolloquial expressions. The consistency of the proposed emotion lexicon in\nemotion extraction was assessed through modification and utilization of three\ndistinct emotion text datasets. This study not only validates the efficacy of\nthe constructed lexicon but also emphasizes that collaborative annotation\nbetween human and artificial intelligence can significantly enhance the quality\nof emotion labels, highlighting the potential of such partnerships in\nfacilitating natural language processing tasks for low-resource languages."
                },
                "authors": [
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Dong Dong"
                    },
                    {
                        "name": "Chi-tim Hung"
                    },
                    {
                        "name": "Leonard Heyerdahl"
                    },
                    {
                        "name": "Tamara Giles-Vernick"
                    },
                    {
                        "name": "Eng-kiong Yeoh"
                    }
                ],
                "author_detail": {
                    "name": "Eng-kiong Yeoh"
                },
                "author": "Eng-kiong Yeoh",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17591v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17591v4",
                "updated": "2024-10-15T11:52:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    52,
                    53,
                    1,
                    289,
                    0
                ],
                "published": "2024-09-26T07:16:38Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    16,
                    38,
                    3,
                    270,
                    0
                ],
                "title": "Conjugate Bayesian Two-step Change Point Detection for Hawkes Process",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conjugate Bayesian Two-step Change Point Detection for Hawkes Process"
                },
                "summary": "The Bayesian two-step change point detection method is popular for the Hawkes\nprocess due to its simplicity and intuitiveness. However, the non-conjugacy\nbetween the point process likelihood and the prior requires most existing\nBayesian two-step change point detection methods to rely on non-conjugate\ninference methods. These methods lack analytical expressions, leading to low\ncomputational efficiency and impeding timely change point detection. To address\nthis issue, this work employs data augmentation to propose a conjugate Bayesian\ntwo-step change point detection method for the Hawkes process, which proves to\nbe more accurate and efficient. Extensive experiments on both synthetic and\nreal data demonstrate the superior effectiveness and efficiency of our method\ncompared to baseline methods. Additionally, we conduct ablation studies to\nexplore the robustness of our method concerning various hyperparameters. Our\ncode is publicly available at https://github.com/Aurora2050/CoBay-CPD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bayesian two-step change point detection method is popular for the Hawkes\nprocess due to its simplicity and intuitiveness. However, the non-conjugacy\nbetween the point process likelihood and the prior requires most existing\nBayesian two-step change point detection methods to rely on non-conjugate\ninference methods. These methods lack analytical expressions, leading to low\ncomputational efficiency and impeding timely change point detection. To address\nthis issue, this work employs data augmentation to propose a conjugate Bayesian\ntwo-step change point detection method for the Hawkes process, which proves to\nbe more accurate and efficient. Extensive experiments on both synthetic and\nreal data demonstrate the superior effectiveness and efficiency of our method\ncompared to baseline methods. Additionally, we conduct ablation studies to\nexplore the robustness of our method concerning various hyperparameters. Our\ncode is publicly available at https://github.com/Aurora2050/CoBay-CPD."
                },
                "authors": [
                    {
                        "name": "Zeyue Zhang"
                    },
                    {
                        "name": "Xiaoling Lu"
                    },
                    {
                        "name": "Feng Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhou"
                },
                "author": "Feng Zhou",
                "arxiv_comment": "10 pages, accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17591v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17591v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11522v1",
                "updated": "2024-10-15T11:48:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    48,
                    31,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T11:48:31Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    48,
                    31,
                    1,
                    289,
                    0
                ],
                "title": "Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero\n  Shot Music Emotion Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero\n  Shot Music Emotion Prediction"
                },
                "summary": "In this work, we present a novel method for music emotion recognition that\nleverages Large Language Model (LLM) embeddings for label alignment across\nmultiple datasets and zero-shot prediction on novel categories. First, we\ncompute LLM embeddings for emotion labels and apply non-parametric clustering\nto group similar labels, across multiple datasets containing disjoint labels.\nWe use these cluster centers to map music features (MERT) to the LLM embedding\nspace. To further enhance the model, we introduce an alignment regularization\nthat enables dissociation of MERT embeddings from different clusters. This\nfurther enhances the model's ability to better adaptation to unseen datasets.\nWe demonstrate the effectiveness of our approach by performing zero-shot\ninference on a new dataset, showcasing its ability to generalize to unseen\nlabels without additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present a novel method for music emotion recognition that\nleverages Large Language Model (LLM) embeddings for label alignment across\nmultiple datasets and zero-shot prediction on novel categories. First, we\ncompute LLM embeddings for emotion labels and apply non-parametric clustering\nto group similar labels, across multiple datasets containing disjoint labels.\nWe use these cluster centers to map music features (MERT) to the LLM embedding\nspace. To further enhance the model, we introduce an alignment regularization\nthat enables dissociation of MERT embeddings from different clusters. This\nfurther enhances the model's ability to better adaptation to unseen datasets.\nWe demonstrate the effectiveness of our approach by performing zero-shot\ninference on a new dataset, showcasing its ability to generalize to unseen\nlabels without additional training."
                },
                "authors": [
                    {
                        "name": "Renhang Liu"
                    },
                    {
                        "name": "Abhinaba Roy"
                    },
                    {
                        "name": "Dorien Herremans"
                    }
                ],
                "author_detail": {
                    "name": "Dorien Herremans"
                },
                "author": "Dorien Herremans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14162v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14162v2",
                "updated": "2024-10-15T11:37:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    37,
                    4,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-20T10:04:09Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    10,
                    4,
                    9,
                    3,
                    172,
                    0
                ],
                "title": "DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation"
                },
                "summary": "Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}."
                },
                "authors": [
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Tobias Schimanski"
                    },
                    {
                        "name": "Meihong Lin"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Markus Leippold"
                    }
                ],
                "author_detail": {
                    "name": "Markus Leippold"
                },
                "author": "Markus Leippold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14162v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14162v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04467v3",
                "updated": "2024-10-15T11:29:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    29,
                    10,
                    1,
                    289,
                    0
                ],
                "published": "2024-07-05T12:30:02Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    12,
                    30,
                    2,
                    4,
                    187,
                    0
                ],
                "title": "Are Large Language Models Strategic Decision Makers? A Study of\n  Performance and Bias in Two-Player Non-Zero-Sum Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Strategic Decision Makers? A Study of\n  Performance and Bias in Two-Player Non-Zero-Sum Games"
                },
                "summary": "Large Language Models (LLMs) have been increasingly used in real-world\nsettings, yet their strategic decision-making abilities remain largely\nunexplored. To fully benefit from the potential of LLMs, it's essential to\nunderstand their ability to function in complex social scenarios. Game theory,\nwhich is already used to understand real-world interactions, provides a good\nframework for assessing these abilities. This work investigates the performance\nand merits of LLMs in canonical game-theoretic two-player non-zero-sum games,\nStag Hunt and Prisoner Dilemma. Our structured evaluation of GPT-3.5,\nGPT-4-Turbo, GPT-4o, and Llama-3-8B shows that these models, when making\ndecisions in these games, are affected by at least one of the following\nsystematic biases: positional bias, payoff bias, or behavioural bias. This\nindicates that LLMs do not fully rely on logical reasoning when making these\nstrategic decisions. As a result, it was found that the LLMs' performance drops\nwhen the game configuration is misaligned with the affecting biases. When\nmisaligned, GPT-3.5, GPT-4-Turbo, GPT-4o, and Llama-3-8B show an average\nperformance drop of 32\\%, 25\\%, 34\\%, and 29\\% respectively in Stag Hunt, and\n28\\%, 16\\%, 34\\%, and 24\\% respectively in Prisoner's Dilemma. Surprisingly,\nGPT-4o (a top-performing LLM across standard benchmarks) suffers the most\nsubstantial performance drop, suggesting that newer models are not addressing\nthese issues. Interestingly, we found that a commonly used method of improving\nthe reasoning capabilities of LLMs, chain-of-thought (CoT) prompting, reduces\nthe biases in GPT-3.5, GPT-4o, and Llama-3-8B but increases the effect of the\nbias in GPT-4-Turbo, indicating that CoT alone cannot fully serve as a robust\nsolution to this problem. We perform several additional experiments, which\nprovide further insight into these observed behaviours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been increasingly used in real-world\nsettings, yet their strategic decision-making abilities remain largely\nunexplored. To fully benefit from the potential of LLMs, it's essential to\nunderstand their ability to function in complex social scenarios. Game theory,\nwhich is already used to understand real-world interactions, provides a good\nframework for assessing these abilities. This work investigates the performance\nand merits of LLMs in canonical game-theoretic two-player non-zero-sum games,\nStag Hunt and Prisoner Dilemma. Our structured evaluation of GPT-3.5,\nGPT-4-Turbo, GPT-4o, and Llama-3-8B shows that these models, when making\ndecisions in these games, are affected by at least one of the following\nsystematic biases: positional bias, payoff bias, or behavioural bias. This\nindicates that LLMs do not fully rely on logical reasoning when making these\nstrategic decisions. As a result, it was found that the LLMs' performance drops\nwhen the game configuration is misaligned with the affecting biases. When\nmisaligned, GPT-3.5, GPT-4-Turbo, GPT-4o, and Llama-3-8B show an average\nperformance drop of 32\\%, 25\\%, 34\\%, and 29\\% respectively in Stag Hunt, and\n28\\%, 16\\%, 34\\%, and 24\\% respectively in Prisoner's Dilemma. Surprisingly,\nGPT-4o (a top-performing LLM across standard benchmarks) suffers the most\nsubstantial performance drop, suggesting that newer models are not addressing\nthese issues. Interestingly, we found that a commonly used method of improving\nthe reasoning capabilities of LLMs, chain-of-thought (CoT) prompting, reduces\nthe biases in GPT-3.5, GPT-4o, and Llama-3-8B but increases the effect of the\nbias in GPT-4-Turbo, indicating that CoT alone cannot fully serve as a robust\nsolution to this problem. We perform several additional experiments, which\nprovide further insight into these observed behaviours."
                },
                "authors": [
                    {
                        "name": "Nathan Herr"
                    },
                    {
                        "name": "Fernando Acero"
                    },
                    {
                        "name": "Roberta Raileanu"
                    },
                    {
                        "name": "María Pérez-Ortiz"
                    },
                    {
                        "name": "Zhibin Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhibin Li"
                },
                "author": "Zhibin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11507v1",
                "updated": "2024-10-15T11:20:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    20,
                    42,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T11:20:42Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    20,
                    42,
                    1,
                    289,
                    0
                ],
                "title": "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic\n  Evaluation Framework for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic\n  Evaluation Framework for LLMs"
                },
                "summary": "While various vertical domain large language models (LLMs) have been\ndeveloped, the challenge of automatically evaluating their performance across\ndifferent domains remains significant in addressing real-world user needs.\nCurrent benchmark-based evaluation methods exhibit rigid, purposeless\ninteractions and rely on pre-collected static datasets that are costly to\nbuild, inflexible across domains, and misaligned with practical user needs. To\naddress this, we revisit the evaluation components and introduce two\ndefinitions: **Benchmark+**, which extends traditional QA benchmarks into a\nmore flexible ``strategy-criterion'' format; and **Assessment+**, which\nenhances the interaction process for greater exploration and enables both\nquantitative metrics and qualitative insights that capture nuanced target LLM\nbehaviors from richer multi-turn interactions. We propose an agent-based\nevaluation framework called *TestAgent*, which implements these two concepts\nthrough retrieval augmented generation and reinforcement learning. Experiments\non tasks ranging from building vertical domain evaluation from scratch to\nactivating existing benchmarks demonstrate the effectiveness of *TestAgent*\nacross various scenarios. We believe this work offers an interesting\nperspective on automatic evaluation for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While various vertical domain large language models (LLMs) have been\ndeveloped, the challenge of automatically evaluating their performance across\ndifferent domains remains significant in addressing real-world user needs.\nCurrent benchmark-based evaluation methods exhibit rigid, purposeless\ninteractions and rely on pre-collected static datasets that are costly to\nbuild, inflexible across domains, and misaligned with practical user needs. To\naddress this, we revisit the evaluation components and introduce two\ndefinitions: **Benchmark+**, which extends traditional QA benchmarks into a\nmore flexible ``strategy-criterion'' format; and **Assessment+**, which\nenhances the interaction process for greater exploration and enables both\nquantitative metrics and qualitative insights that capture nuanced target LLM\nbehaviors from richer multi-turn interactions. We propose an agent-based\nevaluation framework called *TestAgent*, which implements these two concepts\nthrough retrieval augmented generation and reinforcement learning. Experiments\non tasks ranging from building vertical domain evaluation from scratch to\nactivating existing benchmarks demonstrate the effectiveness of *TestAgent*\nacross various scenarios. We believe this work offers an interesting\nperspective on automatic evaluation for LLMs."
                },
                "authors": [
                    {
                        "name": "Wanying Wang"
                    },
                    {
                        "name": "Zeyu Ma"
                    },
                    {
                        "name": "Pengfei Liu"
                    },
                    {
                        "name": "Mingang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingang Chen"
                },
                "author": "Mingang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10645v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10645v2",
                "updated": "2024-10-15T11:01:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    1,
                    33,
                    1,
                    289,
                    0
                ],
                "published": "2024-08-20T08:36:59Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    36,
                    59,
                    1,
                    233,
                    0
                ],
                "title": "CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation"
                },
                "summary": "Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, CoRA (an acronym for Collaborative LoRA),\nwith a collaborative weights generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings, converting them into collaborative\nweights with low-rank properties through the collaborative weights generator.\nWe then merge the collaborative weights into LLM's weights, enabling LLM to\nperceive the collaborative signals and generate personalized recommendations\nwithout fine-tuning or extra collaborative tokens in prompts. Extensive\nexperiments confirm that CoRA effectively integrates collaborative information\ninto LLM, enhancing recommendation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, CoRA (an acronym for Collaborative LoRA),\nwith a collaborative weights generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings, converting them into collaborative\nweights with low-rank properties through the collaborative weights generator.\nWe then merge the collaborative weights into LLM's weights, enabling LLM to\nperceive the collaborative signals and generate personalized recommendations\nwithout fine-tuning or extra collaborative tokens in prompts. Extensive\nexperiments confirm that CoRA effectively integrates collaborative information\ninto LLM, enhancing recommendation performance."
                },
                "authors": [
                    {
                        "name": "Yuting Liu"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Yizhou Dang"
                    },
                    {
                        "name": "Yuliang Liang"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Jianzhe Zhao"
                    },
                    {
                        "name": "Xingwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingwei Wang"
                },
                "author": "Xingwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10645v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10645v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11890v2",
                "updated": "2024-10-15T10:53:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    10,
                    53,
                    55,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-14T03:34:02Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    3,
                    34,
                    2,
                    4,
                    166,
                    0
                ],
                "title": "Unraveling the Mechanics of Learning-Based Demonstration Selection for\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling the Mechanics of Learning-Based Demonstration Selection for\n  In-Context Learning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive in-context learning\n(ICL) capabilities from few-shot demonstration exemplars. While recent\nlearning-based demonstration selection methods have proven beneficial to ICL by\nchoosing more useful exemplars, their underlying mechanisms are opaque,\nhindering efforts to address limitations such as high training costs and poor\ngeneralization across tasks. These methods generally assume the selection\nprocess captures similarities between the exemplar and the target instance,\nhowever, it remains unknown what kinds of similarities are captured and vital\nto performing ICL. To dive into this question, we analyze the working\nmechanisms of the learning-based demonstration selection methods and\nempirically identify two important factors related to similarity measurement:\n1) The ability to integrate different levels of task-agnostic text similarities\nbetween the input of exemplars and test cases enhances generalization power\nacross different tasks. 2) Incorporating task-specific labels when measuring\nthe similarities significantly improves the performance on each specific task.\nWe validate these two findings through extensive quantitative and qualitative\nanalyses across ten datasets and various LLMs. Based on our findings, we\nintroduce two effective yet simplified exemplar selection methods catering to\ntask-agnostic and task-specific demands, eliminating the costly LLM inference\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive in-context learning\n(ICL) capabilities from few-shot demonstration exemplars. While recent\nlearning-based demonstration selection methods have proven beneficial to ICL by\nchoosing more useful exemplars, their underlying mechanisms are opaque,\nhindering efforts to address limitations such as high training costs and poor\ngeneralization across tasks. These methods generally assume the selection\nprocess captures similarities between the exemplar and the target instance,\nhowever, it remains unknown what kinds of similarities are captured and vital\nto performing ICL. To dive into this question, we analyze the working\nmechanisms of the learning-based demonstration selection methods and\nempirically identify two important factors related to similarity measurement:\n1) The ability to integrate different levels of task-agnostic text similarities\nbetween the input of exemplars and test cases enhances generalization power\nacross different tasks. 2) Incorporating task-specific labels when measuring\nthe similarities significantly improves the performance on each specific task.\nWe validate these two findings through extensive quantitative and qualitative\nanalyses across ten datasets and various LLMs. Based on our findings, we\nintroduce two effective yet simplified exemplar selection methods catering to\ntask-agnostic and task-specific demands, eliminating the costly LLM inference\noverhead."
                },
                "authors": [
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Wenya Wang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Chris Xing Tian"
                    },
                    {
                        "name": "Chenqi Kong"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Haoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Haoliang Li"
                },
                "author": "Haoliang Li",
                "arxiv_comment": "17 pages, 7 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11548v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11548v5",
                "updated": "2024-10-16T06:29:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    6,
                    29,
                    55,
                    2,
                    290,
                    0
                ],
                "published": "2024-06-17T13:44:53Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    13,
                    44,
                    53,
                    0,
                    169,
                    0
                ],
                "title": "AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIC MLLM: Autonomous Interactive Correction MLLM for Robust Robotic\n  Manipulation"
                },
                "summary": "The ability to reflect on and correct failures is crucial for robotic systems\nto interact stably with real-life objects. Observing the generalization and\nreasoning capabilities of Multimodal Large Language Models (MLLMs), previous\napproaches have aimed to utilize these models to enhance robotic systems\naccordingly. However, these methods typically focus on high-level planning\ncorrections using an additional MLLM, with limited utilization of failed\nsamples to correct low-level contact poses which is particularly prone to occur\nduring articulated object manipulation. To address this gap, we propose an\nAutonomous Interactive Correction (AIC) MLLM, which makes use of previous\nlow-level interaction experiences to correct SE(3) pose predictions for\narticulated object. Specifically, AIC MLLM is initially fine-tuned to acquire\nboth pose prediction and feedback prompt comprehension abilities. We design two\ntypes of prompt instructions for interactions with objects: 1) visual masks to\nhighlight unmovable parts for position correction, and 2) textual descriptions\nto indicate potential directions for rotation correction. During inference, a\nFeedback Information Extraction module is introduced to recognize the failure\ncause, allowing AIC MLLM to adaptively correct the pose prediction using the\ncorresponding prompts. To further enhance manipulation stability, we devise a\nTest Time Adaptation strategy that enables AIC MLLM to better adapt to the\ncurrent scene configuration. Finally, extensive experiments are conducted in\nboth simulated and real-world environments to evaluate the proposed method. The\nresults demonstrate that our AIC MLLM can efficiently correct failure samples\nby leveraging interaction experience prompts. Our project website is\nhttps://sites.google.com/view/aic-mllm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to reflect on and correct failures is crucial for robotic systems\nto interact stably with real-life objects. Observing the generalization and\nreasoning capabilities of Multimodal Large Language Models (MLLMs), previous\napproaches have aimed to utilize these models to enhance robotic systems\naccordingly. However, these methods typically focus on high-level planning\ncorrections using an additional MLLM, with limited utilization of failed\nsamples to correct low-level contact poses which is particularly prone to occur\nduring articulated object manipulation. To address this gap, we propose an\nAutonomous Interactive Correction (AIC) MLLM, which makes use of previous\nlow-level interaction experiences to correct SE(3) pose predictions for\narticulated object. Specifically, AIC MLLM is initially fine-tuned to acquire\nboth pose prediction and feedback prompt comprehension abilities. We design two\ntypes of prompt instructions for interactions with objects: 1) visual masks to\nhighlight unmovable parts for position correction, and 2) textual descriptions\nto indicate potential directions for rotation correction. During inference, a\nFeedback Information Extraction module is introduced to recognize the failure\ncause, allowing AIC MLLM to adaptively correct the pose prediction using the\ncorresponding prompts. To further enhance manipulation stability, we devise a\nTest Time Adaptation strategy that enables AIC MLLM to better adapt to the\ncurrent scene configuration. Finally, extensive experiments are conducted in\nboth simulated and real-world environments to evaluate the proposed method. The\nresults demonstrate that our AIC MLLM can efficiently correct failure samples\nby leveraging interaction experience prompts. Our project website is\nhttps://sites.google.com/view/aic-mllm."
                },
                "authors": [
                    {
                        "name": "Chuyan Xiong"
                    },
                    {
                        "name": "Chengyu Shen"
                    },
                    {
                        "name": "Xiaoqi Li"
                    },
                    {
                        "name": "Kaichen Zhou"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Ruiping Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11548v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11548v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12572v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12572v3",
                "updated": "2024-10-15T10:35:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    10,
                    35,
                    17,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-18T13:02:12Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    2,
                    12,
                    1,
                    170,
                    0
                ],
                "title": "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large\n  Language Models"
                },
                "summary": "We introduce Mathador-LM, a new benchmark for evaluating the mathematical\nreasoning on large language models (LLMs), combining ruleset interpretation,\nplanning, and problem-solving. This benchmark is inspired by the Mathador game,\nwhere the objective is to reach a target number using basic arithmetic\noperations on a given set of base numbers, following a simple set of rules. We\nshow that, across leading LLMs, we obtain stable average performance while\ngenerating benchmark instances \\emph{dynamically}, following a target\ndifficulty level. Thus, our benchmark alleviates concerns about test-set\nleakage into training data, an issue that often undermines popular benchmarks.\nAdditionally, we conduct a comprehensive evaluation of both open and\nclosed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that\ncontemporary models struggle with Mathador-LM, scoring significantly lower than\naverage 3rd graders. This stands in stark contrast to their strong performance\non popular mathematical reasoning benchmarks. The implementation of Mathador-LM\nbenchmark is available at\n\\href{https://github.com/IST-DASLab/Mathador-LM}{github.com/IST-DASLab/Mathador-LM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Mathador-LM, a new benchmark for evaluating the mathematical\nreasoning on large language models (LLMs), combining ruleset interpretation,\nplanning, and problem-solving. This benchmark is inspired by the Mathador game,\nwhere the objective is to reach a target number using basic arithmetic\noperations on a given set of base numbers, following a simple set of rules. We\nshow that, across leading LLMs, we obtain stable average performance while\ngenerating benchmark instances \\emph{dynamically}, following a target\ndifficulty level. Thus, our benchmark alleviates concerns about test-set\nleakage into training data, an issue that often undermines popular benchmarks.\nAdditionally, we conduct a comprehensive evaluation of both open and\nclosed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that\ncontemporary models struggle with Mathador-LM, scoring significantly lower than\naverage 3rd graders. This stands in stark contrast to their strong performance\non popular mathematical reasoning benchmarks. The implementation of Mathador-LM\nbenchmark is available at\n\\href{https://github.com/IST-DASLab/Mathador-LM}{github.com/IST-DASLab/Mathador-LM}."
                },
                "authors": [
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Amir Moeini"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12572v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12572v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.11842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11842v1",
                "updated": "2024-10-15T17:59:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    59,
                    44,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:59:44Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    59,
                    44,
                    1,
                    289,
                    0
                ],
                "title": "MoH: Multi-Head Attention as Mixture-of-Head Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoH: Multi-Head Attention as Mixture-of-Head Attention"
                },
                "summary": "In this work, we upgrade the multi-head attention mechanism, the core of the\nTransformer model, to improve efficiency while maintaining or surpassing the\nprevious accuracy level. We show that multi-head attention can be expressed in\nthe summation form. Drawing on the insight that not all attention heads hold\nequal significance, we propose Mixture-of-Head attention (MoH), a new\narchitecture that treats attention heads as experts in the Mixture-of-Experts\n(MoE) mechanism. MoH has two significant advantages: First, MoH enables each\ntoken to select the appropriate attention heads, enhancing inference efficiency\nwithout compromising accuracy or increasing the number of parameters. Second,\nMoH replaces the standard summation in multi-head attention with a weighted\nsummation, introducing flexibility to the attention mechanism and unlocking\nextra performance potential. Extensive experiments on ViT, DiT, and LLMs\ndemonstrate that MoH outperforms multi-head attention by using only 50%-90% of\nthe attention heads. Moreover, we demonstrate that pre-trained multi-head\nattention models, such as LLaMA3-8B, can be further continue-tuned into our MoH\nmodels. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14\nbenchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the\nattention heads. We believe the proposed MoH is a promising alternative to\nmulti-head attention and provides a strong foundation for developing advanced\nand efficient attention-based models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we upgrade the multi-head attention mechanism, the core of the\nTransformer model, to improve efficiency while maintaining or surpassing the\nprevious accuracy level. We show that multi-head attention can be expressed in\nthe summation form. Drawing on the insight that not all attention heads hold\nequal significance, we propose Mixture-of-Head attention (MoH), a new\narchitecture that treats attention heads as experts in the Mixture-of-Experts\n(MoE) mechanism. MoH has two significant advantages: First, MoH enables each\ntoken to select the appropriate attention heads, enhancing inference efficiency\nwithout compromising accuracy or increasing the number of parameters. Second,\nMoH replaces the standard summation in multi-head attention with a weighted\nsummation, introducing flexibility to the attention mechanism and unlocking\nextra performance potential. Extensive experiments on ViT, DiT, and LLMs\ndemonstrate that MoH outperforms multi-head attention by using only 50%-90% of\nthe attention heads. Moreover, we demonstrate that pre-trained multi-head\nattention models, such as LLaMA3-8B, can be further continue-tuned into our MoH\nmodels. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14\nbenchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the\nattention heads. We believe the proposed MoH is a promising alternative to\nmulti-head attention and provides a strong foundation for developing advanced\nand efficient attention-based models."
                },
                "authors": [
                    {
                        "name": "Peng Jin"
                    },
                    {
                        "name": "Bo Zhu"
                    },
                    {
                        "name": "Li Yuan"
                    },
                    {
                        "name": "Shuicheng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Shuicheng Yan"
                },
                "author": "Shuicheng Yan",
                "arxiv_comment": "23 pages, code: https://github.com/SkyworkAI/MoH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11841v1",
                "updated": "2024-10-15T17:59:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    59,
                    30,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:59:30Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    59,
                    30,
                    1,
                    289,
                    0
                ],
                "title": "GaVaMoE: Gaussian-Variational Gated Mixture of Experts for Explainable\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaVaMoE: Gaussian-Variational Gated Mixture of Experts for Explainable\n  Recommendation"
                },
                "summary": "Large language model-based explainable recommendation (LLM-based ER) systems\nshow promise in generating human-like explanations for recommendations.\nHowever, they face challenges in modeling user-item collaborative preferences,\npersonalizing explanations, and handling sparse user-item interactions. To\naddress these issues, we propose GaVaMoE, a novel Gaussian-Variational Gated\nMixture of Experts framework for explainable recommendation. GaVaMoE introduces\ntwo key components: (1) a rating reconstruction module that employs Variational\nAutoencoder (VAE) with a Gaussian Mixture Model (GMM) to capture complex\nuser-item collaborative preferences, serving as a pre-trained multi-gating\nmechanism; and (2) a set of fine-grained expert models coupled with the\nmulti-gating mechanism for generating highly personalized explanations. The VAE\ncomponent models latent factors in user-item interactions, while the GMM\nclusters users with similar behaviors. Each cluster corresponds to a gate in\nthe multi-gating mechanism, routing user-item pairs to appropriate expert\nmodels. This architecture enables GaVaMoE to generate tailored explanations for\nspecific user types and preferences, mitigating data sparsity by leveraging\nuser similarities. Extensive experiments on three real-world datasets\ndemonstrate that GaVaMoE significantly outperforms existing methods in\nexplanation quality, personalization, and consistency. Notably, GaVaMoE\nexhibits robust performance in scenarios with sparse user-item interactions,\nmaintaining high-quality explanations even for users with limited historical\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-based explainable recommendation (LLM-based ER) systems\nshow promise in generating human-like explanations for recommendations.\nHowever, they face challenges in modeling user-item collaborative preferences,\npersonalizing explanations, and handling sparse user-item interactions. To\naddress these issues, we propose GaVaMoE, a novel Gaussian-Variational Gated\nMixture of Experts framework for explainable recommendation. GaVaMoE introduces\ntwo key components: (1) a rating reconstruction module that employs Variational\nAutoencoder (VAE) with a Gaussian Mixture Model (GMM) to capture complex\nuser-item collaborative preferences, serving as a pre-trained multi-gating\nmechanism; and (2) a set of fine-grained expert models coupled with the\nmulti-gating mechanism for generating highly personalized explanations. The VAE\ncomponent models latent factors in user-item interactions, while the GMM\nclusters users with similar behaviors. Each cluster corresponds to a gate in\nthe multi-gating mechanism, routing user-item pairs to appropriate expert\nmodels. This architecture enables GaVaMoE to generate tailored explanations for\nspecific user types and preferences, mitigating data sparsity by leveraging\nuser similarities. Extensive experiments on three real-world datasets\ndemonstrate that GaVaMoE significantly outperforms existing methods in\nexplanation quality, personalization, and consistency. Notably, GaVaMoE\nexhibits robust performance in scenarios with sparse user-item interactions,\nmaintaining high-quality explanations even for users with limited historical\ndata."
                },
                "authors": [
                    {
                        "name": "Fei Tang"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Zeqi Tan"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Guiyang Hou"
                    },
                    {
                        "name": "Kaitao Song"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05278v2",
                "updated": "2024-10-15T17:55:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    55,
                    50,
                    1,
                    289,
                    0
                ],
                "published": "2024-08-09T18:07:36Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    18,
                    7,
                    36,
                    4,
                    222,
                    0
                ],
                "title": "Problem of Locating and Allocating Charging Equipment for Battery\n  Electric Buses under Stochastic Charging Demand",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem of Locating and Allocating Charging Equipment for Battery\n  Electric Buses under Stochastic Charging Demand"
                },
                "summary": "Bus electrification plays a crucial role in advancing urban transportation\nsustainability. Battery Electric Buses (BEBs), however, often need recharging,\nmaking the Problem of Locating and Allocating Charging Equipment for BEBs\n(PLACE-BEB) essential for efficient operations. This study proposes an\noptimization framework to solve the PLACE-BEB by determining the optimal\nplacement of charger types at potential locations under the stochastic charging\ndemand. Leveraging the existing stochastic location literature, we develop a\nMixed-Integer Non-Linear Program (MINLP) to model the problem. To solve this\nproblem, we develop an exact solution method that minimizes the costs related\nto building charging stations, charger allocation, travel to stations, and\naverage queueing and charging times. Queueing dynamics are modeled using an\nM/M/s queue, with the number of servers at each location treated as a decision\nvariable. To improve scalability, we implement a Simulated Annealing (SA) and a\nGenetic Algorithm (GA) allowing for efficient solutions to large-scale\nproblems. The computational performance of the methods was thoroughly\nevaluated, revealing that SA was effective for small-scale problems, while GA\noutperformed others for large-scale instances. A case study comparing\ngarage-only, other-only, and mixed scenarios, along with joint deployment,\nhighlighted the cost benefits of a collaborative and a comprehensive approach.\nSensitivity analyses showed that the waiting time is a key factor to consider\nin the decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bus electrification plays a crucial role in advancing urban transportation\nsustainability. Battery Electric Buses (BEBs), however, often need recharging,\nmaking the Problem of Locating and Allocating Charging Equipment for BEBs\n(PLACE-BEB) essential for efficient operations. This study proposes an\noptimization framework to solve the PLACE-BEB by determining the optimal\nplacement of charger types at potential locations under the stochastic charging\ndemand. Leveraging the existing stochastic location literature, we develop a\nMixed-Integer Non-Linear Program (MINLP) to model the problem. To solve this\nproblem, we develop an exact solution method that minimizes the costs related\nto building charging stations, charger allocation, travel to stations, and\naverage queueing and charging times. Queueing dynamics are modeled using an\nM/M/s queue, with the number of servers at each location treated as a decision\nvariable. To improve scalability, we implement a Simulated Annealing (SA) and a\nGenetic Algorithm (GA) allowing for efficient solutions to large-scale\nproblems. The computational performance of the methods was thoroughly\nevaluated, revealing that SA was effective for small-scale problems, while GA\noutperformed others for large-scale instances. A case study comparing\ngarage-only, other-only, and mixed scenarios, along with joint deployment,\nhighlighted the cost benefits of a collaborative and a comprehensive approach.\nSensitivity analyses showed that the waiting time is a key factor to consider\nin the decision-making."
                },
                "authors": [
                    {
                        "name": "Sadjad Bazarnovi"
                    },
                    {
                        "name": "Taner Cokyasar"
                    },
                    {
                        "name": "Omer Verbas"
                    },
                    {
                        "name": "Abolfazl Kouros Mohammadian"
                    }
                ],
                "author_detail": {
                    "name": "Abolfazl Kouros Mohammadian"
                },
                "author": "Abolfazl Kouros Mohammadian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10818v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10818v2",
                "updated": "2024-10-15T17:55:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    55,
                    46,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for\n  Multimodal Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TemporalBench: Benchmarking Fine-grained Temporal Understanding for\n  Multimodal Video Models"
                },
                "summary": "Understanding fine-grained temporal dynamics is crucial for multimodal video\ncomprehension and generation. Due to the lack of fine-grained temporal\nannotations, existing video benchmarks mostly resemble static image benchmarks\nand are incompetent at evaluating models for temporal understanding. In this\npaper, we introduce TemporalBench, a new benchmark dedicated to evaluating\nfine-grained temporal understanding in videos. TemporalBench consists of ~10K\nvideo question-answer pairs, derived from ~2K high-quality human annotations\ndetailing the temporal dynamics in video clips. As a result, our benchmark\nprovides a unique testbed for evaluating various temporal understanding and\nreasoning abilities such as action frequency, motion magnitude, event order,\netc. Moreover, it enables evaluations on various tasks like both video question\nanswering and captioning, both short and long video understanding, as well as\ndifferent models such as multimodal video embedding models and text generation\nmodels. Results show that state-of-the-art models like GPT-4o achieve only\n38.5% question answering accuracy on TemporalBench, demonstrating a significant\ngap (~30%) between humans and AI in temporal understanding. Furthermore, we\nnotice a critical pitfall for multi-choice QA where LLMs can detect the subtle\nchanges in negative captions and find a centralized description as a cue for\nits prediction, where we propose Multiple Binary Accuracy (MBA) to correct such\nbias. We hope that TemporalBench can foster research on improving models'\ntemporal reasoning capabilities. Both dataset and evaluation code will be made\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding fine-grained temporal dynamics is crucial for multimodal video\ncomprehension and generation. Due to the lack of fine-grained temporal\nannotations, existing video benchmarks mostly resemble static image benchmarks\nand are incompetent at evaluating models for temporal understanding. In this\npaper, we introduce TemporalBench, a new benchmark dedicated to evaluating\nfine-grained temporal understanding in videos. TemporalBench consists of ~10K\nvideo question-answer pairs, derived from ~2K high-quality human annotations\ndetailing the temporal dynamics in video clips. As a result, our benchmark\nprovides a unique testbed for evaluating various temporal understanding and\nreasoning abilities such as action frequency, motion magnitude, event order,\netc. Moreover, it enables evaluations on various tasks like both video question\nanswering and captioning, both short and long video understanding, as well as\ndifferent models such as multimodal video embedding models and text generation\nmodels. Results show that state-of-the-art models like GPT-4o achieve only\n38.5% question answering accuracy on TemporalBench, demonstrating a significant\ngap (~30%) between humans and AI in temporal understanding. Furthermore, we\nnotice a critical pitfall for multi-choice QA where LLMs can detect the subtle\nchanges in negative captions and find a centralized description as a cue for\nits prediction, where we propose Multiple Binary Accuracy (MBA) to correct such\nbias. We hope that TemporalBench can foster research on improving models'\ntemporal reasoning capabilities. Both dataset and evaluation code will be made\navailable."
                },
                "authors": [
                    {
                        "name": "Mu Cai"
                    },
                    {
                        "name": "Reuben Tan"
                    },
                    {
                        "name": "Jianrui Zhang"
                    },
                    {
                        "name": "Bocheng Zou"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Fangrui Zhu"
                    },
                    {
                        "name": "Jing Gu"
                    },
                    {
                        "name": "Yiwu Zhong"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yao Dou"
                    },
                    {
                        "name": "Jaden Park"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Jianwei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yang"
                },
                "author": "Jianwei Yang",
                "arxiv_comment": "Project Page: https://temporalbench.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10818v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10818v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20635v2",
                "updated": "2024-10-15T17:54:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    54,
                    17,
                    1,
                    289,
                    0
                ],
                "published": "2024-07-30T08:26:44Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    26,
                    44,
                    1,
                    212,
                    0
                ],
                "title": "Autonomous Improvement of Instruction Following Skills via Foundation\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Improvement of Instruction Following Skills via Foundation\n  Models"
                },
                "summary": "Intelligent instruction-following robots capable of improving from\nautonomously collected experience have the potential to transform robot\nlearning: instead of collecting costly teleoperated demonstration data,\nlarge-scale deployment of fleets of robots can quickly collect larger\nquantities of autonomous data that can collectively improve their performance.\nHowever, autonomous improvement requires solving two key problems: (i) fully\nautomating a scalable data collection procedure that can collect diverse and\nsemantically meaningful robot data and (ii) learning from non-optimal,\nautonomous data with no human annotations. To this end, we propose a novel\napproach that addresses these challenges, allowing instruction-following\npolicies to improve from autonomously collected data without human supervision.\nOur framework leverages vision-language models to collect and evaluate\nsemantically meaningful experiences in new environments, and then utilizes a\ndecomposition of instruction following tasks into (semantic)\nlanguage-conditioned image generation and (non-semantic) goal reaching, which\nmakes it significantly more practical to improve from this autonomously\ncollected data without any human annotations. We carry out extensive\nexperiments in the real world to demonstrate the effectiveness of our approach,\nand find that in a suite of unseen environments, the robot policy can be\nimproved 2x with autonomously collected data. We open-source the code for our\nsemantic autonomous improvement pipeline, as well as our autonomous dataset of\n30.5K trajectories collected across five tabletop environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent instruction-following robots capable of improving from\nautonomously collected experience have the potential to transform robot\nlearning: instead of collecting costly teleoperated demonstration data,\nlarge-scale deployment of fleets of robots can quickly collect larger\nquantities of autonomous data that can collectively improve their performance.\nHowever, autonomous improvement requires solving two key problems: (i) fully\nautomating a scalable data collection procedure that can collect diverse and\nsemantically meaningful robot data and (ii) learning from non-optimal,\nautonomous data with no human annotations. To this end, we propose a novel\napproach that addresses these challenges, allowing instruction-following\npolicies to improve from autonomously collected data without human supervision.\nOur framework leverages vision-language models to collect and evaluate\nsemantically meaningful experiences in new environments, and then utilizes a\ndecomposition of instruction following tasks into (semantic)\nlanguage-conditioned image generation and (non-semantic) goal reaching, which\nmakes it significantly more practical to improve from this autonomously\ncollected data without any human annotations. We carry out extensive\nexperiments in the real world to demonstrate the effectiveness of our approach,\nand find that in a suite of unseen environments, the robot policy can be\nimproved 2x with autonomously collected data. We open-source the code for our\nsemantic autonomous improvement pipeline, as well as our autonomous dataset of\n30.5K trajectories collected across five tabletop environments."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Zhou"
                    },
                    {
                        "name": "Pranav Atreya"
                    },
                    {
                        "name": "Abraham Lee"
                    },
                    {
                        "name": "Homer Walke"
                    },
                    {
                        "name": "Oier Mees"
                    },
                    {
                        "name": "Sergey Levine"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Levine"
                },
                "author": "Sergey Levine",
                "arxiv_comment": "2024 Conference on Robot Learning (CoRL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11825v1",
                "updated": "2024-10-15T17:52:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    52,
                    20,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:52:20Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    52,
                    20,
                    1,
                    289,
                    0
                ],
                "title": "Learning Smooth Humanoid Locomotion through Lipschitz-Constrained\n  Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Smooth Humanoid Locomotion through Lipschitz-Constrained\n  Policies"
                },
                "summary": "Reinforcement learning combined with sim-to-real transfer offers a general\nframework for developing locomotion controllers for legged robots. To\nfacilitate successful deployment in the real world, smoothing techniques, such\nas low-pass filters and smoothness rewards, are often employed to develop\npolicies with smooth behaviors. However, because these techniques are\nnon-differentiable and usually require tedious tuning of a large set of\nhyperparameters, they tend to require extensive manual tuning for each robotic\nplatform. To address this challenge and establish a general technique for\nenforcing smooth behaviors, we propose a simple and effective method that\nimposes a Lipschitz constraint on a learned policy, which we refer to as\nLipschitz-Constrained Policies (LCP). We show that the Lipschitz constraint can\nbe implemented in the form of a gradient penalty, which provides a\ndifferentiable objective that can be easily incorporated with automatic\ndifferentiation frameworks. We demonstrate that LCP effectively replaces the\nneed for smoothing rewards or low-pass filters and can be easily integrated\ninto training frameworks for many distinct humanoid robots. We extensively\nevaluate LCP in both simulation and real-world humanoid robots, producing\nsmooth and robust locomotion controllers. All simulation and deployment code,\nalong with complete checkpoints, is available on our project page:\nhttps://lipschitz-constrained-policy.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning combined with sim-to-real transfer offers a general\nframework for developing locomotion controllers for legged robots. To\nfacilitate successful deployment in the real world, smoothing techniques, such\nas low-pass filters and smoothness rewards, are often employed to develop\npolicies with smooth behaviors. However, because these techniques are\nnon-differentiable and usually require tedious tuning of a large set of\nhyperparameters, they tend to require extensive manual tuning for each robotic\nplatform. To address this challenge and establish a general technique for\nenforcing smooth behaviors, we propose a simple and effective method that\nimposes a Lipschitz constraint on a learned policy, which we refer to as\nLipschitz-Constrained Policies (LCP). We show that the Lipschitz constraint can\nbe implemented in the form of a gradient penalty, which provides a\ndifferentiable objective that can be easily incorporated with automatic\ndifferentiation frameworks. We demonstrate that LCP effectively replaces the\nneed for smoothing rewards or low-pass filters and can be easily integrated\ninto training frameworks for many distinct humanoid robots. We extensively\nevaluate LCP in both simulation and real-world humanoid robots, producing\nsmooth and robust locomotion controllers. All simulation and deployment code,\nalong with complete checkpoints, is available on our project page:\nhttps://lipschitz-constrained-policy.github.io."
                },
                "authors": [
                    {
                        "name": "Zixuan Chen"
                    },
                    {
                        "name": "Xialin He"
                    },
                    {
                        "name": "Yen-Jen Wang"
                    },
                    {
                        "name": "Qiayuan Liao"
                    },
                    {
                        "name": "Yanjie Ze"
                    },
                    {
                        "name": "Zhongyu Li"
                    },
                    {
                        "name": "S. Shankar Sastry"
                    },
                    {
                        "name": "Jiajun Wu"
                    },
                    {
                        "name": "Koushil Sreenath"
                    },
                    {
                        "name": "Saurabh Gupta"
                    },
                    {
                        "name": "Xue Bin Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Bin Peng"
                },
                "author": "Xue Bin Peng",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10909v2",
                "updated": "2024-10-15T17:44:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    44,
                    57,
                    1,
                    289,
                    0
                ],
                "published": "2024-07-15T17:09:44Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    9,
                    44,
                    0,
                    197,
                    0
                ],
                "title": "FinDKG: Dynamic Knowledge Graphs with Large Language Models for\n  Detecting Global Trends in Financial Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinDKG: Dynamic Knowledge Graphs with Large Language Models for\n  Detecting Global Trends in Financial Markets"
                },
                "summary": "Dynamic knowledge graphs (DKGs) are popular structures to express different\ntypes of connections between objects over time. They can also serve as an\nefficient mathematical tool to represent information extracted from complex\nunstructured data sources, such as text or images. Within financial\napplications, DKGs could be used to detect trends for strategic thematic\ninvesting, based on information obtained from financial news articles. In this\nwork, we explore the properties of large language models (LLMs) as dynamic\nknowledge graph generators, proposing a novel open-source fine-tuned LLM for\nthis purpose, called the Integrated Contextual Knowledge Graph Generator\n(ICKG). We use ICKG to produce a novel open-source DKG from a corpus of\nfinancial news articles, called FinDKG, and we propose an attention-based GNN\narchitecture for analysing it, called KGTransformer. We test the performance of\nthe proposed model on benchmark datasets and FinDKG, demonstrating superior\nperformance on link prediction tasks. Additionally, we evaluate the performance\nof the KGTransformer on FinDKG for thematic investing, showing it can\noutperform existing thematic ETFs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic knowledge graphs (DKGs) are popular structures to express different\ntypes of connections between objects over time. They can also serve as an\nefficient mathematical tool to represent information extracted from complex\nunstructured data sources, such as text or images. Within financial\napplications, DKGs could be used to detect trends for strategic thematic\ninvesting, based on information obtained from financial news articles. In this\nwork, we explore the properties of large language models (LLMs) as dynamic\nknowledge graph generators, proposing a novel open-source fine-tuned LLM for\nthis purpose, called the Integrated Contextual Knowledge Graph Generator\n(ICKG). We use ICKG to produce a novel open-source DKG from a corpus of\nfinancial news articles, called FinDKG, and we propose an attention-based GNN\narchitecture for analysing it, called KGTransformer. We test the performance of\nthe proposed model on benchmark datasets and FinDKG, demonstrating superior\nperformance on link prediction tasks. Additionally, we evaluate the performance\nof the KGTransformer on FinDKG for thematic investing, showing it can\noutperform existing thematic ETFs."
                },
                "authors": [
                    {
                        "name": "Xiaohui Victor Li"
                    },
                    {
                        "name": "Francesco Sanna Passino"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Sanna Passino"
                },
                "author": "Francesco Sanna Passino",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11815v1",
                "updated": "2024-10-15T17:40:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    40,
                    48,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:40:48Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    40,
                    48,
                    1,
                    289,
                    0
                ],
                "title": "SGEdit: Bridging LLM with Text2Image Generative Model for Scene\n  Graph-based Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEdit: Bridging LLM with Text2Image Generative Model for Scene\n  Graph-based Image Editing"
                },
                "summary": "Scene graphs offer a structured, hierarchical representation of images, with\nnodes and edges symbolizing objects and the relationships among them. It can\nserve as a natural interface for image editing, dramatically improving\nprecision and flexibility. Leveraging this benefit, we introduce a new\nframework that integrates large language model (LLM) with Text2Image generative\nmodel for scene graph-based image editing. This integration enables precise\nmodifications at the object level and creative recomposition of scenes without\ncompromising overall image integrity. Our approach involves two primary stages:\n1) Utilizing a LLM-driven scene parser, we construct an image's scene graph,\ncapturing key objects and their interrelationships, as well as parsing\nfine-grained attributes such as object masks and descriptions. These\nannotations facilitate concept learning with a fine-tuned diffusion model,\nrepresenting each object with an optimized token and detailed description\nprompt. 2) During the image editing phase, a LLM editing controller guides the\nedits towards specific areas. These edits are then implemented by an\nattention-modulated diffusion editor, utilizing the fine-tuned model to perform\nobject additions, deletions, replacements, and adjustments. Through extensive\nexperiments, we demonstrate that our framework significantly outperforms\nexisting image editing methods in terms of editing precision and scene\naesthetics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scene graphs offer a structured, hierarchical representation of images, with\nnodes and edges symbolizing objects and the relationships among them. It can\nserve as a natural interface for image editing, dramatically improving\nprecision and flexibility. Leveraging this benefit, we introduce a new\nframework that integrates large language model (LLM) with Text2Image generative\nmodel for scene graph-based image editing. This integration enables precise\nmodifications at the object level and creative recomposition of scenes without\ncompromising overall image integrity. Our approach involves two primary stages:\n1) Utilizing a LLM-driven scene parser, we construct an image's scene graph,\ncapturing key objects and their interrelationships, as well as parsing\nfine-grained attributes such as object masks and descriptions. These\nannotations facilitate concept learning with a fine-tuned diffusion model,\nrepresenting each object with an optimized token and detailed description\nprompt. 2) During the image editing phase, a LLM editing controller guides the\nedits towards specific areas. These edits are then implemented by an\nattention-modulated diffusion editor, utilizing the fine-tuned model to perform\nobject additions, deletions, replacements, and adjustments. Through extensive\nexperiments, we demonstrate that our framework significantly outperforms\nexisting image editing methods in terms of editing precision and scene\naesthetics."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Zhang"
                    },
                    {
                        "name": "DongDong Chen"
                    },
                    {
                        "name": "Jing Liao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liao"
                },
                "author": "Jing Liao",
                "arxiv_comment": "Accepted by ACM Transactions on Graphics and SIGGRAPH Asia 2024.\n  Project page: https://bestzzhang.github.io/SGEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05717v2",
                "updated": "2024-10-15T17:37:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    37,
                    45,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-08T06:24:15Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    6,
                    24,
                    15,
                    1,
                    282,
                    0
                ],
                "title": "Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of\n  Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of\n  Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"
                },
                "summary": "This research addresses the need for high-definition (HD) maps for autonomous\nvehicles (AVs), focusing on road lane information derived from aerial imagery.\nWhile Earth observation data offers valuable resources for map creation,\nspecialized models for road lane extraction are still underdeveloped in remote\nsensing. In this study, we perform an extensive comparison of twelve\nfoundational deep learning-based semantic segmentation models for road lane\nmarking extraction from high-definition remote sensing images, assessing their\nperformance under transfer learning with partially labeled datasets. These\nmodels were fine-tuned on the partially labeled Waterloo Urban Scene dataset,\nand pre-trained on the SkyScapes dataset, simulating a likely scenario of\nreal-life model deployment under partial labeling. We observed and assessed the\nfine-tuning performance and overall performance. Models showed significant\nperformance improvements after fine-tuning, with mean IoU scores ranging from\n33.56% to 76.11%, and recall ranging from 66.0% to 98.96%. Transformer-based\nmodels outperformed convolutional neural networks, emphasizing the importance\nof model pre-training and fine-tuning in enhancing HD map development for AV\nnavigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research addresses the need for high-definition (HD) maps for autonomous\nvehicles (AVs), focusing on road lane information derived from aerial imagery.\nWhile Earth observation data offers valuable resources for map creation,\nspecialized models for road lane extraction are still underdeveloped in remote\nsensing. In this study, we perform an extensive comparison of twelve\nfoundational deep learning-based semantic segmentation models for road lane\nmarking extraction from high-definition remote sensing images, assessing their\nperformance under transfer learning with partially labeled datasets. These\nmodels were fine-tuned on the partially labeled Waterloo Urban Scene dataset,\nand pre-trained on the SkyScapes dataset, simulating a likely scenario of\nreal-life model deployment under partial labeling. We observed and assessed the\nfine-tuning performance and overall performance. Models showed significant\nperformance improvements after fine-tuning, with mean IoU scores ranging from\n33.56% to 76.11%, and recall ranging from 66.0% to 98.96%. Transformer-based\nmodels outperformed convolutional neural networks, emphasizing the importance\nof model pre-training and fine-tuning in enhancing HD map development for AV\nnavigation."
                },
                "authors": [
                    {
                        "name": "Willow Liu"
                    },
                    {
                        "name": "Shuxin Qiao"
                    },
                    {
                        "name": "Kyle Gao"
                    },
                    {
                        "name": "Hongjie He"
                    },
                    {
                        "name": "Michael A. Chapman"
                    },
                    {
                        "name": "Linlin Xu"
                    },
                    {
                        "name": "Jonathan Li"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Li"
                },
                "author": "Jonathan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11805v1",
                "updated": "2024-10-15T17:33:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    33,
                    43,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:33:43Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    33,
                    43,
                    1,
                    289,
                    0
                ],
                "title": "NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NesTools: A Dataset for Evaluating Nested Tool Learning Abilities of\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) combined with tool learning have gained\nimpressive results in real-world applications. During tool learning, LLMs may\ncall multiple tools in nested orders, where the latter tool call may take the\nformer response as its input parameters. However, current research on the\nnested tool learning capabilities is still under-explored, since the existing\nbenchmarks lack of relevant data instances. To address this problem, we\nintroduce NesTools to bridge the current gap in comprehensive nested tool\nlearning evaluations. NesTools comprises a novel automatic data generation\nmethod to construct large-scale nested tool calls with different nesting\nstructures. With manual review and refinement, the dataset is in high quality\nand closely aligned with real-world scenarios. Therefore, NesTools can serve as\na new benchmark to evaluate the nested tool learning abilities of LLMs. We\nconduct extensive experiments on 22 LLMs, and provide in-depth analyses with\nNesTools, which shows that current LLMs still suffer from the complex nested\ntool learning task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) combined with tool learning have gained\nimpressive results in real-world applications. During tool learning, LLMs may\ncall multiple tools in nested orders, where the latter tool call may take the\nformer response as its input parameters. However, current research on the\nnested tool learning capabilities is still under-explored, since the existing\nbenchmarks lack of relevant data instances. To address this problem, we\nintroduce NesTools to bridge the current gap in comprehensive nested tool\nlearning evaluations. NesTools comprises a novel automatic data generation\nmethod to construct large-scale nested tool calls with different nesting\nstructures. With manual review and refinement, the dataset is in high quality\nand closely aligned with real-world scenarios. Therefore, NesTools can serve as\na new benchmark to evaluate the nested tool learning abilities of LLMs. We\nconduct extensive experiments on 22 LLMs, and provide in-depth analyses with\nNesTools, which shows that current LLMs still suffer from the complex nested\ntool learning task."
                },
                "authors": [
                    {
                        "name": "Han Han"
                    },
                    {
                        "name": "Tong Zhu"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Mengsong Wu"
                    },
                    {
                        "name": "Hao Xiong"
                    },
                    {
                        "name": "Wenliang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenliang Chen"
                },
                "author": "Wenliang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11795v1",
                "updated": "2024-10-15T17:19:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    19,
                    46,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:19:46Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    19,
                    46,
                    1,
                    289,
                    0
                ],
                "title": "Efficient Diffusion Models: A Comprehensive Survey from Principles to\n  Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Models: A Comprehensive Survey from Principles to\n  Practices"
                },
                "summary": "As one of the most popular and sought-after generative models in the recent\nyears, diffusion models have sparked the interests of many researchers and\nsteadily shown excellent advantage in various generative tasks such as image\nsynthesis, video generation, molecule design, 3D scene rendering and multimodal\ngeneration, relying on their dense theoretical principles and reliable\napplication practices. The remarkable success of these recent efforts on\ndiffusion models comes largely from progressive design principles and efficient\narchitecture, training, inference, and deployment methodologies. However, there\nhas not been a comprehensive and in-depth review to summarize these principles\nand practices to help the rapid understanding and application of diffusion\nmodels. In this survey, we provide a new efficiency-oriented perspective on\nthese existing efforts, which mainly focuses on the profound principles and\nefficient practices in architecture designs, model training, fast inference and\nreliable deployment, to guide further theoretical research, algorithm migration\nand model application for new scenarios in a reader-friendly way.\n\\url{https://github.com/ponyzym/Efficient-DMs-Survey}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As one of the most popular and sought-after generative models in the recent\nyears, diffusion models have sparked the interests of many researchers and\nsteadily shown excellent advantage in various generative tasks such as image\nsynthesis, video generation, molecule design, 3D scene rendering and multimodal\ngeneration, relying on their dense theoretical principles and reliable\napplication practices. The remarkable success of these recent efforts on\ndiffusion models comes largely from progressive design principles and efficient\narchitecture, training, inference, and deployment methodologies. However, there\nhas not been a comprehensive and in-depth review to summarize these principles\nand practices to help the rapid understanding and application of diffusion\nmodels. In this survey, we provide a new efficiency-oriented perspective on\nthese existing efforts, which mainly focuses on the profound principles and\nefficient practices in architecture designs, model training, fast inference and\nreliable deployment, to guide further theoretical research, algorithm migration\nand model application for new scenarios in a reader-friendly way.\n\\url{https://github.com/ponyzym/Efficient-DMs-Survey}"
                },
                "authors": [
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Yuzhu Zhang"
                    },
                    {
                        "name": "Guoli Jia"
                    },
                    {
                        "name": "Liangliang Zhao"
                    },
                    {
                        "name": "Yichao Ma"
                    },
                    {
                        "name": "Mingjie Ma"
                    },
                    {
                        "name": "Gaofeng Liu"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Jianjun Li"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11792v1",
                "updated": "2024-10-15T17:17:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    17,
                    54,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:17:54Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    17,
                    54,
                    1,
                    289,
                    0
                ],
                "title": "OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video\n  Imitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video\n  Imitation"
                },
                "summary": "We study the problem of teaching humanoid robots manipulation skills by\nimitating from single video demonstrations. We introduce OKAMI, a method that\ngenerates a manipulation plan from a single RGB-D video and derives a policy\nfor execution. At the heart of our approach is object-aware retargeting, which\nenables the humanoid robot to mimic the human motions in an RGB-D video while\nadjusting to different object locations during deployment. OKAMI uses\nopen-world vision models to identify task-relevant objects and retarget the\nbody motions and hand poses separately. Our experiments show that OKAMI\nachieves strong generalizations across varying visual and spatial conditions,\noutperforming the state-of-the-art baseline on open-world imitation from\nobservation. Furthermore, OKAMI rollout trajectories are leveraged to train\nclosed-loop visuomotor policies, which achieve an average success rate of 79.2%\nwithout the need for labor-intensive teleoperation. More videos can be found on\nour website https://ut-austin-rpl.github.io/OKAMI/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of teaching humanoid robots manipulation skills by\nimitating from single video demonstrations. We introduce OKAMI, a method that\ngenerates a manipulation plan from a single RGB-D video and derives a policy\nfor execution. At the heart of our approach is object-aware retargeting, which\nenables the humanoid robot to mimic the human motions in an RGB-D video while\nadjusting to different object locations during deployment. OKAMI uses\nopen-world vision models to identify task-relevant objects and retarget the\nbody motions and hand poses separately. Our experiments show that OKAMI\nachieves strong generalizations across varying visual and spatial conditions,\noutperforming the state-of-the-art baseline on open-world imitation from\nobservation. Furthermore, OKAMI rollout trajectories are leveraged to train\nclosed-loop visuomotor policies, which achieve an average success rate of 79.2%\nwithout the need for labor-intensive teleoperation. More videos can be found on\nour website https://ut-austin-rpl.github.io/OKAMI/."
                },
                "authors": [
                    {
                        "name": "Jinhan Li"
                    },
                    {
                        "name": "Yifeng Zhu"
                    },
                    {
                        "name": "Yuqi Xie"
                    },
                    {
                        "name": "Zhenyu Jiang"
                    },
                    {
                        "name": "Mingyo Seo"
                    },
                    {
                        "name": "Georgios Pavlakos"
                    },
                    {
                        "name": "Yuke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Zhu"
                },
                "author": "Yuke Zhu",
                "arxiv_comment": "Accepted for oral presentation at 8th Annual Conference on Robot\n  Learning. Project website: https://ut-austin-rpl.github.io/OKAMI/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03730v2",
                "updated": "2024-10-15T17:09:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    9,
                    40,
                    1,
                    289,
                    0
                ],
                "published": "2024-09-30T16:05:38Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    16,
                    5,
                    38,
                    0,
                    274,
                    0
                ],
                "title": "Teuken-7B-Base & Teuken-7B-Instruct: Towards European LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teuken-7B-Base & Teuken-7B-Instruct: Towards European LLMs"
                },
                "summary": "We present two multilingual LLMs designed to embrace Europe's linguistic\ndiversity by supporting all 24 official languages of the European Union.\nTrained on a dataset comprising around 60% non-English data and utilizing a\ncustom multilingual tokenizer, our models address the limitations of existing\nLLMs that predominantly focus on English or a few high-resource languages. We\ndetail the models' development principles, i.e., data composition, tokenizer\noptimization, and training methodologies. The models demonstrate competitive\nperformance across multilingual benchmarks, as evidenced by their performance\non European versions of ARC, HellaSwag, MMLU, and TruthfulQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present two multilingual LLMs designed to embrace Europe's linguistic\ndiversity by supporting all 24 official languages of the European Union.\nTrained on a dataset comprising around 60% non-English data and utilizing a\ncustom multilingual tokenizer, our models address the limitations of existing\nLLMs that predominantly focus on English or a few high-resource languages. We\ndetail the models' development principles, i.e., data composition, tokenizer\noptimization, and training methodologies. The models demonstrate competitive\nperformance across multilingual benchmarks, as evidenced by their performance\non European versions of ARC, HellaSwag, MMLU, and TruthfulQA."
                },
                "authors": [
                    {
                        "name": "Mehdi Ali"
                    },
                    {
                        "name": "Michael Fromm"
                    },
                    {
                        "name": "Klaudia Thellmann"
                    },
                    {
                        "name": "Jan Ebert"
                    },
                    {
                        "name": "Alexander Arno Weber"
                    },
                    {
                        "name": "Richard Rutmann"
                    },
                    {
                        "name": "Charvi Jain"
                    },
                    {
                        "name": "Max Lübbering"
                    },
                    {
                        "name": "Daniel Steinigen"
                    },
                    {
                        "name": "Johannes Leveling"
                    },
                    {
                        "name": "Katrin Klug"
                    },
                    {
                        "name": "Jasper Schulze Buschhoff"
                    },
                    {
                        "name": "Lena Jurkschat"
                    },
                    {
                        "name": "Hammam Abdelwahab"
                    },
                    {
                        "name": "Benny Jörg Stein"
                    },
                    {
                        "name": "Karl-Heinz Sylla"
                    },
                    {
                        "name": "Pavel Denisov"
                    },
                    {
                        "name": "Nicolo' Brandizzi"
                    },
                    {
                        "name": "Qasid Saleem"
                    },
                    {
                        "name": "Anirban Bhowmick"
                    },
                    {
                        "name": "Lennard Helmer"
                    },
                    {
                        "name": "Chelsea John"
                    },
                    {
                        "name": "Pedro Ortiz Suarez"
                    },
                    {
                        "name": "Malte Ostendorff"
                    },
                    {
                        "name": "Alex Jude"
                    },
                    {
                        "name": "Lalith Manjunath"
                    },
                    {
                        "name": "Samuel Weinbach"
                    },
                    {
                        "name": "Carolin Penke"
                    },
                    {
                        "name": "Oleg Filatov"
                    },
                    {
                        "name": "Shima Asaadi"
                    },
                    {
                        "name": "Fabio Barth"
                    },
                    {
                        "name": "Rafet Sifa"
                    },
                    {
                        "name": "Fabian Küch"
                    },
                    {
                        "name": "Andreas Herten"
                    },
                    {
                        "name": "René Jäkel"
                    },
                    {
                        "name": "Georg Rehm"
                    },
                    {
                        "name": "Stefan Kesselheim"
                    },
                    {
                        "name": "Joachim Köhler"
                    },
                    {
                        "name": "Nicolas Flores-Herr"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Flores-Herr"
                },
                "author": "Nicolas Flores-Herr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11786v1",
                "updated": "2024-10-15T17:05:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    5,
                    25,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:05:25Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    5,
                    25,
                    1,
                    289,
                    0
                ],
                "title": "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for\n  Faithfulness and Transferability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selection-p: Self-Supervised Task-Agnostic Prompt Compression for\n  Faithfulness and Transferability"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in a\nwide range of natural language processing tasks when leveraging in-context\nlearning. To mitigate the additional computational and financial costs\nassociated with in-context learning, several prompt compression methods have\nbeen proposed to compress the in-context learning prompts. Despite their\nsuccess, these methods face challenges with transferability due to\nmodel-specific compression, or rely on external training data, such as GPT-4.\nIn this paper, we investigate the ability of LLMs to develop a unified\ncompression method that discretizes uninformative tokens, utilizing a\nself-supervised pre-training technique. By introducing a small number of\nparameters during the continual pre-training, the proposed Selection-p produces\na probability for each input token, indicating whether to preserve or discard\nit. Experiments show Selection-p achieves state-of-the-art performance across\nnumerous classification tasks, achieving compression rates of up to 10 times\nwhile experiencing only a marginal 0.8% decrease in performance. Moreover, it\nexhibits superior transferability to different models compared to prior work.\nAdditionally, we further analyze how Selection-p helps maintain performance on\nin-context learning with long contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in a\nwide range of natural language processing tasks when leveraging in-context\nlearning. To mitigate the additional computational and financial costs\nassociated with in-context learning, several prompt compression methods have\nbeen proposed to compress the in-context learning prompts. Despite their\nsuccess, these methods face challenges with transferability due to\nmodel-specific compression, or rely on external training data, such as GPT-4.\nIn this paper, we investigate the ability of LLMs to develop a unified\ncompression method that discretizes uninformative tokens, utilizing a\nself-supervised pre-training technique. By introducing a small number of\nparameters during the continual pre-training, the proposed Selection-p produces\na probability for each input token, indicating whether to preserve or discard\nit. Experiments show Selection-p achieves state-of-the-art performance across\nnumerous classification tasks, achieving compression rates of up to 10 times\nwhile experiencing only a marginal 0.8% decrease in performance. Moreover, it\nexhibits superior transferability to different models compared to prior work.\nAdditionally, we further analyze how Selection-p helps maintain performance on\nin-context learning with long contexts."
                },
                "authors": [
                    {
                        "name": "Tsz Ting Chung"
                    },
                    {
                        "name": "Leyang Cui"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Shuming Shi"
                    },
                    {
                        "name": "Dit-Yan Yeung"
                    }
                ],
                "author_detail": {
                    "name": "Dit-Yan Yeung"
                },
                "author": "Dit-Yan Yeung",
                "arxiv_comment": "14 pages, 5 figures, 10 tables, EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.09969v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.09969v3",
                "updated": "2024-10-15T17:04:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    4,
                    20,
                    1,
                    289,
                    0
                ],
                "published": "2023-09-18T17:50:17Z",
                "published_parsed": [
                    2023,
                    9,
                    18,
                    17,
                    50,
                    17,
                    0,
                    261,
                    0
                ],
                "title": "Prompt a Robot to Walk with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt a Robot to Walk with Large Language Models"
                },
                "summary": "Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) pre-trained on vast internet-scale data have\nshowcased remarkable capabilities across diverse domains. Recently, there has\nbeen escalating interest in deploying LLMs for robotics, aiming to harness the\npower of foundation models in real-world settings. However, this approach faces\nsignificant challenges, particularly in grounding these models in the physical\nworld and in generating dynamic robot motions. To address these issues, we\nintroduce a novel paradigm in which we use few-shot prompts collected from the\nphysical environment, enabling the LLM to autoregressively generate low-level\ncontrol commands for robots without task-specific fine-tuning. Experiments\nacross various robots and environments validate that our method can effectively\nprompt a robot to walk. We thus illustrate how LLMs can proficiently function\nas low-level feedback controllers for dynamic motion control even in\nhigh-dimensional robotic systems. The project website and source code can be\nfound at: https://prompt2walk.github.io/ ."
                },
                "authors": [
                    {
                        "name": "Yen-Jen Wang"
                    },
                    {
                        "name": "Bike Zhang"
                    },
                    {
                        "name": "Jianyu Chen"
                    },
                    {
                        "name": "Koushil Sreenath"
                    }
                ],
                "author_detail": {
                    "name": "Koushil Sreenath"
                },
                "author": "Koushil Sreenath",
                "arxiv_comment": "Conference on Decision and Control (CDC), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.09969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.09969v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11782v1",
                "updated": "2024-10-15T17:01:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    1,
                    21,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:01:21Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    1,
                    21,
                    1,
                    289,
                    0
                ],
                "title": "G-Designer: Architecting Multi-agent Communication Topologies via Graph\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "G-Designer: Architecting Multi-agent Communication Topologies via Graph\n  Neural Networks"
                },
                "summary": "Recent advancements in large language model (LLM)-based agents have\ndemonstrated that collective intelligence can significantly surpass the\ncapabilities of individual agents, primarily due to well-crafted inter-agent\ncommunication topologies. Despite the diverse and high-performing designs\navailable, practitioners often face confusion when selecting the most effective\npipeline for their specific task: \\textit{Which topology is the best choice for\nmy task, avoiding unnecessary communication token overhead while ensuring\nhigh-quality solution?} In response to this dilemma, we introduce G-Designer,\nan adaptive, efficient, and robust solution for multi-agent deployment, which\ndynamically designs task-aware, customized communication topologies.\nSpecifically, G-Designer models the multi-agent system as a multi-agent\nnetwork, leveraging a variational graph auto-encoder to encode both the nodes\n(agents) and a task-specific virtual node, and decodes a task-adaptive and\nhigh-performing communication topology. Extensive experiments on six benchmarks\nshowcase that G-Designer is: \\textbf{(1) high-performing}, achieving superior\nresults on MMLU with accuracy at $84.50\\%$ and on HumanEval with pass@1 at\n$89.90\\%$; \\textbf{(2) task-adaptive}, architecting communication protocols\ntailored to task difficulty, reducing token consumption by up to $95.33\\%$ on\nHumanEval; and \\textbf{(3) adversarially robust}, defending against agent\nadversarial attacks with merely $0.3\\%$ accuracy drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language model (LLM)-based agents have\ndemonstrated that collective intelligence can significantly surpass the\ncapabilities of individual agents, primarily due to well-crafted inter-agent\ncommunication topologies. Despite the diverse and high-performing designs\navailable, practitioners often face confusion when selecting the most effective\npipeline for their specific task: \\textit{Which topology is the best choice for\nmy task, avoiding unnecessary communication token overhead while ensuring\nhigh-quality solution?} In response to this dilemma, we introduce G-Designer,\nan adaptive, efficient, and robust solution for multi-agent deployment, which\ndynamically designs task-aware, customized communication topologies.\nSpecifically, G-Designer models the multi-agent system as a multi-agent\nnetwork, leveraging a variational graph auto-encoder to encode both the nodes\n(agents) and a task-specific virtual node, and decodes a task-adaptive and\nhigh-performing communication topology. Extensive experiments on six benchmarks\nshowcase that G-Designer is: \\textbf{(1) high-performing}, achieving superior\nresults on MMLU with accuracy at $84.50\\%$ and on HumanEval with pass@1 at\n$89.90\\%$; \\textbf{(2) task-adaptive}, architecting communication protocols\ntailored to task difficulty, reducing token consumption by up to $95.33\\%$ on\nHumanEval; and \\textbf{(3) adversarially robust}, defending against agent\nadversarial attacks with merely $0.3\\%$ accuracy drop."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Yanwei Yue"
                    },
                    {
                        "name": "Xiangguo Sun"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Miao Yu"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Dawei Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Cheng"
                },
                "author": "Dawei Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11781v1",
                "updated": "2024-10-15T17:00:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    0,
                    15,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T17:00:15Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    0,
                    15,
                    1,
                    289,
                    0
                ],
                "title": "Language Models Encode Numbers Using Digit Representations in Base 10",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Encode Numbers Using Digit Representations in Base 10"
                },
                "summary": "Large language models (LLMs) frequently make errors when handling even simple\nnumerical problems, such as comparing two small numbers. A natural hypothesis\nis that these errors stem from how LLMs represent numbers, and specifically,\nwhether their representations of numbers capture their numeric values. We\ntackle this question from the observation that LLM errors on numerical tasks\nare often distributed across \\textit{the digits} of the answer rather than\nnormally around \\textit{its numeric value}. Through a series of probing\nexperiments and causal interventions, we show that LLMs internally represent\nnumbers with individual circular representations per-digit in base 10. This\ndigit-wise representation, as opposed to a value representation, sheds light on\nthe error patterns of models on tasks involving numerical reasoning and could\nserve as a basis for future studies on analyzing numerical mechanisms in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently make errors when handling even simple\nnumerical problems, such as comparing two small numbers. A natural hypothesis\nis that these errors stem from how LLMs represent numbers, and specifically,\nwhether their representations of numbers capture their numeric values. We\ntackle this question from the observation that LLM errors on numerical tasks\nare often distributed across \\textit{the digits} of the answer rather than\nnormally around \\textit{its numeric value}. Through a series of probing\nexperiments and causal interventions, we show that LLMs internally represent\nnumbers with individual circular representations per-digit in base 10. This\ndigit-wise representation, as opposed to a value representation, sheds light on\nthe error patterns of models on tasks involving numerical reasoning and could\nserve as a basis for future studies on analyzing numerical mechanisms in LLMs."
                },
                "authors": [
                    {
                        "name": "Amit Arnold Levy"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09982v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09982v2",
                "updated": "2024-10-15T16:57:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    57,
                    41,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-13T19:53:40Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    19,
                    53,
                    40,
                    6,
                    287,
                    0
                ],
                "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Data Distillation for Recovering Quality in Pruned Large Language\n  Models"
                },
                "summary": "Large language models have driven significant progress in natural language\nprocessing, but their deployment requires substantial compute and memory\nresources. As models scale, compression techniques become essential for\nbalancing model quality with computational efficiency. Structured pruning,\nwhich removes less critical components of the model, is a promising strategy\nfor reducing complexity. However, one-shot pruning often results in significant\nquality degradation, particularly in tasks requiring multi-step reasoning. To\nrecover lost quality, supervised fine-tuning (SFT) is commonly applied, but it\ncan lead to catastrophic forgetting by shifting the model's learned data\ndistribution. Therefore, addressing the degradation from both pruning and SFT\nis essential to preserve the original model's quality. In this work, we propose\nself-data distilled fine-tuning to address these challenges. Our approach\nleverages the original, unpruned model to generate a distilled dataset that\npreserves semantic richness and mitigates catastrophic forgetting by\nmaintaining alignment with the base model's knowledge. Empirically, we\ndemonstrate that self-data distillation consistently outperforms standard SFT,\nimproving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard\nv1. Specifically, when pruning 6 decoder blocks on Llama3.1-8B Instruct (i.e.,\n32 to 26 layers, reducing the model size from 8.03B to 6.72B parameters), our\nmethod retains 91.2% of the original model's accuracy compared to 81.7% with\nSFT, while reducing real-world FLOPs by 16.30%. Furthermore, our approach\nscales effectively across datasets, with the quality improving as the dataset\nsize increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have driven significant progress in natural language\nprocessing, but their deployment requires substantial compute and memory\nresources. As models scale, compression techniques become essential for\nbalancing model quality with computational efficiency. Structured pruning,\nwhich removes less critical components of the model, is a promising strategy\nfor reducing complexity. However, one-shot pruning often results in significant\nquality degradation, particularly in tasks requiring multi-step reasoning. To\nrecover lost quality, supervised fine-tuning (SFT) is commonly applied, but it\ncan lead to catastrophic forgetting by shifting the model's learned data\ndistribution. Therefore, addressing the degradation from both pruning and SFT\nis essential to preserve the original model's quality. In this work, we propose\nself-data distilled fine-tuning to address these challenges. Our approach\nleverages the original, unpruned model to generate a distilled dataset that\npreserves semantic richness and mitigates catastrophic forgetting by\nmaintaining alignment with the base model's knowledge. Empirically, we\ndemonstrate that self-data distillation consistently outperforms standard SFT,\nimproving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard\nv1. Specifically, when pruning 6 decoder blocks on Llama3.1-8B Instruct (i.e.,\n32 to 26 layers, reducing the model size from 8.03B to 6.72B parameters), our\nmethod retains 91.2% of the original model's accuracy compared to 81.7% with\nSFT, while reducing real-world FLOPs by 16.30%. Furthermore, our approach\nscales effectively across datasets, with the quality improving as the dataset\nsize increases."
                },
                "authors": [
                    {
                        "name": "Vithursan Thangarasa"
                    },
                    {
                        "name": "Ganesh Venkatesh"
                    },
                    {
                        "name": "Nish Sinnadurai"
                    },
                    {
                        "name": "Sean Lie"
                    }
                ],
                "author_detail": {
                    "name": "Sean Lie"
                },
                "author": "Sean Lie",
                "arxiv_comment": "Accepted at the NeurIPS 2024 Machine Learning and Compression\n  Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09982v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09982v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11772v1",
                "updated": "2024-10-15T16:53:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    53,
                    26,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T16:53:26Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    53,
                    26,
                    1,
                    289,
                    0
                ],
                "title": "Layer-wise Importance Matters: Less Memory for Better Performance in\n  Parameter-efficient Fine-tuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Layer-wise Importance Matters: Less Memory for Better Performance in\n  Parameter-efficient Fine-tuning of Large Language Models"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant\npopularity for adapting pre-trained Large Language Models (LLMs) to downstream\ntasks, primarily due to their potential to significantly reduce memory and\ncomputational overheads. However, a common limitation in most PEFT approaches\nis their application of a uniform architectural design across all layers. This\nuniformity involves identical trainable modules and ignores the varying\nimportance of each layer, leading to sub-optimal fine-tuning results. To\novercome the above limitation and obtain better performance, we develop a novel\napproach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent\nsparsity and select the most important subset of full layers with effective\nlayer-wise importance scoring. The proposed IST is a versatile and\nplug-and-play technique compatible with various PEFT methods that operate on a\nper-layer basis. By leveraging the estimated importance scores, IST dynamically\nupdates these selected layers in PEFT modules, leading to reduced memory\ndemands. We further provide theoretical proof of convergence and empirical\nevidence of superior performance to demonstrate the advantages of IST over\nuniform updating strategies. Extensive experiments on a range of LLMs, PEFTs,\nand downstream tasks substantiate the effectiveness of our proposed method,\nshowcasing IST's capacity to enhance existing layer-based PEFT methods. Our\ncode is available at https://github.com/Kaiseem/IST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant\npopularity for adapting pre-trained Large Language Models (LLMs) to downstream\ntasks, primarily due to their potential to significantly reduce memory and\ncomputational overheads. However, a common limitation in most PEFT approaches\nis their application of a uniform architectural design across all layers. This\nuniformity involves identical trainable modules and ignores the varying\nimportance of each layer, leading to sub-optimal fine-tuning results. To\novercome the above limitation and obtain better performance, we develop a novel\napproach, Importance-aware Sparse Tuning (IST), to fully utilize the inherent\nsparsity and select the most important subset of full layers with effective\nlayer-wise importance scoring. The proposed IST is a versatile and\nplug-and-play technique compatible with various PEFT methods that operate on a\nper-layer basis. By leveraging the estimated importance scores, IST dynamically\nupdates these selected layers in PEFT modules, leading to reduced memory\ndemands. We further provide theoretical proof of convergence and empirical\nevidence of superior performance to demonstrate the advantages of IST over\nuniform updating strategies. Extensive experiments on a range of LLMs, PEFTs,\nand downstream tasks substantiate the effectiveness of our proposed method,\nshowcasing IST's capacity to enhance existing layer-based PEFT methods. Our\ncode is available at https://github.com/Kaiseem/IST."
                },
                "authors": [
                    {
                        "name": "Kai Yao"
                    },
                    {
                        "name": "Penlei Gao"
                    },
                    {
                        "name": "Lichun Li"
                    },
                    {
                        "name": "Yuan Zhao"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Jianke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jianke Zhu"
                },
                "author": "Jianke Zhu",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13198v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13198v2",
                "updated": "2024-10-15T16:35:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    35,
                    56,
                    1,
                    289,
                    0
                ],
                "published": "2024-03-19T23:18:40Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    23,
                    18,
                    40,
                    1,
                    79,
                    0
                ],
                "title": "LAP, Using Action Feasibility for Improved Uncertainty Alignment of\n  Large Language Model Planners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAP, Using Action Feasibility for Improved Uncertainty Alignment of\n  Large Language Model Planners"
                },
                "summary": "Large language models (LLMs) showcase many desirable traits for intelligent\nand helpful robots. However, they are also known to hallucinate predictions.\nThis issue is exacerbated in robotics where LLM hallucinations may result in\nrobots confidently executing plans that are contrary to user goals, relying\nmore frequently on human assistance, or preventing the robot from asking for\nhelp at all. In this work, we present LAP, a novel approach for utilizing\noff-the-shelf LLMs, alongside a novel Action feasibility metric, in robotic\nPlanners that minimize harmful hallucinations and human intervention. Our key\nfinding is that calculating and leveraging a new metric, which we call\nA-Feasibility, a measure of whether a given action is possible and safe in the\nprovided scene, helps to mitigate hallucinations in LLM predictions and better\nalign the LLM's confidence measure with the probability of success. We\nspecifically propose an A-Feasibility metric which both combines scene context\nand prompting a LLM to determine if a given action is possible and safe in the\nscene, using the LLM's response to compute the score. Through experiments in\nboth simulation and the real world on tasks with a variety of ambiguities, we\nshow that LAP significantly increases success rate and decreases the amount of\nhuman intervention required relative to prior art. For example, in our\nreal-world testing paradigm, LAP decreases the human help rate of previous\nmethods by over 33% at a success rate of 70%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) showcase many desirable traits for intelligent\nand helpful robots. However, they are also known to hallucinate predictions.\nThis issue is exacerbated in robotics where LLM hallucinations may result in\nrobots confidently executing plans that are contrary to user goals, relying\nmore frequently on human assistance, or preventing the robot from asking for\nhelp at all. In this work, we present LAP, a novel approach for utilizing\noff-the-shelf LLMs, alongside a novel Action feasibility metric, in robotic\nPlanners that minimize harmful hallucinations and human intervention. Our key\nfinding is that calculating and leveraging a new metric, which we call\nA-Feasibility, a measure of whether a given action is possible and safe in the\nprovided scene, helps to mitigate hallucinations in LLM predictions and better\nalign the LLM's confidence measure with the probability of success. We\nspecifically propose an A-Feasibility metric which both combines scene context\nand prompting a LLM to determine if a given action is possible and safe in the\nscene, using the LLM's response to compute the score. Through experiments in\nboth simulation and the real world on tasks with a variety of ambiguities, we\nshow that LAP significantly increases success rate and decreases the amount of\nhuman intervention required relative to prior art. For example, in our\nreal-world testing paradigm, LAP decreases the human help rate of previous\nmethods by over 33% at a success rate of 70%."
                },
                "authors": [
                    {
                        "name": "James F. Mullen Jr."
                    },
                    {
                        "name": "Dinesh Manocha"
                    }
                ],
                "author_detail": {
                    "name": "Dinesh Manocha"
                },
                "author": "Dinesh Manocha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13198v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13198v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09043v2",
                "updated": "2024-10-15T16:29:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    29,
                    55,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-11T17:57:16Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    17,
                    57,
                    16,
                    4,
                    285,
                    0
                ],
                "title": "Transforming In-Vehicle Network Intrusion Detection: VAE-based Knowledge\n  Distillation Meets Explainable AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming In-Vehicle Network Intrusion Detection: VAE-based Knowledge\n  Distillation Meets Explainable AI"
                },
                "summary": "In the evolving landscape of autonomous vehicles, ensuring robust in-vehicle\nnetwork (IVN) security is paramount. This paper introduces an advanced\nintrusion detection system (IDS) called KD-XVAE that uses a Variational\nAutoencoder (VAE)-based knowledge distillation approach to enhance both\nperformance and efficiency. Our model significantly reduces complexity,\noperating with just 1669 parameters and achieving an inference time of 0.3 ms\nper batch, making it highly suitable for resource-constrained automotive\nenvironments. Evaluations in the HCRL Car-Hacking dataset demonstrate\nexceptional capabilities, attaining perfect scores (Recall, Precision, F1 Score\nof 100%, and FNR of 0%) under multiple attack types, including DoS, Fuzzing,\nGear Spoofing, and RPM Spoofing. Comparative analysis on the CICIoV2024 dataset\nfurther underscores its superiority over traditional machine learning models,\nachieving perfect detection metrics. We furthermore integrate Explainable AI\n(XAI) techniques to ensure transparency in the model's decisions. The VAE\ncompresses the original feature space into a latent space, on which the\ndistilled model is trained. SHAP(SHapley Additive exPlanations) values provide\ninsights into the importance of each latent dimension, mapped back to original\nfeatures for intuitive understanding. Our paper advances the field by\nintegrating state-of-the-art techniques, addressing critical challenges in the\ndeployment of efficient, trustworthy, and reliable IDSes for autonomous\nvehicles, ensuring enhanced protection against emerging cyber threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the evolving landscape of autonomous vehicles, ensuring robust in-vehicle\nnetwork (IVN) security is paramount. This paper introduces an advanced\nintrusion detection system (IDS) called KD-XVAE that uses a Variational\nAutoencoder (VAE)-based knowledge distillation approach to enhance both\nperformance and efficiency. Our model significantly reduces complexity,\noperating with just 1669 parameters and achieving an inference time of 0.3 ms\nper batch, making it highly suitable for resource-constrained automotive\nenvironments. Evaluations in the HCRL Car-Hacking dataset demonstrate\nexceptional capabilities, attaining perfect scores (Recall, Precision, F1 Score\nof 100%, and FNR of 0%) under multiple attack types, including DoS, Fuzzing,\nGear Spoofing, and RPM Spoofing. Comparative analysis on the CICIoV2024 dataset\nfurther underscores its superiority over traditional machine learning models,\nachieving perfect detection metrics. We furthermore integrate Explainable AI\n(XAI) techniques to ensure transparency in the model's decisions. The VAE\ncompresses the original feature space into a latent space, on which the\ndistilled model is trained. SHAP(SHapley Additive exPlanations) values provide\ninsights into the importance of each latent dimension, mapped back to original\nfeatures for intuitive understanding. Our paper advances the field by\nintegrating state-of-the-art techniques, addressing critical challenges in the\ndeployment of efficient, trustworthy, and reliable IDSes for autonomous\nvehicles, ensuring enhanced protection against emerging cyber threats."
                },
                "authors": [
                    {
                        "name": "Muhammet Anil Yagiz"
                    },
                    {
                        "name": "Pedram MohajerAnsari"
                    },
                    {
                        "name": "Mert D. Pese"
                    },
                    {
                        "name": "Polat Goktas"
                    }
                ],
                "author_detail": {
                    "name": "Polat Goktas"
                },
                "author": "Polat Goktas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11745v1",
                "updated": "2024-10-15T16:22:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    22,
                    49,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T16:22:49Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    22,
                    49,
                    1,
                    289,
                    0
                ],
                "title": "Personas with Attitudes: Controlling LLMs for Diverse Data Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personas with Attitudes: Controlling LLMs for Diverse Data Annotation"
                },
                "summary": "We present a novel approach for enhancing diversity and control in data\nannotation tasks by personalizing large language models (LLMs). We investigate\nthe impact of injecting diverse persona descriptions into LLM prompts across\ntwo studies, exploring whether personas increase annotation diversity and\nwhether the impacts of individual personas on the resulting annotations are\nconsistent and controllable. Our results show that persona-prompted LLMs\nproduce more diverse annotations than LLMs prompted without personas and that\nthese effects are both controllable and repeatable, making our approach a\nsuitable tool for improving data annotation in subjective NLP tasks like\ntoxicity detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach for enhancing diversity and control in data\nannotation tasks by personalizing large language models (LLMs). We investigate\nthe impact of injecting diverse persona descriptions into LLM prompts across\ntwo studies, exploring whether personas increase annotation diversity and\nwhether the impacts of individual personas on the resulting annotations are\nconsistent and controllable. Our results show that persona-prompted LLMs\nproduce more diverse annotations than LLMs prompted without personas and that\nthese effects are both controllable and repeatable, making our approach a\nsuitable tool for improving data annotation in subjective NLP tasks like\ntoxicity detection."
                },
                "authors": [
                    {
                        "name": "Leon Fröhling"
                    },
                    {
                        "name": "Gianluca Demartini"
                    },
                    {
                        "name": "Dennis Assenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Dennis Assenmacher"
                },
                "author": "Dennis Assenmacher",
                "arxiv_comment": "21 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11744v1",
                "updated": "2024-10-15T16:21:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    21,
                    15,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T16:21:15Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    21,
                    15,
                    1,
                    289,
                    0
                ],
                "title": "DySpec: Faster Speculative Decoding with Dynamic Token Tree Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DySpec: Faster Speculative Decoding with Dynamic Token Tree Structure"
                },
                "summary": "While speculative decoding has recently appeared as a promising direction for\naccelerating the inference of large language models (LLMs), the speedup and\nscalability are strongly bounded by the token acceptance rate. Prevalent\nmethods usually organize predicted tokens as independent chains or fixed token\ntrees, which fails to generalize to diverse query distributions. In this paper,\nwe propose DySpec, a faster speculative decoding algorithm with a novel dynamic\ntoken tree structure. We begin by bridging the draft distribution and\nacceptance rate from intuitive and empirical clues, and successfully show that\nthe two variables are strongly correlated. Based on this, we employ a greedy\nstrategy to dynamically expand the token tree at run time. Theoretically, we\nshow that our method can achieve optimal results under mild assumptions.\nEmpirically, DySpec yields a higher acceptance rate and speedup than fixed\ntrees. DySpec can drastically improve the throughput and reduce the latency of\ntoken generation across various data distribution and model sizes, which\nsignificantly outperforms strong competitors, including Specinfer and Sequoia.\nUnder low temperature setting, DySpec can improve the throughput up to\n9.1$\\times$ and reduce the latency up to 9.4$\\times$ on Llama2-70B. Under high\ntemperature setting, DySpec can also improve the throughput up to 6.21$\\times$,\ndespite the increasing difficulty of speculating more than one token per step\nfor draft model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While speculative decoding has recently appeared as a promising direction for\naccelerating the inference of large language models (LLMs), the speedup and\nscalability are strongly bounded by the token acceptance rate. Prevalent\nmethods usually organize predicted tokens as independent chains or fixed token\ntrees, which fails to generalize to diverse query distributions. In this paper,\nwe propose DySpec, a faster speculative decoding algorithm with a novel dynamic\ntoken tree structure. We begin by bridging the draft distribution and\nacceptance rate from intuitive and empirical clues, and successfully show that\nthe two variables are strongly correlated. Based on this, we employ a greedy\nstrategy to dynamically expand the token tree at run time. Theoretically, we\nshow that our method can achieve optimal results under mild assumptions.\nEmpirically, DySpec yields a higher acceptance rate and speedup than fixed\ntrees. DySpec can drastically improve the throughput and reduce the latency of\ntoken generation across various data distribution and model sizes, which\nsignificantly outperforms strong competitors, including Specinfer and Sequoia.\nUnder low temperature setting, DySpec can improve the throughput up to\n9.1$\\times$ and reduce the latency up to 9.4$\\times$ on Llama2-70B. Under high\ntemperature setting, DySpec can also improve the throughput up to 6.21$\\times$,\ndespite the increasing difficulty of speculating more than one token per step\nfor draft model."
                },
                "authors": [
                    {
                        "name": "Yunfan Xiong"
                    },
                    {
                        "name": "Ruoyu Zhang"
                    },
                    {
                        "name": "Yanzeng Li"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Lei Zou"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zou"
                },
                "author": "Lei Zou",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05874v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05874v2",
                "updated": "2024-10-15T16:18:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    18,
                    10,
                    1,
                    289,
                    0
                ],
                "published": "2024-08-11T22:59:32Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    22,
                    59,
                    32,
                    6,
                    224,
                    0
                ],
                "title": "LLM-Based Robust Product Classification in Commerce and Compliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Robust Product Classification in Commerce and Compliance"
                },
                "summary": "Product classification is a crucial task in international trade, as\ncompliance regulations are verified and taxes and duties are applied based on\nproduct categories. Manual classification of products is time-consuming and\nerror-prone, and the sheer volume of products imported and exported renders the\nmanual process infeasible. Consequently, e-commerce platforms and enterprises\ninvolved in international trade have turned to automatic product classification\nusing machine learning. However, current approaches do not consider the\nreal-world challenges associated with product classification, such as very\nabbreviated and incomplete product descriptions. In addition, recent\nadvancements in generative Large Language Models (LLMs) and their reasoning\ncapabilities are mainly untapped in product classification and e-commerce. In\nthis research, we explore the real-life challenges of industrial classification\nand we propose data perturbations that allow for realistic data simulation.\nFurthermore, we employ LLM-based product classification to improve the\nrobustness of the prediction in presence of incomplete data. Our research shows\nthat LLMs with in-context learning outperform the supervised approaches in the\nclean-data scenario. Additionally, we illustrate that LLMs are significantly\nmore robust than the supervised approaches when data attacks are present.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Product classification is a crucial task in international trade, as\ncompliance regulations are verified and taxes and duties are applied based on\nproduct categories. Manual classification of products is time-consuming and\nerror-prone, and the sheer volume of products imported and exported renders the\nmanual process infeasible. Consequently, e-commerce platforms and enterprises\ninvolved in international trade have turned to automatic product classification\nusing machine learning. However, current approaches do not consider the\nreal-world challenges associated with product classification, such as very\nabbreviated and incomplete product descriptions. In addition, recent\nadvancements in generative Large Language Models (LLMs) and their reasoning\ncapabilities are mainly untapped in product classification and e-commerce. In\nthis research, we explore the real-life challenges of industrial classification\nand we propose data perturbations that allow for realistic data simulation.\nFurthermore, we employ LLM-based product classification to improve the\nrobustness of the prediction in presence of incomplete data. Our research shows\nthat LLMs with in-context learning outperform the supervised approaches in the\nclean-data scenario. Additionally, we illustrate that LLMs are significantly\nmore robust than the supervised approaches when data attacks are present."
                },
                "authors": [
                    {
                        "name": "Sina Gholamian"
                    },
                    {
                        "name": "Gianfranco Romani"
                    },
                    {
                        "name": "Bartosz Rudnikowicz"
                    },
                    {
                        "name": "Stavroula Skylaki"
                    }
                ],
                "author_detail": {
                    "name": "Stavroula Skylaki"
                },
                "author": "Stavroula Skylaki",
                "arxiv_comment": "Camera-ready version for Customizable NLP Workshop at EMNLP 2024. 11\n  pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05874v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05874v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09962v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09962v2",
                "updated": "2024-10-15T16:10:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    10,
                    26,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-13T18:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    18,
                    59,
                    58,
                    6,
                    287,
                    0
                ],
                "title": "LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large\n  Language Models"
                },
                "summary": "Hallucination, a phenomenon where multimodal large language models~(MLLMs)\ntend to generate textual responses that are plausible but unaligned with the\nimage, has become one major hurdle in various MLLM-related applications.\nSeveral benchmarks have been created to gauge the hallucination levels of\nMLLMs, by either raising discriminative questions about the existence of\nobjects or introducing LLM evaluators to score the generated text from MLLMs.\nHowever, the discriminative data largely involve simple questions that are not\naligned with real-world text, while the generative data involve LLM evaluators\nthat are computationally intensive and unstable due to their inherent\nrandomness. We propose LongHalQA, an LLM-free hallucination benchmark that\ncomprises 6K long and complex hallucination text. LongHalQA is featured by\nGPT4V-generated hallucinatory data that are well aligned with real-world\nscenarios, including object/image descriptions and multi-round conversations\nwith 14/130 words and 189 words, respectively, on average. It introduces two\nnew tasks, hallucination discrimination and hallucination completion, unifying\nboth discriminative and generative evaluations in a single\nmultiple-choice-question form and leading to more reliable and efficient\nevaluations without the need for LLM evaluators. Further, we propose an\nadvanced pipeline that greatly facilitates the construction of future\nhallucination benchmarks with long and complex questions and descriptions.\nExtensive experiments over multiple recent MLLMs reveal various new challenges\nwhen they are handling hallucinations with long and complex textual data.\nDataset and evaluation code are available at\nhttps://github.com/hanqiu-hq/LongHalQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination, a phenomenon where multimodal large language models~(MLLMs)\ntend to generate textual responses that are plausible but unaligned with the\nimage, has become one major hurdle in various MLLM-related applications.\nSeveral benchmarks have been created to gauge the hallucination levels of\nMLLMs, by either raising discriminative questions about the existence of\nobjects or introducing LLM evaluators to score the generated text from MLLMs.\nHowever, the discriminative data largely involve simple questions that are not\naligned with real-world text, while the generative data involve LLM evaluators\nthat are computationally intensive and unstable due to their inherent\nrandomness. We propose LongHalQA, an LLM-free hallucination benchmark that\ncomprises 6K long and complex hallucination text. LongHalQA is featured by\nGPT4V-generated hallucinatory data that are well aligned with real-world\nscenarios, including object/image descriptions and multi-round conversations\nwith 14/130 words and 189 words, respectively, on average. It introduces two\nnew tasks, hallucination discrimination and hallucination completion, unifying\nboth discriminative and generative evaluations in a single\nmultiple-choice-question form and leading to more reliable and efficient\nevaluations without the need for LLM evaluators. Further, we propose an\nadvanced pipeline that greatly facilitates the construction of future\nhallucination benchmarks with long and complex questions and descriptions.\nExtensive experiments over multiple recent MLLMs reveal various new challenges\nwhen they are handling hallucinations with long and complex textual data.\nDataset and evaluation code are available at\nhttps://github.com/hanqiu-hq/LongHalQA."
                },
                "authors": [
                    {
                        "name": "Han Qiu"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Qin Qi"
                    },
                    {
                        "name": "Xiaoqin Zhang"
                    },
                    {
                        "name": "Ling Shao"
                    },
                    {
                        "name": "Shijian Lu"
                    }
                ],
                "author_detail": {
                    "name": "Shijian Lu"
                },
                "author": "Shijian Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09962v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11353v2",
                "updated": "2024-10-15T16:05:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    5,
                    27,
                    1,
                    289,
                    0
                ],
                "published": "2024-09-17T16:55:25Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    55,
                    25,
                    1,
                    261,
                    0
                ],
                "title": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation\n  in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation\n  in Large Language Models"
                },
                "summary": "Hallucination, the generation of factually incorrect content, is a growing\nchallenge in Large Language Models (LLMs). Existing detection and mitigation\nmethods are often isolated and insufficient for domain-specific needs, lacking\na standardized pipeline. This paper introduces THaMES (Tool for Hallucination\nMitigations and EvaluationS), an integrated framework and library addressing\nthis gap. THaMES offers an end-to-end solution for evaluating and mitigating\nhallucinations in LLMs, featuring automated test set generation, multifaceted\nbenchmarking, and adaptable mitigation strategies. It automates test set\ncreation from any corpus, ensuring high data quality, diversity, and\ncost-efficiency through techniques like batch processing, weighted sampling,\nand counterfactual validation. THaMES assesses a model's ability to detect and\nreduce hallucinations across various tasks, including text generation and\nbinary classification, applying optimal mitigation strategies like In-Context\nLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient\nFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base\nof academic papers, political news, and Wikipedia reveal that commercial models\nlike GPT-4o benefit more from RAG than ICL, while open-weight models like\nLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT\nsignificantly enhances the performance of Llama-3.1-8B-Instruct in both\nevaluation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination, the generation of factually incorrect content, is a growing\nchallenge in Large Language Models (LLMs). Existing detection and mitigation\nmethods are often isolated and insufficient for domain-specific needs, lacking\na standardized pipeline. This paper introduces THaMES (Tool for Hallucination\nMitigations and EvaluationS), an integrated framework and library addressing\nthis gap. THaMES offers an end-to-end solution for evaluating and mitigating\nhallucinations in LLMs, featuring automated test set generation, multifaceted\nbenchmarking, and adaptable mitigation strategies. It automates test set\ncreation from any corpus, ensuring high data quality, diversity, and\ncost-efficiency through techniques like batch processing, weighted sampling,\nand counterfactual validation. THaMES assesses a model's ability to detect and\nreduce hallucinations across various tasks, including text generation and\nbinary classification, applying optimal mitigation strategies like In-Context\nLearning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient\nFine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base\nof academic papers, political news, and Wikipedia reveal that commercial models\nlike GPT-4o benefit more from RAG than ICL, while open-weight models like\nLlama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT\nsignificantly enhances the performance of Llama-3.1-8B-Instruct in both\nevaluation tasks."
                },
                "authors": [
                    {
                        "name": "Mengfei Liang"
                    },
                    {
                        "name": "Archish Arun"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Cristian Munoz"
                    },
                    {
                        "name": "Jonathan Lutch"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    },
                    {
                        "name": "Philip Treleaven"
                    }
                ],
                "author_detail": {
                    "name": "Philip Treleaven"
                },
                "author": "Philip Treleaven",
                "arxiv_comment": "Accepted in NeurIPS 2024 SoLaR (Socially Responsible Language\n  Modelling Research ) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03764v2",
                "updated": "2024-10-15T16:01:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    16,
                    1,
                    11,
                    1,
                    289,
                    0
                ],
                "published": "2024-05-06T18:02:00Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    18,
                    2,
                    0,
                    0,
                    127,
                    0
                ],
                "title": "GOVERN: Gradient Orientation Vote Ensemble for Multi-Teacher Reinforced\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GOVERN: Gradient Orientation Vote Ensemble for Multi-Teacher Reinforced\n  Distillation"
                },
                "summary": "Pre-trained language models have become an integral component of\nquestion-answering systems, achieving remarkable performance. However, for\npractical deployment, it is crucial to perform knowledge distillation to\nmaintain high performance while operating under computational constraints. In\nthis paper, we address a key question: given the importance of unsupervised\ndistillation for student model performance, how can knowledge from multiple\nteacher models be effectively ensemble during this stage without the guidance\nof labels? We propose a novel algorithm, GOVERN, to tackle this issue. GOVERN\nhas demonstrated significant improvements in both offline and online\nexperiments, enabling the student model to achieve results comparable to that\nof teacher ensembles. Our experiments show that GOVERN remarkably requires a\nmere 1\\% of the ensemble method's inference budget to achieve 99.5\\% of\nperformance. The proposed algorithm has been successfully deployed in a\nreal-world commercial question-answering system, demonstrating its real-world\napplicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained language models have become an integral component of\nquestion-answering systems, achieving remarkable performance. However, for\npractical deployment, it is crucial to perform knowledge distillation to\nmaintain high performance while operating under computational constraints. In\nthis paper, we address a key question: given the importance of unsupervised\ndistillation for student model performance, how can knowledge from multiple\nteacher models be effectively ensemble during this stage without the guidance\nof labels? We propose a novel algorithm, GOVERN, to tackle this issue. GOVERN\nhas demonstrated significant improvements in both offline and online\nexperiments, enabling the student model to achieve results comparable to that\nof teacher ensembles. Our experiments show that GOVERN remarkably requires a\nmere 1\\% of the ensemble method's inference budget to achieve 99.5\\% of\nperformance. The proposed algorithm has been successfully deployed in a\nreal-world commercial question-answering system, demonstrating its real-world\napplicability."
                },
                "authors": [
                    {
                        "name": "Wenjie Zhou"
                    },
                    {
                        "name": "Zhenxin Ding"
                    },
                    {
                        "name": "Xiaodong Zhang"
                    },
                    {
                        "name": "Haibo Shi"
                    },
                    {
                        "name": "Junfeng Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "arxiv_comment": "Accepted by EMNLP 2024 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v2",
                "updated": "2024-10-15T15:58:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    58,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20485v2",
                "updated": "2024-10-15T15:56:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    56,
                    58,
                    1,
                    289,
                    0
                ],
                "published": "2024-05-30T21:19:24Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    21,
                    19,
                    24,
                    3,
                    151,
                    0
                ],
                "title": "Phantom: General Trigger Attacks on Retrieval Augmented Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phantom: General Trigger Attacks on Retrieval Augmented Language\n  Generation"
                },
                "summary": "Retrieval Augmented Generation (RAG) expands the capabilities of modern large\nlanguage models (LLMs), by anchoring, adapting, and personalizing their\nresponses to the most relevant knowledge sources. It is particularly useful in\nchatbot applications, allowing developers to customize LLM output without\nexpensive retraining. Despite their significant utility in various\napplications, RAG systems present new security risks. In this work, we propose\nnew attack vectors that allow an adversary to inject a single malicious\ndocument into a RAG system's knowledge base, and mount a backdoor poisoning\nattack. We design Phantom, a general two-stage optimization framework against\nRAG systems, that crafts a malicious poisoned document leading to an integrity\nviolation in the model's output. First, the document is constructed to be\nretrieved only when a specific trigger sequence of tokens appears in the\nvictim's queries. Second, the document is further optimized with crafted\nadversarial text that induces various adversarial objectives on the LLM output,\nincluding refusal to answer, reputation damage, privacy violations, and harmful\nbehaviors. We demonstrate our attacks on multiple LLM architectures, including\nGemma, Vicuna, and Llama, and show that they transfer to GPT-3.5 Turbo and\nGPT-4. Finally, we successfully conducted a Phantom attack on NVIDIA's\nblack-box production RAG system, \"Chat with RTX\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) expands the capabilities of modern large\nlanguage models (LLMs), by anchoring, adapting, and personalizing their\nresponses to the most relevant knowledge sources. It is particularly useful in\nchatbot applications, allowing developers to customize LLM output without\nexpensive retraining. Despite their significant utility in various\napplications, RAG systems present new security risks. In this work, we propose\nnew attack vectors that allow an adversary to inject a single malicious\ndocument into a RAG system's knowledge base, and mount a backdoor poisoning\nattack. We design Phantom, a general two-stage optimization framework against\nRAG systems, that crafts a malicious poisoned document leading to an integrity\nviolation in the model's output. First, the document is constructed to be\nretrieved only when a specific trigger sequence of tokens appears in the\nvictim's queries. Second, the document is further optimized with crafted\nadversarial text that induces various adversarial objectives on the LLM output,\nincluding refusal to answer, reputation damage, privacy violations, and harmful\nbehaviors. We demonstrate our attacks on multiple LLM architectures, including\nGemma, Vicuna, and Llama, and show that they transfer to GPT-3.5 Turbo and\nGPT-4. Finally, we successfully conducted a Phantom attack on NVIDIA's\nblack-box production RAG system, \"Chat with RTX\"."
                },
                "authors": [
                    {
                        "name": "Harsh Chaudhari"
                    },
                    {
                        "name": "Giorgio Severi"
                    },
                    {
                        "name": "John Abascal"
                    },
                    {
                        "name": "Matthew Jagielski"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "Milad Nasr"
                    },
                    {
                        "name": "Cristina Nita-Rotaru"
                    },
                    {
                        "name": "Alina Oprea"
                    }
                ],
                "author_detail": {
                    "name": "Alina Oprea"
                },
                "author": "Alina Oprea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11722v1",
                "updated": "2024-10-15T15:55:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    55,
                    0,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:55:00Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    55,
                    0,
                    1,
                    289,
                    0
                ],
                "title": "RClicks: Realistic Click Simulation for Benchmarking Interactive\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RClicks: Realistic Click Simulation for Benchmarking Interactive\n  Segmentation"
                },
                "summary": "The emergence of Segment Anything (SAM) sparked research interest in the\nfield of interactive segmentation, especially in the context of image editing\ntasks and speeding up data annotation. Unlike common semantic segmentation,\ninteractive segmentation methods allow users to directly influence their output\nthrough prompts (e.g. clicks). However, click patterns in real-world\ninteractive segmentation scenarios remain largely unexplored. Most methods rely\non the assumption that users would click in the center of the largest erroneous\narea. Nevertheless, recent studies show that this is not always the case. Thus,\nmethods may have poor performance in real-world deployment despite high metrics\nin a baseline benchmark. To accurately simulate real-user clicks, we conducted\na large crowdsourcing study of click patterns in an interactive segmentation\nscenario and collected 475K real-user clicks. Drawing on ideas from saliency\ntasks, we develop a clickability model that enables sampling clicks, which\nclosely resemble actual user inputs. Using our model and dataset, we propose\nRClicks benchmark for a comprehensive comparison of existing interactive\nsegmentation methods on realistic clicks. Specifically, we evaluate not only\nthe average quality of methods, but also the robustness w.r.t. click patterns.\nAccording to our benchmark, in real-world usage interactive segmentation models\nmay perform worse than it has been reported in the baseline benchmark, and most\nof the methods are not robust. We believe that RClicks is a significant step\ntowards creating interactive segmentation methods that provide the best user\nexperience in real-world cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Segment Anything (SAM) sparked research interest in the\nfield of interactive segmentation, especially in the context of image editing\ntasks and speeding up data annotation. Unlike common semantic segmentation,\ninteractive segmentation methods allow users to directly influence their output\nthrough prompts (e.g. clicks). However, click patterns in real-world\ninteractive segmentation scenarios remain largely unexplored. Most methods rely\non the assumption that users would click in the center of the largest erroneous\narea. Nevertheless, recent studies show that this is not always the case. Thus,\nmethods may have poor performance in real-world deployment despite high metrics\nin a baseline benchmark. To accurately simulate real-user clicks, we conducted\na large crowdsourcing study of click patterns in an interactive segmentation\nscenario and collected 475K real-user clicks. Drawing on ideas from saliency\ntasks, we develop a clickability model that enables sampling clicks, which\nclosely resemble actual user inputs. Using our model and dataset, we propose\nRClicks benchmark for a comprehensive comparison of existing interactive\nsegmentation methods on realistic clicks. Specifically, we evaluate not only\nthe average quality of methods, but also the robustness w.r.t. click patterns.\nAccording to our benchmark, in real-world usage interactive segmentation models\nmay perform worse than it has been reported in the baseline benchmark, and most\nof the methods are not robust. We believe that RClicks is a significant step\ntowards creating interactive segmentation methods that provide the best user\nexperience in real-world cases."
                },
                "authors": [
                    {
                        "name": "Anton Antonov"
                    },
                    {
                        "name": "Andrey Moskalenko"
                    },
                    {
                        "name": "Denis Shepelev"
                    },
                    {
                        "name": "Alexander Krapukhin"
                    },
                    {
                        "name": "Konstantin Soshin"
                    },
                    {
                        "name": "Anton Konushin"
                    },
                    {
                        "name": "Vlad Shakhuro"
                    }
                ],
                "author_detail": {
                    "name": "Vlad Shakhuro"
                },
                "author": "Vlad Shakhuro",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18679v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18679v4",
                "updated": "2024-10-15T15:52:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    52,
                    57,
                    1,
                    289,
                    0
                ],
                "published": "2024-02-28T19:49:55Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    49,
                    55,
                    2,
                    59,
                    0
                ],
                "title": "Data Interpreter: An LLM Agent For Data Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Interpreter: An LLM Agent For Data Science"
                },
                "summary": "Large Language Model (LLM)-based agents have shown effectiveness across many\napplications. However, their use in data science scenarios requiring solving\nlong-term interconnected tasks, dynamic data adjustments and domain expertise\nremains challenging. Previous approaches primarily focus on individual tasks,\nmaking it difficult to assess the complete data science workflow. Moreover,\nthey struggle to handle real-time changes in intermediate data and fail to\nadapt dynamically to evolving task dependencies inherent to data science\nproblems. In this paper, we present Data Interpreter, an LLM-based agent\ndesigned to automatically solve various data science problems end-to-end. Our\nData Interpreter incorporates two key modules: 1) Hierarchical Graph Modeling,\nwhich breaks down complex problems into manageable subproblems, enabling\ndynamic node generation and graph optimization; and 2) Programmable Node\nGeneration, a technique that refines and verifies each subproblem to\niteratively improve code generation results and robustness. Extensive\nexperiments consistently demonstrate the superiority of Data Interpreter. On\nInfiAgent-DABench, it achieves a 25% performance boost, raising accuracy from\n75.9% to 94.9%. For machine learning and open-ended tasks, it improves\nperformance from 88% to 95%, and from 60% to 97%, respectively. Moreover, on\nthe MATH dataset, Data Interpreter achieves remarkable performance with a 26%\nimprovement compared to state-of-the-art baselines. The code is available at\nhttps://github.com/geekan/MetaGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have shown effectiveness across many\napplications. However, their use in data science scenarios requiring solving\nlong-term interconnected tasks, dynamic data adjustments and domain expertise\nremains challenging. Previous approaches primarily focus on individual tasks,\nmaking it difficult to assess the complete data science workflow. Moreover,\nthey struggle to handle real-time changes in intermediate data and fail to\nadapt dynamically to evolving task dependencies inherent to data science\nproblems. In this paper, we present Data Interpreter, an LLM-based agent\ndesigned to automatically solve various data science problems end-to-end. Our\nData Interpreter incorporates two key modules: 1) Hierarchical Graph Modeling,\nwhich breaks down complex problems into manageable subproblems, enabling\ndynamic node generation and graph optimization; and 2) Programmable Node\nGeneration, a technique that refines and verifies each subproblem to\niteratively improve code generation results and robustness. Extensive\nexperiments consistently demonstrate the superiority of Data Interpreter. On\nInfiAgent-DABench, it achieves a 25% performance boost, raising accuracy from\n75.9% to 94.9%. For machine learning and open-ended tasks, it improves\nperformance from 88% to 95%, and from 60% to 97%, respectively. Moreover, on\nthe MATH dataset, Data Interpreter achieves remarkable performance with a 26%\nimprovement compared to state-of-the-art baselines. The code is available at\nhttps://github.com/geekan/MetaGPT."
                },
                "authors": [
                    {
                        "name": "Sirui Hong"
                    },
                    {
                        "name": "Yizhang Lin"
                    },
                    {
                        "name": "Bang Liu"
                    },
                    {
                        "name": "Bangbang Liu"
                    },
                    {
                        "name": "Binhao Wu"
                    },
                    {
                        "name": "Ceyao Zhang"
                    },
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Danyang Li"
                    },
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Jinlin Wang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Lingyao Zhang"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Mingchen Zhuge"
                    },
                    {
                        "name": "Taicheng Guo"
                    },
                    {
                        "name": "Tuo Zhou"
                    },
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Xiangtao Lu"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Xinbing Liang"
                    },
                    {
                        "name": "Yaying Fei"
                    },
                    {
                        "name": "Yuheng Cheng"
                    },
                    {
                        "name": "Zhibin Gou"
                    },
                    {
                        "name": "Zongze Xu"
                    },
                    {
                        "name": "Chenglin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chenglin Wu"
                },
                "author": "Chenglin Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18679v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18679v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11720v1",
                "updated": "2024-10-15T15:52:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    52,
                    45,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:52:45Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    52,
                    45,
                    1,
                    289,
                    0
                ],
                "title": "Light-Weight Fault Tolerant Attention for Large Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light-Weight Fault Tolerant Attention for Large Language Model Training"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks. However, the training of these\nmodels is computationally intensive and susceptible to faults, particularly in\nthe attention mechanism, which is a critical component of transformer-based\nLLMs. In this paper, we investigate the impact of faults on LLM training,\nfocusing on INF, NaN, and near-INF values in the computation results with\nsystematic fault injection experiments. We observe the propagation patterns of\nthese errors, which can trigger non-trainable states in the model and disrupt\ntraining, forcing the procedure to load from checkpoints.To mitigate the impact\nof these faults, we propose ATTNChecker, the first Algorithm-Based Fault\nTolerance (ABFT) technique tailored for the attention mechanism in LLMs.\nATTNChecker is designed based on fault propagation patterns of LLM and\nincorporates performance optimization to adapt to both system reliability and\nmodel vulnerability while providing lightweight protection for fast LLM\ntraining. Evaluations on four LLMs show that ATTNChecker on average incurs on\naverage 7% overhead on training while detecting and correcting all extreme\nerrors. Compared with the state-of-the-art checkpoint/restore approach,\nATTNChecker reduces recovery overhead by up to 49x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance in\nvarious natural language processing tasks. However, the training of these\nmodels is computationally intensive and susceptible to faults, particularly in\nthe attention mechanism, which is a critical component of transformer-based\nLLMs. In this paper, we investigate the impact of faults on LLM training,\nfocusing on INF, NaN, and near-INF values in the computation results with\nsystematic fault injection experiments. We observe the propagation patterns of\nthese errors, which can trigger non-trainable states in the model and disrupt\ntraining, forcing the procedure to load from checkpoints.To mitigate the impact\nof these faults, we propose ATTNChecker, the first Algorithm-Based Fault\nTolerance (ABFT) technique tailored for the attention mechanism in LLMs.\nATTNChecker is designed based on fault propagation patterns of LLM and\nincorporates performance optimization to adapt to both system reliability and\nmodel vulnerability while providing lightweight protection for fast LLM\ntraining. Evaluations on four LLMs show that ATTNChecker on average incurs on\naverage 7% overhead on training while detecting and correcting all extreme\nerrors. Compared with the state-of-the-art checkpoint/restore approach,\nATTNChecker reduces recovery overhead by up to 49x."
                },
                "authors": [
                    {
                        "name": "Yuhang Liang"
                    },
                    {
                        "name": "Xinyi Li"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Bo Fang"
                    },
                    {
                        "name": "Jieyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jieyang Chen"
                },
                "author": "Jieyang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; B.2.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11718v1",
                "updated": "2024-10-15T15:49:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    49,
                    15,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:49:15Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    49,
                    15,
                    1,
                    289,
                    0
                ],
                "title": "Converging to a Lingua Franca: Evolution of Linguistic Regions and\n  Semantics Alignment in Multilingual Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Converging to a Lingua Franca: Evolution of Linguistic Regions and\n  Semantics Alignment in Multilingual Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance,\nparticularly in multilingual contexts. While recent studies suggest that LLMs\ncan transfer skills learned in one language to others, the internal mechanisms\nbehind this ability remain unclear. We observed that the neuron activation\npatterns of LLMs exhibit similarities when processing the same language,\nrevealing the existence and location of key linguistic regions. Additionally,\nwe found that neuron activation patterns are similar when processing sentences\nwith the same semantic meaning in different languages. This indicates that LLMs\nmap semantically identical inputs from different languages into a \"Lingua\nFranca\", a common semantic latent space that allows for consistent processing\nacross languages. This semantic alignment becomes more pronounced with training\nand increased model size, resulting in a more language-agnostic activation\npattern. Moreover, we found that key linguistic neurons are concentrated in the\nfirst and last layers of LLMs, becoming denser in the first layers as training\nprogresses. Experiments on BLOOM and LLaMA2 support these findings,\nhighlighting the structural evolution of multilingual LLMs during training and\nscaling up. This paper provides insights into the internal workings of LLMs,\noffering a foundation for future improvements in their cross-lingual\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance,\nparticularly in multilingual contexts. While recent studies suggest that LLMs\ncan transfer skills learned in one language to others, the internal mechanisms\nbehind this ability remain unclear. We observed that the neuron activation\npatterns of LLMs exhibit similarities when processing the same language,\nrevealing the existence and location of key linguistic regions. Additionally,\nwe found that neuron activation patterns are similar when processing sentences\nwith the same semantic meaning in different languages. This indicates that LLMs\nmap semantically identical inputs from different languages into a \"Lingua\nFranca\", a common semantic latent space that allows for consistent processing\nacross languages. This semantic alignment becomes more pronounced with training\nand increased model size, resulting in a more language-agnostic activation\npattern. Moreover, we found that key linguistic neurons are concentrated in the\nfirst and last layers of LLMs, becoming denser in the first layers as training\nprogresses. Experiments on BLOOM and LLaMA2 support these findings,\nhighlighting the structural evolution of multilingual LLMs during training and\nscaling up. This paper provides insights into the internal workings of LLMs,\noffering a foundation for future improvements in their cross-lingual\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Hongchuan Zeng"
                    },
                    {
                        "name": "Senyu Han"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "16 pages, 11 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17695v2",
                "updated": "2024-10-15T15:48:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    48,
                    4,
                    1,
                    289,
                    0
                ],
                "published": "2024-07-25T01:32:41Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    1,
                    32,
                    41,
                    3,
                    207,
                    0
                ],
                "title": "Enhancing Agent Learning through World Dynamics Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Agent Learning through World Dynamics Modeling"
                },
                "summary": "Large language models (LLMs) have been increasingly applied to tasks in\nlanguage understanding and interactive decision-making, with their impressive\nperformance largely attributed to the extensive domain knowledge embedded\nwithin them. However, the depth and breadth of this knowledge can vary across\ndomains. Many existing approaches assume that LLMs possess a comprehensive\nunderstanding of their environment, often overlooking potential gaps in their\ngrasp of actual world dynamics. To address this, we introduce Discover, Verify,\nand Evolve (DiVE), a framework that discovers world dynamics from a small\nnumber of demonstrations, verifies the accuracy of these dynamics, and evolves\nnew, advanced dynamics tailored to the current situation. Through extensive\nevaluations, we assess the impact of each component on performance and compare\nthe dynamics generated by DiVE to human-annotated dynamics. Our results show\nthat LLMs guided by DiVE make more informed decisions, achieving rewards\ncomparable to human players in the Crafter environment and surpassing methods\nthat require prior task-specific training in the MiniHack environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly applied to tasks in\nlanguage understanding and interactive decision-making, with their impressive\nperformance largely attributed to the extensive domain knowledge embedded\nwithin them. However, the depth and breadth of this knowledge can vary across\ndomains. Many existing approaches assume that LLMs possess a comprehensive\nunderstanding of their environment, often overlooking potential gaps in their\ngrasp of actual world dynamics. To address this, we introduce Discover, Verify,\nand Evolve (DiVE), a framework that discovers world dynamics from a small\nnumber of demonstrations, verifies the accuracy of these dynamics, and evolves\nnew, advanced dynamics tailored to the current situation. Through extensive\nevaluations, we assess the impact of each component on performance and compare\nthe dynamics generated by DiVE to human-annotated dynamics. Our results show\nthat LLMs guided by DiVE make more informed decisions, achieving rewards\ncomparable to human players in the Crafter environment and surpassing methods\nthat require prior task-specific training in the MiniHack environment."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Sun"
                    },
                    {
                        "name": "Haochen Shi"
                    },
                    {
                        "name": "Marc-Alexandre Côté"
                    },
                    {
                        "name": "Glen Berseth"
                    },
                    {
                        "name": "Xingdi Yuan"
                    },
                    {
                        "name": "Bang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bang Liu"
                },
                "author": "Bang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11711v1",
                "updated": "2024-10-15T15:46:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    46,
                    53,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:46:53Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    46,
                    53,
                    1,
                    289,
                    0
                ],
                "title": "Zero-shot Model-based Reinforcement Learning using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Model-based Reinforcement Learning using Large Language Models"
                },
                "summary": "The emerging zero-shot capabilities of Large Language Models (LLMs) have led\nto their applications in areas extending well beyond natural language\nprocessing tasks. In reinforcement learning, while LLMs have been extensively\nused in text-based environments, their integration with continuous state spaces\nremains understudied. In this paper, we investigate how pre-trained LLMs can be\nleveraged to predict in context the dynamics of continuous Markov decision\nprocesses. We identify handling multivariate data and incorporating the control\nsignal as key challenges that limit the potential of LLMs' deployment in this\nsetup and propose Disentangled In-Context Learning (DICL) to address them. We\npresent proof-of-concept applications in two reinforcement learning settings:\nmodel-based policy evaluation and data-augmented off-policy reinforcement\nlearning, supported by theoretical analysis of the proposed methods. Our\nexperiments further demonstrate that our approach produces well-calibrated\nuncertainty estimates. We release the code at\nhttps://github.com/abenechehab/dicl.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emerging zero-shot capabilities of Large Language Models (LLMs) have led\nto their applications in areas extending well beyond natural language\nprocessing tasks. In reinforcement learning, while LLMs have been extensively\nused in text-based environments, their integration with continuous state spaces\nremains understudied. In this paper, we investigate how pre-trained LLMs can be\nleveraged to predict in context the dynamics of continuous Markov decision\nprocesses. We identify handling multivariate data and incorporating the control\nsignal as key challenges that limit the potential of LLMs' deployment in this\nsetup and propose Disentangled In-Context Learning (DICL) to address them. We\npresent proof-of-concept applications in two reinforcement learning settings:\nmodel-based policy evaluation and data-augmented off-policy reinforcement\nlearning, supported by theoretical analysis of the proposed methods. Our\nexperiments further demonstrate that our approach produces well-calibrated\nuncertainty estimates. We release the code at\nhttps://github.com/abenechehab/dicl."
                },
                "authors": [
                    {
                        "name": "Abdelhakim Benechehab"
                    },
                    {
                        "name": "Youssef Attia El Hili"
                    },
                    {
                        "name": "Ambroise Odonnat"
                    },
                    {
                        "name": "Oussama Zekri"
                    },
                    {
                        "name": "Albert Thomas"
                    },
                    {
                        "name": "Giuseppe Paolo"
                    },
                    {
                        "name": "Maurizio Filippone"
                    },
                    {
                        "name": "Ievgen Redko"
                    },
                    {
                        "name": "Balázs Kégl"
                    }
                ],
                "author_detail": {
                    "name": "Balázs Kégl"
                },
                "author": "Balázs Kégl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11710v1",
                "updated": "2024-10-15T15:46:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    46,
                    17,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:46:17Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    46,
                    17,
                    1,
                    289,
                    0
                ],
                "title": "MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have displayed massive improvements in reasoning\nand decision-making skills and can hold natural conversations with users.\nRecently, many tool-use benchmark datasets have been proposed. However,\nexisting datasets have the following limitations: (1). Insufficient evaluation\nscenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation\ncosts (e.g., GPT API costs). To address these limitations, in this work, we\npropose a multi-granularity tool-use benchmark for large language models called\nMTU-Bench. For the \"multi-granularity\" property, our MTU-Bench covers five tool\nusage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool,\nmultiple-turn and single-tool, multiple-turn and multiple-tool, and\nout-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench\nare based on the prediction results and the ground truth without using any GPT\nor human evaluation metrics. Moreover, our MTU-Bench is collected by\ntransforming existing high-quality datasets to simulate real-world tool usage\nscenarios, and we also propose an instruction dataset called MTU-Instruct data\nto enhance the tool-use abilities of existing LLMs. Comprehensive experimental\nresults demonstrate the effectiveness of our MTU-Bench. Code and data will be\nreleased at https: //github.com/MTU-Bench-Team/MTU-Bench.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have displayed massive improvements in reasoning\nand decision-making skills and can hold natural conversations with users.\nRecently, many tool-use benchmark datasets have been proposed. However,\nexisting datasets have the following limitations: (1). Insufficient evaluation\nscenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation\ncosts (e.g., GPT API costs). To address these limitations, in this work, we\npropose a multi-granularity tool-use benchmark for large language models called\nMTU-Bench. For the \"multi-granularity\" property, our MTU-Bench covers five tool\nusage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool,\nmultiple-turn and single-tool, multiple-turn and multiple-tool, and\nout-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench\nare based on the prediction results and the ground truth without using any GPT\nor human evaluation metrics. Moreover, our MTU-Bench is collected by\ntransforming existing high-quality datasets to simulate real-world tool usage\nscenarios, and we also propose an instruction dataset called MTU-Instruct data\nto enhance the tool-use abilities of existing LLMs. Comprehensive experimental\nresults demonstrate the effectiveness of our MTU-Bench. Code and data will be\nreleased at https: //github.com/MTU-Bench-Team/MTU-Bench.git."
                },
                "authors": [
                    {
                        "name": "Pei Wang"
                    },
                    {
                        "name": "Yanan Wu"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xiaoshuai Song"
                    },
                    {
                        "name": "Zhongyuan Peng"
                    },
                    {
                        "name": "Ken Deng"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Jiakai Wang"
                    },
                    {
                        "name": "Junran Peng"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11693v2",
                "updated": "2024-10-16T01:45:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    1,
                    45,
                    28,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-15T15:26:28Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    26,
                    28,
                    1,
                    289,
                    0
                ],
                "title": "IntGrad MT: Eliciting LLMs' Machine Translation Capabilities with\n  Sentence Interpolation and Gradual MT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IntGrad MT: Eliciting LLMs' Machine Translation Capabilities with\n  Sentence Interpolation and Gradual MT"
                },
                "summary": "Recent Large Language Models (LLMs) have demonstrated strong performance in\ntranslation without needing to be finetuned on additional parallel corpora.\nHowever, they still underperform for low-resource language pairs. Previous\nworks have focused on mitigating this issue by leveraging relevant few-shot\nexamples or external resources such as dictionaries or grammar books, making\nmodels heavily reliant on these nonparametric sources of information. In this\npaper, we propose a novel method named IntGrad MT that focuses on fully\nexploiting an LLM's inherent translation capability. IntGrad MT achieves this\nby constructing a chain of few-shot examples, each consisting of a source\nsentence and the model's own translation, that rise incrementally in\ndifficulty. IntGrad MT employs two techniques: Sentence Interpolation, which\ngenerates a sequence of sentences that gradually change from an easy sentence\nto translate to a difficult one, and Gradual MT, which sequentially translates\nthis chain using translations of earlier sentences as few-shot examples for the\ntranslation of subsequent ones. With this approach, we observe a substantial\nenhancement in the xCOMET scores of various LLMs for multiple languages,\nespecially in low-resource languages such as Hindi(8.26), Swahili(7.10),\nBengali(6.97) and Marathi(13.03). Our approach presents a practical way of\nenhancing LLMs' performance without extra training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Language Models (LLMs) have demonstrated strong performance in\ntranslation without needing to be finetuned on additional parallel corpora.\nHowever, they still underperform for low-resource language pairs. Previous\nworks have focused on mitigating this issue by leveraging relevant few-shot\nexamples or external resources such as dictionaries or grammar books, making\nmodels heavily reliant on these nonparametric sources of information. In this\npaper, we propose a novel method named IntGrad MT that focuses on fully\nexploiting an LLM's inherent translation capability. IntGrad MT achieves this\nby constructing a chain of few-shot examples, each consisting of a source\nsentence and the model's own translation, that rise incrementally in\ndifficulty. IntGrad MT employs two techniques: Sentence Interpolation, which\ngenerates a sequence of sentences that gradually change from an easy sentence\nto translate to a difficult one, and Gradual MT, which sequentially translates\nthis chain using translations of earlier sentences as few-shot examples for the\ntranslation of subsequent ones. With this approach, we observe a substantial\nenhancement in the xCOMET scores of various LLMs for multiple languages,\nespecially in low-resource languages such as Hindi(8.26), Swahili(7.10),\nBengali(6.97) and Marathi(13.03). Our approach presents a practical way of\nenhancing LLMs' performance without extra training."
                },
                "authors": [
                    {
                        "name": "Seung-Woo Choi"
                    },
                    {
                        "name": "Ga-Hyun Yoo"
                    },
                    {
                        "name": "Jay-Yoon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jay-Yoon Lee"
                },
                "author": "Jay-Yoon Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11674v1",
                "updated": "2024-10-15T15:08:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    8,
                    57,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:08:57Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    8,
                    57,
                    1,
                    289,
                    0
                ],
                "title": "LLM-Mixer: Multiscale Mixing in LLMs for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Mixer: Multiscale Mixing in LLMs for Time Series Forecasting"
                },
                "summary": "Time series forecasting remains a challenging task, particularly in the\ncontext of complex multiscale temporal patterns. This study presents LLM-Mixer,\na framework that improves forecasting accuracy through the combination of\nmultiscale time-series decomposition with pre-trained LLMs (Large Language\nModels). LLM-Mixer captures both short-term fluctuations and long-term trends\nby decomposing the data into multiple temporal resolutions and processing them\nwith a frozen LLM, guided by a textual prompt specifically designed for\ntime-series data. Extensive experiments conducted on multivariate and\nunivariate datasets demonstrate that LLM-Mixer achieves competitive\nperformance, outperforming recent state-of-the-art models across various\nforecasting horizons. This work highlights the potential of combining\nmultiscale analysis and LLMs for effective and scalable time-series\nforecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting remains a challenging task, particularly in the\ncontext of complex multiscale temporal patterns. This study presents LLM-Mixer,\na framework that improves forecasting accuracy through the combination of\nmultiscale time-series decomposition with pre-trained LLMs (Large Language\nModels). LLM-Mixer captures both short-term fluctuations and long-term trends\nby decomposing the data into multiple temporal resolutions and processing them\nwith a frozen LLM, guided by a textual prompt specifically designed for\ntime-series data. Extensive experiments conducted on multivariate and\nunivariate datasets demonstrate that LLM-Mixer achieves competitive\nperformance, outperforming recent state-of-the-art models across various\nforecasting horizons. This work highlights the potential of combining\nmultiscale analysis and LLMs for effective and scalable time-series\nforecasting."
                },
                "authors": [
                    {
                        "name": "Md Kowsher"
                    },
                    {
                        "name": "Md. Shohanur Islam Sobuj"
                    },
                    {
                        "name": "Nusrat Jahan Prottasha"
                    },
                    {
                        "name": "E. Alejandro Alanis"
                    },
                    {
                        "name": "Ozlem Ozmen Garibay"
                    },
                    {
                        "name": "Niloofar Yousefi"
                    }
                ],
                "author_detail": {
                    "name": "Niloofar Yousefi"
                },
                "author": "Niloofar Yousefi",
                "arxiv_comment": "Time series forecasting using LLMs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02953v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02953v2",
                "updated": "2024-10-15T15:08:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    8,
                    32,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-03T19:53:47Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    19,
                    53,
                    47,
                    3,
                    277,
                    0
                ],
                "title": "Unlocking Structured Thinking in Language Models with Cognitive\n  Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Structured Thinking in Language Models with Cognitive\n  Prompting"
                },
                "summary": "We propose cognitive prompting as a novel approach to guide problem-solving\nin large language models (LLMs) through structured, human-like cognitive\noperations such as goal clarification, decomposition, filtering, abstraction,\nand pattern recognition. By employing systematic, step-by-step reasoning,\ncognitive prompting enables LLMs to efficiently tackle complex, multi-step\ntasks. We evaluate the effectiveness of cognitive prompting on Meta's LLaMA\nmodels, comparing performance on arithmetic reasoning tasks using the GSM8K\ndataset and on commonsense reasoning benchmarks. Our analysis includes\ncomparisons between models without cognitive prompting, models with a static\nsequence of cognitive operations, and models using reflective cognitive\nprompting, where the LLM dynamically self-selects the sequence of cognitive\noperations. The results show that cognitive prompting, particularly when\ndynamically adapted, significantly improves the performance of larger models,\nsuch as LLaMA3.1 70B, and enhances their ability to handle multi-step reasoning\ntasks. This approach also improves interpretability and flexibility,\nhighlighting cognitive prompting as a promising strategy for general-purpose AI\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose cognitive prompting as a novel approach to guide problem-solving\nin large language models (LLMs) through structured, human-like cognitive\noperations such as goal clarification, decomposition, filtering, abstraction,\nand pattern recognition. By employing systematic, step-by-step reasoning,\ncognitive prompting enables LLMs to efficiently tackle complex, multi-step\ntasks. We evaluate the effectiveness of cognitive prompting on Meta's LLaMA\nmodels, comparing performance on arithmetic reasoning tasks using the GSM8K\ndataset and on commonsense reasoning benchmarks. Our analysis includes\ncomparisons between models without cognitive prompting, models with a static\nsequence of cognitive operations, and models using reflective cognitive\nprompting, where the LLM dynamically self-selects the sequence of cognitive\noperations. The results show that cognitive prompting, particularly when\ndynamically adapted, significantly improves the performance of larger models,\nsuch as LLaMA3.1 70B, and enhances their ability to handle multi-step reasoning\ntasks. This approach also improves interpretability and flexibility,\nhighlighting cognitive prompting as a promising strategy for general-purpose AI\nreasoning."
                },
                "authors": [
                    {
                        "name": "Oliver Kramer"
                    },
                    {
                        "name": "Jill Baumann"
                    }
                ],
                "author_detail": {
                    "name": "Jill Baumann"
                },
                "author": "Jill Baumann",
                "arxiv_comment": "11 pages, submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02953v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02953v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11672v1",
                "updated": "2024-10-15T15:05:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    5,
                    41,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T15:05:41Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    5,
                    41,
                    1,
                    289,
                    0
                ],
                "title": "Leaving the barn door open for Clever Hans: Simple features predict LLM\n  benchmark answers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leaving the barn door open for Clever Hans: Simple features predict LLM\n  benchmark answers"
                },
                "summary": "The integrity of AI benchmarks is fundamental to accurately assess the\ncapabilities of AI systems. The internal validity of these benchmarks - i.e.,\nmaking sure they are free from confounding factors - is crucial for ensuring\nthat they are measuring what they are designed to measure. In this paper, we\nexplore a key issue related to internal validity: the possibility that AI\nsystems can solve benchmarks in unintended ways, bypassing the capability being\ntested. This phenomenon, widely known in human and animal experiments, is often\nreferred to as the 'Clever Hans' effect, where tasks are solved using spurious\ncues, often involving much simpler processes than those putatively assessed.\nPrevious research suggests that language models can exhibit this behaviour as\nwell. In several older Natural Language Processing (NLP) benchmarks, individual\n$n$-grams like \"not\" have been found to be highly predictive of the correct\nlabels, and supervised NLP models have been shown to exploit these patterns. In\nthis work, we investigate the extent to which simple $n$-grams extracted from\nbenchmark instances can be combined to predict labels in modern multiple-choice\nbenchmarks designed for LLMs, and whether LLMs might be using such $n$-gram\npatterns to solve these benchmarks. We show how simple classifiers trained on\nthese $n$-grams can achieve high scores on several benchmarks, despite lacking\nthe capabilities being tested. Additionally, we provide evidence that modern\nLLMs might be using these superficial patterns to solve benchmarks. This\nsuggests that the internal validity of these benchmarks may be compromised and\ncaution should be exercised when interpreting LLM performance results on them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integrity of AI benchmarks is fundamental to accurately assess the\ncapabilities of AI systems. The internal validity of these benchmarks - i.e.,\nmaking sure they are free from confounding factors - is crucial for ensuring\nthat they are measuring what they are designed to measure. In this paper, we\nexplore a key issue related to internal validity: the possibility that AI\nsystems can solve benchmarks in unintended ways, bypassing the capability being\ntested. This phenomenon, widely known in human and animal experiments, is often\nreferred to as the 'Clever Hans' effect, where tasks are solved using spurious\ncues, often involving much simpler processes than those putatively assessed.\nPrevious research suggests that language models can exhibit this behaviour as\nwell. In several older Natural Language Processing (NLP) benchmarks, individual\n$n$-grams like \"not\" have been found to be highly predictive of the correct\nlabels, and supervised NLP models have been shown to exploit these patterns. In\nthis work, we investigate the extent to which simple $n$-grams extracted from\nbenchmark instances can be combined to predict labels in modern multiple-choice\nbenchmarks designed for LLMs, and whether LLMs might be using such $n$-gram\npatterns to solve these benchmarks. We show how simple classifiers trained on\nthese $n$-grams can achieve high scores on several benchmarks, despite lacking\nthe capabilities being tested. Additionally, we provide evidence that modern\nLLMs might be using these superficial patterns to solve benchmarks. This\nsuggests that the internal validity of these benchmarks may be compromised and\ncaution should be exercised when interpreting LLM performance results on them."
                },
                "authors": [
                    {
                        "name": "Lorenzo Pacchiardi"
                    },
                    {
                        "name": "Marko Tesic"
                    },
                    {
                        "name": "Lucy G. Cheke"
                    },
                    {
                        "name": "José Hernández-Orallo"
                    }
                ],
                "author_detail": {
                    "name": "José Hernández-Orallo"
                },
                "author": "José Hernández-Orallo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11919v2",
                "updated": "2024-10-15T14:52:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    52,
                    55,
                    1,
                    289,
                    0
                ],
                "published": "2024-09-18T12:32:25Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    12,
                    32,
                    25,
                    2,
                    262,
                    0
                ],
                "title": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language\n  Models for Referring Expression Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language\n  Models for Referring Expression Comprehension"
                },
                "summary": "Vision Language Models (VLMs) have demonstrated remarkable capabilities in\nvarious open-vocabulary tasks, yet their zero-shot performance lags behind\ntask-specific finetuned models, particularly in complex tasks like Referring\nExpression Comprehension (REC). Fine-tuning usually requires 'white-box' access\nto the model's architecture and weights, which is not always feasible due to\nproprietary or privacy concerns. In this work, we propose LLM-wrapper, a method\nfor 'black-box' adaptation of VLMs for the REC task using Large Language Models\n(LLMs). LLM-wrapper capitalizes on the reasoning abilities of LLMs, improved\nwith a light fine-tuning, to select the most relevant bounding box matching the\nreferring expression, from candidates generated by a zero-shot black-box VLM.\nOur approach offers several advantages: it enables the adaptation of\nclosed-source models without needing access to their internal workings, it is\nversatile as it works with any VLM, it transfers to new VLMs, and it allows for\nthe adaptation of an ensemble of VLMs. We evaluate LLM-wrapper on multiple\ndatasets using different VLMs and LLMs, demonstrating significant performance\nimprovements and highlighting the versatility of our method. While LLM-wrapper\nis not meant to directly compete with standard white-box fine-tuning, it offers\na practical and effective alternative for black-box VLM adaptation. The code\nwill be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have demonstrated remarkable capabilities in\nvarious open-vocabulary tasks, yet their zero-shot performance lags behind\ntask-specific finetuned models, particularly in complex tasks like Referring\nExpression Comprehension (REC). Fine-tuning usually requires 'white-box' access\nto the model's architecture and weights, which is not always feasible due to\nproprietary or privacy concerns. In this work, we propose LLM-wrapper, a method\nfor 'black-box' adaptation of VLMs for the REC task using Large Language Models\n(LLMs). LLM-wrapper capitalizes on the reasoning abilities of LLMs, improved\nwith a light fine-tuning, to select the most relevant bounding box matching the\nreferring expression, from candidates generated by a zero-shot black-box VLM.\nOur approach offers several advantages: it enables the adaptation of\nclosed-source models without needing access to their internal workings, it is\nversatile as it works with any VLM, it transfers to new VLMs, and it allows for\nthe adaptation of an ensemble of VLMs. We evaluate LLM-wrapper on multiple\ndatasets using different VLMs and LLMs, demonstrating significant performance\nimprovements and highlighting the versatility of our method. While LLM-wrapper\nis not meant to directly compete with standard white-box fine-tuning, it offers\na practical and effective alternative for black-box VLM adaptation. The code\nwill be open-sourced."
                },
                "authors": [
                    {
                        "name": "Amaia Cardiel"
                    },
                    {
                        "name": "Eloi Zablocki"
                    },
                    {
                        "name": "Elias Ramzi"
                    },
                    {
                        "name": "Oriane Siméoni"
                    },
                    {
                        "name": "Matthieu Cord"
                    }
                ],
                "author_detail": {
                    "name": "Matthieu Cord"
                },
                "author": "Matthieu Cord",
                "arxiv_comment": "v1 at EVAL-FoMo workshop, ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11655v1",
                "updated": "2024-10-15T14:42:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    42,
                    18,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T14:42:18Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    42,
                    18,
                    1,
                    289,
                    0
                ],
                "title": "Retrieval Augmented Spelling Correction for E-Commerce Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Spelling Correction for E-Commerce Applications"
                },
                "summary": "The rapid introduction of new brand names into everyday language poses a\nunique challenge for e-commerce spelling correction services, which must\ndistinguish genuine misspellings from novel brand names that use unconventional\nspelling. We seek to address this challenge via Retrieval Augmented Generation\n(RAG). On this approach, product names are retrieved from a catalog and\nincorporated into the context used by a large language model (LLM) that has\nbeen fine-tuned to do contextual spelling correction. Through quantitative\nevaluation and qualitative error analyses, we find improvements in spelling\ncorrection utilizing the RAG framework beyond a stand-alone LLM. We also\ndemonstrate the value of additional finetuning of the LLM to incorporate\nretrieved context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid introduction of new brand names into everyday language poses a\nunique challenge for e-commerce spelling correction services, which must\ndistinguish genuine misspellings from novel brand names that use unconventional\nspelling. We seek to address this challenge via Retrieval Augmented Generation\n(RAG). On this approach, product names are retrieved from a catalog and\nincorporated into the context used by a large language model (LLM) that has\nbeen fine-tuned to do contextual spelling correction. Through quantitative\nevaluation and qualitative error analyses, we find improvements in spelling\ncorrection utilizing the RAG framework beyond a stand-alone LLM. We also\ndemonstrate the value of additional finetuning of the LLM to incorporate\nretrieved context."
                },
                "authors": [
                    {
                        "name": "Xuan Guo"
                    },
                    {
                        "name": "Rohit Patki"
                    },
                    {
                        "name": "Dante Everaert"
                    },
                    {
                        "name": "Christopher Potts"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Potts"
                },
                "author": "Christopher Potts",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11654v1",
                "updated": "2024-10-15T14:41:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    41,
                    44,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T14:41:44Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    41,
                    44,
                    1,
                    289,
                    0
                ],
                "title": "Transformer Layer Injection: A Novel Approach for Efficient Upscaling of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer Layer Injection: A Novel Approach for Efficient Upscaling of\n  Large Language Models"
                },
                "summary": "In this paper, we propose Transformer Layer Injection (TLI), a novel method\nfor efficiently upscaling large language models (LLMs) while minimizing\ncomputational costs and maintaining model performance. Model scale is a key\nfactor in enhancing the quality of machine learning models, and TLI addresses\nthe challenge of scaling by reducing initial loss, minimizing fine-tuning\nrequirements, and preserving model complexity. Our approach improves upon the\nconventional Depth Up-Scaling (DUS) technique by injecting new layers into\nevery set of K layers, enabling hidden representations to pass through\ntransformer blocks with minimal disruption. We compare TLI with existing\napproaches, including Mixture of Experts (MoE) and DUS, and validate its\nefficiency through experiments on small LLMs (LLama3 1B, 3B, and 8B). Results\nshow that TLI achieves better initialization, requires fewer training steps,\nand delivers superior accuracy on tasks such as KoBEST and KMCQA, with models\nperforming effectively even without additional training. TLI is demonstrated to\nbe both data-efficient and cost-effective, significantly outperforming existing\nmethods. Its scalability and simplicity make it a promising solution for\nupscaling transformer-based models, with potential applications in scaling\nmodels from 10B to 405B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Transformer Layer Injection (TLI), a novel method\nfor efficiently upscaling large language models (LLMs) while minimizing\ncomputational costs and maintaining model performance. Model scale is a key\nfactor in enhancing the quality of machine learning models, and TLI addresses\nthe challenge of scaling by reducing initial loss, minimizing fine-tuning\nrequirements, and preserving model complexity. Our approach improves upon the\nconventional Depth Up-Scaling (DUS) technique by injecting new layers into\nevery set of K layers, enabling hidden representations to pass through\ntransformer blocks with minimal disruption. We compare TLI with existing\napproaches, including Mixture of Experts (MoE) and DUS, and validate its\nefficiency through experiments on small LLMs (LLama3 1B, 3B, and 8B). Results\nshow that TLI achieves better initialization, requires fewer training steps,\nand delivers superior accuracy on tasks such as KoBEST and KMCQA, with models\nperforming effectively even without additional training. TLI is demonstrated to\nbe both data-efficient and cost-effective, significantly outperforming existing\nmethods. Its scalability and simplicity make it a promising solution for\nupscaling transformer-based models, with potential applications in scaling\nmodels from 10B to 405B parameters."
                },
                "authors": [
                    {
                        "name": "James Vo"
                    }
                ],
                "author_detail": {
                    "name": "James Vo"
                },
                "author": "James Vo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11650v1",
                "updated": "2024-10-15T14:38:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    38,
                    14,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T14:38:14Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    38,
                    14,
                    1,
                    289,
                    0
                ],
                "title": "ED-ViT: Splitting Vision Transformer for Distributed Inference on Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ED-ViT: Splitting Vision Transformer for Distributed Inference on Edge\n  Devices"
                },
                "summary": "Deep learning models are increasingly deployed on resource-constrained edge\ndevices for real-time data analytics. In recent years, Vision Transformer\nmodels and their variants have demonstrated outstanding performance across\nvarious computer vision tasks. However, their high computational demands and\ninference latency pose significant challenges for model deployment on\nresource-constraint edge devices. To address this issue, we propose a novel\nVision Transformer splitting framework, ED-ViT, designed to execute complex\nmodels across multiple edge devices efficiently. Specifically, we partition\nVision Transformer models into several sub-models, where each sub-model is\ntailored to handle a specific subset of data classes. To further minimize\ncomputation overhead and inference latency, we introduce a class-wise pruning\ntechnique that reduces the size of each sub-model. We conduct extensive\nexperiments on five datasets with three model structures, demonstrating that\nour approach significantly reduces inference latency on edge devices and\nachieves a model size reduction of up to 28.9 times and 34.1 times,\nrespectively, while maintaining test accuracy comparable to the original Vision\nTransformer. Additionally, we compare ED-ViT with two state-of-the-art methods\nthat deploy CNN and SNN models on edge devices, evaluating accuracy, inference\ntime, and overall model size. Our comprehensive evaluation underscores the\neffectiveness of the proposed ED-ViT framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models are increasingly deployed on resource-constrained edge\ndevices for real-time data analytics. In recent years, Vision Transformer\nmodels and their variants have demonstrated outstanding performance across\nvarious computer vision tasks. However, their high computational demands and\ninference latency pose significant challenges for model deployment on\nresource-constraint edge devices. To address this issue, we propose a novel\nVision Transformer splitting framework, ED-ViT, designed to execute complex\nmodels across multiple edge devices efficiently. Specifically, we partition\nVision Transformer models into several sub-models, where each sub-model is\ntailored to handle a specific subset of data classes. To further minimize\ncomputation overhead and inference latency, we introduce a class-wise pruning\ntechnique that reduces the size of each sub-model. We conduct extensive\nexperiments on five datasets with three model structures, demonstrating that\nour approach significantly reduces inference latency on edge devices and\nachieves a model size reduction of up to 28.9 times and 34.1 times,\nrespectively, while maintaining test accuracy comparable to the original Vision\nTransformer. Additionally, we compare ED-ViT with two state-of-the-art methods\nthat deploy CNN and SNN models on edge devices, evaluating accuracy, inference\ntime, and overall model size. Our comprehensive evaluation underscores the\neffectiveness of the proposed ED-ViT framework."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Yijun Song"
                    },
                    {
                        "name": "Xia Li"
                    },
                    {
                        "name": "Yifei Sun"
                    },
                    {
                        "name": "Huiying Lan"
                    },
                    {
                        "name": "Zemin Liu"
                    },
                    {
                        "name": "Linshan Jiang"
                    },
                    {
                        "name": "Jialin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Li"
                },
                "author": "Jialin Li",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11647v1",
                "updated": "2024-10-15T14:33:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    33,
                    23,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T14:33:23Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    33,
                    23,
                    1,
                    289,
                    0
                ],
                "title": "Measuring Spiritual Values and Bias of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Spiritual Values and Bias of Large Language Models"
                },
                "summary": "Large language models (LLMs) have become integral tool for users from various\nbackgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural\nnuances embedded in their pre-training data. However, the values and\nperspectives inherent in this data can influence the behavior of LLMs, leading\nto potential biases. As a result, the use of LLMs in contexts involving\nspiritual or moral values necessitates careful consideration of these\nunderlying biases. Our work starts with verification of our hypothesis by\ntesting the spiritual values of popular LLMs. Experimental results show that\nLLMs' spiritual values are quite diverse, as opposed to the stereotype of\natheists or secularists. We then investigate how different spiritual values\naffect LLMs in social-fairness scenarios e.g., hate speech identification). Our\nfindings reveal that different spiritual values indeed lead to different\nsensitivity to different hate target groups. Furthermore, we propose to\ncontinue pre-training LLMs on spiritual texts, and empirical results\ndemonstrate the effectiveness of this approach in mitigating spiritual bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become integral tool for users from various\nbackgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural\nnuances embedded in their pre-training data. However, the values and\nperspectives inherent in this data can influence the behavior of LLMs, leading\nto potential biases. As a result, the use of LLMs in contexts involving\nspiritual or moral values necessitates careful consideration of these\nunderlying biases. Our work starts with verification of our hypothesis by\ntesting the spiritual values of popular LLMs. Experimental results show that\nLLMs' spiritual values are quite diverse, as opposed to the stereotype of\natheists or secularists. We then investigate how different spiritual values\naffect LLMs in social-fairness scenarios e.g., hate speech identification). Our\nfindings reveal that different spiritual values indeed lead to different\nsensitivity to different hate target groups. Furthermore, we propose to\ncontinue pre-training LLMs on spiritual texts, and empirical results\ndemonstrate the effectiveness of this approach in mitigating spiritual bias."
                },
                "authors": [
                    {
                        "name": "Songyuan Liu"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Runze Yan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Carl Yang"
                    },
                    {
                        "name": "Jiaying Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaying Lu"
                },
                "author": "Jiaying Lu",
                "arxiv_comment": "9 pages including appendix; 5 figures; 5 tables; submitted to ARR -\n  Octobor 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18921v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18921v3",
                "updated": "2024-10-15T14:24:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    24,
                    56,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-27T06:24:00Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    6,
                    24,
                    0,
                    3,
                    179,
                    0
                ],
                "title": "Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models\n  with Personality-Indicative Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing Minds, Not Just Words: Enhancing Role-Playing Language Models\n  with Personality-Indicative Data"
                },
                "summary": "Role-playing agents (RPA) have been a popular application area for large\nlanguage models (LLMs), attracting significant interest from both industry and\nacademia.While existing RPAs well portray the characters' knowledge and tones,\nthey face challenges in capturing their minds, especially for small\nrole-playing language models (RPLMs). In this paper, we propose to enhance\nRPLMs via personality-indicative data. Specifically, we leverage questions from\npsychological scales and distill advanced RPAs to generate dialogues that grasp\nthe minds of characters. Experimental results validate that RPLMs trained with\nour dataset exhibit advanced role-playing capabilities for both general and\npersonality-related evaluations. Code and data are available at\n\\href{https://github.com/alienet1109/RolePersonality}{this URL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-playing agents (RPA) have been a popular application area for large\nlanguage models (LLMs), attracting significant interest from both industry and\nacademia.While existing RPAs well portray the characters' knowledge and tones,\nthey face challenges in capturing their minds, especially for small\nrole-playing language models (RPLMs). In this paper, we propose to enhance\nRPLMs via personality-indicative data. Specifically, we leverage questions from\npsychological scales and distill advanced RPAs to generate dialogues that grasp\nthe minds of characters. Experimental results validate that RPLMs trained with\nour dataset exhibit advanced role-playing capabilities for both general and\npersonality-related evaluations. Code and data are available at\n\\href{https://github.com/alienet1109/RolePersonality}{this URL}."
                },
                "authors": [
                    {
                        "name": "Yiting Ran"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Xinfeng Yuan"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Deqing Yang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yanghua Xiao"
                },
                "author": "Yanghua Xiao",
                "arxiv_comment": "11 pages, 1 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18921v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18921v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12701v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12701v3",
                "updated": "2024-10-15T14:21:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    21,
                    54,
                    1,
                    289,
                    0
                ],
                "published": "2024-05-21T11:50:16Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    11,
                    50,
                    16,
                    1,
                    142,
                    0
                ],
                "title": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering"
                },
                "summary": "In the medical domain, numerous scenarios necessitate the long-form\ngeneration ability of large language models (LLMs). Specifically, when\naddressing patients' questions, it is essential that the model's response\nconveys factual claims, highlighting the need for an automated method to\nevaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset\nreconstructed using long-form question-answering datasets related to the\nbiomedical domain. We use MedLFQA to facilitate a cost-effective automatic\nevaluations of factuality. We also propose OLAPH, a simple and novel framework\nthat utilizes cost-effective and multifaceted automatic evaluation to construct\na synthetic preference set and answers questions in our preferred manner. Our\nframework leads us to train LLMs step-by-step to reduce hallucinations and\ninclude crucial medical claims. We highlight that, even on evaluation metrics\nnot used during training, LLMs trained with our OLAPH framework demonstrate\nsignificant performance improvement in factuality. Our findings reveal that a\n7B LLM trained with our OLAPH framework can provide long answers comparable to\nthe medical experts' answers in terms of factuality. We believe that our work\ncould shed light on gauging the long-text generation ability of LLMs in the\nmedical domain. Our code and datasets are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the medical domain, numerous scenarios necessitate the long-form\ngeneration ability of large language models (LLMs). Specifically, when\naddressing patients' questions, it is essential that the model's response\nconveys factual claims, highlighting the need for an automated method to\nevaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset\nreconstructed using long-form question-answering datasets related to the\nbiomedical domain. We use MedLFQA to facilitate a cost-effective automatic\nevaluations of factuality. We also propose OLAPH, a simple and novel framework\nthat utilizes cost-effective and multifaceted automatic evaluation to construct\na synthetic preference set and answers questions in our preferred manner. Our\nframework leads us to train LLMs step-by-step to reduce hallucinations and\ninclude crucial medical claims. We highlight that, even on evaluation metrics\nnot used during training, LLMs trained with our OLAPH framework demonstrate\nsignificant performance improvement in factuality. Our findings reveal that a\n7B LLM trained with our OLAPH framework can provide long answers comparable to\nthe medical experts' answers in terms of factuality. We believe that our work\ncould shed light on gauging the long-text generation ability of LLMs in the\nmedical domain. Our code and datasets are available."
                },
                "authors": [
                    {
                        "name": "Minbyul Jeong"
                    },
                    {
                        "name": "Hyeon Hwang"
                    },
                    {
                        "name": "Chanwoong Yoon"
                    },
                    {
                        "name": "Taewhoo Lee"
                    },
                    {
                        "name": "Jaewoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Kang"
                },
                "author": "Jaewoo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12701v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12701v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.11111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.11111v2",
                "updated": "2024-10-15T14:20:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    20,
                    51,
                    1,
                    289,
                    0
                ],
                "published": "2023-04-21T16:29:43Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    16,
                    29,
                    43,
                    4,
                    111,
                    0
                ],
                "title": "Inducing anxiety in large language models can induce bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inducing anxiety in large language models can induce bias"
                },
                "summary": "Large language models (LLMs) are transforming research on machine learning\nwhile galvanizing public debates. Understanding not only when these models work\nwell and succeed but also why they fail and misbehave is of great societal\nrelevance. We propose to turn the lens of psychiatry, a framework used to\ndescribe and modify maladaptive behavior, to the outputs produced by these\nmodels. We focus on twelve established LLMs and subject them to a questionnaire\ncommonly used in psychiatry. Our results show that six of the latest LLMs\nrespond robustly to the anxiety questionnaire, producing comparable anxiety\nscores to humans. Moreover, the LLMs' responses can be predictably changed by\nusing anxiety-inducing prompts. Anxiety-induction not only influences LLMs'\nscores on an anxiety questionnaire but also influences their behavior in a\npreviously-established benchmark measuring biases such as racism and ageism.\nImportantly, greater anxiety-inducing text leads to stronger increases in\nbiases, suggesting that how anxiously a prompt is communicated to large\nlanguage models has a strong influence on their behavior in applied settings.\nThese results demonstrate the usefulness of methods taken from psychiatry for\nstudying the capable algorithms to which we increasingly delegate authority and\nautonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are transforming research on machine learning\nwhile galvanizing public debates. Understanding not only when these models work\nwell and succeed but also why they fail and misbehave is of great societal\nrelevance. We propose to turn the lens of psychiatry, a framework used to\ndescribe and modify maladaptive behavior, to the outputs produced by these\nmodels. We focus on twelve established LLMs and subject them to a questionnaire\ncommonly used in psychiatry. Our results show that six of the latest LLMs\nrespond robustly to the anxiety questionnaire, producing comparable anxiety\nscores to humans. Moreover, the LLMs' responses can be predictably changed by\nusing anxiety-inducing prompts. Anxiety-induction not only influences LLMs'\nscores on an anxiety questionnaire but also influences their behavior in a\npreviously-established benchmark measuring biases such as racism and ageism.\nImportantly, greater anxiety-inducing text leads to stronger increases in\nbiases, suggesting that how anxiously a prompt is communicated to large\nlanguage models has a strong influence on their behavior in applied settings.\nThese results demonstrate the usefulness of methods taken from psychiatry for\nstudying the capable algorithms to which we increasingly delegate authority and\nautonomy."
                },
                "authors": [
                    {
                        "name": "Julian Coda-Forno"
                    },
                    {
                        "name": "Kristin Witte"
                    },
                    {
                        "name": "Akshay K. Jagadish"
                    },
                    {
                        "name": "Marcel Binz"
                    },
                    {
                        "name": "Zeynep Akata"
                    },
                    {
                        "name": "Eric Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Eric Schulz"
                },
                "author": "Eric Schulz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.11111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.11111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11625v1",
                "updated": "2024-10-15T14:14:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    14,
                    6,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T14:14:06Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    14,
                    14,
                    6,
                    1,
                    289,
                    0
                ],
                "title": "Fast Local Neural Regression for Low-Cost, Path Traced Lambertian Global\n  Illumination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Local Neural Regression for Low-Cost, Path Traced Lambertian Global\n  Illumination"
                },
                "summary": "Despite recent advances in hardware acceleration of ray tracing, real-time\nray budgets remain stubbornly limited at a handful of samples per pixel (spp)\non commodity hardware, placing the onus on denoising algorithms to achieve high\nvisual quality for path traced global illumination. Neural network-based\nsolutions give excellent result quality at the cost of increased execution time\nrelative to hand-engineered methods, making them less suitable for deployment\non resource-constrained systems. We therefore propose incorporating a neural\nnetwork into a computationally-efficient local linear model-based denoiser, and\ndemonstrate faithful single-frame reconstruction of global illumination for\nLambertian scenes at very low sample counts (1spp) and for low computational\ncost. Other contributions include improving the quality and performance of\nlocal linear model-based denoising through a simplified mathematical treatment,\nand demonstration of the surprising usefulness of ambient occlusion as a guide\nchannel. We also show how our technique is straightforwardly extensible to\njoint denoising and upsampling of path traced renders with reference to\nlow-cost, rasterized guide channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in hardware acceleration of ray tracing, real-time\nray budgets remain stubbornly limited at a handful of samples per pixel (spp)\non commodity hardware, placing the onus on denoising algorithms to achieve high\nvisual quality for path traced global illumination. Neural network-based\nsolutions give excellent result quality at the cost of increased execution time\nrelative to hand-engineered methods, making them less suitable for deployment\non resource-constrained systems. We therefore propose incorporating a neural\nnetwork into a computationally-efficient local linear model-based denoiser, and\ndemonstrate faithful single-frame reconstruction of global illumination for\nLambertian scenes at very low sample counts (1spp) and for low computational\ncost. Other contributions include improving the quality and performance of\nlocal linear model-based denoising through a simplified mathematical treatment,\nand demonstration of the surprising usefulness of ambient occlusion as a guide\nchannel. We also show how our technique is straightforwardly extensible to\njoint denoising and upsampling of path traced renders with reference to\nlow-cost, rasterized guide channels."
                },
                "authors": [
                    {
                        "name": "Arturo Salmi"
                    },
                    {
                        "name": "Szabolcs Cséfalvay"
                    },
                    {
                        "name": "James Imber"
                    }
                ],
                "author_detail": {
                    "name": "James Imber"
                },
                "author": "James Imber",
                "arxiv_comment": "11 pages, 10 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.3; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14205v2",
                "updated": "2024-10-15T13:58:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    58,
                    17,
                    1,
                    289,
                    0
                ],
                "published": "2024-05-23T06:03:19Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    6,
                    3,
                    19,
                    3,
                    144,
                    0
                ],
                "title": "Agent Planning with World Knowledge Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Planning with World Knowledge Model"
                },
                "summary": "Recent endeavors towards directly using large language models (LLMs) as agent\nmodels to execute interactive planning tasks have shown commendable results.\nDespite their achievements, however, they still struggle with brainless\ntrial-and-error in global planning and generating hallucinatory actions in\nlocal planning due to their poor understanding of the ``real'' physical world.\nImitating humans' mental world knowledge model which provides global prior\nknowledge before the task and maintains local dynamic knowledge during the\ntask, in this paper, we introduce parametric World Knowledge Model (WKM) to\nfacilitate agent planning. Concretely, we steer the agent model to\nself-synthesize knowledge from both expert and sampled trajectories. Then we\ndevelop WKM, providing prior task knowledge to guide the global planning and\ndynamic state knowledge to assist the local planning. Experimental results on\nthree complex real-world simulated datasets with three state-of-the-art\nopen-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our\nmethod can achieve superior performance compared to various strong baselines.\nBesides, we analyze to illustrate that our WKM can effectively alleviate the\nblind trial-and-error and hallucinatory action issues, providing strong support\nfor the agent's understanding of the world. Other interesting findings include:\n1) our instance-level task knowledge can generalize better to unseen tasks, 2)\nweak WKM can guide strong agent model planning, and 3) unified WKM training has\npromising potential for further development. The code is available at\nhttps://github.com/zjunlp/WKM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent endeavors towards directly using large language models (LLMs) as agent\nmodels to execute interactive planning tasks have shown commendable results.\nDespite their achievements, however, they still struggle with brainless\ntrial-and-error in global planning and generating hallucinatory actions in\nlocal planning due to their poor understanding of the ``real'' physical world.\nImitating humans' mental world knowledge model which provides global prior\nknowledge before the task and maintains local dynamic knowledge during the\ntask, in this paper, we introduce parametric World Knowledge Model (WKM) to\nfacilitate agent planning. Concretely, we steer the agent model to\nself-synthesize knowledge from both expert and sampled trajectories. Then we\ndevelop WKM, providing prior task knowledge to guide the global planning and\ndynamic state knowledge to assist the local planning. Experimental results on\nthree complex real-world simulated datasets with three state-of-the-art\nopen-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our\nmethod can achieve superior performance compared to various strong baselines.\nBesides, we analyze to illustrate that our WKM can effectively alleviate the\nblind trial-and-error and hallucinatory action issues, providing strong support\nfor the agent's understanding of the world. Other interesting findings include:\n1) our instance-level task knowledge can generalize better to unseen tasks, 2)\nweak WKM can guide strong agent model planning, and 3) unified WKM training has\npromising potential for further development. The code is available at\nhttps://github.com/zjunlp/WKM."
                },
                "authors": [
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11594v1",
                "updated": "2024-10-15T13:29:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    29,
                    22,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T13:29:22Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    29,
                    22,
                    1,
                    289,
                    0
                ],
                "title": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box Uncertainty Quantification Method for LLM-as-a-Judge"
                },
                "summary": "LLM-as-a-Judge is a widely used method for evaluating the performance of\nLarge Language Models (LLMs) across various tasks. We address the challenge of\nquantifying the uncertainty of LLM-as-a-Judge evaluations. While uncertainty\nquantification has been well-studied in other domains, applying it effectively\nto LLMs poses unique challenges due to their complex decision-making\ncapabilities and computational demands. In this paper, we introduce a novel\nmethod for quantifying uncertainty designed to enhance the trustworthiness of\nLLM-as-a-Judge evaluations. The method quantifies uncertainty by analyzing the\nrelationships between generated assessments and possible ratings. By\ncross-evaluating these relationships and constructing a confusion matrix based\non token probabilities, the method derives labels of high or low uncertainty.\nWe evaluate our method across multiple benchmarks, demonstrating a strong\ncorrelation between the accuracy of LLM evaluations and the derived uncertainty\nscores. Our findings suggest that this method can significantly improve the\nreliability and consistency of LLM-as-a-Judge evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge is a widely used method for evaluating the performance of\nLarge Language Models (LLMs) across various tasks. We address the challenge of\nquantifying the uncertainty of LLM-as-a-Judge evaluations. While uncertainty\nquantification has been well-studied in other domains, applying it effectively\nto LLMs poses unique challenges due to their complex decision-making\ncapabilities and computational demands. In this paper, we introduce a novel\nmethod for quantifying uncertainty designed to enhance the trustworthiness of\nLLM-as-a-Judge evaluations. The method quantifies uncertainty by analyzing the\nrelationships between generated assessments and possible ratings. By\ncross-evaluating these relationships and constructing a confusion matrix based\non token probabilities, the method derives labels of high or low uncertainty.\nWe evaluate our method across multiple benchmarks, demonstrating a strong\ncorrelation between the accuracy of LLM evaluations and the derived uncertainty\nscores. Our findings suggest that this method can significantly improve the\nreliability and consistency of LLM-as-a-Judge evaluations."
                },
                "authors": [
                    {
                        "name": "Nico Wagner"
                    },
                    {
                        "name": "Michael Desmond"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Zahra Ashktorab"
                    },
                    {
                        "name": "Elizabeth M. Daly"
                    },
                    {
                        "name": "Qian Pan"
                    },
                    {
                        "name": "Martín Santillán Cooper"
                    },
                    {
                        "name": "James M. Johnson"
                    },
                    {
                        "name": "Werner Geyer"
                    }
                ],
                "author_detail": {
                    "name": "Werner Geyer"
                },
                "author": "Werner Geyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11591v1",
                "updated": "2024-10-15T13:25:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    25,
                    43,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T13:25:43Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    25,
                    43,
                    1,
                    289,
                    0
                ],
                "title": "PaSTe: Improving the Efficiency of Visual Anomaly Detection at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaSTe: Improving the Efficiency of Visual Anomaly Detection at the Edge"
                },
                "summary": "Visual Anomaly Detection (VAD) has gained significant research attention for\nits ability to identify anomalous images and pinpoint the specific areas\nresponsible for the anomaly. A key advantage of VAD is its unsupervised nature,\nwhich eliminates the need for costly and time-consuming labeled data\ncollection. However, despite its potential for real-world applications, the\nliterature has given limited focus to resource-efficient VAD, particularly for\ndeployment on edge devices. This work addresses this gap by leveraging\nlightweight neural networks to reduce memory and computation requirements,\nenabling VAD deployment on resource-constrained edge devices. We benchmark the\nmajor VAD algorithms within this framework and demonstrate the feasibility of\nedge-based VAD using the well-known MVTec dataset. Furthermore, we introduce a\nnovel algorithm, Partially Shared Teacher-student (PaSTe), designed to address\nthe high resource demands of the existing Student Teacher Feature Pyramid\nMatching (STFPM) approach. Our results show that PaSTe decreases the inference\ntime by 25%, while reducing the training time by 33% and peak RAM usage during\ntraining by 76%. These improvements make the VAD process significantly more\nefficient, laying a solid foundation for real-world deployment on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Anomaly Detection (VAD) has gained significant research attention for\nits ability to identify anomalous images and pinpoint the specific areas\nresponsible for the anomaly. A key advantage of VAD is its unsupervised nature,\nwhich eliminates the need for costly and time-consuming labeled data\ncollection. However, despite its potential for real-world applications, the\nliterature has given limited focus to resource-efficient VAD, particularly for\ndeployment on edge devices. This work addresses this gap by leveraging\nlightweight neural networks to reduce memory and computation requirements,\nenabling VAD deployment on resource-constrained edge devices. We benchmark the\nmajor VAD algorithms within this framework and demonstrate the feasibility of\nedge-based VAD using the well-known MVTec dataset. Furthermore, we introduce a\nnovel algorithm, Partially Shared Teacher-student (PaSTe), designed to address\nthe high resource demands of the existing Student Teacher Feature Pyramid\nMatching (STFPM) approach. Our results show that PaSTe decreases the inference\ntime by 25%, while reducing the training time by 33% and peak RAM usage during\ntraining by 76%. These improvements make the VAD process significantly more\nefficient, laying a solid foundation for real-world deployment on edge devices."
                },
                "authors": [
                    {
                        "name": "Manuel Barusco"
                    },
                    {
                        "name": "Francesco Borsatti"
                    },
                    {
                        "name": "Davide Dalle Pezze"
                    },
                    {
                        "name": "Francesco Paissan"
                    },
                    {
                        "name": "Elisabetta Farella"
                    },
                    {
                        "name": "Gian Antonio Susto"
                    }
                ],
                "author_detail": {
                    "name": "Gian Antonio Susto"
                },
                "author": "Gian Antonio Susto",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11588v1",
                "updated": "2024-10-15T13:24:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    24,
                    44,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T13:24:44Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    24,
                    44,
                    1,
                    289,
                    0
                ],
                "title": "Causal Reasoning in Large Language Models: A Knowledge Graph Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Reasoning in Large Language Models: A Knowledge Graph Approach"
                },
                "summary": "Large language models (LLMs) typically improve performance by either\nretrieving semantically similar information, or enhancing reasoning abilities\nthrough structured prompts like chain-of-thought. While both strategies are\nconsidered crucial, it remains unclear which has a greater impact on model\nperformance or whether a combination of both is necessary. This paper answers\nthis question by proposing a knowledge graph (KG)-based random-walk reasoning\napproach that leverages causal relationships. We conduct experiments on the\ncommonsense question answering task that is based on a KG. The KG inherently\nprovides both relevant information, such as related entity keywords, and a\nreasoning structure through the connections between nodes. Experimental results\nshow that the proposed KG-based random-walk reasoning method improves the\nreasoning ability and performance of LLMs. Interestingly, incorporating three\nseemingly irrelevant sentences into the query using KG-based random-walk\nreasoning enhances LLM performance, contrary to conventional wisdom. These\nfindings suggest that integrating causal structures into prompts can\nsignificantly improve reasoning capabilities, providing new insights into the\nrole of causality in optimizing LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) typically improve performance by either\nretrieving semantically similar information, or enhancing reasoning abilities\nthrough structured prompts like chain-of-thought. While both strategies are\nconsidered crucial, it remains unclear which has a greater impact on model\nperformance or whether a combination of both is necessary. This paper answers\nthis question by proposing a knowledge graph (KG)-based random-walk reasoning\napproach that leverages causal relationships. We conduct experiments on the\ncommonsense question answering task that is based on a KG. The KG inherently\nprovides both relevant information, such as related entity keywords, and a\nreasoning structure through the connections between nodes. Experimental results\nshow that the proposed KG-based random-walk reasoning method improves the\nreasoning ability and performance of LLMs. Interestingly, incorporating three\nseemingly irrelevant sentences into the query using KG-based random-walk\nreasoning enhances LLM performance, contrary to conventional wisdom. These\nfindings suggest that integrating causal structures into prompts can\nsignificantly improve reasoning capabilities, providing new insights into the\nrole of causality in optimizing LLM performance."
                },
                "authors": [
                    {
                        "name": "Yejin Kim"
                    },
                    {
                        "name": "Eojin Kang"
                    },
                    {
                        "name": "Juae Kim"
                    },
                    {
                        "name": "H. Howie Huang"
                    }
                ],
                "author_detail": {
                    "name": "H. Howie Huang"
                },
                "author": "H. Howie Huang",
                "arxiv_comment": "Accepted at NeurIPS 2024 Workshop on Causality and Large Models\n  (CaLM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10210v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10210v2",
                "updated": "2024-10-15T13:21:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    13,
                    21,
                    19,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-14T07:09:02Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    9,
                    2,
                    0,
                    288,
                    0
                ],
                "title": "Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as\n  the Key",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as\n  the Key"
                },
                "summary": "As large language models rapidly evolve to support longer context, there is a\nnotable disparity in their capability to generate output at greater lengths.\nRecent study suggests that the primary cause for this imbalance may arise from\nthe lack of data with long-output during alignment training. In light of this\nobservation, attempts are made to re-align foundation models with data that\nfills the gap, which result in models capable of generating lengthy output when\ninstructed. In this paper, we explore the impact of data-quality in tuning a\nmodel for long output, and the possibility of doing so from the starting points\nof human-aligned (instruct or chat) models. With careful data curation, we show\nthat it possible to achieve similar performance improvement in our tuned\nmodels, with only a small fraction of training data instances and compute. In\naddition, we assess the generalizability of such approaches by applying our\ntuning-recipes to several models. our findings suggest that, while capacities\nfor generating long output vary across different models out-of-the-box, our\napproach to tune them with high-quality data using lite compute, consistently\nyields notable improvement across all models we experimented on. We have made\npublic our curated dataset for tuning long-writing capability, the\nimplementations of model tuning and evaluation, as well as the fine-tuned\nmodels, all of which can be openly-accessed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models rapidly evolve to support longer context, there is a\nnotable disparity in their capability to generate output at greater lengths.\nRecent study suggests that the primary cause for this imbalance may arise from\nthe lack of data with long-output during alignment training. In light of this\nobservation, attempts are made to re-align foundation models with data that\nfills the gap, which result in models capable of generating lengthy output when\ninstructed. In this paper, we explore the impact of data-quality in tuning a\nmodel for long output, and the possibility of doing so from the starting points\nof human-aligned (instruct or chat) models. With careful data curation, we show\nthat it possible to achieve similar performance improvement in our tuned\nmodels, with only a small fraction of training data instances and compute. In\naddition, we assess the generalizability of such approaches by applying our\ntuning-recipes to several models. our findings suggest that, while capacities\nfor generating long output vary across different models out-of-the-box, our\napproach to tune them with high-quality data using lite compute, consistently\nyields notable improvement across all models we experimented on. We have made\npublic our curated dataset for tuning long-writing capability, the\nimplementations of model tuning and evaluation, as well as the fine-tuned\nmodels, all of which can be openly-accessed."
                },
                "authors": [
                    {
                        "name": "Yingda Chen"
                    },
                    {
                        "name": "Xingjun Wang"
                    },
                    {
                        "name": "Jintao Huang"
                    },
                    {
                        "name": "Yunlin Mao"
                    },
                    {
                        "name": "Daoze Zhang"
                    },
                    {
                        "name": "Yuze Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yuze Zhao"
                },
                "author": "Yuze Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10210v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10210v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19020v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19020v2",
                "updated": "2024-10-15T12:55:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    55,
                    27,
                    1,
                    289,
                    0
                ],
                "published": "2024-09-25T07:03:31Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    3,
                    31,
                    2,
                    269,
                    0
                ],
                "title": "DiaSynth: Synthetic Dialogue Generation Framework for Low Resource\n  Dialogue Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiaSynth: Synthetic Dialogue Generation Framework for Low Resource\n  Dialogue Applications"
                },
                "summary": "The scarcity of domain-specific dialogue datasets limits the development of\ndialogue systems across applications. Existing research is constrained by\ngeneral or niche datasets that lack sufficient scale for training dialogue\nsystems. To address this gap, we introduce DiaSynth - a synthetic dialogue\ngeneration framework capable of generating high-quality, contextually rich\ndialogues across a wide range of domains. Unlike existing frameworks, DiaSynth\nuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to\ngenerate dynamic, domain-specific dialogues with simulated personas and diverse\nconversational features. We perform our experiments by generating synthetic\ndata using different LLMs and few-shot examples from DialogSum and SAMSum. The\npretrained language models fine-tuned on the synthetic data outperform the base\nmodels by 16.47% on dialogue summarization, while the comparison between models\nfine-tuned on in-domain data and synthetic data shows that the synthetic data\nis able to capture 90.48% of the performance distribution of the in-domain data\non dialogue summarization. The quality of the data generated also increases as\nwe increase the size of LLM from 3B to 8B. These results validate DiaSynth's\npotential as a robust alternative to traditional data collection methods. We\nopen source the code and data generated for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of domain-specific dialogue datasets limits the development of\ndialogue systems across applications. Existing research is constrained by\ngeneral or niche datasets that lack sufficient scale for training dialogue\nsystems. To address this gap, we introduce DiaSynth - a synthetic dialogue\ngeneration framework capable of generating high-quality, contextually rich\ndialogues across a wide range of domains. Unlike existing frameworks, DiaSynth\nuses Large Language Models (LLMs) and Chain of Thought (CoT) reasoning to\ngenerate dynamic, domain-specific dialogues with simulated personas and diverse\nconversational features. We perform our experiments by generating synthetic\ndata using different LLMs and few-shot examples from DialogSum and SAMSum. The\npretrained language models fine-tuned on the synthetic data outperform the base\nmodels by 16.47% on dialogue summarization, while the comparison between models\nfine-tuned on in-domain data and synthetic data shows that the synthetic data\nis able to capture 90.48% of the performance distribution of the in-domain data\non dialogue summarization. The quality of the data generated also increases as\nwe increase the size of LLM from 3B to 8B. These results validate DiaSynth's\npotential as a robust alternative to traditional data collection methods. We\nopen source the code and data generated for future research."
                },
                "authors": [
                    {
                        "name": "Sathya Krishnan Suresh"
                    },
                    {
                        "name": "Wu Mengjun"
                    },
                    {
                        "name": "Tushar Pranav"
                    },
                    {
                        "name": "Eng Siong Chng"
                    }
                ],
                "author_detail": {
                    "name": "Eng Siong Chng"
                },
                "author": "Eng Siong Chng",
                "arxiv_comment": "13 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19020v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19020v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11565v1",
                "updated": "2024-10-15T12:54:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    54,
                    13,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T12:54:13Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    54,
                    13,
                    1,
                    289,
                    0
                ],
                "title": "Demo: Testing AI-driven MAC Learning in Autonomic Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: Testing AI-driven MAC Learning in Autonomic Networks"
                },
                "summary": "6G networks will be highly dynamic, re-configurable, and resilient. To enable\nand support such features, employing AI has been suggested. Integrating AIin\nnetworks will likely require distributed AI deployments with resilient\nconnectivity, e.g., for communication between RL agents and environment. Such\napproaches need to be validated in realistic network environments. In this\ndemo, we use ContainerNet to emulate AI-capable and autonomic networks that\nemploy the routing protocol KIRA to provide resilient connectivity and service\ndiscovery. As an example AI application, we train and infer deep RL agents\nlearning medium access control (MAC) policies for a wireless network\nenvironment in the emulated network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G networks will be highly dynamic, re-configurable, and resilient. To enable\nand support such features, employing AI has been suggested. Integrating AIin\nnetworks will likely require distributed AI deployments with resilient\nconnectivity, e.g., for communication between RL agents and environment. Such\napproaches need to be validated in realistic network environments. In this\ndemo, we use ContainerNet to emulate AI-capable and autonomic networks that\nemploy the routing protocol KIRA to provide resilient connectivity and service\ndiscovery. As an example AI application, we train and infer deep RL agents\nlearning medium access control (MAC) policies for a wireless network\nenvironment in the emulated network."
                },
                "authors": [
                    {
                        "name": "Leonard Paeleke"
                    },
                    {
                        "name": "Navid Keshtiarast"
                    },
                    {
                        "name": "Paul Seehofer"
                    },
                    {
                        "name": "Roland Bless"
                    },
                    {
                        "name": "Holger Karl"
                    },
                    {
                        "name": "Marina Petrova"
                    },
                    {
                        "name": "Martina Zitterbart"
                    }
                ],
                "author_detail": {
                    "name": "Martina Zitterbart"
                },
                "author": "Martina Zitterbart",
                "arxiv_comment": "Accepted for presentation in the Demo Session at the IEEE\n  International Conference on Network Protocols (ICNP), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11564v1",
                "updated": "2024-10-15T12:53:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    53,
                    42,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T12:53:42Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    53,
                    42,
                    1,
                    289,
                    0
                ],
                "title": "PAVLM: Advancing Point Cloud based Affordance Understanding Via\n  Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAVLM: Advancing Point Cloud based Affordance Understanding Via\n  Vision-Language Model"
                },
                "summary": "Affordance understanding, the task of identifying actionable regions on 3D\nobjects, plays a vital role in allowing robotic systems to engage with and\noperate within the physical world. Although Visual Language Models (VLMs) have\nexcelled in high-level reasoning and long-horizon planning for robotic\nmanipulation, they still fall short in grasping the nuanced physical properties\nrequired for effective human-robot interaction. In this paper, we introduce\nPAVLM (Point cloud Affordance Vision-Language Model), an innovative framework\nthat utilizes the extensive multimodal knowledge embedded in pre-trained\nlanguage models to enhance 3D affordance understanding of point cloud. PAVLM\nintegrates a geometric-guided propagation module with hidden embeddings from\nlarge language models (LLMs) to enrich visual semantics. On the language side,\nwe prompt Llama-3.1 models to generate refined context-aware text, augmenting\nthe instructional input with deeper semantic cues. Experimental results on the\n3D-AffordanceNet benchmark demonstrate that PAVLM outperforms baseline methods\nfor both full and partial point clouds, particularly excelling in its\ngeneralization to novel open-world affordance tasks of 3D objects. For more\ninformation, visit our project site: pavlm-source.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Affordance understanding, the task of identifying actionable regions on 3D\nobjects, plays a vital role in allowing robotic systems to engage with and\noperate within the physical world. Although Visual Language Models (VLMs) have\nexcelled in high-level reasoning and long-horizon planning for robotic\nmanipulation, they still fall short in grasping the nuanced physical properties\nrequired for effective human-robot interaction. In this paper, we introduce\nPAVLM (Point cloud Affordance Vision-Language Model), an innovative framework\nthat utilizes the extensive multimodal knowledge embedded in pre-trained\nlanguage models to enhance 3D affordance understanding of point cloud. PAVLM\nintegrates a geometric-guided propagation module with hidden embeddings from\nlarge language models (LLMs) to enrich visual semantics. On the language side,\nwe prompt Llama-3.1 models to generate refined context-aware text, augmenting\nthe instructional input with deeper semantic cues. Experimental results on the\n3D-AffordanceNet benchmark demonstrate that PAVLM outperforms baseline methods\nfor both full and partial point clouds, particularly excelling in its\ngeneralization to novel open-world affordance tasks of 3D objects. For more\ninformation, visit our project site: pavlm-source.github.io."
                },
                "authors": [
                    {
                        "name": "Shang-Ching Liu"
                    },
                    {
                        "name": "Van Nhiem Tran"
                    },
                    {
                        "name": "Wenkai Chen"
                    },
                    {
                        "name": "Wei-Lun Cheng"
                    },
                    {
                        "name": "Yen-Lin Huang"
                    },
                    {
                        "name": "I-Bin Liao"
                    },
                    {
                        "name": "Yung-Hui Li"
                    },
                    {
                        "name": "Jianwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Zhang"
                },
                "author": "Jianwei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11550v1",
                "updated": "2024-10-15T12:39:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    39,
                    20,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T12:39:20Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    39,
                    20,
                    1,
                    289,
                    0
                ],
                "title": "Y-Mol: A Multiscale Biomedical Knowledge-Guided Large Language Model for\n  Drug Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Y-Mol: A Multiscale Biomedical Knowledge-Guided Large Language Model for\n  Drug Development"
                },
                "summary": "Large Language Models (LLMs) have recently demonstrated remarkable\nperformance in general tasks across various fields. However, their\neffectiveness within specific domains such as drug development remains\nchallenges. To solve these challenges, we introduce \\textbf{Y-Mol}, forming a\nwell-established LLM paradigm for the flow of drug development. Y-Mol is a\nmultiscale biomedical knowledge-guided LLM designed to accomplish tasks across\nlead compound discovery, pre-clinic, and clinic prediction. By integrating\nmillions of multiscale biomedical knowledge and using LLaMA2 as the base LLM,\nY-Mol augments the reasoning capability in the biomedical domain by learning\nfrom a corpus of publications, knowledge graphs, and expert-designed synthetic\ndata. The capability is further enriched with three types of drug-oriented\ninstructions: description-based prompts from processed publications,\nsemantic-based prompts for extracting associations from knowledge graphs, and\ntemplate-based prompts for understanding expert knowledge from biomedical\ntools. Besides, Y-Mol offers a set of LLM paradigms that can autonomously\nexecute the downstream tasks across the entire process of drug development,\nincluding virtual screening, drug design, pharmacological properties\nprediction, and drug-related interaction prediction. Our extensive evaluations\nof various biomedical sources demonstrate that Y-Mol significantly outperforms\ngeneral-purpose LLMs in discovering lead compounds, predicting molecular\nproperties, and identifying drug interaction events.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently demonstrated remarkable\nperformance in general tasks across various fields. However, their\neffectiveness within specific domains such as drug development remains\nchallenges. To solve these challenges, we introduce \\textbf{Y-Mol}, forming a\nwell-established LLM paradigm for the flow of drug development. Y-Mol is a\nmultiscale biomedical knowledge-guided LLM designed to accomplish tasks across\nlead compound discovery, pre-clinic, and clinic prediction. By integrating\nmillions of multiscale biomedical knowledge and using LLaMA2 as the base LLM,\nY-Mol augments the reasoning capability in the biomedical domain by learning\nfrom a corpus of publications, knowledge graphs, and expert-designed synthetic\ndata. The capability is further enriched with three types of drug-oriented\ninstructions: description-based prompts from processed publications,\nsemantic-based prompts for extracting associations from knowledge graphs, and\ntemplate-based prompts for understanding expert knowledge from biomedical\ntools. Besides, Y-Mol offers a set of LLM paradigms that can autonomously\nexecute the downstream tasks across the entire process of drug development,\nincluding virtual screening, drug design, pharmacological properties\nprediction, and drug-related interaction prediction. Our extensive evaluations\nof various biomedical sources demonstrate that Y-Mol significantly outperforms\ngeneral-purpose LLMs in discovering lead compounds, predicting molecular\nproperties, and identifying drug interaction events."
                },
                "authors": [
                    {
                        "name": "Tengfei Ma"
                    },
                    {
                        "name": "Xuan Lin"
                    },
                    {
                        "name": "Tianle Li"
                    },
                    {
                        "name": "Chaoyi Li"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Xibao Cai"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Daojian Zeng"
                    },
                    {
                        "name": "Dongsheng Cao"
                    },
                    {
                        "name": "Xiangxiang Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Xiangxiang Zeng"
                },
                "author": "Xiangxiang Zeng",
                "arxiv_comment": "12 pages, Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11540v1",
                "updated": "2024-10-15T12:14:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    14,
                    57,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T12:14:57Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    14,
                    57,
                    1,
                    289,
                    0
                ],
                "title": "Data Quality Control in Federated Instruction-tuning of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Quality Control in Federated Instruction-tuning of Large Language\n  Models"
                },
                "summary": "By leveraging massively distributed data, federated learning (FL) enables\ncollaborative instruction tuning of large language models (LLMs) in a\nprivacy-preserving way. While FL effectively expands the data quantity, the\nissue of data quality remains under-explored in the current literature on FL\nfor LLMs. To address this gap, we propose a new framework of federated\ninstruction tuning of LLMs with data quality control (FedDQC), which measures\ndata quality to facilitate the subsequent filtering and hierarchical training\nprocesses. Our approach introduces an efficient metric to assess each client's\ninstruction-response alignment (IRA), identifying potentially noisy data\nthrough single-shot inference. Low-IRA samples are potentially noisy and\nfiltered to mitigate their negative impacts. To further utilize this IRA value,\nwe propose a quality-aware hierarchical training paradigm, where LLM is\nprogressively fine-tuned from high-IRA to low-IRA data, mirroring the\neasy-to-hard learning process. We conduct extensive experiments on 4 synthetic\nand a real-world dataset, and compare our method with baselines adapted from\ncentralized setting. Results show that our method consistently and\nsignificantly improves the performance of LLMs trained on mix-quality data in\nFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By leveraging massively distributed data, federated learning (FL) enables\ncollaborative instruction tuning of large language models (LLMs) in a\nprivacy-preserving way. While FL effectively expands the data quantity, the\nissue of data quality remains under-explored in the current literature on FL\nfor LLMs. To address this gap, we propose a new framework of federated\ninstruction tuning of LLMs with data quality control (FedDQC), which measures\ndata quality to facilitate the subsequent filtering and hierarchical training\nprocesses. Our approach introduces an efficient metric to assess each client's\ninstruction-response alignment (IRA), identifying potentially noisy data\nthrough single-shot inference. Low-IRA samples are potentially noisy and\nfiltered to mitigate their negative impacts. To further utilize this IRA value,\nwe propose a quality-aware hierarchical training paradigm, where LLM is\nprogressively fine-tuned from high-IRA to low-IRA data, mirroring the\neasy-to-hard learning process. We conduct extensive experiments on 4 synthetic\nand a real-world dataset, and compare our method with baselines adapted from\ncentralized setting. Results show that our method consistently and\nsignificantly improves the performance of LLMs trained on mix-quality data in\nFL."
                },
                "authors": [
                    {
                        "name": "Yaxin Du"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Fengting Yuchi"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "Jingjing Qu"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11533v1",
                "updated": "2024-10-15T12:08:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    8,
                    14,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T12:08:14Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    8,
                    14,
                    1,
                    289,
                    0
                ],
                "title": "Multi-round jailbreak attack on large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-round jailbreak attack on large language models"
                },
                "summary": "Ensuring the safety and alignment of large language models (LLMs) with human\nvalues is crucial for generating responses that are beneficial to humanity.\nWhile LLMs have the capability to identify and avoid harmful queries, they\nremain vulnerable to \"jailbreak\" attacks, where carefully crafted prompts can\ninduce the generation of toxic content. Traditional single-round jailbreak\nattacks, such as GCG and AutoDAN, do not alter the sensitive words in the\ndangerous prompts. Although they can temporarily bypass the model's safeguards\nthrough prompt engineering, their success rate drops significantly as the LLM\nis further fine-tuned, and they cannot effectively circumvent static rule-based\nfilters that remove the hazardous vocabulary.\n  In this study, to better understand jailbreak attacks, we introduce a\nmulti-round jailbreak approach. This method can rewrite the dangerous prompts,\ndecomposing them into a series of less harmful sub-questions to bypass the\nLLM's safety checks. We first use the LLM to perform a decomposition task,\nbreaking down a set of natural language questions into a sequence of\nprogressive sub-questions, which are then used to fine-tune the Llama3-8B\nmodel, enabling it to decompose hazardous prompts. The fine-tuned model is then\nused to break down the problematic prompt, and the resulting sub-questions are\nsequentially asked to the victim model. If the victim model rejects a\nsub-question, a new decomposition is generated, and the process is repeated\nuntil the final objective is achieved. Our experimental results show a 94\\%\nsuccess rate on the llama2-7B and demonstrate the effectiveness of this\napproach in circumventing static rule-based filters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safety and alignment of large language models (LLMs) with human\nvalues is crucial for generating responses that are beneficial to humanity.\nWhile LLMs have the capability to identify and avoid harmful queries, they\nremain vulnerable to \"jailbreak\" attacks, where carefully crafted prompts can\ninduce the generation of toxic content. Traditional single-round jailbreak\nattacks, such as GCG and AutoDAN, do not alter the sensitive words in the\ndangerous prompts. Although they can temporarily bypass the model's safeguards\nthrough prompt engineering, their success rate drops significantly as the LLM\nis further fine-tuned, and they cannot effectively circumvent static rule-based\nfilters that remove the hazardous vocabulary.\n  In this study, to better understand jailbreak attacks, we introduce a\nmulti-round jailbreak approach. This method can rewrite the dangerous prompts,\ndecomposing them into a series of less harmful sub-questions to bypass the\nLLM's safety checks. We first use the LLM to perform a decomposition task,\nbreaking down a set of natural language questions into a sequence of\nprogressive sub-questions, which are then used to fine-tune the Llama3-8B\nmodel, enabling it to decompose hazardous prompts. The fine-tuned model is then\nused to break down the problematic prompt, and the resulting sub-questions are\nsequentially asked to the victim model. If the victim model rejects a\nsub-question, a new decomposition is generated, and the process is repeated\nuntil the final objective is achieved. Our experimental results show a 94\\%\nsuccess rate on the llama2-7B and demonstrate the effectiveness of this\napproach in circumventing static rule-based filters."
                },
                "authors": [
                    {
                        "name": "Yihua Zhou"
                    },
                    {
                        "name": "Xiaochuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Xiaochuan Shi"
                },
                "author": "Xiaochuan Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10271v2",
                "updated": "2024-10-15T12:06:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    6,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-05-16T17:27:41Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    17,
                    27,
                    41,
                    3,
                    137,
                    0
                ],
                "title": "Adaptive Hybrid Model Pruning in Federated Learning through Loss\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Hybrid Model Pruning in Federated Learning through Loss\n  Exploration"
                },
                "summary": "The rapid proliferation of smart devices coupled with the advent of 6G\nnetworks has profoundly reshaped the domain of collaborative machine learning.\nAlongside growing privacy-security concerns in sensitive fields, these\ndevelopments have positioned federated learning (FL) as a pivotal technology\nfor decentralized model training. Despite its vast potential, specially in the\nage of complex foundation models, FL encounters challenges such as elevated\ncommunication costs, computational constraints, and the complexities of non-IID\ndata distributions. We introduce AutoFLIP, an innovative approach that utilizes\na federated loss exploration phase to drive adaptive hybrid pruning, operating\nin a structured and unstructured way. This innovative mechanism automatically\nidentifies and prunes model substructure by distilling knowledge on model\ngradients behavior across different non-IID client losses topology, thereby\noptimizing computational efficiency and enhancing model performance on resource\nconstrained scenarios. Extensive experiments on various datasets and FL tasks\nreveal that AutoFLIP not only efficiently accelerates global convergence, but\nalso achieves superior accuracy and robustness compared to traditional methods.\nOn average, AutoFLIP reduces computational overhead by 48.8% and communication\ncosts by 35.5%, while improving global accuracy. By significantly reducing\nthese overheads, AutoFLIP offer the way for efficient FL deployment in\nreal-world applications for a scalable and broad applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of smart devices coupled with the advent of 6G\nnetworks has profoundly reshaped the domain of collaborative machine learning.\nAlongside growing privacy-security concerns in sensitive fields, these\ndevelopments have positioned federated learning (FL) as a pivotal technology\nfor decentralized model training. Despite its vast potential, specially in the\nage of complex foundation models, FL encounters challenges such as elevated\ncommunication costs, computational constraints, and the complexities of non-IID\ndata distributions. We introduce AutoFLIP, an innovative approach that utilizes\na federated loss exploration phase to drive adaptive hybrid pruning, operating\nin a structured and unstructured way. This innovative mechanism automatically\nidentifies and prunes model substructure by distilling knowledge on model\ngradients behavior across different non-IID client losses topology, thereby\noptimizing computational efficiency and enhancing model performance on resource\nconstrained scenarios. Extensive experiments on various datasets and FL tasks\nreveal that AutoFLIP not only efficiently accelerates global convergence, but\nalso achieves superior accuracy and robustness compared to traditional methods.\nOn average, AutoFLIP reduces computational overhead by 48.8% and communication\ncosts by 35.5%, while improving global accuracy. By significantly reducing\nthese overheads, AutoFLIP offer the way for efficient FL deployment in\nreal-world applications for a scalable and broad applicability."
                },
                "authors": [
                    {
                        "name": "Christian Internò"
                    },
                    {
                        "name": "Elena Raponi"
                    },
                    {
                        "name": "Niki van Stein"
                    },
                    {
                        "name": "Thomas Bäck"
                    },
                    {
                        "name": "Markus Olhofer"
                    },
                    {
                        "name": "Yaochu Jin"
                    },
                    {
                        "name": "Barbara Hammer"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Hammer"
                },
                "author": "Barbara Hammer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11531v1",
                "updated": "2024-10-15T12:05:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    5,
                    58,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T12:05:58Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    12,
                    5,
                    58,
                    1,
                    289,
                    0
                ],
                "title": "AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based\n  Chatbots Utilizing Private Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGENTiGraph: An Interactive Knowledge Graph Platform for LLM-based\n  Chatbots Utilizing Private Data"
                },
                "summary": "Large Language Models~(LLMs) have demonstrated capabilities across various\napplications but face challenges such as hallucination, limited reasoning\nabilities, and factual inconsistencies, especially when tackling complex,\ndomain-specific tasks like question answering~(QA). While Knowledge\nGraphs~(KGs) have been shown to help mitigate these issues, research on the\nintegration of LLMs with background KGs remains limited. In particular, user\naccessibility and the flexibility of the underlying KG have not been thoroughly\nexplored. We introduce AGENTiGraph (Adaptive Generative ENgine for Task-based\nInteraction and Graphical Representation), a platform for knowledge management\nthrough natural language interaction. It integrates knowledge extraction,\nintegration, and real-time visualization. AGENTiGraph employs a multi-agent\narchitecture to dynamically interpret user intents, manage tasks, and integrate\nnew knowledge, ensuring adaptability to evolving user requirements and data\ncontexts. Our approach demonstrates superior performance in knowledge graph\ninteractions, particularly for complex domain-specific tasks. Experimental\nresults on a dataset of 3,500 test cases show AGENTiGraph significantly\noutperforms state-of-the-art zero-shot baselines, achieving 95.12\\% accuracy in\ntask classification and 90.45\\% success rate in task execution. User studies\ncorroborate its effectiveness in real-world scenarios. To showcase versatility,\nwe extended AGENTiGraph to legislation and healthcare domains, constructing\nspecialized KGs capable of answering complex queries in legal and medical\ncontexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models~(LLMs) have demonstrated capabilities across various\napplications but face challenges such as hallucination, limited reasoning\nabilities, and factual inconsistencies, especially when tackling complex,\ndomain-specific tasks like question answering~(QA). While Knowledge\nGraphs~(KGs) have been shown to help mitigate these issues, research on the\nintegration of LLMs with background KGs remains limited. In particular, user\naccessibility and the flexibility of the underlying KG have not been thoroughly\nexplored. We introduce AGENTiGraph (Adaptive Generative ENgine for Task-based\nInteraction and Graphical Representation), a platform for knowledge management\nthrough natural language interaction. It integrates knowledge extraction,\nintegration, and real-time visualization. AGENTiGraph employs a multi-agent\narchitecture to dynamically interpret user intents, manage tasks, and integrate\nnew knowledge, ensuring adaptability to evolving user requirements and data\ncontexts. Our approach demonstrates superior performance in knowledge graph\ninteractions, particularly for complex domain-specific tasks. Experimental\nresults on a dataset of 3,500 test cases show AGENTiGraph significantly\noutperforms state-of-the-art zero-shot baselines, achieving 95.12\\% accuracy in\ntask classification and 90.45\\% success rate in task execution. User studies\ncorroborate its effectiveness in real-world scenarios. To showcase versatility,\nwe extended AGENTiGraph to legislation and healthcare domains, constructing\nspecialized KGs capable of answering complex queries in legal and medical\ncontexts."
                },
                "authors": [
                    {
                        "name": "Xinjie Zhao"
                    },
                    {
                        "name": "Moritz Blum"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Boming Yang"
                    },
                    {
                        "name": "Luis Márquez Carpintero"
                    },
                    {
                        "name": "Mónica Pina-Navarro"
                    },
                    {
                        "name": "Tony Wang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Huitao Li"
                    },
                    {
                        "name": "Yanran Fu"
                    },
                    {
                        "name": "Rongrong Wang"
                    },
                    {
                        "name": "Juntao Zhang"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "arxiv_comment": "30 pages, 7 figures; Submitted to COLING 2025 System Demonstrations\n  Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11526v1",
                "updated": "2024-10-15T11:57:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    57,
                    34,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T11:57:34Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    57,
                    34,
                    1,
                    289,
                    0
                ],
                "title": "Human-LLM Collaborative Construction of a Cantonese Emotion Lexicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-LLM Collaborative Construction of a Cantonese Emotion Lexicon"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nlanguage understanding and generation. Advanced utilization of the knowledge\nembedded in LLMs for automated annotation has consistently been explored. This\nstudy proposed to develop an emotion lexicon for Cantonese, a low-resource\nlanguage, through collaborative efforts between LLM and human annotators. By\nintegrating emotion labels provided by LLM and human annotators, the study\nleveraged existing linguistic resources including lexicons in other languages\nand local forums to construct a Cantonese emotion lexicon enriched with\ncolloquial expressions. The consistency of the proposed emotion lexicon in\nemotion extraction was assessed through modification and utilization of three\ndistinct emotion text datasets. This study not only validates the efficacy of\nthe constructed lexicon but also emphasizes that collaborative annotation\nbetween human and artificial intelligence can significantly enhance the quality\nof emotion labels, highlighting the potential of such partnerships in\nfacilitating natural language processing tasks for low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nlanguage understanding and generation. Advanced utilization of the knowledge\nembedded in LLMs for automated annotation has consistently been explored. This\nstudy proposed to develop an emotion lexicon for Cantonese, a low-resource\nlanguage, through collaborative efforts between LLM and human annotators. By\nintegrating emotion labels provided by LLM and human annotators, the study\nleveraged existing linguistic resources including lexicons in other languages\nand local forums to construct a Cantonese emotion lexicon enriched with\ncolloquial expressions. The consistency of the proposed emotion lexicon in\nemotion extraction was assessed through modification and utilization of three\ndistinct emotion text datasets. This study not only validates the efficacy of\nthe constructed lexicon but also emphasizes that collaborative annotation\nbetween human and artificial intelligence can significantly enhance the quality\nof emotion labels, highlighting the potential of such partnerships in\nfacilitating natural language processing tasks for low-resource languages."
                },
                "authors": [
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Dong Dong"
                    },
                    {
                        "name": "Chi-tim Hung"
                    },
                    {
                        "name": "Leonard Heyerdahl"
                    },
                    {
                        "name": "Tamara Giles-Vernick"
                    },
                    {
                        "name": "Eng-kiong Yeoh"
                    }
                ],
                "author_detail": {
                    "name": "Eng-kiong Yeoh"
                },
                "author": "Eng-kiong Yeoh",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11522v1",
                "updated": "2024-10-15T11:48:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    48,
                    31,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T11:48:31Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    48,
                    31,
                    1,
                    289,
                    0
                ],
                "title": "Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero\n  Shot Music Emotion Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLM Embeddings for Cross Dataset Label Alignment and Zero\n  Shot Music Emotion Prediction"
                },
                "summary": "In this work, we present a novel method for music emotion recognition that\nleverages Large Language Model (LLM) embeddings for label alignment across\nmultiple datasets and zero-shot prediction on novel categories. First, we\ncompute LLM embeddings for emotion labels and apply non-parametric clustering\nto group similar labels, across multiple datasets containing disjoint labels.\nWe use these cluster centers to map music features (MERT) to the LLM embedding\nspace. To further enhance the model, we introduce an alignment regularization\nthat enables dissociation of MERT embeddings from different clusters. This\nfurther enhances the model's ability to better adaptation to unseen datasets.\nWe demonstrate the effectiveness of our approach by performing zero-shot\ninference on a new dataset, showcasing its ability to generalize to unseen\nlabels without additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present a novel method for music emotion recognition that\nleverages Large Language Model (LLM) embeddings for label alignment across\nmultiple datasets and zero-shot prediction on novel categories. First, we\ncompute LLM embeddings for emotion labels and apply non-parametric clustering\nto group similar labels, across multiple datasets containing disjoint labels.\nWe use these cluster centers to map music features (MERT) to the LLM embedding\nspace. To further enhance the model, we introduce an alignment regularization\nthat enables dissociation of MERT embeddings from different clusters. This\nfurther enhances the model's ability to better adaptation to unseen datasets.\nWe demonstrate the effectiveness of our approach by performing zero-shot\ninference on a new dataset, showcasing its ability to generalize to unseen\nlabels without additional training."
                },
                "authors": [
                    {
                        "name": "Renhang Liu"
                    },
                    {
                        "name": "Abhinaba Roy"
                    },
                    {
                        "name": "Dorien Herremans"
                    }
                ],
                "author_detail": {
                    "name": "Dorien Herremans"
                },
                "author": "Dorien Herremans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14162v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14162v2",
                "updated": "2024-10-15T11:37:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    37,
                    4,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-20T10:04:09Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    10,
                    4,
                    9,
                    3,
                    172,
                    0
                ],
                "title": "DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIRAS: Efficient LLM Annotation of Document Relevance in Retrieval\n  Augmented Generation"
                },
                "summary": "Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) is widely employed to ground responses\nto queries on domain-specific documents. But do RAG implementations leave out\nimportant information when answering queries that need an integrated analysis\nof information (e.g., Tell me good news in the stock market today.)? To address\nthese concerns, RAG developers need to annotate information retrieval (IR) data\nfor their domain of interest, which is challenging because (1) domain-specific\nqueries usually need nuanced definitions of relevance beyond shallow semantic\nrelevance; and (2) human or GPT-4 annotation is costly and cannot cover all\n(query, document) pairs (i.e., annotation selection bias), thus harming the\neffectiveness in evaluating IR recall. To address these challenges, we propose\nDIRAS (Domain-specific Information Retrieval Annotation with Scalability), a\nmanual-annotation-free schema that fine-tunes open-sourced LLMs to consider\nnuanced relevance definition and annotate (partial) relevance labels with\ncalibrated relevance scores. Extensive evaluation shows that DIRAS enables\nsmaller (8B) LLMs to achieve GPT-4-level performance on annotating and ranking\nunseen (query, document) pairs, and is helpful for real-world RAG development.\nAll code, LLM generations, and human annotations can be found in\n\\url{https://github.com/EdisonNi-hku/DIRAS}."
                },
                "authors": [
                    {
                        "name": "Jingwei Ni"
                    },
                    {
                        "name": "Tobias Schimanski"
                    },
                    {
                        "name": "Meihong Lin"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Elliott Ash"
                    },
                    {
                        "name": "Markus Leippold"
                    }
                ],
                "author_detail": {
                    "name": "Markus Leippold"
                },
                "author": "Markus Leippold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14162v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14162v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04467v3",
                "updated": "2024-10-15T11:29:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    29,
                    10,
                    1,
                    289,
                    0
                ],
                "published": "2024-07-05T12:30:02Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    12,
                    30,
                    2,
                    4,
                    187,
                    0
                ],
                "title": "Are Large Language Models Strategic Decision Makers? A Study of\n  Performance and Bias in Two-Player Non-Zero-Sum Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Strategic Decision Makers? A Study of\n  Performance and Bias in Two-Player Non-Zero-Sum Games"
                },
                "summary": "Large Language Models (LLMs) have been increasingly used in real-world\nsettings, yet their strategic decision-making abilities remain largely\nunexplored. To fully benefit from the potential of LLMs, it's essential to\nunderstand their ability to function in complex social scenarios. Game theory,\nwhich is already used to understand real-world interactions, provides a good\nframework for assessing these abilities. This work investigates the performance\nand merits of LLMs in canonical game-theoretic two-player non-zero-sum games,\nStag Hunt and Prisoner Dilemma. Our structured evaluation of GPT-3.5,\nGPT-4-Turbo, GPT-4o, and Llama-3-8B shows that these models, when making\ndecisions in these games, are affected by at least one of the following\nsystematic biases: positional bias, payoff bias, or behavioural bias. This\nindicates that LLMs do not fully rely on logical reasoning when making these\nstrategic decisions. As a result, it was found that the LLMs' performance drops\nwhen the game configuration is misaligned with the affecting biases. When\nmisaligned, GPT-3.5, GPT-4-Turbo, GPT-4o, and Llama-3-8B show an average\nperformance drop of 32\\%, 25\\%, 34\\%, and 29\\% respectively in Stag Hunt, and\n28\\%, 16\\%, 34\\%, and 24\\% respectively in Prisoner's Dilemma. Surprisingly,\nGPT-4o (a top-performing LLM across standard benchmarks) suffers the most\nsubstantial performance drop, suggesting that newer models are not addressing\nthese issues. Interestingly, we found that a commonly used method of improving\nthe reasoning capabilities of LLMs, chain-of-thought (CoT) prompting, reduces\nthe biases in GPT-3.5, GPT-4o, and Llama-3-8B but increases the effect of the\nbias in GPT-4-Turbo, indicating that CoT alone cannot fully serve as a robust\nsolution to this problem. We perform several additional experiments, which\nprovide further insight into these observed behaviours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been increasingly used in real-world\nsettings, yet their strategic decision-making abilities remain largely\nunexplored. To fully benefit from the potential of LLMs, it's essential to\nunderstand their ability to function in complex social scenarios. Game theory,\nwhich is already used to understand real-world interactions, provides a good\nframework for assessing these abilities. This work investigates the performance\nand merits of LLMs in canonical game-theoretic two-player non-zero-sum games,\nStag Hunt and Prisoner Dilemma. Our structured evaluation of GPT-3.5,\nGPT-4-Turbo, GPT-4o, and Llama-3-8B shows that these models, when making\ndecisions in these games, are affected by at least one of the following\nsystematic biases: positional bias, payoff bias, or behavioural bias. This\nindicates that LLMs do not fully rely on logical reasoning when making these\nstrategic decisions. As a result, it was found that the LLMs' performance drops\nwhen the game configuration is misaligned with the affecting biases. When\nmisaligned, GPT-3.5, GPT-4-Turbo, GPT-4o, and Llama-3-8B show an average\nperformance drop of 32\\%, 25\\%, 34\\%, and 29\\% respectively in Stag Hunt, and\n28\\%, 16\\%, 34\\%, and 24\\% respectively in Prisoner's Dilemma. Surprisingly,\nGPT-4o (a top-performing LLM across standard benchmarks) suffers the most\nsubstantial performance drop, suggesting that newer models are not addressing\nthese issues. Interestingly, we found that a commonly used method of improving\nthe reasoning capabilities of LLMs, chain-of-thought (CoT) prompting, reduces\nthe biases in GPT-3.5, GPT-4o, and Llama-3-8B but increases the effect of the\nbias in GPT-4-Turbo, indicating that CoT alone cannot fully serve as a robust\nsolution to this problem. We perform several additional experiments, which\nprovide further insight into these observed behaviours."
                },
                "authors": [
                    {
                        "name": "Nathan Herr"
                    },
                    {
                        "name": "Fernando Acero"
                    },
                    {
                        "name": "Roberta Raileanu"
                    },
                    {
                        "name": "María Pérez-Ortiz"
                    },
                    {
                        "name": "Zhibin Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhibin Li"
                },
                "author": "Zhibin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11507v1",
                "updated": "2024-10-15T11:20:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    20,
                    42,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T11:20:42Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    20,
                    42,
                    1,
                    289,
                    0
                ],
                "title": "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic\n  Evaluation Framework for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic\n  Evaluation Framework for LLMs"
                },
                "summary": "While various vertical domain large language models (LLMs) have been\ndeveloped, the challenge of automatically evaluating their performance across\ndifferent domains remains significant in addressing real-world user needs.\nCurrent benchmark-based evaluation methods exhibit rigid, purposeless\ninteractions and rely on pre-collected static datasets that are costly to\nbuild, inflexible across domains, and misaligned with practical user needs. To\naddress this, we revisit the evaluation components and introduce two\ndefinitions: **Benchmark+**, which extends traditional QA benchmarks into a\nmore flexible ``strategy-criterion'' format; and **Assessment+**, which\nenhances the interaction process for greater exploration and enables both\nquantitative metrics and qualitative insights that capture nuanced target LLM\nbehaviors from richer multi-turn interactions. We propose an agent-based\nevaluation framework called *TestAgent*, which implements these two concepts\nthrough retrieval augmented generation and reinforcement learning. Experiments\non tasks ranging from building vertical domain evaluation from scratch to\nactivating existing benchmarks demonstrate the effectiveness of *TestAgent*\nacross various scenarios. We believe this work offers an interesting\nperspective on automatic evaluation for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While various vertical domain large language models (LLMs) have been\ndeveloped, the challenge of automatically evaluating their performance across\ndifferent domains remains significant in addressing real-world user needs.\nCurrent benchmark-based evaluation methods exhibit rigid, purposeless\ninteractions and rely on pre-collected static datasets that are costly to\nbuild, inflexible across domains, and misaligned with practical user needs. To\naddress this, we revisit the evaluation components and introduce two\ndefinitions: **Benchmark+**, which extends traditional QA benchmarks into a\nmore flexible ``strategy-criterion'' format; and **Assessment+**, which\nenhances the interaction process for greater exploration and enables both\nquantitative metrics and qualitative insights that capture nuanced target LLM\nbehaviors from richer multi-turn interactions. We propose an agent-based\nevaluation framework called *TestAgent*, which implements these two concepts\nthrough retrieval augmented generation and reinforcement learning. Experiments\non tasks ranging from building vertical domain evaluation from scratch to\nactivating existing benchmarks demonstrate the effectiveness of *TestAgent*\nacross various scenarios. We believe this work offers an interesting\nperspective on automatic evaluation for LLMs."
                },
                "authors": [
                    {
                        "name": "Wanying Wang"
                    },
                    {
                        "name": "Zeyu Ma"
                    },
                    {
                        "name": "Pengfei Liu"
                    },
                    {
                        "name": "Mingang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingang Chen"
                },
                "author": "Mingang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10645v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10645v2",
                "updated": "2024-10-15T11:01:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    11,
                    1,
                    33,
                    1,
                    289,
                    0
                ],
                "published": "2024-08-20T08:36:59Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    8,
                    36,
                    59,
                    1,
                    233,
                    0
                ],
                "title": "CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoRA: Collaborative Information Perception by Large Language Model's\n  Weights for Recommendation"
                },
                "summary": "Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, CoRA (an acronym for Collaborative LoRA),\nwith a collaborative weights generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings, converting them into collaborative\nweights with low-rank properties through the collaborative weights generator.\nWe then merge the collaborative weights into LLM's weights, enabling LLM to\nperceive the collaborative signals and generate personalized recommendations\nwithout fine-tuning or extra collaborative tokens in prompts. Extensive\nexperiments confirm that CoRA effectively integrates collaborative information\ninto LLM, enhancing recommendation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Involving collaborative information in Large Language Models (LLMs) is a\npromising technique for adapting LLMs for recommendation. Existing methods\nachieve this by concatenating collaborative features with text tokens into a\nunified sequence input and then fine-tuning to align these features with LLM's\ninput space. Although effective, in this work, we identify two limitations when\nadapting LLMs to recommendation tasks, which hinder the integration of general\nknowledge and collaborative information, resulting in sub-optimal\nrecommendation performance. (1) Fine-tuning LLM with recommendation data can\nundermine its inherent world knowledge and fundamental competencies, which are\ncrucial for interpreting and inferring recommendation text. (2) Incorporating\ncollaborative features into textual prompts disrupts the semantics of the\noriginal prompts, preventing LLM from generating appropriate outputs. In this\npaper, we propose a new paradigm, CoRA (an acronym for Collaborative LoRA),\nwith a collaborative weights generator. Rather than input space alignment, this\nmethod aligns collaborative information with LLM's parameter space,\nrepresenting them as incremental weights to update LLM's output. This way, LLM\nperceives collaborative information without altering its general knowledge and\ntext inference capabilities. Specifically, we employ a collaborative filtering\nmodel to extract user and item embeddings, converting them into collaborative\nweights with low-rank properties through the collaborative weights generator.\nWe then merge the collaborative weights into LLM's weights, enabling LLM to\nperceive the collaborative signals and generate personalized recommendations\nwithout fine-tuning or extra collaborative tokens in prompts. Extensive\nexperiments confirm that CoRA effectively integrates collaborative information\ninto LLM, enhancing recommendation performance."
                },
                "authors": [
                    {
                        "name": "Yuting Liu"
                    },
                    {
                        "name": "Jinghao Zhang"
                    },
                    {
                        "name": "Yizhou Dang"
                    },
                    {
                        "name": "Yuliang Liang"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Guibing Guo"
                    },
                    {
                        "name": "Jianzhe Zhao"
                    },
                    {
                        "name": "Xingwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingwei Wang"
                },
                "author": "Xingwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10645v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10645v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11890v2",
                "updated": "2024-10-15T10:53:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    10,
                    53,
                    55,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-14T03:34:02Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    3,
                    34,
                    2,
                    4,
                    166,
                    0
                ],
                "title": "Unraveling the Mechanics of Learning-Based Demonstration Selection for\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling the Mechanics of Learning-Based Demonstration Selection for\n  In-Context Learning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive in-context learning\n(ICL) capabilities from few-shot demonstration exemplars. While recent\nlearning-based demonstration selection methods have proven beneficial to ICL by\nchoosing more useful exemplars, their underlying mechanisms are opaque,\nhindering efforts to address limitations such as high training costs and poor\ngeneralization across tasks. These methods generally assume the selection\nprocess captures similarities between the exemplar and the target instance,\nhowever, it remains unknown what kinds of similarities are captured and vital\nto performing ICL. To dive into this question, we analyze the working\nmechanisms of the learning-based demonstration selection methods and\nempirically identify two important factors related to similarity measurement:\n1) The ability to integrate different levels of task-agnostic text similarities\nbetween the input of exemplars and test cases enhances generalization power\nacross different tasks. 2) Incorporating task-specific labels when measuring\nthe similarities significantly improves the performance on each specific task.\nWe validate these two findings through extensive quantitative and qualitative\nanalyses across ten datasets and various LLMs. Based on our findings, we\nintroduce two effective yet simplified exemplar selection methods catering to\ntask-agnostic and task-specific demands, eliminating the costly LLM inference\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive in-context learning\n(ICL) capabilities from few-shot demonstration exemplars. While recent\nlearning-based demonstration selection methods have proven beneficial to ICL by\nchoosing more useful exemplars, their underlying mechanisms are opaque,\nhindering efforts to address limitations such as high training costs and poor\ngeneralization across tasks. These methods generally assume the selection\nprocess captures similarities between the exemplar and the target instance,\nhowever, it remains unknown what kinds of similarities are captured and vital\nto performing ICL. To dive into this question, we analyze the working\nmechanisms of the learning-based demonstration selection methods and\nempirically identify two important factors related to similarity measurement:\n1) The ability to integrate different levels of task-agnostic text similarities\nbetween the input of exemplars and test cases enhances generalization power\nacross different tasks. 2) Incorporating task-specific labels when measuring\nthe similarities significantly improves the performance on each specific task.\nWe validate these two findings through extensive quantitative and qualitative\nanalyses across ten datasets and various LLMs. Based on our findings, we\nintroduce two effective yet simplified exemplar selection methods catering to\ntask-agnostic and task-specific demands, eliminating the costly LLM inference\noverhead."
                },
                "authors": [
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Wenya Wang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Chris Xing Tian"
                    },
                    {
                        "name": "Chenqi Kong"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Haoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Haoliang Li"
                },
                "author": "Haoliang Li",
                "arxiv_comment": "17 pages, 7 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12572v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12572v3",
                "updated": "2024-10-15T10:35:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    10,
                    35,
                    17,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-18T13:02:12Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    2,
                    12,
                    1,
                    170,
                    0
                ],
                "title": "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large\n  Language Models"
                },
                "summary": "We introduce Mathador-LM, a new benchmark for evaluating the mathematical\nreasoning on large language models (LLMs), combining ruleset interpretation,\nplanning, and problem-solving. This benchmark is inspired by the Mathador game,\nwhere the objective is to reach a target number using basic arithmetic\noperations on a given set of base numbers, following a simple set of rules. We\nshow that, across leading LLMs, we obtain stable average performance while\ngenerating benchmark instances \\emph{dynamically}, following a target\ndifficulty level. Thus, our benchmark alleviates concerns about test-set\nleakage into training data, an issue that often undermines popular benchmarks.\nAdditionally, we conduct a comprehensive evaluation of both open and\nclosed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that\ncontemporary models struggle with Mathador-LM, scoring significantly lower than\naverage 3rd graders. This stands in stark contrast to their strong performance\non popular mathematical reasoning benchmarks. The implementation of Mathador-LM\nbenchmark is available at\n\\href{https://github.com/IST-DASLab/Mathador-LM}{github.com/IST-DASLab/Mathador-LM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Mathador-LM, a new benchmark for evaluating the mathematical\nreasoning on large language models (LLMs), combining ruleset interpretation,\nplanning, and problem-solving. This benchmark is inspired by the Mathador game,\nwhere the objective is to reach a target number using basic arithmetic\noperations on a given set of base numbers, following a simple set of rules. We\nshow that, across leading LLMs, we obtain stable average performance while\ngenerating benchmark instances \\emph{dynamically}, following a target\ndifficulty level. Thus, our benchmark alleviates concerns about test-set\nleakage into training data, an issue that often undermines popular benchmarks.\nAdditionally, we conduct a comprehensive evaluation of both open and\nclosed-source state-of-the-art LLMs on Mathador-LM. Our findings reveal that\ncontemporary models struggle with Mathador-LM, scoring significantly lower than\naverage 3rd graders. This stands in stark contrast to their strong performance\non popular mathematical reasoning benchmarks. The implementation of Mathador-LM\nbenchmark is available at\n\\href{https://github.com/IST-DASLab/Mathador-LM}{github.com/IST-DASLab/Mathador-LM}."
                },
                "authors": [
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Amir Moeini"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12572v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12572v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11469v1",
                "updated": "2024-10-15T10:16:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    10,
                    16,
                    45,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T10:16:45Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    10,
                    16,
                    45,
                    1,
                    289,
                    0
                ],
                "title": "O-Edit: Orthogonal Subspace Editing for Language Model Sequential\n  Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "O-Edit: Orthogonal Subspace Editing for Language Model Sequential\n  Editing"
                },
                "summary": "Large language models (LLMs) acquire knowledge during pre-training, but over\ntime, this knowledge may become incorrect or outdated, necessitating updates\nafter training. Knowledge editing techniques address this issue without the\nneed for costly re-training. However, most existing methods are designed for\nsingle edits, and as the number of edits increases, they often cause a decline\nin the model's overall performance, posing significant challenges for\nsequential editing. To overcome this, we propose Orthogonal Subspace Editing,\nO-Edit. This algorithm orthogonalizes the direction of each knowledge update,\nminimizing interference between successive updates and reducing the impact of\nnew updates on unrelated knowledge. Our approach does not require replaying\npreviously edited data and processes each edit knowledge on time. It can\nperform thousands of edits on mainstream LLMs, achieving an average performance\nimprovement that is 4.2 times better than existing methods while effectively\npreserving the model's performance on downstream tasks, all with minimal\nadditional parameter overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) acquire knowledge during pre-training, but over\ntime, this knowledge may become incorrect or outdated, necessitating updates\nafter training. Knowledge editing techniques address this issue without the\nneed for costly re-training. However, most existing methods are designed for\nsingle edits, and as the number of edits increases, they often cause a decline\nin the model's overall performance, posing significant challenges for\nsequential editing. To overcome this, we propose Orthogonal Subspace Editing,\nO-Edit. This algorithm orthogonalizes the direction of each knowledge update,\nminimizing interference between successive updates and reducing the impact of\nnew updates on unrelated knowledge. Our approach does not require replaying\npreviously edited data and processes each edit knowledge on time. It can\nperform thousands of edits on mainstream LLMs, achieving an average performance\nimprovement that is 4.2 times better than existing methods while effectively\npreserving the model's performance on downstream tasks, all with minimal\nadditional parameter overhead."
                },
                "authors": [
                    {
                        "name": "Yuchen Cai"
                    },
                    {
                        "name": "Ding Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ding Cao"
                },
                "author": "Ding Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11463v1",
                "updated": "2024-10-15T10:10:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    10,
                    10,
                    33,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T10:10:33Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    10,
                    10,
                    33,
                    1,
                    289,
                    0
                ],
                "title": "Advanced Persistent Threats (APT) Attribution Using Deep Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Persistent Threats (APT) Attribution Using Deep Reinforcement\n  Learning"
                },
                "summary": "This paper investigates the application of Deep Reinforcement Learning (DRL)\nfor attributing malware to specific Advanced Persistent Threat (APT) groups\nthrough detailed behavioural analysis. By analysing over 3500 malware samples\nfrom 12 distinct APT groups, the study utilises sophisticated tools like Cuckoo\nSandbox to extract behavioural data, providing a deep insight into the\noperational patterns of malware. The research demonstrates that the DRL model\nsignificantly outperforms traditional machine learning approaches such as SGD,\nSVC, KNN, MLP, and Decision Tree Classifiers, achieving an impressive test\naccuracy of 89.27 %. It highlights the model capability to adeptly manage\ncomplex, variable, and elusive malware attributes. Furthermore, the paper\ndiscusses the considerable computational resources and extensive data\ndependencies required for deploying these advanced AI models in cybersecurity\nframeworks. Future research is directed towards enhancing the efficiency of DRL\nmodels, expanding the diversity of the datasets, addressing ethical concerns,\nand leveraging Large Language Models (LLMs) to refine reward mechanisms and\noptimise the DRL framework. By showcasing the transformative potential of DRL\nin malware attribution, this research advocates for a responsible and balanced\napproach to AI integration, with the goal of advancing cybersecurity through\nmore adaptable, accurate, and robust systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the application of Deep Reinforcement Learning (DRL)\nfor attributing malware to specific Advanced Persistent Threat (APT) groups\nthrough detailed behavioural analysis. By analysing over 3500 malware samples\nfrom 12 distinct APT groups, the study utilises sophisticated tools like Cuckoo\nSandbox to extract behavioural data, providing a deep insight into the\noperational patterns of malware. The research demonstrates that the DRL model\nsignificantly outperforms traditional machine learning approaches such as SGD,\nSVC, KNN, MLP, and Decision Tree Classifiers, achieving an impressive test\naccuracy of 89.27 %. It highlights the model capability to adeptly manage\ncomplex, variable, and elusive malware attributes. Furthermore, the paper\ndiscusses the considerable computational resources and extensive data\ndependencies required for deploying these advanced AI models in cybersecurity\nframeworks. Future research is directed towards enhancing the efficiency of DRL\nmodels, expanding the diversity of the datasets, addressing ethical concerns,\nand leveraging Large Language Models (LLMs) to refine reward mechanisms and\noptimise the DRL framework. By showcasing the transformative potential of DRL\nin malware attribution, this research advocates for a responsible and balanced\napproach to AI integration, with the goal of advancing cybersecurity through\nmore adaptable, accurate, and robust systems."
                },
                "authors": [
                    {
                        "name": "Animesh Singh Basnet"
                    },
                    {
                        "name": "Mohamed Chahine Ghanem"
                    },
                    {
                        "name": "Dipo Dunsin"
                    },
                    {
                        "name": "Wiktor Sowinski-Mydlarz"
                    }
                ],
                "author_detail": {
                    "name": "Wiktor Sowinski-Mydlarz"
                },
                "author": "Wiktor Sowinski-Mydlarz",
                "arxiv_comment": "21 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10908v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10908v5",
                "updated": "2024-10-15T10:09:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    10,
                    9,
                    1,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-16T12:11:46Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    12,
                    11,
                    46,
                    6,
                    168,
                    0
                ],
                "title": "Logit Separability-Driven Samples and Multiple Class-Related Words\n  Selection for Advancing In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logit Separability-Driven Samples and Multiple Class-Related Words\n  Selection for Advancing In-Context Learning"
                },
                "summary": "Effective organization of in-context learning (ICL) demonstrations is key to\nimproving the quality of large language model (LLM) responses. To create better\nsample-label pairs that instruct LLM understanding, we introduce logit\nseparability, a criterion to assess the clarity of both samples and\nclass-related words at the logit level. This facilitates the optimization of\nsample and label selection, enhancing the precision of information provided in\nICL demonstrations. Additionally, we find that incorporating multiple\nclass-related words for each sample, rather than relying on a single class\nname, improves performance by offering a broader range of label information.\nBuilding on these insights, we propose LICL, a logit separability-based method\nthat jointly organizes samples and integrates multiple class-related words into\neach sample-label pair. Evaluations across seven classification datasets show\nthat this approach significantly improves ICL performance by providing clearer\ninstructions and richer label information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective organization of in-context learning (ICL) demonstrations is key to\nimproving the quality of large language model (LLM) responses. To create better\nsample-label pairs that instruct LLM understanding, we introduce logit\nseparability, a criterion to assess the clarity of both samples and\nclass-related words at the logit level. This facilitates the optimization of\nsample and label selection, enhancing the precision of information provided in\nICL demonstrations. Additionally, we find that incorporating multiple\nclass-related words for each sample, rather than relying on a single class\nname, improves performance by offering a broader range of label information.\nBuilding on these insights, we propose LICL, a logit separability-based method\nthat jointly organizes samples and integrates multiple class-related words into\neach sample-label pair. Evaluations across seven classification datasets show\nthat this approach significantly improves ICL performance by providing clearer\ninstructions and richer label information."
                },
                "authors": [
                    {
                        "name": "Zhu Zixiao"
                    },
                    {
                        "name": "Feng Zijian"
                    },
                    {
                        "name": "Zhou Hanzhang"
                    },
                    {
                        "name": "Qian Junlang"
                    },
                    {
                        "name": "Mao Kezhi"
                    }
                ],
                "author_detail": {
                    "name": "Mao Kezhi"
                },
                "author": "Mao Kezhi",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10908v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10908v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11459v1",
                "updated": "2024-10-15T10:07:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    10,
                    7,
                    15,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T10:07:15Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    10,
                    7,
                    15,
                    1,
                    289,
                    0
                ],
                "title": "Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have exhibited outstanding performance in\nengaging with humans and addressing complex questions by leveraging their vast\nimplicit knowledge and robust reasoning capabilities. However, such models are\nvulnerable to jailbreak attacks, leading to the generation of harmful\nresponses. Despite recent research on single-turn jailbreak strategies to\nfacilitate the development of defence mechanisms, the challenge of revealing\nvulnerabilities under multi-turn setting remains relatively under-explored. In\nthis work, we propose Jigsaw Puzzles (JSP), a straightforward yet effective\nmulti-turn jailbreak strategy against the advanced LLMs. JSP splits questions\ninto harmless fractions as the input of each turn, and requests LLMs to\nreconstruct and respond to questions under multi-turn interaction. Our\nexperimental results demonstrate that the proposed JSP jailbreak bypasses\noriginal safeguards against explicitly harmful content, achieving an average\nattack success rate of 93.76% on 189 harmful queries across 5 advanced LLMs\n(Gemini-1.5-Pro, Llama-3.1-70B, GPT-4, GPT-4o, GPT-4o-mini). Moreover, JSP\nachieves a state-of-the-art attack success rate of 92% on GPT-4 on the harmful\nquery benchmark, and exhibits strong resistant to defence strategies. Warning:\nthis paper contains offensive examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited outstanding performance in\nengaging with humans and addressing complex questions by leveraging their vast\nimplicit knowledge and robust reasoning capabilities. However, such models are\nvulnerable to jailbreak attacks, leading to the generation of harmful\nresponses. Despite recent research on single-turn jailbreak strategies to\nfacilitate the development of defence mechanisms, the challenge of revealing\nvulnerabilities under multi-turn setting remains relatively under-explored. In\nthis work, we propose Jigsaw Puzzles (JSP), a straightforward yet effective\nmulti-turn jailbreak strategy against the advanced LLMs. JSP splits questions\ninto harmless fractions as the input of each turn, and requests LLMs to\nreconstruct and respond to questions under multi-turn interaction. Our\nexperimental results demonstrate that the proposed JSP jailbreak bypasses\noriginal safeguards against explicitly harmful content, achieving an average\nattack success rate of 93.76% on 189 harmful queries across 5 advanced LLMs\n(Gemini-1.5-Pro, Llama-3.1-70B, GPT-4, GPT-4o, GPT-4o-mini). Moreover, JSP\nachieves a state-of-the-art attack success rate of 92% on GPT-4 on the harmful\nquery benchmark, and exhibits strong resistant to defence strategies. Warning:\nthis paper contains offensive examples."
                },
                "authors": [
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Lizhen Qu"
                    },
                    {
                        "name": "Ehsan Shareghi"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    }
                ],
                "author_detail": {
                    "name": "Gholamreza Haffari"
                },
                "author": "Gholamreza Haffari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11450v1",
                "updated": "2024-10-15T09:53:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    53,
                    40,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T09:53:40Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    53,
                    40,
                    1,
                    289,
                    0
                ],
                "title": "A Cross-Lingual Statutory Article Retrieval Dataset for Taiwan Legal\n  Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cross-Lingual Statutory Article Retrieval Dataset for Taiwan Legal\n  Studies"
                },
                "summary": "This paper introduces a cross-lingual statutory article retrieval (SAR)\ndataset designed to enhance legal information retrieval in multilingual\nsettings. Our dataset features spoken-language-style legal inquiries in\nEnglish, paired with corresponding Chinese versions and relevant statutes,\ncovering all Taiwanese civil, criminal, and administrative laws. This dataset\naims to improve access to legal information for non-native speakers,\nparticularly for foreign nationals in Taiwan. We propose several LLM-based\nmethods as baselines for evaluating retrieval effectiveness, focusing on\nmitigating translation errors and improving cross-lingual retrieval\nperformance. Our work provides a valuable resource for developing inclusive\nlegal information retrieval systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a cross-lingual statutory article retrieval (SAR)\ndataset designed to enhance legal information retrieval in multilingual\nsettings. Our dataset features spoken-language-style legal inquiries in\nEnglish, paired with corresponding Chinese versions and relevant statutes,\ncovering all Taiwanese civil, criminal, and administrative laws. This dataset\naims to improve access to legal information for non-native speakers,\nparticularly for foreign nationals in Taiwan. We propose several LLM-based\nmethods as baselines for evaluating retrieval effectiveness, focusing on\nmitigating translation errors and improving cross-lingual retrieval\nperformance. Our work provides a valuable resource for developing inclusive\nlegal information retrieval systems."
                },
                "authors": [
                    {
                        "name": "Yen-Hsiang Wang"
                    },
                    {
                        "name": "Feng-Dian Su"
                    },
                    {
                        "name": "Tzu-Yu Yeh"
                    },
                    {
                        "name": "Yao-Chung Fan"
                    }
                ],
                "author_detail": {
                    "name": "Yao-Chung Fan"
                },
                "author": "Yao-Chung Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11437v1",
                "updated": "2024-10-15T09:40:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    40,
                    50,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T09:40:50Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    40,
                    50,
                    1,
                    289,
                    0
                ],
                "title": "Difficult Task Yes but Simple Task No: Unveiling the Laziness in\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Difficult Task Yes but Simple Task No: Unveiling the Laziness in\n  Multimodal LLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) demonstrate a strong understanding\nof the real world and can even handle complex tasks. However, they still fail\non some straightforward visual question-answering (VQA) problems. This paper\ndives deeper into this issue, revealing that models tend to err when answering\neasy questions (e.g. Yes/No questions) about an image, even though they can\ncorrectly describe it. We refer to this model behavior discrepancy between\ndifficult and simple questions as model laziness. To systematically investigate\nmodel laziness, we manually construct LazyBench, a benchmark that includes\nYes/No, multiple choice, short answer questions, and image description tasks\nthat are related to the same subjects in the images. Based on LazyBench, we\nobserve that laziness widely exists in current advanced MLLMs (e.g. GPT-4o,\nGemini-1.5-pro, Claude 3 and LLaVA-v1.5-13B), and it is more pronounced on\nstronger models. We also analyze the VQA v2 (LLaVA-v1.5-13B) benchmark and find\nthat about half of its failure cases are caused by model laziness, which\nfurther highlights the importance of ensuring that the model fully utilizes its\ncapability. To this end, we conduct preliminary exploration on how to mitigate\nlaziness and find that chain of thought (CoT) can effectively address this\nissue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) demonstrate a strong understanding\nof the real world and can even handle complex tasks. However, they still fail\non some straightforward visual question-answering (VQA) problems. This paper\ndives deeper into this issue, revealing that models tend to err when answering\neasy questions (e.g. Yes/No questions) about an image, even though they can\ncorrectly describe it. We refer to this model behavior discrepancy between\ndifficult and simple questions as model laziness. To systematically investigate\nmodel laziness, we manually construct LazyBench, a benchmark that includes\nYes/No, multiple choice, short answer questions, and image description tasks\nthat are related to the same subjects in the images. Based on LazyBench, we\nobserve that laziness widely exists in current advanced MLLMs (e.g. GPT-4o,\nGemini-1.5-pro, Claude 3 and LLaVA-v1.5-13B), and it is more pronounced on\nstronger models. We also analyze the VQA v2 (LLaVA-v1.5-13B) benchmark and find\nthat about half of its failure cases are caused by model laziness, which\nfurther highlights the importance of ensuring that the model fully utilizes its\ncapability. To this end, we conduct preliminary exploration on how to mitigate\nlaziness and find that chain of thought (CoT) can effectively address this\nissue."
                },
                "authors": [
                    {
                        "name": "Sihang Zhao"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Xiaoying Tang"
                    },
                    {
                        "name": "Pinjia He"
                    }
                ],
                "author_detail": {
                    "name": "Pinjia He"
                },
                "author": "Pinjia He",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00658v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00658v3",
                "updated": "2024-10-15T09:16:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    16,
                    38,
                    1,
                    289,
                    0
                ],
                "published": "2024-02-01T15:18:33Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    15,
                    18,
                    33,
                    3,
                    32,
                    0
                ],
                "title": "Learning Planning-based Reasoning by Trajectories Collection and Process\n  Reward Synthesizing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Planning-based Reasoning by Trajectories Collection and Process\n  Reward Synthesizing"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nhandling complex reasoning tasks through step-by-step rationale generation.\nHowever, recent studies have raised concerns regarding the hallucination and\nflaws in their reasoning process. Substantial efforts are being made to improve\nthe reliability and faithfulness of the generated rationales. Some approaches\nmodel reasoning as planning, while others focus on annotating for process\nsupervision. Nevertheless, the planning-based search process often results in\nhigh latency due to the frequent assessment of intermediate reasoning states\nand the extensive exploration space. Additionally, supervising the reasoning\nprocess with human annotation is costly and challenging to scale for LLM\ntraining. To address these issues, in this paper, we propose a framework to\nlearn planning-based reasoning through Direct Preference Optimization (DPO) on\ncollected trajectories, which are ranked according to synthesized process\nrewards. Our results on challenging logical reasoning benchmarks demonstrate\nthe effectiveness of our learning framework, showing that our 7B model can\nsurpass the strong counterparts like GPT-3.5-Turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in\nhandling complex reasoning tasks through step-by-step rationale generation.\nHowever, recent studies have raised concerns regarding the hallucination and\nflaws in their reasoning process. Substantial efforts are being made to improve\nthe reliability and faithfulness of the generated rationales. Some approaches\nmodel reasoning as planning, while others focus on annotating for process\nsupervision. Nevertheless, the planning-based search process often results in\nhigh latency due to the frequent assessment of intermediate reasoning states\nand the extensive exploration space. Additionally, supervising the reasoning\nprocess with human annotation is costly and challenging to scale for LLM\ntraining. To address these issues, in this paper, we propose a framework to\nlearn planning-based reasoning through Direct Preference Optimization (DPO) on\ncollected trajectories, which are ranked according to synthesized process\nrewards. Our results on challenging logical reasoning benchmarks demonstrate\nthe effectiveness of our learning framework, showing that our 7B model can\nsurpass the strong counterparts like GPT-3.5-Turbo."
                },
                "authors": [
                    {
                        "name": "Fangkai Jiao"
                    },
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Zhengyuan Liu"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "17 pages, 9 figures. EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00658v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00658v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03122v2",
                "updated": "2024-10-15T09:13:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    13,
                    52,
                    1,
                    289,
                    0
                ],
                "published": "2024-05-06T02:35:10Z",
                "published_parsed": [
                    2024,
                    5,
                    6,
                    2,
                    35,
                    10,
                    0,
                    127,
                    0
                ],
                "title": "Automatic Retrieval-augmented Generation of 6G Network Specifications\n  for Use Cases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Retrieval-augmented Generation of 6G Network Specifications\n  for Use Cases"
                },
                "summary": "6G Open Radio Access Networks (O-RAN) promises to open data interfaces to\nenable plug-and-play service Apps, many of which are consumer and\nbusiness-facing. Opening up 6G access lowers the barrier to innovation but\nraises the challenge that the required communication specifications are not\nfully known to all service designers. As such, business innovators must either\nbe familiar with 6G standards or consult with experts. Enabling consistent,\nunbiased, rapid, and low-cost requirement assessment and specification\ngeneration is crucial to the O-RAN innovation ecosystem.\n  Here, we discuss our initiative to bridge service specification gaps between\nnetwork service providers and business innovators leveraging Large Language\nModels (LLMs). We first review the state-of-the-art and motivation in 6G\nplug-and-play services, capabilities, potential use cases and LLMs. We identify\nan ample innovation space for hybrid use cases that may require diverse and\nvariational wireless functionalities across its operating time. We show that\nthe network specification can be automated and present the first automatic\nretrieval-augmented network service specification framework for 6G use cases.\nTo enable public acceptance and feedback, a website interface is published for\nthe research and industrial community to experiment with the framework. We hope\nthis review highlights the need for emerging foundation models for this area\nand motivates researcher engagement and contribution to the community through\nour framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G Open Radio Access Networks (O-RAN) promises to open data interfaces to\nenable plug-and-play service Apps, many of which are consumer and\nbusiness-facing. Opening up 6G access lowers the barrier to innovation but\nraises the challenge that the required communication specifications are not\nfully known to all service designers. As such, business innovators must either\nbe familiar with 6G standards or consult with experts. Enabling consistent,\nunbiased, rapid, and low-cost requirement assessment and specification\ngeneration is crucial to the O-RAN innovation ecosystem.\n  Here, we discuss our initiative to bridge service specification gaps between\nnetwork service providers and business innovators leveraging Large Language\nModels (LLMs). We first review the state-of-the-art and motivation in 6G\nplug-and-play services, capabilities, potential use cases and LLMs. We identify\nan ample innovation space for hybrid use cases that may require diverse and\nvariational wireless functionalities across its operating time. We show that\nthe network specification can be automated and present the first automatic\nretrieval-augmented network service specification framework for 6G use cases.\nTo enable public acceptance and feedback, a website interface is published for\nthe research and industrial community to experiment with the framework. We hope\nthis review highlights the need for emerging foundation models for this area\nand motivates researcher engagement and contribution to the community through\nour framework."
                },
                "authors": [
                    {
                        "name": "Yun Tang"
                    },
                    {
                        "name": "Weisi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Weisi Guo"
                },
                "author": "Weisi Guo",
                "arxiv_comment": "7 pages, 6 figures, Submitted to IEEE Comm Magazine for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08760v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08760v4",
                "updated": "2024-10-15T09:10:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    10,
                    9,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-12T18:36:20Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    18,
                    36,
                    20,
                    4,
                    103,
                    0
                ],
                "title": "The Generation Gap: Exploring Age Bias in the Value Systems of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Generation Gap: Exploring Age Bias in the Value Systems of Large\n  Language Models"
                },
                "summary": "We explore the alignment of values in Large Language Models (LLMs) with\nspecific age groups, leveraging data from the World Value Survey across\nthirteen categories. Through a diverse set of prompts tailored to ensure\nresponse robustness, we find a general inclination of LLM values towards\nyounger demographics, especially when compared to the US population. Although a\ngeneral inclination can be observed, we also found that this inclination toward\nyounger groups can be different across different value categories.\nAdditionally, we explore the impact of incorporating age identity information\nin prompts and observe challenges in mitigating value discrepancies with\ndifferent age cohorts. Our findings highlight the age bias in LLMs and provide\ninsights for future work. Materials for our analysis are available at \\url{\nhttps://github.com/MichiganNLP/Age-Bias-In-LLMs}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the alignment of values in Large Language Models (LLMs) with\nspecific age groups, leveraging data from the World Value Survey across\nthirteen categories. Through a diverse set of prompts tailored to ensure\nresponse robustness, we find a general inclination of LLM values towards\nyounger demographics, especially when compared to the US population. Although a\ngeneral inclination can be observed, we also found that this inclination toward\nyounger groups can be different across different value categories.\nAdditionally, we explore the impact of incorporating age identity information\nin prompts and observe challenges in mitigating value discrepancies with\ndifferent age cohorts. Our findings highlight the age bias in LLMs and provide\ninsights for future work. Materials for our analysis are available at \\url{\nhttps://github.com/MichiganNLP/Age-Bias-In-LLMs}"
                },
                "authors": [
                    {
                        "name": "Siyang Liu"
                    },
                    {
                        "name": "Trish Maturi"
                    },
                    {
                        "name": "Bowen Yi"
                    },
                    {
                        "name": "Siqi Shen"
                    },
                    {
                        "name": "Rada Mihalcea"
                    }
                ],
                "author_detail": {
                    "name": "Rada Mihalcea"
                },
                "author": "Rada Mihalcea",
                "arxiv_comment": "5 pages",
                "arxiv_journal_ref": "The 2024 Conference on Empirical Methods in Natural Language\n  Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08760v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08760v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11417v1",
                "updated": "2024-10-15T09:07:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T09:07:25Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "title": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models"
                },
                "summary": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11414v1",
                "updated": "2024-10-15T09:02:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    2,
                    9,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T09:02:09Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    2,
                    9,
                    1,
                    289,
                    0
                ],
                "title": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via\n  Mechanistic Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via\n  Mechanistic Interpretability"
                },
                "summary": "Retrieval-Augmented Generation (RAG) models are designed to incorporate\nexternal knowledge, reducing hallucinations caused by insufficient parametric\n(internal) knowledge. However, even with accurate and relevant retrieved\ncontent, RAG models can still produce hallucinations by generating outputs that\nconflict with the retrieved information. Detecting such hallucinations requires\ndisentangling how Large Language Models (LLMs) utilize external and parametric\nknowledge. Current detection methods often focus on one of these mechanisms or\nwithout decoupling their intertwined effects, making accurate detection\ndifficult. In this paper, we investigate the internal mechanisms behind\nhallucinations in RAG scenarios. We discover hallucinations occur when the\nKnowledge FFNs in LLMs overemphasize parametric knowledge in the residual\nstream, while Copying Heads fail to effectively retain or integrate external\nknowledge from retrieved content. Based on these findings, we propose ReDeEP, a\nnovel method that detects hallucinations by decoupling LLM's utilization of\nexternal context and parametric knowledge. Our experiments show that ReDeEP\nsignificantly improves RAG hallucination detection accuracy. Additionally, we\nintroduce AARF, which mitigates hallucinations by modulating the contributions\nof Knowledge FFNs and Copying Heads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) models are designed to incorporate\nexternal knowledge, reducing hallucinations caused by insufficient parametric\n(internal) knowledge. However, even with accurate and relevant retrieved\ncontent, RAG models can still produce hallucinations by generating outputs that\nconflict with the retrieved information. Detecting such hallucinations requires\ndisentangling how Large Language Models (LLMs) utilize external and parametric\nknowledge. Current detection methods often focus on one of these mechanisms or\nwithout decoupling their intertwined effects, making accurate detection\ndifficult. In this paper, we investigate the internal mechanisms behind\nhallucinations in RAG scenarios. We discover hallucinations occur when the\nKnowledge FFNs in LLMs overemphasize parametric knowledge in the residual\nstream, while Copying Heads fail to effectively retain or integrate external\nknowledge from retrieved content. Based on these findings, we propose ReDeEP, a\nnovel method that detects hallucinations by decoupling LLM's utilization of\nexternal context and parametric knowledge. Our experiments show that ReDeEP\nsignificantly improves RAG hallucination detection accuracy. Additionally, we\nintroduce AARF, which mitigates hallucinations by modulating the contributions\nof Knowledge FFNs and Copying Heads."
                },
                "authors": [
                    {
                        "name": "Zhongxiang Sun"
                    },
                    {
                        "name": "Xiaoxue Zang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Weijie Yu"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Han Li"
                    }
                ],
                "author_detail": {
                    "name": "Han Li"
                },
                "author": "Han Li",
                "arxiv_comment": "23pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10570v2",
                "updated": "2024-10-15T08:56:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    56,
                    32,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-14T14:47:32Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    47,
                    32,
                    0,
                    288,
                    0
                ],
                "title": "Mindalogue: LLM-Powered Nonlinear Interaction for Effective Learning and\n  Task Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mindalogue: LLM-Powered Nonlinear Interaction for Effective Learning and\n  Task Exploration"
                },
                "summary": "Current generative AI models like ChatGPT, Claude, and Gemini are widely used\nfor knowledge dissemination, task decomposition, and creative thinking.\nHowever, their linear interaction methods often force users to repeatedly\ncompare and copy contextual information when handling complex tasks, increasing\ncognitive load and operational costs. Moreover, the ambiguity in model\nresponses requires users to refine and simplify the information further. To\naddress these issues, we developed \"Mindalogue\", a system using a non-linear\ninteraction model based on \"nodes + canvas\" to enhance user efficiency and\nfreedom while generating structured responses. A formative study with 11 users\ninformed the design of Mindalogue, which was then evaluated through a study\nwith 16 participants. The results showed that Mindalogue significantly reduced\ntask steps and improved users' comprehension of complex information. This study\nhighlights the potential of non-linear interaction in improving AI tool\nefficiency and user experience in the HCI field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current generative AI models like ChatGPT, Claude, and Gemini are widely used\nfor knowledge dissemination, task decomposition, and creative thinking.\nHowever, their linear interaction methods often force users to repeatedly\ncompare and copy contextual information when handling complex tasks, increasing\ncognitive load and operational costs. Moreover, the ambiguity in model\nresponses requires users to refine and simplify the information further. To\naddress these issues, we developed \"Mindalogue\", a system using a non-linear\ninteraction model based on \"nodes + canvas\" to enhance user efficiency and\nfreedom while generating structured responses. A formative study with 11 users\ninformed the design of Mindalogue, which was then evaluated through a study\nwith 16 participants. The results showed that Mindalogue significantly reduced\ntask steps and improved users' comprehension of complex information. This study\nhighlights the potential of non-linear interaction in improving AI tool\nefficiency and user experience in the HCI field."
                },
                "authors": [
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Ziyao Zhang"
                    },
                    {
                        "name": "Fengliang Zhu"
                    },
                    {
                        "name": "Jiajie Zhou"
                    },
                    {
                        "name": "Anyi Rao"
                    }
                ],
                "author_detail": {
                    "name": "Anyi Rao"
                },
                "author": "Anyi Rao",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U35(Primary), 68T20(Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11410v1",
                "updated": "2024-10-15T08:54:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    54,
                    27,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T08:54:27Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    54,
                    27,
                    1,
                    289,
                    0
                ],
                "title": "PMMT: Preference Alignment in Multilingual Machine Translation via LLM\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PMMT: Preference Alignment in Multilingual Machine Translation via LLM\n  Distillation"
                },
                "summary": "Translation is important for cross-language communication, and many efforts\nhave been made to improve its accuracy. However, less investment is conducted\nin aligning translations with human preferences, such as translation tones or\nstyles. In this paper, a new method is proposed to effectively generate\nlarge-scale multilingual parallel corpora with specific translation preferences\nusing Large Language Models (LLMs). Meanwhile, an automatic pipeline is\ndesigned to distill human preferences into smaller Machine Translation (MT)\nmodels for efficiently and economically supporting large-scale calls in online\nservices. Experiments indicate that the proposed method takes the lead in\ntranslation tasks with aligned human preferences by a large margin. Meanwhile,\non popular public benchmarks like WMT and Flores, on which our models were not\ntrained, the proposed method also shows a competitive performance compared to\nSOTA works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translation is important for cross-language communication, and many efforts\nhave been made to improve its accuracy. However, less investment is conducted\nin aligning translations with human preferences, such as translation tones or\nstyles. In this paper, a new method is proposed to effectively generate\nlarge-scale multilingual parallel corpora with specific translation preferences\nusing Large Language Models (LLMs). Meanwhile, an automatic pipeline is\ndesigned to distill human preferences into smaller Machine Translation (MT)\nmodels for efficiently and economically supporting large-scale calls in online\nservices. Experiments indicate that the proposed method takes the lead in\ntranslation tasks with aligned human preferences by a large margin. Meanwhile,\non popular public benchmarks like WMT and Flores, on which our models were not\ntrained, the proposed method also shows a competitive performance compared to\nSOTA works."
                },
                "authors": [
                    {
                        "name": "Shuqiao Sun"
                    },
                    {
                        "name": "Yutong Yao"
                    },
                    {
                        "name": "Peiwen Wu"
                    },
                    {
                        "name": "Feijun Jiang"
                    },
                    {
                        "name": "Kaifu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifu Zhang"
                },
                "author": "Kaifu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11404v1",
                "updated": "2024-10-15T08:49:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    49,
                    59,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T08:49:59Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    49,
                    59,
                    1,
                    289,
                    0
                ],
                "title": "MoChat: Joints-Grouped Spatio-Temporal Grounding LLM for Multi-Turn\n  Motion Comprehension and Description",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoChat: Joints-Grouped Spatio-Temporal Grounding LLM for Multi-Turn\n  Motion Comprehension and Description"
                },
                "summary": "Despite continuous advancements in deep learning for understanding human\nmotion, existing models often struggle to accurately identify action timing and\nspecific body parts, typically supporting only single-round interaction. Such\nlimitations in capturing fine-grained motion details reduce their effectiveness\nin motion understanding tasks. In this paper, we propose MoChat, a multimodal\nlarge language model capable of spatio-temporal grounding of human motion and\nunderstanding multi-turn dialogue context. To achieve these capabilities, we\ngroup the spatial information of each skeleton frame based on human anatomical\nstructure and then apply them with Joints-Grouped Skeleton Encoder, whose\noutputs are combined with LLM embeddings to create spatio-aware and\ntemporal-aware embeddings separately. Additionally, we develop a pipeline for\nextracting timestamps from skeleton sequences based on textual annotations, and\nconstruct multi-turn dialogues for spatially grounding. Finally, various task\ninstructions are generated for jointly training. Experimental results\ndemonstrate that MoChat achieves state-of-the-art performance across multiple\nmetrics in motion understanding tasks, making it as the first model capable of\nfine-grained spatio-temporal grounding of human motion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite continuous advancements in deep learning for understanding human\nmotion, existing models often struggle to accurately identify action timing and\nspecific body parts, typically supporting only single-round interaction. Such\nlimitations in capturing fine-grained motion details reduce their effectiveness\nin motion understanding tasks. In this paper, we propose MoChat, a multimodal\nlarge language model capable of spatio-temporal grounding of human motion and\nunderstanding multi-turn dialogue context. To achieve these capabilities, we\ngroup the spatial information of each skeleton frame based on human anatomical\nstructure and then apply them with Joints-Grouped Skeleton Encoder, whose\noutputs are combined with LLM embeddings to create spatio-aware and\ntemporal-aware embeddings separately. Additionally, we develop a pipeline for\nextracting timestamps from skeleton sequences based on textual annotations, and\nconstruct multi-turn dialogues for spatially grounding. Finally, various task\ninstructions are generated for jointly training. Experimental results\ndemonstrate that MoChat achieves state-of-the-art performance across multiple\nmetrics in motion understanding tasks, making it as the first model capable of\nfine-grained spatio-temporal grounding of human motion."
                },
                "authors": [
                    {
                        "name": "Jiawei Mo"
                    },
                    {
                        "name": "Yixuan Chen"
                    },
                    {
                        "name": "Rifen Lin"
                    },
                    {
                        "name": "Yongkang Ni"
                    },
                    {
                        "name": "Min Zeng"
                    },
                    {
                        "name": "Xiping Hu"
                    },
                    {
                        "name": "Min Li"
                    }
                ],
                "author_detail": {
                    "name": "Min Li"
                },
                "author": "Min Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01121v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01121v3",
                "updated": "2024-10-15T08:48:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    48,
                    3,
                    1,
                    289,
                    0
                ],
                "published": "2024-05-02T09:35:06Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    9,
                    35,
                    6,
                    3,
                    123,
                    0
                ],
                "title": "Efficient Data Generation for Source-grounded Information-seeking\n  Dialogs: A Use Case for Meeting Transcripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Data Generation for Source-grounded Information-seeking\n  Dialogs: A Use Case for Meeting Transcripts"
                },
                "summary": "Automating data generation with Large Language Models (LLMs) has become\nincreasingly popular. In this work, we investigate the feasibility and\neffectiveness of LLM-based data generation in the challenging setting of\nsource-grounded information-seeking dialogs, with response attribution, over\nlong documents. Our source texts consist of long and noisy meeting transcripts,\nadding to the task complexity. Since automating attribution remains difficult,\nwe propose a semi-automatic approach: dialog queries and responses are\ngenerated with LLMs, followed by human verification and identification of\nattribution spans. Using this approach, we created MISeD -- Meeting Information\nSeeking Dialogs dataset -- a dataset of information-seeking dialogs focused on\nmeeting transcripts. Models finetuned with MISeD demonstrate superior\nperformance compared to off-the-shelf models, even those of larger size.\nFinetuning on MISeD gives comparable response generation quality to finetuning\non fully manual data, while improving attribution quality and reducing time and\neffort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating data generation with Large Language Models (LLMs) has become\nincreasingly popular. In this work, we investigate the feasibility and\neffectiveness of LLM-based data generation in the challenging setting of\nsource-grounded information-seeking dialogs, with response attribution, over\nlong documents. Our source texts consist of long and noisy meeting transcripts,\nadding to the task complexity. Since automating attribution remains difficult,\nwe propose a semi-automatic approach: dialog queries and responses are\ngenerated with LLMs, followed by human verification and identification of\nattribution spans. Using this approach, we created MISeD -- Meeting Information\nSeeking Dialogs dataset -- a dataset of information-seeking dialogs focused on\nmeeting transcripts. Models finetuned with MISeD demonstrate superior\nperformance compared to off-the-shelf models, even those of larger size.\nFinetuning on MISeD gives comparable response generation quality to finetuning\non fully manual data, while improving attribution quality and reducing time and\neffort."
                },
                "authors": [
                    {
                        "name": "Lotem Golany"
                    },
                    {
                        "name": "Filippo Galgani"
                    },
                    {
                        "name": "Maya Mamo"
                    },
                    {
                        "name": "Nimrod Parasol"
                    },
                    {
                        "name": "Omer Vandsburger"
                    },
                    {
                        "name": "Nadav Bar"
                    },
                    {
                        "name": "Ido Dagan"
                    }
                ],
                "author_detail": {
                    "name": "Ido Dagan"
                },
                "author": "Ido Dagan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01121v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01121v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10329v2",
                "updated": "2024-10-15T08:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    45,
                    35,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-14T09:40:52Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    40,
                    52,
                    0,
                    288,
                    0
                ],
                "title": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for\n  Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for\n  Text-Attributed Graphs"
                },
                "summary": "Recently, research on Text-Attributed Graphs (TAGs) has gained significant\nattention due to the prevalence of free-text node features in real-world\napplications and the advancements in Large Language Models (LLMs) that bolster\nTAG methodologies. However, current TAG approaches face two primary challenges:\n(i) Heavy reliance on label information and (ii) Limited cross-domain\nzero/few-shot transferability. These issues constrain the scaling of both data\nand model size, owing to high labor costs and scaling laws, complicating the\ndevelopment of graph foundation models with strong transferability. In this\nwork, we propose the GraphCLIP framework to address these challenges by\nlearning graph foundation models with strong cross-domain zero/few-shot\ntransferability through a self-supervised contrastive graph-summary pretraining\nmethod. Specifically, we generate and curate large-scale graph-summary pair\ndata with the assistance of LLMs, and introduce a novel graph-summary\npretraining method, combined with invariant learning, to enhance graph\nfoundation models with strong cross-domain zero-shot transferability. For\nfew-shot learning, we propose a novel graph prompt tuning technique aligned\nwith our pretraining objective to mitigate catastrophic forgetting and minimize\nlearning costs. Extensive experiments show the superiority of GraphCLIP in both\nzero-shot and few-shot settings, while evaluations across various downstream\ntasks confirm the versatility of GraphCLIP. Our code is available at:\nhttps://github.com/ZhuYun97/GraphCLIP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, research on Text-Attributed Graphs (TAGs) has gained significant\nattention due to the prevalence of free-text node features in real-world\napplications and the advancements in Large Language Models (LLMs) that bolster\nTAG methodologies. However, current TAG approaches face two primary challenges:\n(i) Heavy reliance on label information and (ii) Limited cross-domain\nzero/few-shot transferability. These issues constrain the scaling of both data\nand model size, owing to high labor costs and scaling laws, complicating the\ndevelopment of graph foundation models with strong transferability. In this\nwork, we propose the GraphCLIP framework to address these challenges by\nlearning graph foundation models with strong cross-domain zero/few-shot\ntransferability through a self-supervised contrastive graph-summary pretraining\nmethod. Specifically, we generate and curate large-scale graph-summary pair\ndata with the assistance of LLMs, and introduce a novel graph-summary\npretraining method, combined with invariant learning, to enhance graph\nfoundation models with strong cross-domain zero-shot transferability. For\nfew-shot learning, we propose a novel graph prompt tuning technique aligned\nwith our pretraining objective to mitigate catastrophic forgetting and minimize\nlearning costs. Extensive experiments show the superiority of GraphCLIP in both\nzero-shot and few-shot settings, while evaluations across various downstream\ntasks confirm the versatility of GraphCLIP. Our code is available at:\nhttps://github.com/ZhuYun97/GraphCLIP"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Xiaotang Wang"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Boci Peng"
                    },
                    {
                        "name": "Chuntao Hong"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09297v3",
                "updated": "2024-10-15T08:45:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    45,
                    18,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-13T16:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    33,
                    44,
                    3,
                    165,
                    0
                ],
                "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding"
                },
                "summary": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv"
                },
                "authors": [
                    {
                        "name": "Zayd Muhammad Kawakibi Zuhri"
                    },
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11396v1",
                "updated": "2024-10-15T08:39:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    39,
                    28,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T08:39:28Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    39,
                    28,
                    1,
                    289,
                    0
                ],
                "title": "Implementing Derivations of Definite Logic Programs with Self-Attention\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementing Derivations of Definite Logic Programs with Self-Attention\n  Networks"
                },
                "summary": "In this paper we propose that a restricted version of logical inference can\nbe implemented with self-attention networks. We are aiming at showing that LLMs\n(Large Language Models) constructed with transformer networks can make logical\ninferences. We would reveal the potential of LLMs by analyzing self-attention\nnetworks, which are main components of transformer networks. Our approach is\nnot based on semantics of natural languages but operations of logical\ninference. %point of view. We show that hierarchical constructions of\nself-attention networks with feed forward networks (FFNs) can implement\ntop-down derivations for a class of logical formulae. We also show bottom-up\nderivations are also implemented for the same class. We believe that our\nresults show that LLMs implicitly have the power of logical inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we propose that a restricted version of logical inference can\nbe implemented with self-attention networks. We are aiming at showing that LLMs\n(Large Language Models) constructed with transformer networks can make logical\ninferences. We would reveal the potential of LLMs by analyzing self-attention\nnetworks, which are main components of transformer networks. Our approach is\nnot based on semantics of natural languages but operations of logical\ninference. %point of view. We show that hierarchical constructions of\nself-attention networks with feed forward networks (FFNs) can implement\ntop-down derivations for a class of logical formulae. We also show bottom-up\nderivations are also implemented for the same class. We believe that our\nresults show that LLMs implicitly have the power of logical inference."
                },
                "authors": [
                    {
                        "name": "Phan Thi Thanh Thuy"
                    },
                    {
                        "name": "Akihiro Yamamoto"
                    }
                ],
                "author_detail": {
                    "name": "Akihiro Yamamoto"
                },
                "author": "Akihiro Yamamoto",
                "arxiv_comment": "Presented at NeLaMKRR@KR, 2024 (arXiv:2410.05339)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11387v1",
                "updated": "2024-10-15T08:24:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    24,
                    5,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T08:24:05Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    24,
                    5,
                    1,
                    289,
                    0
                ],
                "title": "LLM2Swarm: Robot Swarms that Responsively Reason, Plan, and Collaborate\n  through LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM2Swarm: Robot Swarms that Responsively Reason, Plan, and Collaborate\n  through LLMs"
                },
                "summary": "Robot swarms are composed of many simple robots that communicate and\ncollaborate to fulfill complex tasks. Robot controllers usually need to be\nspecified by experts on a case-by-case basis via programming code. This process\nis time-consuming, prone to errors, and unable to take into account all\nsituations that may be encountered during deployment. On the other hand, recent\nLarge Language Models (LLMs) have demonstrated reasoning and planning\ncapabilities, introduced new ways to interact with and program machines, and\nrepresent domain and commonsense knowledge. Hence, we propose to address the\naforementioned challenges by integrating LLMs with robot swarms and show the\npotential in proofs of concept (showcases). For this integration, we explore\ntwo approaches. The first approach is 'indirect integration,' where LLMs are\nused to synthesize and validate the robot controllers. This approach may reduce\ndevelopment time and human error before deployment. Moreover, during\ndeployment, it could be used for on-the-fly creation of new robot behaviors.\nThe second approach is 'direct integration,' where each robot locally executes\na separate LLM instance during deployment for robot-robot collaboration and\nhuman-swarm interaction. These local LLM instances enable each robot to reason,\nplan, and collaborate using natural language. To enable further research on our\nmainly conceptual contribution, we release the software and videos for our\nLLM2Swarm system: https://github.com/Pold87/LLM2Swarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot swarms are composed of many simple robots that communicate and\ncollaborate to fulfill complex tasks. Robot controllers usually need to be\nspecified by experts on a case-by-case basis via programming code. This process\nis time-consuming, prone to errors, and unable to take into account all\nsituations that may be encountered during deployment. On the other hand, recent\nLarge Language Models (LLMs) have demonstrated reasoning and planning\ncapabilities, introduced new ways to interact with and program machines, and\nrepresent domain and commonsense knowledge. Hence, we propose to address the\naforementioned challenges by integrating LLMs with robot swarms and show the\npotential in proofs of concept (showcases). For this integration, we explore\ntwo approaches. The first approach is 'indirect integration,' where LLMs are\nused to synthesize and validate the robot controllers. This approach may reduce\ndevelopment time and human error before deployment. Moreover, during\ndeployment, it could be used for on-the-fly creation of new robot behaviors.\nThe second approach is 'direct integration,' where each robot locally executes\na separate LLM instance during deployment for robot-robot collaboration and\nhuman-swarm interaction. These local LLM instances enable each robot to reason,\nplan, and collaborate using natural language. To enable further research on our\nmainly conceptual contribution, we release the software and videos for our\nLLM2Swarm system: https://github.com/Pold87/LLM2Swarm."
                },
                "authors": [
                    {
                        "name": "Volker Strobel"
                    },
                    {
                        "name": "Marco Dorigo"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "arxiv_comment": "Accepted at NeurIPS 2024 Workshop on Open-World Agents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11385v1",
                "updated": "2024-10-15T08:23:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    23,
                    31,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T08:23:31Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    23,
                    31,
                    1,
                    289,
                    0
                ],
                "title": "Do LLMs Have the Generalization Ability in Conducting Causal Inference?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Have the Generalization Ability in Conducting Causal Inference?"
                },
                "summary": "In causal inference, generalization capability refers to the ability to\nconduct causal inference methods on new data to estimate the causal-effect\nbetween unknown phenomenon, which is crucial for expanding the boundaries of\nknowledge. Studies have evaluated the causal inference capabilities of Large\nLanguage Models (LLMs) concerning known phenomena, yet the generalization\ncapabilities of LLMs concerning unseen phenomena remain unexplored. In this\npaper, we selected four tasks: Causal Path Discovery (CP), Backdoor Adjustment\n(BA), Factual Inference (FI), and Counterfactual Inference (CI) as\nrepresentatives of causal inference tasks. To generate evaluation questions\nabout previously unseen phenomena in new data on the four tasks, we propose a\nbenchmark generation framework, which employs randomly generated graphs and\nnode names to formulate questions within hypothetical new causal scenarios.\nBased on this framework, we compile a benchmark dataset of varying levels of\nquestion complexity. We extensively tested the generalization capabilities of\nfive leading LLMs across four tasks. Experiment results reveal that while LLMs\nexhibit good generalization performance in solving simple CP, FI, and complex\nCI questions, they encounter difficulties when tackling BA questions and face\nobvious performance fluctuations as the problem complexity changes.\nFurthermore, when the names of phenomena incorporate existing terms, even if\nthese names are entirely novel, their generalization performance can still be\nhindered by interference from familiar terms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In causal inference, generalization capability refers to the ability to\nconduct causal inference methods on new data to estimate the causal-effect\nbetween unknown phenomenon, which is crucial for expanding the boundaries of\nknowledge. Studies have evaluated the causal inference capabilities of Large\nLanguage Models (LLMs) concerning known phenomena, yet the generalization\ncapabilities of LLMs concerning unseen phenomena remain unexplored. In this\npaper, we selected four tasks: Causal Path Discovery (CP), Backdoor Adjustment\n(BA), Factual Inference (FI), and Counterfactual Inference (CI) as\nrepresentatives of causal inference tasks. To generate evaluation questions\nabout previously unseen phenomena in new data on the four tasks, we propose a\nbenchmark generation framework, which employs randomly generated graphs and\nnode names to formulate questions within hypothetical new causal scenarios.\nBased on this framework, we compile a benchmark dataset of varying levels of\nquestion complexity. We extensively tested the generalization capabilities of\nfive leading LLMs across four tasks. Experiment results reveal that while LLMs\nexhibit good generalization performance in solving simple CP, FI, and complex\nCI questions, they encounter difficulties when tackling BA questions and face\nobvious performance fluctuations as the problem complexity changes.\nFurthermore, when the names of phenomena incorporate existing terms, even if\nthese names are entirely novel, their generalization performance can still be\nhindered by interference from familiar terms."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Dongming Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Ruifang He"
                    },
                    {
                        "name": "Yuexian Hou"
                    }
                ],
                "author_detail": {
                    "name": "Yuexian Hou"
                },
                "author": "Yuexian Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11381v1",
                "updated": "2024-10-15T08:19:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    19,
                    24,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T08:19:24Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    19,
                    24,
                    1,
                    289,
                    0
                ],
                "title": "Survey and Evaluation of Converging Architecture in LLMs based on\n  Footsteps of Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey and Evaluation of Converging Architecture in LLMs based on\n  Footsteps of Operations"
                },
                "summary": "The advent of the Attention mechanism and Transformer architecture enables\ncontextually natural text generation and compresses the burden of processing\nentire source information into singular vectors. Based on these two main ideas,\nmodel sizes gradually increases to accommodate more precise and comprehensive\ninformation, leading to the current state-of-the-art LLMs being very large,\nwith parameters around 70 billion. As the model sizes are growing, the demand\nfor substantial storage and computational capacity increases. This leads to the\ndevelopment of high-bandwidth memory and accelerators, as well as a variety of\nmodel architectures designed to meet these requirements. We note that LLM\narchitectures have increasingly converged. This paper analyzes how these\nconverged architectures perform in terms of layer configurations, operational\nmechanisms, and model sizes, considering various hyperparameter settings. In\nthis paper, we conduct a concise survey of the history of LLMs by tracing the\nevolution of their operational improvements. Furthermore, we summarize the\nperformance trends of LLMs under various hyperparameter settings using the RTX\n6000, which features the state-of-the-art Ada Lovelace architecture. We\nconclude that even the same model can exhibit different behaviors depending on\nthe hyperparameters or whether it is deployed in server or edge environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of the Attention mechanism and Transformer architecture enables\ncontextually natural text generation and compresses the burden of processing\nentire source information into singular vectors. Based on these two main ideas,\nmodel sizes gradually increases to accommodate more precise and comprehensive\ninformation, leading to the current state-of-the-art LLMs being very large,\nwith parameters around 70 billion. As the model sizes are growing, the demand\nfor substantial storage and computational capacity increases. This leads to the\ndevelopment of high-bandwidth memory and accelerators, as well as a variety of\nmodel architectures designed to meet these requirements. We note that LLM\narchitectures have increasingly converged. This paper analyzes how these\nconverged architectures perform in terms of layer configurations, operational\nmechanisms, and model sizes, considering various hyperparameter settings. In\nthis paper, we conduct a concise survey of the history of LLMs by tracing the\nevolution of their operational improvements. Furthermore, we summarize the\nperformance trends of LLMs under various hyperparameter settings using the RTX\n6000, which features the state-of-the-art Ada Lovelace architecture. We\nconclude that even the same model can exhibit different behaviors depending on\nthe hyperparameters or whether it is deployed in server or edge environments."
                },
                "authors": [
                    {
                        "name": "Seongho Kim"
                    },
                    {
                        "name": "Jihyun Moon"
                    },
                    {
                        "name": "Juntaek Oh"
                    },
                    {
                        "name": "Insu Choi"
                    },
                    {
                        "name": "Joon-Sung Yang"
                    }
                ],
                "author_detail": {
                    "name": "Joon-Sung Yang"
                },
                "author": "Joon-Sung Yang",
                "arxiv_comment": "13 pages and 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11377v1",
                "updated": "2024-10-15T08:16:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    16,
                    43,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T08:16:43Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    16,
                    43,
                    1,
                    289,
                    0
                ],
                "title": "A Framework for Adapting Human-Robot Interaction to Diverse User Groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Adapting Human-Robot Interaction to Diverse User Groups"
                },
                "summary": "To facilitate natural and intuitive interactions with diverse user groups in\nreal-world settings, social robots must be capable of addressing the varying\nrequirements and expectations of these groups while adapting their behavior\nbased on user feedback. While previous research often focuses on specific\ndemographics, we present a novel framework for adaptive Human-Robot Interaction\n(HRI) that tailors interactions to different user groups and enables individual\nusers to modulate interactions through both minor and major interruptions. Our\nprimary contributions include the development of an adaptive, ROS-based HRI\nframework with an open-source code base. This framework supports natural\ninteractions through advanced speech recognition and voice activity detection,\nand leverages a large language model (LLM) as a dialogue bridge. We validate\nthe efficiency of our framework through module tests and system trials,\ndemonstrating its high accuracy in age recognition and its robustness to\nrepeated user inputs and plan changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To facilitate natural and intuitive interactions with diverse user groups in\nreal-world settings, social robots must be capable of addressing the varying\nrequirements and expectations of these groups while adapting their behavior\nbased on user feedback. While previous research often focuses on specific\ndemographics, we present a novel framework for adaptive Human-Robot Interaction\n(HRI) that tailors interactions to different user groups and enables individual\nusers to modulate interactions through both minor and major interruptions. Our\nprimary contributions include the development of an adaptive, ROS-based HRI\nframework with an open-source code base. This framework supports natural\ninteractions through advanced speech recognition and voice activity detection,\nand leverages a large language model (LLM) as a dialogue bridge. We validate\nthe efficiency of our framework through module tests and system trials,\ndemonstrating its high accuracy in age recognition and its robustness to\nrepeated user inputs and plan changes."
                },
                "authors": [
                    {
                        "name": "Theresa Pekarek Rosin"
                    },
                    {
                        "name": "Vanessa Hassouna"
                    },
                    {
                        "name": "Xiaowen Sun"
                    },
                    {
                        "name": "Luca Krohm"
                    },
                    {
                        "name": "Henri-Leon Kordt"
                    },
                    {
                        "name": "Michael Beetz"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "Accepted at the 16th International Conference on Social Robotics\n  (ICSR) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14963v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14963v4",
                "updated": "2024-10-15T08:11:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    11,
                    25,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-23T12:16:05Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    12,
                    16,
                    5,
                    1,
                    114,
                    0
                ],
                "title": "Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs\n  Better Solvers for Math Word Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs\n  Better Solvers for Math Word Problems"
                },
                "summary": "Chain-of-Thought (CoT) prompting has enhanced the performance of Large\nLanguage Models (LLMs) across various reasoning tasks. However, CoT still falls\nshort in dealing with complex math word problems, as it usually suffers from\nthree pitfalls: semantic misunderstanding errors, calculation errors, and\nstep-missing errors. Prior studies involve addressing the calculation errors\nand step-missing errors, but neglect the semantic misunderstanding errors,\nwhich is the major factor limiting the reasoning performance of LLMs. To this\nend, we propose a simple-yet-effective method, namely Deeply Understanding the\nProblems (DUP), to improve the LLMs' math problem-solving ability by addressing\nsemantic misunderstanding errors. The core of our method is to encourage the\nLLMs to deeply understand the problems and extract the key problem-solving\ninformation used for better reasoning. Extensive experiments on 10 diverse\nreasoning benchmarks show that our DUP method consistently outperforms the\nother counterparts by a large margin. More encouragingly, DUP achieves a new\nSOTA result on the GSM8K benchmark, with an accuracy of 97.1% under the\nzero-shot setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting has enhanced the performance of Large\nLanguage Models (LLMs) across various reasoning tasks. However, CoT still falls\nshort in dealing with complex math word problems, as it usually suffers from\nthree pitfalls: semantic misunderstanding errors, calculation errors, and\nstep-missing errors. Prior studies involve addressing the calculation errors\nand step-missing errors, but neglect the semantic misunderstanding errors,\nwhich is the major factor limiting the reasoning performance of LLMs. To this\nend, we propose a simple-yet-effective method, namely Deeply Understanding the\nProblems (DUP), to improve the LLMs' math problem-solving ability by addressing\nsemantic misunderstanding errors. The core of our method is to encourage the\nLLMs to deeply understand the problems and extract the key problem-solving\ninformation used for better reasoning. Extensive experiments on 10 diverse\nreasoning benchmarks show that our DUP method consistently outperforms the\nother counterparts by a large margin. More encouragingly, DUP achieves a new\nSOTA result on the GSM8K benchmark, with an accuracy of 97.1% under the\nzero-shot setting."
                },
                "authors": [
                    {
                        "name": "Qihuang Zhong"
                    },
                    {
                        "name": "Kang Wang"
                    },
                    {
                        "name": "Ziyang Xu"
                    },
                    {
                        "name": "Juhua Liu"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Bo Du"
                    }
                ],
                "author_detail": {
                    "name": "Bo Du"
                },
                "author": "Bo Du",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14963v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14963v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11371v1",
                "updated": "2024-10-15T07:51:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    7,
                    51,
                    0,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T07:51:00Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    7,
                    51,
                    0,
                    1,
                    289,
                    0
                ],
                "title": "Learning from Imperfect Data: Towards Efficient Knowledge Distillation\n  of Autoregressive Language Models for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Imperfect Data: Towards Efficient Knowledge Distillation\n  of Autoregressive Language Models for Text-to-SQL"
                },
                "summary": "Large Language Models (LLMs) have shown promising performance in text-to-SQL,\nwhich involves translating natural language questions into SQL queries.\nHowever, current text-to-SQL LLMs are computationally expensive and challenging\nto deploy in real-world applications, highlighting the importance of\ncompressing them. To achieve this goal, knowledge distillation (KD) is a common\napproach, which aims to distill the larger teacher model into a smaller student\nmodel. While numerous KD methods for autoregressive LLMs have emerged recently,\nit is still under-explored whether they work well in complex text-to-SQL\nscenarios. To this end, we conduct a series of analyses and reveal that these\nKD methods generally fall short in balancing performance and efficiency. In\nresponse to this problem, we propose to improve the KD with Imperfect Data,\nnamely KID, which effectively boosts the performance without introducing much\ntraining budget. The core of KID is to efficiently mitigate the\ntraining-inference mismatch by simulating the cascading effect of inference in\nthe imperfect training data. Extensive experiments on 5 text-to-SQL benchmarks\nshow that, KID can not only achieve consistent and significant performance\ngains (up to +5.83% average score) across all model types and sizes, but also\neffectively improve the training efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown promising performance in text-to-SQL,\nwhich involves translating natural language questions into SQL queries.\nHowever, current text-to-SQL LLMs are computationally expensive and challenging\nto deploy in real-world applications, highlighting the importance of\ncompressing them. To achieve this goal, knowledge distillation (KD) is a common\napproach, which aims to distill the larger teacher model into a smaller student\nmodel. While numerous KD methods for autoregressive LLMs have emerged recently,\nit is still under-explored whether they work well in complex text-to-SQL\nscenarios. To this end, we conduct a series of analyses and reveal that these\nKD methods generally fall short in balancing performance and efficiency. In\nresponse to this problem, we propose to improve the KD with Imperfect Data,\nnamely KID, which effectively boosts the performance without introducing much\ntraining budget. The core of KID is to efficiently mitigate the\ntraining-inference mismatch by simulating the cascading effect of inference in\nthe imperfect training data. Extensive experiments on 5 text-to-SQL benchmarks\nshow that, KID can not only achieve consistent and significant performance\ngains (up to +5.83% average score) across all model types and sizes, but also\neffectively improve the training efficiency."
                },
                "authors": [
                    {
                        "name": "Qihuang Zhong"
                    },
                    {
                        "name": "Kunfeng Chen"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Juhua Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted to EMNLP2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11370v1",
                "updated": "2024-10-15T07:50:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    7,
                    50,
                    34,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T07:50:34Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    7,
                    50,
                    34,
                    1,
                    289,
                    0
                ],
                "title": "Enhance Graph Alignment for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhance Graph Alignment for Large Language Models"
                },
                "summary": "Graph-structured data is prevalent in the real world. Recently, due to the\npowerful emergent capabilities, Large Language Models (LLMs) have shown\npromising performance in modeling graphs. The key to effectively applying LLMs\non graphs is converting graph data into a format LLMs can comprehend.\nGraph-to-token approaches are popular in enabling LLMs to process graph\ninformation. They transform graphs into sequences of tokens and align them with\ntext tokens through instruction tuning, where self-supervised instruction\ntuning helps LLMs acquire general knowledge about graphs, and supervised\nfine-tuning specializes LLMs for the downstream tasks on graphs. Despite their\ninitial success, we find that existing methods have a misalignment between\nself-supervised tasks and supervised downstream tasks, resulting in negative\ntransfer from self-supervised fine-tuning to downstream tasks. To address these\nissues, we propose Graph Alignment Large Language Models (GALLM) to benefit\nfrom aligned task templates. In the self-supervised tuning stage, we introduce\na novel text matching task using templates aligned with downstream tasks. In\nthe task-specific tuning stage, we propose two category prompt methods that\nlearn supervision information from additional explanation with further aligned\ntemplates. Experimental evaluations on four datasets demonstrate substantial\nimprovements in supervised learning, multi-dataset generalizability, and\nparticularly in zero-shot capability, highlighting the model's potential as a\ngraph foundation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-structured data is prevalent in the real world. Recently, due to the\npowerful emergent capabilities, Large Language Models (LLMs) have shown\npromising performance in modeling graphs. The key to effectively applying LLMs\non graphs is converting graph data into a format LLMs can comprehend.\nGraph-to-token approaches are popular in enabling LLMs to process graph\ninformation. They transform graphs into sequences of tokens and align them with\ntext tokens through instruction tuning, where self-supervised instruction\ntuning helps LLMs acquire general knowledge about graphs, and supervised\nfine-tuning specializes LLMs for the downstream tasks on graphs. Despite their\ninitial success, we find that existing methods have a misalignment between\nself-supervised tasks and supervised downstream tasks, resulting in negative\ntransfer from self-supervised fine-tuning to downstream tasks. To address these\nissues, we propose Graph Alignment Large Language Models (GALLM) to benefit\nfrom aligned task templates. In the self-supervised tuning stage, we introduce\na novel text matching task using templates aligned with downstream tasks. In\nthe task-specific tuning stage, we propose two category prompt methods that\nlearn supervision information from additional explanation with further aligned\ntemplates. Experimental evaluations on four datasets demonstrate substantial\nimprovements in supervised learning, multi-dataset generalizability, and\nparticularly in zero-shot capability, highlighting the model's potential as a\ngraph foundation model."
                },
                "authors": [
                    {
                        "name": "Haitong Luo"
                    },
                    {
                        "name": "Xuying Meng"
                    },
                    {
                        "name": "Suhang Wang"
                    },
                    {
                        "name": "Tianxiang Zhao"
                    },
                    {
                        "name": "Fali Wang"
                    },
                    {
                        "name": "Hanyun Cao"
                    },
                    {
                        "name": "Yujun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yujun Zhang"
                },
                "author": "Yujun Zhang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09437v2",
                "updated": "2024-10-15T07:48:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    7,
                    48,
                    55,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-12T08:32:26Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    8,
                    32,
                    26,
                    5,
                    286,
                    0
                ],
                "title": "MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MTL-LoRA: Low-Rank Adaptation for Multi-Task Learning"
                },
                "summary": "Parameter-efficient fine-tuning (PEFT) has been widely employed for domain\nadaptation, with LoRA being one of the most prominent methods due to its\nsimplicity and effectiveness. However, in multi-task learning (MTL) scenarios,\nLoRA tends to obscure the distinction between tasks by projecting sparse\nhigh-dimensional features from different tasks into the same dense\nlow-dimensional intrinsic space. This leads to task interference and suboptimal\nperformance for LoRA and its variants. To tackle this challenge, we propose\nMTL-LoRA, which retains the advantages of low-rank adaptation while\nsignificantly enhancing multi-task learning capabilities. MTL-LoRA augments\nLoRA by incorporating additional task-adaptive parameters that differentiate\ntask-specific information and effectively capture shared knowledge across\nvarious tasks within low-dimensional spaces. This approach enables large\nlanguage models (LLMs) pre-trained on general corpus to adapt to different\ntarget task domains with a limited number of trainable parameters.\nComprehensive experimental results, including evaluations on public academic\nbenchmarks for natural language understanding, commonsense reasoning, and\nimage-text understanding, as well as real-world industrial text Ads relevance\ndatasets, demonstrate that MTL-LoRA outperforms LoRA and its various variants\nwith comparable or even fewer learnable parameters in multitask learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) has been widely employed for domain\nadaptation, with LoRA being one of the most prominent methods due to its\nsimplicity and effectiveness. However, in multi-task learning (MTL) scenarios,\nLoRA tends to obscure the distinction between tasks by projecting sparse\nhigh-dimensional features from different tasks into the same dense\nlow-dimensional intrinsic space. This leads to task interference and suboptimal\nperformance for LoRA and its variants. To tackle this challenge, we propose\nMTL-LoRA, which retains the advantages of low-rank adaptation while\nsignificantly enhancing multi-task learning capabilities. MTL-LoRA augments\nLoRA by incorporating additional task-adaptive parameters that differentiate\ntask-specific information and effectively capture shared knowledge across\nvarious tasks within low-dimensional spaces. This approach enables large\nlanguage models (LLMs) pre-trained on general corpus to adapt to different\ntarget task domains with a limited number of trainable parameters.\nComprehensive experimental results, including evaluations on public academic\nbenchmarks for natural language understanding, commonsense reasoning, and\nimage-text understanding, as well as real-world industrial text Ads relevance\ndatasets, demonstrate that MTL-LoRA outperforms LoRA and its various variants\nwith comparable or even fewer learnable parameters in multitask learning."
                },
                "authors": [
                    {
                        "name": "Yaming Yang"
                    },
                    {
                        "name": "Dilxat Muhtar"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Yuefeng Zhan"
                    },
                    {
                        "name": "Jianfeng Liu"
                    },
                    {
                        "name": "Yujing Wang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Denvy Deng"
                    },
                    {
                        "name": "Feng Sun"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Weizhu Chen"
                    },
                    {
                        "name": "Yunhai Tong"
                    }
                ],
                "author_detail": {
                    "name": "Yunhai Tong"
                },
                "author": "Yunhai Tong",
                "arxiv_comment": "12 Pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04449v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04449v3",
                "updated": "2024-10-15T07:45:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    7,
                    45,
                    31,
                    1,
                    289,
                    0
                ],
                "published": "2024-08-08T13:19:37Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    13,
                    19,
                    37,
                    3,
                    221,
                    0
                ],
                "title": "EAIRiskBench: Towards Evaluating Physical Risk Awareness for Task\n  Planning of Foundation Model-based Embodied AI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EAIRiskBench: Towards Evaluating Physical Risk Awareness for Task\n  Planning of Foundation Model-based Embodied AI Agents"
                },
                "summary": "Embodied artificial intelligence (EAI) integrates advanced AI models into\nphysical entities for real-world interaction. The emergence of foundation\nmodels as the \"brain\" of EAI agents for high-level task planning has shown\npromising results. However, the deployment of these agents in physical\nenvironments presents significant safety challenges. For instance, a\nhousekeeping robot lacking sufficient risk awareness might place a metal\ncontainer in a microwave, potentially causing a fire. To address these critical\nsafety concerns, comprehensive pre-deployment risk assessments are imperative.\nThis study introduces EAIRiskBench, a novel framework for automated physical\nrisk assessment in EAI scenarios. EAIRiskBench employs a multi-agent\ncooperative system that leverages various foundation models to generate safety\nguidelines, create risk-prone scenarios, make task planning, and evaluate\nsafety systematically. Utilizing this framework, we construct EAIRiskDataset,\ncomprising diverse test cases across various domains, encompassing both textual\nand visual scenarios. Our comprehensive evaluation of state-of-the-art\nfoundation models reveals alarming results: all models exhibit high task risk\nrates (TRR), with an average of 95.75% across all evaluated models. To address\nthese challenges, we further propose two prompting-based risk mitigation\nstrategies. While these strategies demonstrate some efficacy in reducing TRR,\nthe improvements are limited, still indicating substantial safety concerns.\nThis study provides the first large-scale assessment of physical risk awareness\nin EAI agents. Our findings underscore the critical need for enhanced safety\nmeasures in EAI systems and provide valuable insights for future research\ndirections in developing safer embodied artificial intelligence system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied artificial intelligence (EAI) integrates advanced AI models into\nphysical entities for real-world interaction. The emergence of foundation\nmodels as the \"brain\" of EAI agents for high-level task planning has shown\npromising results. However, the deployment of these agents in physical\nenvironments presents significant safety challenges. For instance, a\nhousekeeping robot lacking sufficient risk awareness might place a metal\ncontainer in a microwave, potentially causing a fire. To address these critical\nsafety concerns, comprehensive pre-deployment risk assessments are imperative.\nThis study introduces EAIRiskBench, a novel framework for automated physical\nrisk assessment in EAI scenarios. EAIRiskBench employs a multi-agent\ncooperative system that leverages various foundation models to generate safety\nguidelines, create risk-prone scenarios, make task planning, and evaluate\nsafety systematically. Utilizing this framework, we construct EAIRiskDataset,\ncomprising diverse test cases across various domains, encompassing both textual\nand visual scenarios. Our comprehensive evaluation of state-of-the-art\nfoundation models reveals alarming results: all models exhibit high task risk\nrates (TRR), with an average of 95.75% across all evaluated models. To address\nthese challenges, we further propose two prompting-based risk mitigation\nstrategies. While these strategies demonstrate some efficacy in reducing TRR,\nthe improvements are limited, still indicating substantial safety concerns.\nThis study provides the first large-scale assessment of physical risk awareness\nin EAI agents. Our findings underscore the critical need for enhanced safety\nmeasures in EAI systems and provide valuable insights for future research\ndirections in developing safer embodied artificial intelligence system."
                },
                "authors": [
                    {
                        "name": "Zihao Zhu"
                    },
                    {
                        "name": "Bingzhe Wu"
                    },
                    {
                        "name": "Zhengyou Zhang"
                    },
                    {
                        "name": "Lei Han"
                    },
                    {
                        "name": "Baoyuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Baoyuan Wu"
                },
                "author": "Baoyuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04449v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04449v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11368v1",
                "updated": "2024-10-15T07:45:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    7,
                    45,
                    18,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T07:45:18Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    7,
                    45,
                    18,
                    1,
                    289,
                    0
                ],
                "title": "Secure Stateful Aggregation: A Practical Protocol with Applications in\n  Differentially-Private Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Stateful Aggregation: A Practical Protocol with Applications in\n  Differentially-Private Federated Learning"
                },
                "summary": "Recent advances in differentially private federated learning (DPFL)\nalgorithms have found that using correlated noise across the rounds of\nfederated learning (DP-FTRL) yields provably and empirically better accuracy\nthan using independent noise (DP-SGD). While DP-SGD is well-suited to federated\nlearning with a single untrusted central server using lightweight secure\naggregation protocols, secure aggregation is not conducive to implementing\nmodern DP-FTRL techniques without assuming a trusted central server. DP-FTRL\nbased approaches have already seen widespread deployment in industry, albeit\nwith a trusted central curator who provides and applies the correlated noise.\nTo realize a fully private, single untrusted server DP-FTRL federated learning\nprotocol, we introduce secure stateful aggregation: a simple append-only data\nstructure that allows for the private storage of aggregate values and reading\nlinear functions of the aggregates. Assuming Ring Learning with Errors, we\nprovide a lightweight and scalable realization of this protocol for\nhigh-dimensional data in a new security/resource model, Federated MPC : where a\npowerful persistent server interacts with weak, ephemeral clients. We observe\nthat secure stateful aggregation suffices for realizing DP-FTRL-based private\nfederated learning: improving DPFL utility guarantees over the state of the art\nwhile maintaining privacy with an untrusted central party. Our approach has\nminimal overhead relative to existing techniques which do not yield comparable\nutility. The secure stateful aggregation primitive and the federated MPC\nparadigm may be of interest for other practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in differentially private federated learning (DPFL)\nalgorithms have found that using correlated noise across the rounds of\nfederated learning (DP-FTRL) yields provably and empirically better accuracy\nthan using independent noise (DP-SGD). While DP-SGD is well-suited to federated\nlearning with a single untrusted central server using lightweight secure\naggregation protocols, secure aggregation is not conducive to implementing\nmodern DP-FTRL techniques without assuming a trusted central server. DP-FTRL\nbased approaches have already seen widespread deployment in industry, albeit\nwith a trusted central curator who provides and applies the correlated noise.\nTo realize a fully private, single untrusted server DP-FTRL federated learning\nprotocol, we introduce secure stateful aggregation: a simple append-only data\nstructure that allows for the private storage of aggregate values and reading\nlinear functions of the aggregates. Assuming Ring Learning with Errors, we\nprovide a lightweight and scalable realization of this protocol for\nhigh-dimensional data in a new security/resource model, Federated MPC : where a\npowerful persistent server interacts with weak, ephemeral clients. We observe\nthat secure stateful aggregation suffices for realizing DP-FTRL-based private\nfederated learning: improving DPFL utility guarantees over the state of the art\nwhile maintaining privacy with an untrusted central party. Our approach has\nminimal overhead relative to existing techniques which do not yield comparable\nutility. The secure stateful aggregation primitive and the federated MPC\nparadigm may be of interest for other practical applications."
                },
                "authors": [
                    {
                        "name": "Marshall Ball"
                    },
                    {
                        "name": "James Bell-Clark"
                    },
                    {
                        "name": "Adria Gascon"
                    },
                    {
                        "name": "Peter Kairouz"
                    },
                    {
                        "name": "Sewoong Oh"
                    },
                    {
                        "name": "Zhiye Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zhiye Xie"
                },
                "author": "Zhiye Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13720v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13720v3",
                "updated": "2024-10-15T07:43:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    7,
                    43,
                    51,
                    1,
                    289,
                    0
                ],
                "published": "2024-02-21T11:31:28Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    11,
                    31,
                    28,
                    2,
                    52,
                    0
                ],
                "title": "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ouroboros: Generating Longer Drafts Phrase by Phrase for Faster\n  Speculative Decoding"
                },
                "summary": "Speculative decoding is a widely used method that accelerates the generation\nprocess of large language models (LLMs) with no compromise in model\nperformance. It achieves this goal by using an existing smaller model for\ndrafting and then employing the target LLM to verify the draft in a low-cost\nparallel manner. Under such a drafting-verification framework, drafting\nefficiency has become a bottleneck in the final speedup of speculative\ndecoding. Therefore, generating longer drafts at less cost can lead to better\ndecoding speedup. To achieve this, we introduce Ouroboros, which can generate\ndraft phrases to parallelize the drafting process and meanwhile lengthen drafts\nin a training-free manner. The experimental results on various typical text\ngeneration tasks show that Ouroboros can achieve speedups of up to $2.8\\times$\nover speculative decoding and $3.9\\times$ over vanilla decoding, without\nfine-tuning draft and target models. The source code of Ouroboros is available\nat https://github.com/thunlp/Ouroboros.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely used method that accelerates the generation\nprocess of large language models (LLMs) with no compromise in model\nperformance. It achieves this goal by using an existing smaller model for\ndrafting and then employing the target LLM to verify the draft in a low-cost\nparallel manner. Under such a drafting-verification framework, drafting\nefficiency has become a bottleneck in the final speedup of speculative\ndecoding. Therefore, generating longer drafts at less cost can lead to better\ndecoding speedup. To achieve this, we introduce Ouroboros, which can generate\ndraft phrases to parallelize the drafting process and meanwhile lengthen drafts\nin a training-free manner. The experimental results on various typical text\ngeneration tasks show that Ouroboros can achieve speedups of up to $2.8\\times$\nover speculative decoding and $3.9\\times$ over vanilla decoding, without\nfine-tuning draft and target models. The source code of Ouroboros is available\nat https://github.com/thunlp/Ouroboros."
                },
                "authors": [
                    {
                        "name": "Weilin Zhao"
                    },
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Wang Xu"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xinrong Zhang"
                    },
                    {
                        "name": "Yewei Fang"
                    },
                    {
                        "name": "Kaihuo Zhang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13720v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13720v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11366v1",
                "updated": "2024-10-15T07:41:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    7,
                    41,
                    40,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T07:41:40Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    7,
                    41,
                    40,
                    1,
                    289,
                    0
                ],
                "title": "LargePiG: Your Large Language Model is Secretly a Pointer Generator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LargePiG: Your Large Language Model is Secretly a Pointer Generator"
                },
                "summary": "Recent research on query generation has focused on using Large Language\nModels (LLMs), which despite bringing state-of-the-art performance, also\nintroduce issues with hallucinations in the generated queries. In this work, we\nintroduce relevance hallucination and factuality hallucination as a new\ntypology for hallucination problems brought by query generation based on LLMs.\nWe propose an effective way to separate content from form in LLM-generated\nqueries, which preserves the factual knowledge extracted and integrated from\nthe inputs and compiles the syntactic structure, including function words,\nusing the powerful linguistic capabilities of the LLM. Specifically, we\nintroduce a model-agnostic and training-free method that turns the Large\nLanguage Model into a Pointer-Generator (LargePiG), where the pointer attention\ndistribution leverages the LLM's inherent attention weights, and the copy\nprobability is derived from the difference between the vocabulary distribution\nof the model's high layers and the last layer. To validate the effectiveness of\nLargePiG, we constructed two datasets for assessing the hallucination problems\nin query generation, covering both document and video scenarios. Empirical\nstudies on various LLMs demonstrated the superiority of LargePiG on both\ndatasets. Additional experiments also verified that LargePiG could reduce\nhallucination in large vision language models and improve the accuracy of\ndocument-based question-answering and factuality evaluation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on query generation has focused on using Large Language\nModels (LLMs), which despite bringing state-of-the-art performance, also\nintroduce issues with hallucinations in the generated queries. In this work, we\nintroduce relevance hallucination and factuality hallucination as a new\ntypology for hallucination problems brought by query generation based on LLMs.\nWe propose an effective way to separate content from form in LLM-generated\nqueries, which preserves the factual knowledge extracted and integrated from\nthe inputs and compiles the syntactic structure, including function words,\nusing the powerful linguistic capabilities of the LLM. Specifically, we\nintroduce a model-agnostic and training-free method that turns the Large\nLanguage Model into a Pointer-Generator (LargePiG), where the pointer attention\ndistribution leverages the LLM's inherent attention weights, and the copy\nprobability is derived from the difference between the vocabulary distribution\nof the model's high layers and the last layer. To validate the effectiveness of\nLargePiG, we constructed two datasets for assessing the hallucination problems\nin query generation, covering both document and video scenarios. Empirical\nstudies on various LLMs demonstrated the superiority of LargePiG on both\ndatasets. Additional experiments also verified that LargePiG could reduce\nhallucination in large vision language models and improve the accuracy of\ndocument-based question-answering and factuality evaluation tasks."
                },
                "authors": [
                    {
                        "name": "Zhongxiang Sun"
                    },
                    {
                        "name": "Zihua Si"
                    },
                    {
                        "name": "Xiaoxue Zang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Yang Song"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]