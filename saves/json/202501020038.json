[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.21023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v1",
                "updated": "2024-12-30T15:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha RÃ¶sler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v1",
                "updated": "2024-12-29T15:42:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v1",
                "updated": "2024-12-28T14:38:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v1",
                "updated": "2024-12-27T20:47:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v1",
                "updated": "2024-12-27T04:17:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v1",
                "updated": "2024-12-26T15:45:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    }
                ],
                "author_detail": {
                    "name": "Heung-Yeung Shum"
                },
                "author": "Heung-Yeung Shum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia FermÃ¼ller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. JimÃ©nez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v1",
                "updated": "2024-12-15T21:02:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15246v1",
                "updated": "2024-12-14T06:47:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T06:47:56Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "title": "Accelerating Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Retrieval-Augmented Generation"
                },
                "summary": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded."
                },
                "authors": [
                    {
                        "name": "Derrick Quinn"
                    },
                    {
                        "name": "Mohammad Nouri"
                    },
                    {
                        "name": "Neel Patel"
                    },
                    {
                        "name": "John Salihu"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Sukhan Lee"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Mohammad Alian"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alian"
                },
                "author": "Mohammad Alian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas KÃ¶stler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore ThieÃen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v1",
                "updated": "2024-12-13T02:26:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_comment": "Conference submission for IPCCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v2",
                "updated": "2024-12-12T14:43:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter RichtÃ¡rik"
                    }
                ],
                "author_detail": {
                    "name": "Peter RichtÃ¡rik"
                },
                "author": "Peter RichtÃ¡rik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v3",
                "updated": "2024-12-12T12:03:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    3,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v2",
                "updated": "2024-12-12T10:07:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    7,
                    17,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v1",
                "updated": "2024-12-12T08:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09036v1",
                "updated": "2024-12-12T07:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:52:56Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty"
                },
                "summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v3",
                "updated": "2024-12-12T03:21:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    21,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_doi": "10.1145/3669940.3707265",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3669940.3707265",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08890v1",
                "updated": "2024-12-12T03:00:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:00:29Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries"
                },
                "summary": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v3",
                "updated": "2024-12-11T12:03:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    3,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Pushing the Limits of In-Network Caching for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of In-Network Caching for Key-Value Stores"
                },
                "summary": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "arxiv_comment": "To be appeared in USENIX NSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08176v1",
                "updated": "2024-12-11T08:07:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:07:12Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "title": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning"
                },
                "summary": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner"
                },
                "authors": [
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Zhaohong Huang"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08063v1",
                "updated": "2024-12-11T03:15:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:15:49Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "title": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates."
                },
                "authors": [
                    {
                        "name": "Zhanming Guan"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Jierui Liu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Dexin Liu"
                    },
                    {
                        "name": "Ningyuan Sun"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Wenchao Li"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Hang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhu"
                },
                "author": "Hang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12952v2",
                "updated": "2024-12-10T22:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    53,
                    16,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-19T17:54:34Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    54,
                    34,
                    1,
                    79,
                    0
                ],
                "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models"
                },
                "summary": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements."
                },
                "authors": [
                    {
                        "name": "Elaine Sui"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v1",
                "updated": "2024-12-10T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Causal Video Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Causal Video Generators"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v1",
                "updated": "2024-12-10T18:50:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian PÃ¶ppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v1",
                "updated": "2024-12-10T18:13:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v4",
                "updated": "2024-12-10T12:45:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    45,
                    31,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v3",
                "updated": "2024-12-09T01:39:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "arxiv_doi": "10.1145/3706630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05896v1",
                "updated": "2024-12-08T11:32:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T11:32:08Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "title": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference"
                },
                "summary": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x."
                },
                "authors": [
                    {
                        "name": "Weizhuo Li"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.21200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21200v1",
                "updated": "2024-12-30T18:59:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    59,
                    6,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:59:06Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    59,
                    6,
                    0,
                    365,
                    0
                ],
                "title": "Distributed Mixture-of-Agents for Edge Inference with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Mixture-of-Agents for Edge Inference with Large Language\n  Models"
                },
                "summary": "Mixture-of-Agents (MoA) has recently been proposed as a method to enhance\nperformance of large language models (LLMs), enabling multiple individual LLMs\nto work together for collaborative inference. This collaborative approach\nresults in improved responses to user prompts compared to relying on a single\nLLM. In this paper, we consider such an MoA architecture in a distributed\nsetting, where LLMs operate on individual edge devices, each uniquely\nassociated with a user and equipped with its own distributed computing power.\nThese devices exchange information using decentralized gossip algorithms,\nallowing different device nodes to talk without the supervision of a\ncentralized server. In the considered setup, different users have their own LLM\nmodels to address user prompts. Additionally, the devices gossip either their\nown user-specific prompts or augmented prompts to generate more refined answers\nto certain queries. User prompts are temporarily stored in the device queues\nwhen their corresponding LLMs are busy. Given the memory limitations of edge\ndevices, it is crucial to ensure that the average queue sizes in the system\nremain bounded. In this paper, we address this by theoretically calculating the\nqueuing stability conditions for the device queues under reasonable\nassumptions, which we validate experimentally as well. Further, we demonstrate\nthrough experiments, leveraging open-source LLMs for the implementation of\ndistributed MoA, that certain MoA configurations produce higher-quality\nresponses compared to others, as evaluated on AlpacaEval 2.0 benchmark. The\nimplementation is available at:\nhttps://github.com/purbeshmitra/distributed_moa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Agents (MoA) has recently been proposed as a method to enhance\nperformance of large language models (LLMs), enabling multiple individual LLMs\nto work together for collaborative inference. This collaborative approach\nresults in improved responses to user prompts compared to relying on a single\nLLM. In this paper, we consider such an MoA architecture in a distributed\nsetting, where LLMs operate on individual edge devices, each uniquely\nassociated with a user and equipped with its own distributed computing power.\nThese devices exchange information using decentralized gossip algorithms,\nallowing different device nodes to talk without the supervision of a\ncentralized server. In the considered setup, different users have their own LLM\nmodels to address user prompts. Additionally, the devices gossip either their\nown user-specific prompts or augmented prompts to generate more refined answers\nto certain queries. User prompts are temporarily stored in the device queues\nwhen their corresponding LLMs are busy. Given the memory limitations of edge\ndevices, it is crucial to ensure that the average queue sizes in the system\nremain bounded. In this paper, we address this by theoretically calculating the\nqueuing stability conditions for the device queues under reasonable\nassumptions, which we validate experimentally as well. Further, we demonstrate\nthrough experiments, leveraging open-source LLMs for the implementation of\ndistributed MoA, that certain MoA configurations produce higher-quality\nresponses compared to others, as evaluated on AlpacaEval 2.0 benchmark. The\nimplementation is available at:\nhttps://github.com/purbeshmitra/distributed_moa."
                },
                "authors": [
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Priyanka Kaswan"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21199v1",
                "updated": "2024-12-30T18:58:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    58,
                    58,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:58:58Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    58,
                    58,
                    0,
                    365,
                    0
                ],
                "title": "HumanEval Pro and MBPP Pro: Evaluating Large Language Models on\n  Self-invoking Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumanEval Pro and MBPP Pro: Evaluating Large Language Models on\n  Self-invoking Code Generation"
                },
                "summary": "We introduce self-invoking code generation, a new task designed to evaluate\nthe progressive reasoning and problem-solving capabilities of LLMs. In this\ntask, models are presented with a base problem and a related, more complex\nproblem. They must solve the base problem and then utilize its solution to\naddress the more complex one. This work features three key contributions.\nFirst, we propose a general recipe for generating more challenging versions of\nexisting benchmarks, resulting in three new benchmarks: HumanEval Pro, MBPP\nPro, and BigCodeBench-Lite Pro, specifically designed to assess LLMs on\nself-invoking code generation. Second, from the analysis of experimental\nresults over twenty LLMs on our benchmarks, we have two important observations:\n(i) Most LLMs excel in traditional code generation benchmarks like HumanEval\nand MBPP, but their performance declines on self-invoking tasks. For example,\no1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro.\n(ii) On self-invoking code generation task, the instruction-tuned models\ndemonstrate only marginal improvements compared to the base models. Third, we\ndisclose the types of failure modes that exist in our evaluation results. All\nthese results underscore the need for further advancements in self-invoking\ncode generation tasks and provide a new direction for future research on\nenhancing LLMs' code reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce self-invoking code generation, a new task designed to evaluate\nthe progressive reasoning and problem-solving capabilities of LLMs. In this\ntask, models are presented with a base problem and a related, more complex\nproblem. They must solve the base problem and then utilize its solution to\naddress the more complex one. This work features three key contributions.\nFirst, we propose a general recipe for generating more challenging versions of\nexisting benchmarks, resulting in three new benchmarks: HumanEval Pro, MBPP\nPro, and BigCodeBench-Lite Pro, specifically designed to assess LLMs on\nself-invoking code generation. Second, from the analysis of experimental\nresults over twenty LLMs on our benchmarks, we have two important observations:\n(i) Most LLMs excel in traditional code generation benchmarks like HumanEval\nand MBPP, but their performance declines on self-invoking tasks. For example,\no1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro.\n(ii) On self-invoking code generation task, the instruction-tuned models\ndemonstrate only marginal improvements compared to the base models. Third, we\ndisclose the types of failure modes that exist in our evaluation results. All\nthese results underscore the need for further advancements in self-invoking\ncode generation tasks and provide a new direction for future research on\nenhancing LLMs' code reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Zhaojian Yu"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Xiao-Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ping Zhang"
                },
                "author": "Xiao-Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21187v1",
                "updated": "2024-12-30T18:55:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    55,
                    12,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:55:12Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    55,
                    12,
                    0,
                    365,
                    0
                ],
                "title": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs"
                },
                "summary": "The remarkable performance of models like the OpenAI o1 can be attributed to\ntheir ability to emulate human-like long-time thinking during inference. These\nmodels employ extended chain-of-thought (CoT) processes, exploring multiple\nstrategies to enhance problem-solving capabilities. However, a critical\nquestion remains: How to intelligently and efficiently scale computational\nresources during testing. This paper presents the first comprehensive study on\nthe prevalent issue of overthinking in these models, where excessive\ncomputational resources are allocated for simple problems with minimal benefit.\nWe introduce novel efficiency metrics from both outcome and process\nperspectives to evaluate the rational use of computational resources by o1-like\nmodels. Using a self-training paradigm, we propose strategies to mitigate\noverthinking, streamlining reasoning processes without compromising accuracy.\nExperimental results show that our approach successfully reduces computational\noverhead while preserving model performance across a range of testsets with\nvarying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable performance of models like the OpenAI o1 can be attributed to\ntheir ability to emulate human-like long-time thinking during inference. These\nmodels employ extended chain-of-thought (CoT) processes, exploring multiple\nstrategies to enhance problem-solving capabilities. However, a critical\nquestion remains: How to intelligently and efficiently scale computational\nresources during testing. This paper presents the first comprehensive study on\nthe prevalent issue of overthinking in these models, where excessive\ncomputational resources are allocated for simple problems with minimal benefit.\nWe introduce novel efficiency metrics from both outcome and process\nperspectives to evaluate the rational use of computational resources by o1-like\nmodels. Using a self-training paradigm, we propose strategies to mitigate\noverthinking, streamlining reasoning processes without compromising accuracy.\nExperimental results show that our approach successfully reduces computational\noverhead while preserving model performance across a range of testsets with\nvarying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME."
                },
                "authors": [
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Jianhui Pang"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Mengfei Zhou"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21154v1",
                "updated": "2024-12-30T18:33:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    33,
                    28,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:33:28Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    33,
                    28,
                    0,
                    365,
                    0
                ],
                "title": "Aviary: training language agents on challenging scientific tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aviary: training language agents on challenging scientific tasks"
                },
                "summary": "Solving complex real-world tasks requires cycles of actions and observations.\nThis is particularly true in science, where tasks require many cycles of\nanalysis, tool use, and experimentation. Language agents are promising for\nautomating intellectual tasks in science because they can interact with tools\nvia natural language or code. Yet their flexibility creates conceptual and\npractical challenges for software implementations, since agents may comprise\nnon-standard components such as internal reasoning, planning, tool usage, as\nwell as the inherent stochasticity of temperature-sampled language models.\nHere, we introduce Aviary, an extensible gymnasium for language agents. We\nformalize agents as policies solving language-grounded partially observable\nMarkov decision processes, which we term language decision processes. We then\nimplement five environments, including three challenging scientific\nenvironments: (1) manipulating DNA constructs for molecular cloning, (2)\nanswering research questions by accessing scientific literature, and (3)\nengineering protein stability. These environments were selected for their focus\non multi-step reasoning and their relevance to contemporary biology research.\nFinally, with online training and scaling inference-time compute, we show that\nlanguage agents backed by open-source, non-frontier LLMs can match and exceed\nboth frontier LLM agents and human experts on multiple tasks at up to 100x\nlower inference cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving complex real-world tasks requires cycles of actions and observations.\nThis is particularly true in science, where tasks require many cycles of\nanalysis, tool use, and experimentation. Language agents are promising for\nautomating intellectual tasks in science because they can interact with tools\nvia natural language or code. Yet their flexibility creates conceptual and\npractical challenges for software implementations, since agents may comprise\nnon-standard components such as internal reasoning, planning, tool usage, as\nwell as the inherent stochasticity of temperature-sampled language models.\nHere, we introduce Aviary, an extensible gymnasium for language agents. We\nformalize agents as policies solving language-grounded partially observable\nMarkov decision processes, which we term language decision processes. We then\nimplement five environments, including three challenging scientific\nenvironments: (1) manipulating DNA constructs for molecular cloning, (2)\nanswering research questions by accessing scientific literature, and (3)\nengineering protein stability. These environments were selected for their focus\non multi-step reasoning and their relevance to contemporary biology research.\nFinally, with online training and scaling inference-time compute, we show that\nlanguage agents backed by open-source, non-frontier LLMs can match and exceed\nboth frontier LLM agents and human experts on multiple tasks at up to 100x\nlower inference cost."
                },
                "authors": [
                    {
                        "name": "Siddharth Narayanan"
                    },
                    {
                        "name": "James D. Braza"
                    },
                    {
                        "name": "Ryan-Rhys Griffiths"
                    },
                    {
                        "name": "Manu Ponnapati"
                    },
                    {
                        "name": "Albert Bou"
                    },
                    {
                        "name": "Jon Laurent"
                    },
                    {
                        "name": "Ori Kabeli"
                    },
                    {
                        "name": "Geemi Wellawatte"
                    },
                    {
                        "name": "Sam Cox"
                    },
                    {
                        "name": "Samuel G. Rodriques"
                    },
                    {
                        "name": "Andrew D. White"
                    }
                ],
                "author_detail": {
                    "name": "Andrew D. White"
                },
                "author": "Andrew D. White",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21147v1",
                "updated": "2024-12-30T18:26:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    26,
                    46,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:26:46Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    26,
                    46,
                    0,
                    365,
                    0
                ],
                "title": "Using AI for Efficient Statistical Inference of Lattice Correlators\n  Across Mass Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using AI for Efficient Statistical Inference of Lattice Correlators\n  Across Mass Parameters"
                },
                "summary": "Lattice QCD is notorious for its computational expense. Modern lattice\nsimulations require large-scale computational resources to handle the large\nnumber of Dirac operator inversions used to construct correlation functions.\nMachine learning (ML) techniques that can increase, at the analysis level, the\ninformation inferred from the correlation functions would therefore be\nbeneficial. We apply supervised learning to infer two-point lattice correlation\nfunctions at different target masses. Our work proposes a new method for\nseparating data into training and bias correction subsets for efficient\nuncertainty estimation. We also benchmark our ML models against a simple ratio\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice QCD is notorious for its computational expense. Modern lattice\nsimulations require large-scale computational resources to handle the large\nnumber of Dirac operator inversions used to construct correlation functions.\nMachine learning (ML) techniques that can increase, at the analysis level, the\ninformation inferred from the correlation functions would therefore be\nbeneficial. We apply supervised learning to infer two-point lattice correlation\nfunctions at different target masses. Our work proposes a new method for\nseparating data into training and bias correction subsets for efficient\nuncertainty estimation. We also benchmark our ML models against a simple ratio\nmethod."
                },
                "authors": [
                    {
                        "name": "Octavio Vega"
                    },
                    {
                        "name": "Andrew Lytle"
                    },
                    {
                        "name": "Jiayu Shen"
                    },
                    {
                        "name": "Aida X. El-Khadra"
                    }
                ],
                "author_detail": {
                    "name": "Aida X. El-Khadra"
                },
                "author": "Aida X. El-Khadra",
                "arxiv_comment": "7 pages, 3 figures. Contribution to the 41st International Symposium\n  on Lattice Field Theory (LATTICE2024), 28 July - 3 August 2024, Liverpool, UK",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-lat",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-lat",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05093v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05093v3",
                "updated": "2024-12-30T18:21:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    21,
                    8,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-09T14:34:32Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    34,
                    32,
                    4,
                    222,
                    0
                ],
                "title": "Order Matters in Hallucination: Reasoning Order as Benchmark and\n  Reflexive Prompting for Large-Language-Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Order Matters in Hallucination: Reasoning Order as Benchmark and\n  Reflexive Prompting for Large-Language-Models"
                },
                "summary": "Large language models (LLMs) have generated significant attention since their\ninception, finding applications across various academic and industrial domains.\nHowever, these models often suffer from the \"hallucination problem\", where\noutputs, though grammatically and logically coherent, lack factual accuracy or\nare entirely fabricated. A particularly troubling issue discovered and widely\ndiscussed recently is the numerical comparison error where multiple LLMs\nincorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs\ngenerate answers and reasoning impacts their consistency. Specifically, results\nvary significantly when an LLM generates an answer first and then provides the\nreasoning versus generating the reasoning process first and then the\nconclusion. Inspired by this, we propose a new benchmark method for assessing\nLLM consistency: comparing responses generated through these two different\napproaches. This benchmark effectively identifies instances where LLMs\nfabricate answers and subsequently generate justifications. Furthermore, we\nintroduce a novel and straightforward prompt strategy designed to mitigate this\nissue. Experimental results demonstrate that this strategy improves performance\nacross various LLMs compared to direct questioning. This work not only sheds\nlight on a critical flaw in LLMs but also offers a practical solution to\nenhance their reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have generated significant attention since their\ninception, finding applications across various academic and industrial domains.\nHowever, these models often suffer from the \"hallucination problem\", where\noutputs, though grammatically and logically coherent, lack factual accuracy or\nare entirely fabricated. A particularly troubling issue discovered and widely\ndiscussed recently is the numerical comparison error where multiple LLMs\nincorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs\ngenerate answers and reasoning impacts their consistency. Specifically, results\nvary significantly when an LLM generates an answer first and then provides the\nreasoning versus generating the reasoning process first and then the\nconclusion. Inspired by this, we propose a new benchmark method for assessing\nLLM consistency: comparing responses generated through these two different\napproaches. This benchmark effectively identifies instances where LLMs\nfabricate answers and subsequently generate justifications. Furthermore, we\nintroduce a novel and straightforward prompt strategy designed to mitigate this\nissue. Experimental results demonstrate that this strategy improves performance\nacross various LLMs compared to direct questioning. This work not only sheds\nlight on a critical flaw in LLMs but also offers a practical solution to\nenhance their reliability."
                },
                "authors": [
                    {
                        "name": "Zikai Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zikai Xie"
                },
                "author": "Zikai Xie",
                "arxiv_comment": "8 pages, submitted to ACL22025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05093v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05093v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21140v1",
                "updated": "2024-12-30T18:15:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    15,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:15:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    15,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Facilitating large language model Russian adaptation with Learned\n  Embedding Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facilitating large language model Russian adaptation with Learned\n  Embedding Propagation"
                },
                "summary": "Rapid advancements of large language model (LLM) technologies led to the\nintroduction of powerful open-source instruction-tuned LLMs that have the same\ntext generation quality as the state-of-the-art counterparts such as GPT-4.\nWhile the emergence of such models accelerates the adoption of LLM technologies\nin sensitive-information environments the authors of such models don not\ndisclose the training data necessary for replication of the results thus making\nthe achievements model-exclusive. Since those open-source models are also\nmultilingual this in turn reduces the benefits of training a language specific\nLLMs as improved inference computation efficiency becomes the only guaranteed\nadvantage of such costly procedure. More cost-efficient options such as\nvocabulary extension and subsequent continued pre-training are also inhibited\nby the lack of access to high-quality instruction-tuning data since it is the\nmajor factor behind the resulting LLM task-solving capabilities. To address the\nlimitations and cut the costs of the language adaptation pipeline we propose\nLearned Embedding Propagation (LEP). Unlike existing approaches our method has\nlower training data size requirements due to minimal impact on existing LLM\nknowledge which we reinforce using novel ad-hoc embedding propagation procedure\nthat allows to skip the instruction-tuning step and instead implant the new\nlanguage knowledge directly into any existing instruct-tuned variant. We\nevaluated four Russian vocabulary adaptations for LLaMa-3-8B and Mistral-7B,\nshowing that LEP is competitive with traditional instruction-tuning methods,\nachieving performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct, with\nfurther improvements via self-calibration and continued tuning enhancing\ntask-solving capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements of large language model (LLM) technologies led to the\nintroduction of powerful open-source instruction-tuned LLMs that have the same\ntext generation quality as the state-of-the-art counterparts such as GPT-4.\nWhile the emergence of such models accelerates the adoption of LLM technologies\nin sensitive-information environments the authors of such models don not\ndisclose the training data necessary for replication of the results thus making\nthe achievements model-exclusive. Since those open-source models are also\nmultilingual this in turn reduces the benefits of training a language specific\nLLMs as improved inference computation efficiency becomes the only guaranteed\nadvantage of such costly procedure. More cost-efficient options such as\nvocabulary extension and subsequent continued pre-training are also inhibited\nby the lack of access to high-quality instruction-tuning data since it is the\nmajor factor behind the resulting LLM task-solving capabilities. To address the\nlimitations and cut the costs of the language adaptation pipeline we propose\nLearned Embedding Propagation (LEP). Unlike existing approaches our method has\nlower training data size requirements due to minimal impact on existing LLM\nknowledge which we reinforce using novel ad-hoc embedding propagation procedure\nthat allows to skip the instruction-tuning step and instead implant the new\nlanguage knowledge directly into any existing instruct-tuned variant. We\nevaluated four Russian vocabulary adaptations for LLaMa-3-8B and Mistral-7B,\nshowing that LEP is competitive with traditional instruction-tuning methods,\nachieving performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct, with\nfurther improvements via self-calibration and continued tuning enhancing\ntask-solving capabilities."
                },
                "authors": [
                    {
                        "name": "Mikhail Tikhomirov"
                    },
                    {
                        "name": "Daniil Chernyshev"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Chernyshev"
                },
                "author": "Daniil Chernyshev",
                "arxiv_comment": "Preprint version of an article published in the Journal of Language\n  and Education. Copyright held by the owner/author(s). Publication rights\n  licensed to the Journal of Language and Education",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21139v1",
                "updated": "2024-12-30T18:15:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    15,
                    39,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:15:39Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    15,
                    39,
                    0,
                    365,
                    0
                ],
                "title": "Training Software Engineering Agents and Verifiers with SWE-Gym",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Software Engineering Agents and Verifiers with SWE-Gym"
                },
                "summary": "We present SWE-Gym, the first environment for training real-world software\nengineering (SWE) agents. SWE-Gym contains 2,438 real-world Python task\ninstances, each comprising a codebase with an executable runtime environment,\nunit tests, and a task specified in natural language. We use SWE-Gym to train\nlanguage model based SWE agents , achieving up to 19% absolute gains in resolve\nrate on the popular SWE-Bench Verified and Lite test sets. We also experiment\nwith inference-time scaling through verifiers trained on agent trajectories\nsampled from SWE-Gym. When combined with our fine-tuned SWE agents, we achieve\n32.0% and 26.0% on SWE-Bench Verified and Lite, respectively, reflecting a new\nstate-of-the-art for open-weight SWE agents. To facilitate further research, we\npublicly release SWE-Gym, models, and agent trajectories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SWE-Gym, the first environment for training real-world software\nengineering (SWE) agents. SWE-Gym contains 2,438 real-world Python task\ninstances, each comprising a codebase with an executable runtime environment,\nunit tests, and a task specified in natural language. We use SWE-Gym to train\nlanguage model based SWE agents , achieving up to 19% absolute gains in resolve\nrate on the popular SWE-Bench Verified and Lite test sets. We also experiment\nwith inference-time scaling through verifiers trained on agent trajectories\nsampled from SWE-Gym. When combined with our fine-tuned SWE agents, we achieve\n32.0% and 26.0% on SWE-Bench Verified and Lite, respectively, reflecting a new\nstate-of-the-art for open-weight SWE agents. To facilitate further research, we\npublicly release SWE-Gym, models, and agent trajectories."
                },
                "authors": [
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Xingyao Wang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Alane Suhr"
                    },
                    {
                        "name": "Yizhe Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhe Zhang"
                },
                "author": "Yizhe Zhang",
                "arxiv_comment": "Code at https://github.com/SWE-Gym/SWE-Gym",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21123v1",
                "updated": "2024-12-30T17:52:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    52,
                    2,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T17:52:02Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    52,
                    2,
                    0,
                    365,
                    0
                ],
                "title": "ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language\n  Modeling Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language\n  Modeling Exploitation"
                },
                "summary": "As large language models (LLMs) increasingly depend on web-scraped datasets,\nconcerns over unauthorized use of copyrighted or personal content for training\nhave intensified. Despite regulations such as the General Data Protection\nRegulation (GDPR), data owners still have limited control over the use of their\ncontent in model training. To address this, we propose ExpShield, a proactive\nself-guard mechanism that empowers content owners to embed invisible\nperturbations into their text, limiting data misuse in LLMs training without\naffecting readability. This preemptive approach enables data owners to protect\nsensitive content directly, without relying on a third-party to perform\ndefense. Starting from the random perturbation, we demonstrate the rationale\nfor using perturbation to conceal protected content. We further enhance the\nefficiency by identifying memorization triggers and creating pitfalls to\ndiverge the model memorization in a more focused way. To validate our defense's\neffectiveness, we propose a novel metric of instance exploitation which\ncaptures the individual risk raised by model training. The experimental results\nvalidate the effectiveness of our approach as the MIA AUC decreases from 0.95\nto 0.55, and instance exploitation approaches zero. This suggests that the\nindividual risk does not increase after training, underscoring the significance\nof proactive defenses in protecting copyrighted data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly depend on web-scraped datasets,\nconcerns over unauthorized use of copyrighted or personal content for training\nhave intensified. Despite regulations such as the General Data Protection\nRegulation (GDPR), data owners still have limited control over the use of their\ncontent in model training. To address this, we propose ExpShield, a proactive\nself-guard mechanism that empowers content owners to embed invisible\nperturbations into their text, limiting data misuse in LLMs training without\naffecting readability. This preemptive approach enables data owners to protect\nsensitive content directly, without relying on a third-party to perform\ndefense. Starting from the random perturbation, we demonstrate the rationale\nfor using perturbation to conceal protected content. We further enhance the\nefficiency by identifying memorization triggers and creating pitfalls to\ndiverge the model memorization in a more focused way. To validate our defense's\neffectiveness, we propose a novel metric of instance exploitation which\ncaptures the individual risk raised by model training. The experimental results\nvalidate the effectiveness of our approach as the MIA AUC decreases from 0.95\nto 0.55, and instance exploitation approaches zero. This suggests that the\nindividual risk does not increase after training, underscoring the significance\nof proactive defenses in protecting copyrighted data."
                },
                "authors": [
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Toan Tran"
                    },
                    {
                        "name": "Tianhao Wang"
                    },
                    {
                        "name": "Hongsheng Hu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Li Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiong"
                },
                "author": "Li Xiong",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21102v1",
                "updated": "2024-12-30T17:25:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    25,
                    58,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T17:25:58Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    25,
                    58,
                    0,
                    365,
                    0
                ],
                "title": "Exploring and Controlling Diversity in LLM-Agent Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring and Controlling Diversity in LLM-Agent Conversation"
                },
                "summary": "Diversity is a critical aspect of multi-agent communication. In this paper,\nwe focus on controlling and exploring diversity in the context of open-domain\nmulti-agent conversations, particularly for world simulation applications. We\npropose Adaptive Prompt Pruning (APP), a novel method that dynamically adjusts\nthe content of the utterance generation prompt to control diversity using a\nsingle parameter, lambda. Through extensive experiments, we show that APP\neffectively controls the output diversity across models and datasets, with\npruning more information leading to more diverse output. We comprehensively\nanalyze the relationship between prompt content and conversational diversity.\nOur findings reveal that information from all components of the prompt\ngenerally constrains the diversity of the output, with the Memory block\nexerting the most significant influence. APP is compatible with established\ntechniques like temperature sampling and top-p sampling, providing a versatile\ntool for diversity management. To address the trade-offs of increased\ndiversity, such as inconsistencies with omitted information, we incorporate a\npost-generation correction step, which effectively balances diversity\nenhancement with output consistency. Additionally, we examine how prompt\nstructure, including component order and length, impacts diversity. This study\naddresses key questions surrounding diversity in multi-agent world simulation,\noffering insights into its control, influencing factors, and associated\ntrade-offs. Our contributions lay the foundation for systematically engineering\ndiversity in LLM-based multi-agent collaborations, advancing their\neffectiveness in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity is a critical aspect of multi-agent communication. In this paper,\nwe focus on controlling and exploring diversity in the context of open-domain\nmulti-agent conversations, particularly for world simulation applications. We\npropose Adaptive Prompt Pruning (APP), a novel method that dynamically adjusts\nthe content of the utterance generation prompt to control diversity using a\nsingle parameter, lambda. Through extensive experiments, we show that APP\neffectively controls the output diversity across models and datasets, with\npruning more information leading to more diverse output. We comprehensively\nanalyze the relationship between prompt content and conversational diversity.\nOur findings reveal that information from all components of the prompt\ngenerally constrains the diversity of the output, with the Memory block\nexerting the most significant influence. APP is compatible with established\ntechniques like temperature sampling and top-p sampling, providing a versatile\ntool for diversity management. To address the trade-offs of increased\ndiversity, such as inconsistencies with omitted information, we incorporate a\npost-generation correction step, which effectively balances diversity\nenhancement with output consistency. Additionally, we examine how prompt\nstructure, including component order and length, impacts diversity. This study\naddresses key questions surrounding diversity in multi-agent world simulation,\noffering insights into its control, influencing factors, and associated\ntrade-offs. Our contributions lay the foundation for systematically engineering\ndiversity in LLM-based multi-agent collaborations, advancing their\neffectiveness in real-world applications."
                },
                "authors": [
                    {
                        "name": "KuanChao Chu"
                    },
                    {
                        "name": "Yi-Pei Chen"
                    },
                    {
                        "name": "Hideki Nakayama"
                    }
                ],
                "author_detail": {
                    "name": "Hideki Nakayama"
                },
                "author": "Hideki Nakayama",
                "arxiv_comment": "Accepted for the AAAI 2025 Workshop on Advancing LLM-Based\n  Multi-Agent Collaboration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21079v1",
                "updated": "2024-12-30T16:56:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    56,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T16:56:44Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    56,
                    44,
                    0,
                    365,
                    0
                ],
                "title": "Edicho: Consistent Image Editing in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edicho: Consistent Image Editing in the Wild"
                },
                "summary": "As a verified need, consistent editing across in-the-wild images remains a\ntechnical challenge arising from various unmanageable factors, like object\nposes, lighting conditions, and photography environments. Edicho steps in with\na training-free solution based on diffusion models, featuring a fundamental\ndesign principle of using explicit image correspondence to direct editing.\nSpecifically, the key components include an attention manipulation module and a\ncarefully refined classifier-free guidance (CFG) denoising strategy, both of\nwhich take into account the pre-estimated correspondence. Such an\ninference-time algorithm enjoys a plug-and-play nature and is compatible to\nmost diffusion-based editing methods, such as ControlNet and BrushNet.\nExtensive results demonstrate the efficacy of Edicho in consistent cross-image\nediting under diverse settings. We will release the code to facilitate future\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a verified need, consistent editing across in-the-wild images remains a\ntechnical challenge arising from various unmanageable factors, like object\nposes, lighting conditions, and photography environments. Edicho steps in with\na training-free solution based on diffusion models, featuring a fundamental\ndesign principle of using explicit image correspondence to direct editing.\nSpecifically, the key components include an attention manipulation module and a\ncarefully refined classifier-free guidance (CFG) denoising strategy, both of\nwhich take into account the pre-estimated correspondence. Such an\ninference-time algorithm enjoys a plug-and-play nature and is compatible to\nmost diffusion-based editing methods, such as ControlNet and BrushNet.\nExtensive results demonstrate the efficacy of Edicho in consistent cross-image\nediting under diverse settings. We will release the code to facilitate future\nstudies."
                },
                "authors": [
                    {
                        "name": "Qingyan Bai"
                    },
                    {
                        "name": "Hao Ouyang"
                    },
                    {
                        "name": "Yinghao Xu"
                    },
                    {
                        "name": "Qiuyu Wang"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Ka Leong Cheng"
                    },
                    {
                        "name": "Yujun Shen"
                    },
                    {
                        "name": "Qifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qifeng Chen"
                },
                "author": "Qifeng Chen",
                "arxiv_comment": "Project page: https://github.com/EzioBy/edicho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09807v2",
                "updated": "2024-12-30T16:45:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    45,
                    50,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-13T02:48:36Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    48,
                    36,
                    4,
                    348,
                    0
                ],
                "title": "LLM Distillation for Efficient Few-Shot Multiple Choice Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Distillation for Efficient Few-Shot Multiple Choice Question\n  Answering"
                },
                "summary": "Multiple Choice Question Answering (MCQA) is an important problem with\nnumerous real-world applications, such as medicine, law, and education. The\nhigh cost of building MCQA datasets makes few-shot learning pivotal in this\ndomain. While Large Language Models (LLMs) can enable few-shot learning, their\ndirect application in real-world scenarios is often hindered by their high\ncomputational cost. To address this challenge, we propose a simple yet\neffective approach that uses LLMs for data generation and scoring. Our approach\nutilizes LLMs to create MCQA data which contains questions and choices, and to\nassign probability scores to the generated choices. We then use the generated\ndata and LLM-assigned scores to finetune a smaller and more efficient\nencoder-only model, DeBERTa-v3-base by leveraging distillation loss. Extensive\nexperiments on the Massive Multitask Language Understanding (MMLU) benchmark\ndemonstrate that our method improves accuracy from 28.9% to 39.3%, representing\na gain of over 10% compared to a baseline finetuned directly on 5-shot\nexamples. This shows the effectiveness of LLM-driven data generation and\nknowledge distillation for few-shot MCQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple Choice Question Answering (MCQA) is an important problem with\nnumerous real-world applications, such as medicine, law, and education. The\nhigh cost of building MCQA datasets makes few-shot learning pivotal in this\ndomain. While Large Language Models (LLMs) can enable few-shot learning, their\ndirect application in real-world scenarios is often hindered by their high\ncomputational cost. To address this challenge, we propose a simple yet\neffective approach that uses LLMs for data generation and scoring. Our approach\nutilizes LLMs to create MCQA data which contains questions and choices, and to\nassign probability scores to the generated choices. We then use the generated\ndata and LLM-assigned scores to finetune a smaller and more efficient\nencoder-only model, DeBERTa-v3-base by leveraging distillation loss. Extensive\nexperiments on the Massive Multitask Language Understanding (MMLU) benchmark\ndemonstrate that our method improves accuracy from 28.9% to 39.3%, representing\na gain of over 10% compared to a baseline finetuned directly on 5-shot\nexamples. This shows the effectiveness of LLM-driven data generation and\nknowledge distillation for few-shot MCQA."
                },
                "authors": [
                    {
                        "name": "Patrick Sutanto"
                    },
                    {
                        "name": "Joan Santoso"
                    },
                    {
                        "name": "Esther Irawati Setiawan"
                    },
                    {
                        "name": "Aji Prasetya Wibawa"
                    }
                ],
                "author_detail": {
                    "name": "Aji Prasetya Wibawa"
                },
                "author": "Aji Prasetya Wibawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21069v1",
                "updated": "2024-12-30T16:37:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    37,
                    17,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T16:37:17Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    37,
                    17,
                    0,
                    365,
                    0
                ],
                "title": "Privacy-Aware Multi-Device Cooperative Edge Inference with Distributed\n  Resource Bidding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Aware Multi-Device Cooperative Edge Inference with Distributed\n  Resource Bidding"
                },
                "summary": "Mobile edge computing (MEC) has empowered mobile devices (MDs) in supporting\nartificial intelligence (AI) applications through collaborative efforts with\nproximal MEC servers. Unfortunately, despite the great promise of device-edge\ncooperative AI inference, data privacy becomes an increasing concern. In this\npaper, we develop a privacy-aware multi-device cooperative edge inference\nsystem for classification tasks, which integrates a distributed bidding\nmechanism for the MEC server's computational resources. Intermediate feature\ncompression is adopted as a principled approach to minimize data privacy\nleakage. To determine the bidding values and feature compression ratios in a\ndistributed fashion, we formulate a decentralized partially observable Markov\ndecision process (DEC-POMDP) model, for which, a multi-agent deep deterministic\npolicy gradient (MADDPG)-based algorithm is developed. Simulation results\ndemonstrate the effectiveness of the proposed algorithm in privacy-preserving\ncooperative edge inference. Specifically, given a sufficient level of data\nprivacy protection, the proposed algorithm achieves 0.31-0.95% improvements in\nclassification accuracy compared to the approach being agnostic to the wireless\nchannel conditions. The performance is further enhanced by 1.54-1.67% by\nconsidering the difficulties of inference data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge computing (MEC) has empowered mobile devices (MDs) in supporting\nartificial intelligence (AI) applications through collaborative efforts with\nproximal MEC servers. Unfortunately, despite the great promise of device-edge\ncooperative AI inference, data privacy becomes an increasing concern. In this\npaper, we develop a privacy-aware multi-device cooperative edge inference\nsystem for classification tasks, which integrates a distributed bidding\nmechanism for the MEC server's computational resources. Intermediate feature\ncompression is adopted as a principled approach to minimize data privacy\nleakage. To determine the bidding values and feature compression ratios in a\ndistributed fashion, we formulate a decentralized partially observable Markov\ndecision process (DEC-POMDP) model, for which, a multi-agent deep deterministic\npolicy gradient (MADDPG)-based algorithm is developed. Simulation results\ndemonstrate the effectiveness of the proposed algorithm in privacy-preserving\ncooperative edge inference. Specifically, given a sufficient level of data\nprivacy protection, the proposed algorithm achieves 0.31-0.95% improvements in\nclassification accuracy compared to the approach being agnostic to the wireless\nchannel conditions. The performance is further enhanced by 1.54-1.67% by\nconsidering the difficulties of inference data."
                },
                "authors": [
                    {
                        "name": "Wenhao Zhuang"
                    },
                    {
                        "name": "Yuyi Mao"
                    }
                ],
                "author_detail": {
                    "name": "Yuyi Mao"
                },
                "author": "Yuyi Mao",
                "arxiv_comment": "This article was submitted to IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21065v1",
                "updated": "2024-12-30T16:34:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    34,
                    11,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T16:34:11Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    34,
                    11,
                    0,
                    365,
                    0
                ],
                "title": "Efficient Multi-Task Inferencing with a Shared Backbone and Lightweight\n  Task-Specific Adapters for Automatic Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Multi-Task Inferencing with a Shared Backbone and Lightweight\n  Task-Specific Adapters for Automatic Scoring"
                },
                "summary": "The integration of Artificial Intelligence (AI) in education requires\nscalable and efficient frameworks that balance performance, adaptability, and\ncost. This paper addresses these needs by proposing a shared backbone model\narchitecture enhanced with lightweight LoRA adapters for task-specific\nfine-tuning, targeting the automated scoring of student responses across 27\nmutually exclusive tasks. By achieving competitive performance (average QWK of\n0.848 compared to 0.888 for fully fine-tuned models) while reducing GPU memory\nconsumption by 60% and inference latency by 40%, the framework demonstrates\nsignificant efficiency gains. This approach aligns with the workshops' focus on\nimproving language models for educational tasks, creating responsible\ninnovations for cost-sensitive deployment, and supporting educators by\nstreamlining assessment workflows. The findings underscore the potential of\nscalable AI to enhance learning outcomes while maintaining fairness and\ntransparency in automated scoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Artificial Intelligence (AI) in education requires\nscalable and efficient frameworks that balance performance, adaptability, and\ncost. This paper addresses these needs by proposing a shared backbone model\narchitecture enhanced with lightweight LoRA adapters for task-specific\nfine-tuning, targeting the automated scoring of student responses across 27\nmutually exclusive tasks. By achieving competitive performance (average QWK of\n0.848 compared to 0.888 for fully fine-tuned models) while reducing GPU memory\nconsumption by 60% and inference latency by 40%, the framework demonstrates\nsignificant efficiency gains. This approach aligns with the workshops' focus on\nimproving language models for educational tasks, creating responsible\ninnovations for cost-sensitive deployment, and supporting educators by\nstreamlining assessment workflows. The findings underscore the potential of\nscalable AI to enhance learning outcomes while maintaining fairness and\ntransparency in automated scoring systems."
                },
                "authors": [
                    {
                        "name": "Ehsan Latif"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Zhai"
                },
                "author": "Xiaoming Zhai",
                "arxiv_comment": "Accepted by AAAI-iRAISE Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17498v2",
                "updated": "2024-12-30T16:29:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    29,
                    36,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-23T11:55:33Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    55,
                    33,
                    0,
                    358,
                    0
                ],
                "title": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought"
                },
                "summary": "Recently, O1-like models have emerged as representative examples,\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\ntasks such as math and coding tasks. In this paper, we introduce DRT-o1, an\nattempt to bring the success of long CoT to neural machine translation (MT).\nSpecifically, in view of the literature books that might involve similes and\nmetaphors, translating these texts to a target language is very difficult in\npractice due to cultural differences. In such cases, literal translation often\nfails to convey the intended meaning effectively. Even for professional human\ntranslators, considerable thought must be given to preserving semantics\nthroughout the translation process. To simulate LLMs' long thought ability in\nMT, we first mine sentences containing similes or metaphors from existing\nliterature books, and then develop a multi-agent framework to translate these\nsentences via long thought. In the multi-agent framework, a translator is used\nto iteratively translate the source sentence under the suggestions provided by\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\nalso employed to quantify the translation in each round. In this way, we\ncollect tens of thousands of long-thought MT data, which is used to train our\nDRT-o1. Using Qwen2.5 and LLama-3.1 as the backbones, DRT-o1 models can learn\nthe thought process during machine translation, and outperform vanilla LLMs as\nwell as existing O1-like LLMs, showing their effectiveness The project is\navailable at https://github.com/krystalan/DRT-o1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, O1-like models have emerged as representative examples,\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\ntasks such as math and coding tasks. In this paper, we introduce DRT-o1, an\nattempt to bring the success of long CoT to neural machine translation (MT).\nSpecifically, in view of the literature books that might involve similes and\nmetaphors, translating these texts to a target language is very difficult in\npractice due to cultural differences. In such cases, literal translation often\nfails to convey the intended meaning effectively. Even for professional human\ntranslators, considerable thought must be given to preserving semantics\nthroughout the translation process. To simulate LLMs' long thought ability in\nMT, we first mine sentences containing similes or metaphors from existing\nliterature books, and then develop a multi-agent framework to translate these\nsentences via long thought. In the multi-agent framework, a translator is used\nto iteratively translate the source sentence under the suggestions provided by\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\nalso employed to quantify the translation in each round. In this way, we\ncollect tens of thousands of long-thought MT data, which is used to train our\nDRT-o1. Using Qwen2.5 and LLama-3.1 as the backbones, DRT-o1 models can learn\nthe thought process during machine translation, and outperform vanilla LLMs as\nwell as existing O1-like LLMs, showing their effectiveness The project is\navailable at https://github.com/krystalan/DRT-o1"
                },
                "authors": [
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yunlong Liang"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21051v1",
                "updated": "2024-12-30T16:09:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    9,
                    28,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T16:09:28Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    9,
                    28,
                    0,
                    365,
                    0
                ],
                "title": "Toward Intelligent and Secure Cloud: Large Language Model Empowered\n  Proactive Defense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Intelligent and Secure Cloud: Large Language Model Empowered\n  Proactive Defense"
                },
                "summary": "The rapid evolution of cloud computing technologies and the increasing number\nof cloud applications have provided a large number of benefits in daily lives.\nHowever, the diversity and complexity of different components pose a\nsignificant challenge to cloud security, especially when dealing with\nsophisticated and advanced cyberattacks. Recent advancements in generative\nfoundation models (GFMs), particularly in the large language models (LLMs),\noffer promising solutions for security intelligence. By exploiting the powerful\nabilities in language understanding, data analysis, task inference, action\nplanning, and code generation, we present LLM-PD, a novel proactive defense\narchitecture that defeats various threats in a proactive manner. LLM-PD can\nefficiently make a decision through comprehensive data analysis and sequential\nreasoning, as well as dynamically creating and deploying actionable defense\nmechanisms on the target cloud. Furthermore, it can flexibly self-evolve based\non experience learned from previous interactions and adapt to new attack\nscenarios without additional training. The experimental results demonstrate its\nremarkable ability in terms of defense effectiveness and efficiency,\nparticularly highlighting an outstanding success rate when compared with other\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of cloud computing technologies and the increasing number\nof cloud applications have provided a large number of benefits in daily lives.\nHowever, the diversity and complexity of different components pose a\nsignificant challenge to cloud security, especially when dealing with\nsophisticated and advanced cyberattacks. Recent advancements in generative\nfoundation models (GFMs), particularly in the large language models (LLMs),\noffer promising solutions for security intelligence. By exploiting the powerful\nabilities in language understanding, data analysis, task inference, action\nplanning, and code generation, we present LLM-PD, a novel proactive defense\narchitecture that defeats various threats in a proactive manner. LLM-PD can\nefficiently make a decision through comprehensive data analysis and sequential\nreasoning, as well as dynamically creating and deploying actionable defense\nmechanisms on the target cloud. Furthermore, it can flexibly self-evolve based\non experience learned from previous interactions and adapt to new attack\nscenarios without additional training. The experimental results demonstrate its\nremarkable ability in terms of defense effectiveness and efficiency,\nparticularly highlighting an outstanding success rate when compared with other\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Yuyang Zhou"
                    },
                    {
                        "name": "Guang Cheng"
                    },
                    {
                        "name": "Kang Du"
                    },
                    {
                        "name": "Zihan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Chen"
                },
                "author": "Zihan Chen",
                "arxiv_comment": "7 pages; In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21037v1",
                "updated": "2024-12-30T16:02:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    2,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T16:02:44Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    2,
                    44,
                    0,
                    365,
                    0
                ],
                "title": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow\n  Matching and Clap-Ranked Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow\n  Matching and Clap-Ranked Preference Optimization"
                },
                "summary": "We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model\nwith 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio\nin just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models\nlies in the difficulty of creating preference pairs, as TTA lacks structured\nmechanisms like verifiable rewards or gold-standard answers available for Large\nLanguage Models (LLMs). To address this, we propose CLAP-Ranked Preference\nOptimization (CRPO), a novel framework that iteratively generates and optimizes\npreference data to enhance TTA alignment. We demonstrate that the audio\npreference dataset generated using CRPO outperforms existing alternatives. With\nthis framework, TangoFlux achieves state-of-the-art performance across both\nobjective and subjective benchmarks. We open source all code and models to\nsupport further research in TTA generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model\nwith 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio\nin just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models\nlies in the difficulty of creating preference pairs, as TTA lacks structured\nmechanisms like verifiable rewards or gold-standard answers available for Large\nLanguage Models (LLMs). To address this, we propose CLAP-Ranked Preference\nOptimization (CRPO), a novel framework that iteratively generates and optimizes\npreference data to enhance TTA alignment. We demonstrate that the audio\npreference dataset generated using CRPO outperforms existing alternatives. With\nthis framework, TangoFlux achieves state-of-the-art performance across both\nobjective and subjective benchmarks. We open source all code and models to\nsupport further research in TTA generation."
                },
                "authors": [
                    {
                        "name": "Chia-Yu Hung"
                    },
                    {
                        "name": "Navonil Majumder"
                    },
                    {
                        "name": "Zhifeng Kong"
                    },
                    {
                        "name": "Ambuj Mehrish"
                    },
                    {
                        "name": "Rafael Valle"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "arxiv_comment": "https://tangoflux.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15639v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15639v3",
                "updated": "2024-12-30T15:59:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    59,
                    18,
                    0,
                    365,
                    0
                ],
                "published": "2024-04-24T04:25:04Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    4,
                    25,
                    4,
                    2,
                    115,
                    0
                ],
                "title": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models\n  of Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models\n  of Code"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable progress in code\ngeneration. It now becomes crucial to identify whether the code is AI-generated\nand to determine the specific model used, particularly for purposes such as\nprotecting Intellectual Property (IP) in industry and preventing cheating in\nprogramming exercises. To this end, several attempts have been made to insert\nwatermarks into machine-generated code. However, existing approaches are\nlimited to inserting only a single bit of information. In this paper, we\nintroduce CodeIP, a novel multi-bit watermarking technique that inserts\nadditional information to preserve crucial provenance details, such as the\nvendor ID of an LLM, thereby safeguarding the IPs of LLMs in code generation.\nFurthermore, to ensure the syntactical correctness of the generated code, we\npropose constraining the sampling process for predicting the next token by\ntraining a type predictor. Experiments conducted on a real-world dataset across\nfive programming languages demonstrate the effectiveness of CodeIP in\nwatermarking LLMs for code generation while maintaining the syntactical\ncorrectness of code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable progress in code\ngeneration. It now becomes crucial to identify whether the code is AI-generated\nand to determine the specific model used, particularly for purposes such as\nprotecting Intellectual Property (IP) in industry and preventing cheating in\nprogramming exercises. To this end, several attempts have been made to insert\nwatermarks into machine-generated code. However, existing approaches are\nlimited to inserting only a single bit of information. In this paper, we\nintroduce CodeIP, a novel multi-bit watermarking technique that inserts\nadditional information to preserve crucial provenance details, such as the\nvendor ID of an LLM, thereby safeguarding the IPs of LLMs in code generation.\nFurthermore, to ensure the syntactical correctness of the generated code, we\npropose constraining the sampling process for predicting the next token by\ntraining a type predictor. Experiments conducted on a real-world dataset across\nfive programming languages demonstrate the effectiveness of CodeIP in\nwatermarking LLMs for code generation while maintaining the syntactical\ncorrectness of code."
                },
                "authors": [
                    {
                        "name": "Batu Guan"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Zhangqian Bi"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "16 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15639v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15639v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21033v1",
                "updated": "2024-12-30T15:58:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    58,
                    41,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:58:41Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    58,
                    41,
                    0,
                    365,
                    0
                ],
                "title": "Plancraft: an evaluation dataset for planning with LLM agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plancraft: an evaluation dataset for planning with LLM agents"
                },
                "summary": "We present Plancraft, a multi-modal evaluation dataset for LLM agents.\nPlancraft has both a text-only and multi-modal interface, based on the\nMinecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and\nRetrieval Augmented Generation (RAG), as well as an oracle planner and oracle\nRAG information extractor, to ablate the different components of a modern agent\narchitecture. To evaluate decision-making, Plancraft also includes a subset of\nexamples that are intentionally unsolvable, providing a realistic challenge\nthat requires the agent not only to complete tasks but also to decide whether\nthey are solvable at all. We benchmark both open-source and closed-source LLMs\nand strategies on our task and compare their performance to a handcrafted\nplanner. We find that LLMs and VLMs struggle with the planning problems that\nPlancraft introduces, and we offer suggestions on how to improve their\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Plancraft, a multi-modal evaluation dataset for LLM agents.\nPlancraft has both a text-only and multi-modal interface, based on the\nMinecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and\nRetrieval Augmented Generation (RAG), as well as an oracle planner and oracle\nRAG information extractor, to ablate the different components of a modern agent\narchitecture. To evaluate decision-making, Plancraft also includes a subset of\nexamples that are intentionally unsolvable, providing a realistic challenge\nthat requires the agent not only to complete tasks but also to decide whether\nthey are solvable at all. We benchmark both open-source and closed-source LLMs\nand strategies on our task and compare their performance to a handcrafted\nplanner. We find that LLMs and VLMs struggle with the planning problems that\nPlancraft introduces, and we offer suggestions on how to improve their\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Gautier Dagan"
                    },
                    {
                        "name": "Frank Keller"
                    },
                    {
                        "name": "Alex Lascarides"
                    }
                ],
                "author_detail": {
                    "name": "Alex Lascarides"
                },
                "author": "Alex Lascarides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21025v1",
                "updated": "2024-12-30T15:50:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    50,
                    27,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:50:27Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    50,
                    27,
                    0,
                    365,
                    0
                ],
                "title": "Considering experimental frame rates and robust segmentation analysis of\n  piecewise-linear microparticle trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Considering experimental frame rates and robust segmentation analysis of\n  piecewise-linear microparticle trajectories"
                },
                "summary": "The movement of intracellular cargo transported by molecular motors is\ncommonly marked by switches between directed motion and stationary pauses. The\npredominant measure for assessing movement is effective diffusivity, which\npredicts the mean-squared displacement of particles over long time scales. In\nthis work, we consider an alternative analysis regime that focuses on shorter\ntime scales and relies on automated segmentation of paths. Due to intrinsic\nuncertainty in changepoint analysis, we highlight the importance of statistical\nsummaries that are robust with respect to the performance of segmentation\nalgorithms. In contrast to effective diffusivity, which averages over multiple\nbehaviors, we emphasize tools that highlight the different motor-cargo states,\nwith an eye toward identifying biophysical mechanisms that determine emergent\nwhole-cell transport properties. By developing a Markov chain model for noisy,\ncontinuous, piecewise-linear microparticle movement, and associated\nmathematical analysis, we provide insight into a common question posed by\nexperimentalists: how does the choice of observational frame rate affect what\nis inferred about transport properties?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The movement of intracellular cargo transported by molecular motors is\ncommonly marked by switches between directed motion and stationary pauses. The\npredominant measure for assessing movement is effective diffusivity, which\npredicts the mean-squared displacement of particles over long time scales. In\nthis work, we consider an alternative analysis regime that focuses on shorter\ntime scales and relies on automated segmentation of paths. Due to intrinsic\nuncertainty in changepoint analysis, we highlight the importance of statistical\nsummaries that are robust with respect to the performance of segmentation\nalgorithms. In contrast to effective diffusivity, which averages over multiple\nbehaviors, we emphasize tools that highlight the different motor-cargo states,\nwith an eye toward identifying biophysical mechanisms that determine emergent\nwhole-cell transport properties. By developing a Markov chain model for noisy,\ncontinuous, piecewise-linear microparticle movement, and associated\nmathematical analysis, we provide insight into a common question posed by\nexperimentalists: how does the choice of observational frame rate affect what\nis inferred about transport properties?"
                },
                "authors": [
                    {
                        "name": "Keisha J. Cook"
                    },
                    {
                        "name": "Nathan Rayens"
                    },
                    {
                        "name": "Linh Do"
                    },
                    {
                        "name": "Christine K. Payne"
                    },
                    {
                        "name": "Scott A. McKinley"
                    }
                ],
                "author_detail": {
                    "name": "Scott A. McKinley"
                },
                "author": "Scott A. McKinley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21016v1",
                "updated": "2024-12-30T15:33:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    34,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:34Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    34,
                    0,
                    365,
                    0
                ],
                "title": "Automated Robustness Testing for LLM-based NLP Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Robustness Testing for LLM-based NLP Software"
                },
                "summary": "Benefiting from the advancements in LLMs, NLP software has undergone rapid\ndevelopment. Such software is widely employed in various safety-critical tasks,\nsuch as financial sentiment analysis, toxic content moderation, and log\ngeneration. To our knowledge, there are no known automated robustness testing\nmethods specifically designed for LLM-based NLP software. Given the complexity\nof LLMs and the unpredictability of real-world inputs (including prompts and\nexamples), it is essential to examine the robustness of overall inputs to\nensure the safety of such software.\n  To this end, this paper introduces the first AutOmated Robustness Testing\nfrAmework, AORTA, which reconceptualizes the testing process into a\ncombinatorial optimization problem. Existing testing methods designed for\nDNN-based software can be applied to LLM-based software by AORTA, but their\neffectiveness is limited. To address this, we propose a novel testing method\nfor LLM-based software within AORTA called Adaptive Beam Search. ABS is\ntailored for the expansive feature space of LLMs and improves testing\neffectiveness through an adaptive beam width and the capability for\nbacktracking.\n  We successfully embed 18 test methods in the designed framework AORTA and\ncompared the test validity of ABS with three datasets and five threat models.\nABS facilitates a more comprehensive and accurate robustness assessment before\nsoftware deployment, with an average test success rate of 86.138%. Compared to\nthe currently best-performing baseline PWWS, ABS significantly reduces the\ncomputational overhead by up to 3441.895 seconds per successful test case and\ndecreases the number of queries by 218.762 times on average. Furthermore, test\ncases generated by ABS exhibit greater naturalness and transferability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the advancements in LLMs, NLP software has undergone rapid\ndevelopment. Such software is widely employed in various safety-critical tasks,\nsuch as financial sentiment analysis, toxic content moderation, and log\ngeneration. To our knowledge, there are no known automated robustness testing\nmethods specifically designed for LLM-based NLP software. Given the complexity\nof LLMs and the unpredictability of real-world inputs (including prompts and\nexamples), it is essential to examine the robustness of overall inputs to\nensure the safety of such software.\n  To this end, this paper introduces the first AutOmated Robustness Testing\nfrAmework, AORTA, which reconceptualizes the testing process into a\ncombinatorial optimization problem. Existing testing methods designed for\nDNN-based software can be applied to LLM-based software by AORTA, but their\neffectiveness is limited. To address this, we propose a novel testing method\nfor LLM-based software within AORTA called Adaptive Beam Search. ABS is\ntailored for the expansive feature space of LLMs and improves testing\neffectiveness through an adaptive beam width and the capability for\nbacktracking.\n  We successfully embed 18 test methods in the designed framework AORTA and\ncompared the test validity of ABS with three datasets and five threat models.\nABS facilitates a more comprehensive and accurate robustness assessment before\nsoftware deployment, with an average test success rate of 86.138%. Compared to\nthe currently best-performing baseline PWWS, ABS significantly reduces the\ncomputational overhead by up to 3441.895 seconds per successful test case and\ndecreases the number of queries by 218.762 times on average. Furthermore, test\ncases generated by ABS exhibit greater naturalness and transferability."
                },
                "authors": [
                    {
                        "name": "Mingxuan Xiao"
                    },
                    {
                        "name": "Yan Xiao"
                    },
                    {
                        "name": "Shunhui Ji"
                    },
                    {
                        "name": "Hanbo Cai"
                    },
                    {
                        "name": "Lei Xue"
                    },
                    {
                        "name": "Pengcheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Pengcheng Zhang"
                },
                "author": "Pengcheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19312v2",
                "updated": "2024-12-30T15:30:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    30,
                    23,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-26T18:19:53Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    19,
                    53,
                    3,
                    361,
                    0
                ],
                "title": "From Interests to Insights: An LLM Approach to Course Recommendations\n  Using Natural Language Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Interests to Insights: An LLM Approach to Course Recommendations\n  Using Natural Language Queries"
                },
                "summary": "Most universities in the United States encourage their students to explore\nacademic areas before declaring a major and to acquire academic breadth by\nsatisfying a variety of requirements. Each term, students must choose among\nmany thousands of offerings, spanning dozens of subject areas, a handful of\ncourses to take. The curricular environment is also dynamic, and poor\ncommunication and search functions on campus can limit a student's ability to\ndiscover new courses of interest. To support both students and their advisers\nin such a setting, we explore a novel Large Language Model (LLM) course\nrecommendation system that applies a Retrieval Augmented Generation (RAG)\nmethod to the corpus of course descriptions. The system first generates an\n'ideal' course description based on the user's query. This description is\nconverted into a search vector using embeddings, which is then used to find\nactual courses with similar content by comparing embedding similarities. We\ndescribe the method and assess the quality and fairness of some example\nprompts. Steps to deploy a pilot system on campus are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most universities in the United States encourage their students to explore\nacademic areas before declaring a major and to acquire academic breadth by\nsatisfying a variety of requirements. Each term, students must choose among\nmany thousands of offerings, spanning dozens of subject areas, a handful of\ncourses to take. The curricular environment is also dynamic, and poor\ncommunication and search functions on campus can limit a student's ability to\ndiscover new courses of interest. To support both students and their advisers\nin such a setting, we explore a novel Large Language Model (LLM) course\nrecommendation system that applies a Retrieval Augmented Generation (RAG)\nmethod to the corpus of course descriptions. The system first generates an\n'ideal' course description based on the user's query. This description is\nconverted into a search vector using embeddings, which is then used to find\nactual courses with similar content by comparing embedding similarities. We\ndescribe the method and assess the quality and fairness of some example\nprompts. Steps to deploy a pilot system on campus are discussed."
                },
                "authors": [
                    {
                        "name": "Hugh Van Deventer"
                    },
                    {
                        "name": "Mark Mills"
                    },
                    {
                        "name": "August Evrard"
                    }
                ],
                "author_detail": {
                    "name": "August Evrard"
                },
                "author": "August Evrard",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03321v2",
                "updated": "2024-12-30T15:19:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    19,
                    27,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-06T17:57:01Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    57,
                    1,
                    1,
                    219,
                    0
                ],
                "title": "Chasing cosmic inflation: constraints for inflationary models and\n  reheating insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chasing cosmic inflation: constraints for inflationary models and\n  reheating insights"
                },
                "summary": "We investigate the impact of different choice of prior's range for the\nreheating epoch on cosmic inflation parameter inference in light of cosmic\nmicrowave background (CMB) anisotropy measurements from the {\\em Planck} 2018\nlegacy release in combination with BICEP/Keck Array 2018 data and additional\nlate-time cosmological observations such as uncalibrated Type Ia supernovae\nfrom the Pantheon catalogue, baryon acoustic oscillations and redshift space\ndistortions from SDSS/BOSS/eBOSS. Here, we explore in particular the\nimplications for the combination of reheating and inflationary-model parameter\nspace considering $R+R^2$ inflation and a broad class of $\\alpha$-attractor and\nD-brane models. Propagating the uncertainties due to an unknown reheating\nphase, these inflationary models completely cover the $n_{\\rm s}$-$r$ parameter\nspace allowed by {\\em Planck} and BICEP/Keck data and represent good targets\nfor future CMB and large-scale structure experiments. We perform a Bayesian\nmodel comparison of inflationary models, taking into account the reheating\nuncertainties assuming a conservative but accurate modelling of inflationary\npredictions. $R+R^2$ inflation, T-model $\\alpha$-attractor inflation for $n=1$,\nE-model $\\alpha$-attractor inflation for $n=1/2$, and KKLT inflation for $p=5$\nare the better performing models, with none being preferred at a statistically\nsignificant level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the impact of different choice of prior's range for the\nreheating epoch on cosmic inflation parameter inference in light of cosmic\nmicrowave background (CMB) anisotropy measurements from the {\\em Planck} 2018\nlegacy release in combination with BICEP/Keck Array 2018 data and additional\nlate-time cosmological observations such as uncalibrated Type Ia supernovae\nfrom the Pantheon catalogue, baryon acoustic oscillations and redshift space\ndistortions from SDSS/BOSS/eBOSS. Here, we explore in particular the\nimplications for the combination of reheating and inflationary-model parameter\nspace considering $R+R^2$ inflation and a broad class of $\\alpha$-attractor and\nD-brane models. Propagating the uncertainties due to an unknown reheating\nphase, these inflationary models completely cover the $n_{\\rm s}$-$r$ parameter\nspace allowed by {\\em Planck} and BICEP/Keck data and represent good targets\nfor future CMB and large-scale structure experiments. We perform a Bayesian\nmodel comparison of inflationary models, taking into account the reheating\nuncertainties assuming a conservative but accurate modelling of inflationary\npredictions. $R+R^2$ inflation, T-model $\\alpha$-attractor inflation for $n=1$,\nE-model $\\alpha$-attractor inflation for $n=1/2$, and KKLT inflation for $p=5$\nare the better performing models, with none being preferred at a statistically\nsignificant level."
                },
                "authors": [
                    {
                        "name": "Mario Ballardini"
                    }
                ],
                "author_detail": {
                    "name": "Mario Ballardini"
                },
                "author": "Mario Ballardini",
                "arxiv_comment": "35 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21006v1",
                "updated": "2024-12-30T15:15:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    15,
                    8,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:15:08Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    15,
                    8,
                    0,
                    365,
                    0
                ],
                "title": "Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant\n  Rationale via Principled Criteria",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant\n  Rationale via Principled Criteria"
                },
                "summary": "Large Language Models (LLMs) rely on generating extensive intermediate\nreasoning units (e.g., tokens, sentences) to enhance final answer quality\nacross a wide range of complex tasks. While generating multiple reasoning paths\nor iteratively refining rationales proves effective for improving performance,\nthese approaches inevitably result in significantly higher inference costs. In\nthis work, we propose a novel sentence-level rationale reduction training\nframework that leverages likelihood-based criteria, verbosity, to identify and\nremove redundant reasoning sentences. Unlike previous approaches that utilize\ntoken-level reduction, our sentence-level reduction framework maintains model\nperformance while reducing generation length. This preserves the original\nreasoning abilities of LLMs and achieves an average 17.15% reduction in\ngeneration costs across various models and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) rely on generating extensive intermediate\nreasoning units (e.g., tokens, sentences) to enhance final answer quality\nacross a wide range of complex tasks. While generating multiple reasoning paths\nor iteratively refining rationales proves effective for improving performance,\nthese approaches inevitably result in significantly higher inference costs. In\nthis work, we propose a novel sentence-level rationale reduction training\nframework that leverages likelihood-based criteria, verbosity, to identify and\nremove redundant reasoning sentences. Unlike previous approaches that utilize\ntoken-level reduction, our sentence-level reduction framework maintains model\nperformance while reducing generation length. This preserves the original\nreasoning abilities of LLMs and achieves an average 17.15% reduction in\ngeneration costs across various models and tasks."
                },
                "authors": [
                    {
                        "name": "Joonwon Jang"
                    },
                    {
                        "name": "Jaehee Kim"
                    },
                    {
                        "name": "Wonbin Kweon"
                    },
                    {
                        "name": "Hwanjo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjo Yu"
                },
                "author": "Hwanjo Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21004v1",
                "updated": "2024-12-30T15:13:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    13,
                    57,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:13:57Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    13,
                    57,
                    0,
                    365,
                    0
                ],
                "title": "Weber-Fechner Law in Temporal Difference learning derived from Control\n  as Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weber-Fechner Law in Temporal Difference learning derived from Control\n  as Inference"
                },
                "summary": "This paper investigates a novel nonlinear update rule based on temporal\ndifference (TD) errors in reinforcement learning (RL). The update rule in the\nstandard RL states that the TD error is linearly proportional to the degree of\nupdates, treating all rewards equally without no bias. On the other hand, the\nrecent biological studies revealed that there are nonlinearities in the TD\nerror and the degree of updates, biasing policies optimistic or pessimistic.\nSuch biases in learning due to nonlinearities are expected to be useful and\nintentionally leftover features in biological learning. Therefore, this\nresearch explores a theoretical framework that can leverage the nonlinearity\nbetween the degree of the update and TD errors. To this end, we focus on a\ncontrol as inference framework, since it is known as a generalized formulation\nencompassing various RL and optimal control methods. In particular, we\ninvestigate the uncomputable nonlinear term needed to be approximately excluded\nin the derivation of the standard RL from control as inference. By analyzing\nit, Weber-Fechner law (WFL) is found, namely, perception (a.k.a. the degree of\nupdates) in response to stimulus change (a.k.a. TD error) is attenuated by\nincrease in the stimulus intensity (a.k.a. the value function). To numerically\nreveal the utilities of WFL on RL, we then propose a practical implementation\nusing a reward-punishment framework and modifying the definition of optimality.\nAnalysis of this implementation reveals that two utilities can be expected i)\nto increase rewards to a certain level early, and ii) to sufficiently suppress\npunishment. We finally investigate and discuss the expected utilities through\nsimulations and robot experiments. As a result, the proposed RL algorithm with\nWFL shows the expected utilities that accelerate the reward-maximizing startup\nand continue to suppress punishments during learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates a novel nonlinear update rule based on temporal\ndifference (TD) errors in reinforcement learning (RL). The update rule in the\nstandard RL states that the TD error is linearly proportional to the degree of\nupdates, treating all rewards equally without no bias. On the other hand, the\nrecent biological studies revealed that there are nonlinearities in the TD\nerror and the degree of updates, biasing policies optimistic or pessimistic.\nSuch biases in learning due to nonlinearities are expected to be useful and\nintentionally leftover features in biological learning. Therefore, this\nresearch explores a theoretical framework that can leverage the nonlinearity\nbetween the degree of the update and TD errors. To this end, we focus on a\ncontrol as inference framework, since it is known as a generalized formulation\nencompassing various RL and optimal control methods. In particular, we\ninvestigate the uncomputable nonlinear term needed to be approximately excluded\nin the derivation of the standard RL from control as inference. By analyzing\nit, Weber-Fechner law (WFL) is found, namely, perception (a.k.a. the degree of\nupdates) in response to stimulus change (a.k.a. TD error) is attenuated by\nincrease in the stimulus intensity (a.k.a. the value function). To numerically\nreveal the utilities of WFL on RL, we then propose a practical implementation\nusing a reward-punishment framework and modifying the definition of optimality.\nAnalysis of this implementation reveals that two utilities can be expected i)\nto increase rewards to a certain level early, and ii) to sufficiently suppress\npunishment. We finally investigate and discuss the expected utilities through\nsimulations and robot experiments. As a result, the proposed RL algorithm with\nWFL shows the expected utilities that accelerate the reward-maximizing startup\nand continue to suppress punishments during learning."
                },
                "authors": [
                    {
                        "name": "Keiichiro Takahashi"
                    },
                    {
                        "name": "Taisuke Kobayashi"
                    },
                    {
                        "name": "Tomoya Yamanokuchi"
                    },
                    {
                        "name": "Takamitsu Matsubara"
                    }
                ],
                "author_detail": {
                    "name": "Takamitsu Matsubara"
                },
                "author": "Takamitsu Matsubara",
                "arxiv_comment": "36 pages 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08603v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08603v3",
                "updated": "2024-12-30T15:08:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    8,
                    23,
                    0,
                    365,
                    0
                ],
                "published": "2024-05-14T13:42:05Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    13,
                    42,
                    5,
                    1,
                    135,
                    0
                ],
                "title": "A Comprehensive Survey of Large Language Models and Multimodal Large\n  Language Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Large Language Models and Multimodal Large\n  Language Models in Medicine"
                },
                "summary": "Since the release of ChatGPT and GPT-4, large language models (LLMs) and\nmultimodal large language models (MLLMs) have attracted widespread attention\nfor their exceptional capabilities in understanding, reasoning, and generation,\nintroducing transformative paradigms for integrating artificial intelligence\ninto medicine. This survey provides a comprehensive overview of the\ndevelopment, principles, application scenarios, challenges, and future\ndirections of LLMs and MLLMs in medicine. Specifically, it begins by examining\nthe paradigm shift, tracing the transition from traditional models to LLMs and\nMLLMs, and highlighting the unique advantages of these LLMs and MLLMs in\nmedical applications. Next, the survey reviews existing medical LLMs and MLLMs,\nproviding detailed guidance on their construction and evaluation in a clear and\nsystematic manner. Subsequently, to underscore the substantial value of LLMs\nand MLLMs in healthcare, the survey explores five promising applications in the\nfield. Finally, the survey addresses the challenges confronting medical LLMs\nand MLLMs and proposes practical strategies and future directions for their\nintegration into medicine. In summary, this survey offers a comprehensive\nanalysis of the technical methodologies and practical clinical applications of\nmedical LLMs and MLLMs, with the goal of bridging the gap between these\nadvanced technologies and clinical practice, thereby fostering the evolution of\nthe next generation of intelligent healthcare systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the release of ChatGPT and GPT-4, large language models (LLMs) and\nmultimodal large language models (MLLMs) have attracted widespread attention\nfor their exceptional capabilities in understanding, reasoning, and generation,\nintroducing transformative paradigms for integrating artificial intelligence\ninto medicine. This survey provides a comprehensive overview of the\ndevelopment, principles, application scenarios, challenges, and future\ndirections of LLMs and MLLMs in medicine. Specifically, it begins by examining\nthe paradigm shift, tracing the transition from traditional models to LLMs and\nMLLMs, and highlighting the unique advantages of these LLMs and MLLMs in\nmedical applications. Next, the survey reviews existing medical LLMs and MLLMs,\nproviding detailed guidance on their construction and evaluation in a clear and\nsystematic manner. Subsequently, to underscore the substantial value of LLMs\nand MLLMs in healthcare, the survey explores five promising applications in the\nfield. Finally, the survey addresses the challenges confronting medical LLMs\nand MLLMs and proposes practical strategies and future directions for their\nintegration into medicine. In summary, this survey offers a comprehensive\nanalysis of the technical methodologies and practical clinical applications of\nmedical LLMs and MLLMs, with the goal of bridging the gap between these\nadvanced technologies and clinical practice, thereby fostering the evolution of\nthe next generation of intelligent healthcare systems."
                },
                "authors": [
                    {
                        "name": "Hanguang Xiao"
                    },
                    {
                        "name": "Feizhong Zhou"
                    },
                    {
                        "name": "Xingyue Liu"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Zhipeng Li"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xiaoxuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxuan Huang"
                },
                "author": "Xiaoxuan Huang",
                "arxiv_doi": "10.1016/j.inffus.2024.102888",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.inffus.2024.102888",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.08603v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08603v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Information Fusion, 117 (2025) 102888",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20996v1",
                "updated": "2024-12-30T15:01:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    1,
                    48,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:01:48Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    1,
                    48,
                    0,
                    365,
                    0
                ],
                "title": "Plug-and-Play Training Framework for Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play Training Framework for Preference Optimization"
                },
                "summary": "Recently, preference optimization methods such as DPO have significantly\nenhanced large language models (LLMs) in wide tasks including dialogue and\nquestion-answering. However, current methods fail to account for the varying\ndifficulty levels of training samples during preference optimization, leading\nto mediocre performance in tasks with high accuracy requirements, particularly\nin mathematical reasoning. To address this limitation, we propose a novel\ntraining framework, which employs multiple sampling to analyze output\ndistributions, assign different weights to samples, and incorporate these\nweights into the preference optimization process. This plug-and-play approach\nenables LLMs to prioritize challenging examples during training, improving\nlearning efficiency. Experimental results demonstrate that our framework\nintegrates seamlessly with various preference optimization methods and achieves\nconsistent improvements in mathematical reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, preference optimization methods such as DPO have significantly\nenhanced large language models (LLMs) in wide tasks including dialogue and\nquestion-answering. However, current methods fail to account for the varying\ndifficulty levels of training samples during preference optimization, leading\nto mediocre performance in tasks with high accuracy requirements, particularly\nin mathematical reasoning. To address this limitation, we propose a novel\ntraining framework, which employs multiple sampling to analyze output\ndistributions, assign different weights to samples, and incorporate these\nweights into the preference optimization process. This plug-and-play approach\nenables LLMs to prioritize challenging examples during training, improving\nlearning efficiency. Experimental results demonstrate that our framework\nintegrates seamlessly with various preference optimization methods and achieves\nconsistent improvements in mathematical reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Jingyuan Ma"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "arxiv_comment": "12 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04050v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04050v2",
                "updated": "2024-12-30T14:58:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    58,
                    54,
                    0,
                    365,
                    0
                ],
                "published": "2024-09-06T06:46:01Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    46,
                    1,
                    4,
                    250,
                    0
                ],
                "title": "EigenSR: Eigenimage-Bridged Pre-Trained RGB Learners for Single\n  Hyperspectral Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EigenSR: Eigenimage-Bridged Pre-Trained RGB Learners for Single\n  Hyperspectral Image Super-Resolution"
                },
                "summary": "Single hyperspectral image super-resolution (single-HSI-SR) aims to improve\nthe resolution of a single input low-resolution HSI. Due to the bottleneck of\ndata scarcity, the development of single-HSI-SR lags far behind that of RGB\nnatural images. In recent years, research on RGB SR has shown that models\npre-trained on large-scale benchmark datasets can greatly improve performance\non unseen data, which may stand as a remedy for HSI. But how can we transfer\nthe pre-trained RGB model to HSI, to overcome the data-scarcity bottleneck?\nBecause of the significant difference in the channels between the pre-trained\nRGB model and the HSI, the model cannot focus on the correlation along the\nspectral dimension, thus limiting its ability to utilize on HSI. Inspired by\nthe HSI spatial-spectral decoupling, we propose a new framework that first\nfine-tunes the pre-trained model with the spatial components (known as\neigenimages), and then infers on unseen HSI using an iterative spectral\nregularization (ISR) to maintain the spectral correlation. The advantages of\nour method lie in: 1) we effectively inject the spatial texture processing\ncapabilities of the pre-trained RGB model into HSI while keeping spectral\nfidelity, 2) learning in the spectral-decorrelated domain can improve the\ngeneralizability to spectral-agnostic data, and 3) our inference in the\neigenimage domain naturally exploits the spectral low-rank property of HSI,\nthereby reducing the complexity. This work bridges the gap between pre-trained\nRGB models and HSI via eigenimages, addressing the issue of limited HSI\ntraining data, hence the name EigenSR. Extensive experiments show that EigenSR\noutperforms the state-of-the-art (SOTA) methods in both spatial and spectral\nmetrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single hyperspectral image super-resolution (single-HSI-SR) aims to improve\nthe resolution of a single input low-resolution HSI. Due to the bottleneck of\ndata scarcity, the development of single-HSI-SR lags far behind that of RGB\nnatural images. In recent years, research on RGB SR has shown that models\npre-trained on large-scale benchmark datasets can greatly improve performance\non unseen data, which may stand as a remedy for HSI. But how can we transfer\nthe pre-trained RGB model to HSI, to overcome the data-scarcity bottleneck?\nBecause of the significant difference in the channels between the pre-trained\nRGB model and the HSI, the model cannot focus on the correlation along the\nspectral dimension, thus limiting its ability to utilize on HSI. Inspired by\nthe HSI spatial-spectral decoupling, we propose a new framework that first\nfine-tunes the pre-trained model with the spatial components (known as\neigenimages), and then infers on unseen HSI using an iterative spectral\nregularization (ISR) to maintain the spectral correlation. The advantages of\nour method lie in: 1) we effectively inject the spatial texture processing\ncapabilities of the pre-trained RGB model into HSI while keeping spectral\nfidelity, 2) learning in the spectral-decorrelated domain can improve the\ngeneralizability to spectral-agnostic data, and 3) our inference in the\neigenimage domain naturally exploits the spectral low-rank property of HSI,\nthereby reducing the complexity. This work bridges the gap between pre-trained\nRGB models and HSI via eigenimages, addressing the issue of limited HSI\ntraining data, hence the name EigenSR. Extensive experiments show that EigenSR\noutperforms the state-of-the-art (SOTA) methods in both spatial and spectral\nmetrics."
                },
                "authors": [
                    {
                        "name": "Xi Su"
                    },
                    {
                        "name": "Xiangfei Shen"
                    },
                    {
                        "name": "Mingyang Wan"
                    },
                    {
                        "name": "Jing Nie"
                    },
                    {
                        "name": "Lihui Chen"
                    },
                    {
                        "name": "Haijun Liu"
                    },
                    {
                        "name": "Xichuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xichuan Zhou"
                },
                "author": "Xichuan Zhou",
                "arxiv_comment": "AAAI 2025 conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04050v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04050v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20995v1",
                "updated": "2024-12-30T14:58:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    58,
                    46,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T14:58:46Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    58,
                    46,
                    0,
                    365,
                    0
                ],
                "title": "KARPA: A Training-free Method of Adapting Knowledge Graph as References\n  for Large Language Model's Reasoning Path Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KARPA: A Training-free Method of Adapting Knowledge Graph as References\n  for Large Language Model's Reasoning Path Aggregation"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance across a\nvariety of tasks, yet they are often affected by hallucinations and the\ntimeliness of knowledge. Leveraging knowledge graphs (KGs) as external\nknowledge sources has emerged as a viable solution, but existing methods for\nLLM-based knowledge graph question answering (KGQA) are often limited by\nstep-by-step decision-making on KGs, restricting the global planning and\nreasoning capabilities of LLMs, or they require fine-tuning or pre-training on\nspecific KGs. To address these challenges, we propose Knowledge graph Assisted\nReasoning Path Aggregation (KARPA), a novel framework that harnesses the global\nplanning abilities of LLMs for efficient and accurate KG reasoning. KARPA\noperates in three steps: pre-planning relation paths using the LLM's global\nplanning capabilities, matching semantically relevant paths via an embedding\nmodel, and reasoning over these paths to generate answers. Unlike existing KGQA\nmethods, KARPA avoids stepwise traversal, requires no additional training, and\nis adaptable to various LLM architectures. Extensive experimental results show\nthat KARPA achieves state-of-the-art performance in KGQA tasks, delivering both\nhigh efficiency and accuracy. Our code will be available on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance across a\nvariety of tasks, yet they are often affected by hallucinations and the\ntimeliness of knowledge. Leveraging knowledge graphs (KGs) as external\nknowledge sources has emerged as a viable solution, but existing methods for\nLLM-based knowledge graph question answering (KGQA) are often limited by\nstep-by-step decision-making on KGs, restricting the global planning and\nreasoning capabilities of LLMs, or they require fine-tuning or pre-training on\nspecific KGs. To address these challenges, we propose Knowledge graph Assisted\nReasoning Path Aggregation (KARPA), a novel framework that harnesses the global\nplanning abilities of LLMs for efficient and accurate KG reasoning. KARPA\noperates in three steps: pre-planning relation paths using the LLM's global\nplanning capabilities, matching semantically relevant paths via an embedding\nmodel, and reasoning over these paths to generate answers. Unlike existing KGQA\nmethods, KARPA avoids stepwise traversal, requires no additional training, and\nis adaptable to various LLM architectures. Extensive experimental results show\nthat KARPA achieves state-of-the-art performance in KGQA tasks, delivering both\nhigh efficiency and accuracy. Our code will be available on Github."
                },
                "authors": [
                    {
                        "name": "Siyuan Fang"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Ningxuan Lu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Qingkun Tang"
                    }
                ],
                "author_detail": {
                    "name": "Qingkun Tang"
                },
                "author": "Qingkun Tang",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20993v1",
                "updated": "2024-12-30T14:57:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    57,
                    53,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T14:57:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    57,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "Efficiently Serving LLM Reasoning Programs with Certaindex",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving LLM Reasoning Programs with Certaindex"
                },
                "summary": "The rapid evolution of large language models (LLMs) has unlocked their\ncapabilities in advanced reasoning tasks like mathematical problem-solving,\ncode generation, and legal analysis. Central to this progress are\ninference-time reasoning algorithms, which refine outputs by exploring multiple\nsolution paths, at the cost of increasing compute demands and response\nlatencies. Existing serving systems fail to adapt to the scaling behaviors of\nthese algorithms or the varying difficulty of queries, leading to inefficient\nresource use and unmet latency targets.\n  We present Dynasor, a system that optimizes inference-time compute for LLM\nreasoning queries. Unlike traditional engines, Dynasor tracks and schedules\nrequests within reasoning queries and uses Certaindex, a proxy that measures\nstatistical reasoning progress based on model certainty, to guide compute\nallocation dynamically. Dynasor co-adapts scheduling with reasoning progress:\nit allocates more compute to hard queries, reduces compute for simpler ones,\nand terminates unpromising queries early, balancing accuracy, latency, and\ncost. On diverse datasets and algorithms, Dynasor reduces compute by up to 50%\nin batch processing and sustaining 3.3x higher query rates or 4.7x tighter\nlatency SLOs in online serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) has unlocked their\ncapabilities in advanced reasoning tasks like mathematical problem-solving,\ncode generation, and legal analysis. Central to this progress are\ninference-time reasoning algorithms, which refine outputs by exploring multiple\nsolution paths, at the cost of increasing compute demands and response\nlatencies. Existing serving systems fail to adapt to the scaling behaviors of\nthese algorithms or the varying difficulty of queries, leading to inefficient\nresource use and unmet latency targets.\n  We present Dynasor, a system that optimizes inference-time compute for LLM\nreasoning queries. Unlike traditional engines, Dynasor tracks and schedules\nrequests within reasoning queries and uses Certaindex, a proxy that measures\nstatistical reasoning progress based on model certainty, to guide compute\nallocation dynamically. Dynasor co-adapts scheduling with reasoning progress:\nit allocates more compute to hard queries, reduces compute for simpler ones,\nand terminates unpromising queries early, balancing accuracy, latency, and\ncost. On diverse datasets and algorithms, Dynasor reduces compute by up to 50%\nin batch processing and sustaining 3.3x higher query rates or 4.7x tighter\nlatency SLOs in online serving."
                },
                "authors": [
                    {
                        "name": "Yichao Fu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Siqi Zhu"
                    },
                    {
                        "name": "Zheyu Fu"
                    },
                    {
                        "name": "Zhongdongming Dai"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2202.08500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2202.08500v2",
                "updated": "2024-12-30T14:48:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    48,
                    42,
                    0,
                    365,
                    0
                ],
                "published": "2022-02-17T08:05:45Z",
                "published_parsed": [
                    2022,
                    2,
                    17,
                    8,
                    5,
                    45,
                    3,
                    48,
                    0
                ],
                "title": "Causal inference with recurrent and competing events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference with recurrent and competing events"
                },
                "summary": "Many research questions concern treatment effects on outcomes that can recur\nseveral times in the same individual. For example, medical researchers are\ninterested in treatment effects on hospitalizations in heart failure patients\nand sports injuries in athletes. Competing events, such as death, complicate\ncausal inference in studies of recurrent events because once a competing event\noccurs, an individual cannot have more recurrent events. Several statistical\nestimands have been studied in recurrent event settings, with and without\ncompeting events. However, the causal interpretations of these estimands, and\nthe conditions that are required to identify these estimands from observed\ndata, have yet to be formalized. Here we use a formal framework for causal\ninference to formulate several causal estimands in recurrent event settings,\nwith and without competing events. We clarify when commonly used classical\nstatistical estimands can be interpreted as causal quantities from the causal\nmediation literature, such as (controlled) direct effects and total effects.\nFurthermore, we show that recent results on interventionist mediation estimands\nallow us to define new causal estimands with recurrent and competing events\nthat may be of particular clinical relevance in many subject matter settings.\nWe use causal directed acyclic graphs and single world intervention graphs to\nillustrate how to reason about identification conditions for the various causal\nestimands based on subject matter knowledge. Furthermore, using results on\ncounting processes, we show that our causal estimands and their identification\nconditions, which are articulated in discrete time, converge to classical\ncontinuous time counterparts in the limit of fine discretizations of time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many research questions concern treatment effects on outcomes that can recur\nseveral times in the same individual. For example, medical researchers are\ninterested in treatment effects on hospitalizations in heart failure patients\nand sports injuries in athletes. Competing events, such as death, complicate\ncausal inference in studies of recurrent events because once a competing event\noccurs, an individual cannot have more recurrent events. Several statistical\nestimands have been studied in recurrent event settings, with and without\ncompeting events. However, the causal interpretations of these estimands, and\nthe conditions that are required to identify these estimands from observed\ndata, have yet to be formalized. Here we use a formal framework for causal\ninference to formulate several causal estimands in recurrent event settings,\nwith and without competing events. We clarify when commonly used classical\nstatistical estimands can be interpreted as causal quantities from the causal\nmediation literature, such as (controlled) direct effects and total effects.\nFurthermore, we show that recent results on interventionist mediation estimands\nallow us to define new causal estimands with recurrent and competing events\nthat may be of particular clinical relevance in many subject matter settings.\nWe use causal directed acyclic graphs and single world intervention graphs to\nillustrate how to reason about identification conditions for the various causal\nestimands based on subject matter knowledge. Furthermore, using results on\ncounting processes, we show that our causal estimands and their identification\nconditions, which are articulated in discrete time, converge to classical\ncontinuous time counterparts in the limit of fine discretizations of time."
                },
                "authors": [
                    {
                        "name": "Matias Janvin"
                    },
                    {
                        "name": "Jessica G. Young"
                    },
                    {
                        "name": "PÃ¥l C. Ryalen"
                    },
                    {
                        "name": "Mats J. Stensrud"
                    }
                ],
                "author_detail": {
                    "name": "Mats J. Stensrud"
                },
                "author": "Mats J. Stensrud",
                "arxiv_doi": "10.1007/s10985-023-09594-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10985-023-09594-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2202.08500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2202.08500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20973v1",
                "updated": "2024-12-30T14:24:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    24,
                    54,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T14:24:54Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    24,
                    54,
                    0,
                    365,
                    0
                ],
                "title": "A Qualitative Analysis of Kernel Extension for Higher Order Proof\n  Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Qualitative Analysis of Kernel Extension for Higher Order Proof\n  Checking"
                },
                "summary": "For the sake of reliability, the kernels of Interactive Theorem Provers\n(ITPs) are generally kept relatively small. On top of the kernel, additional\nsymbols and inference rules are defined. This paper presents an analysis of how\nkernel extension reduces the size of proofs and impacts proof checking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For the sake of reliability, the kernels of Interactive Theorem Provers\n(ITPs) are generally kept relatively small. On top of the kernel, additional\nsymbols and inference rules are defined. This paper presents an analysis of how\nkernel extension reduces the size of proofs and impacts proof checking."
                },
                "authors": [
                    {
                        "name": "Shuai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Wang"
                },
                "author": "Shuai Wang",
                "arxiv_comment": "The paper was presented in the student session of the European Summer\n  School in Logic, Language, and Information (ESSLLI) in 2016",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q60, 68V30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1; F.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20956v1",
                "updated": "2024-12-30T13:53:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    53,
                    51,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T13:53:51Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    53,
                    51,
                    0,
                    365,
                    0
                ],
                "title": "QuantumLLMInstruct: A 500k LLM Instruction-Tuning Dataset with\n  Problem-Solution Pairs for Quantum Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantumLLMInstruct: A 500k LLM Instruction-Tuning Dataset with\n  Problem-Solution Pairs for Quantum Computing"
                },
                "summary": "We present QuantumLLMInstruct (QLMMI), an innovative dataset featuring over\n500,000 meticulously curated instruction-following problem-solution pairs\ndesigned specifically for quantum computing - the largest and most\ncomprehensive dataset of its kind. Originating from over 90 primary seed\ndomains and encompassing hundreds of subdomains autonomously generated by LLMs,\nQLMMI marks a transformative step in the diversity and richness of quantum\ncomputing datasets.\n  Designed for instruction fine-tuning, QLMMI seeks to significantly improve\nLLM performance in addressing complex quantum computing challenges across a\nwide range of quantum physics topics. While Large Language Models (LLMs) have\npropelled advancements in computational science with datasets like Omni-MATH\nand OpenMathInstruct, these primarily target Olympiad-level mathematics,\nleaving quantum computing largely unexplored.\n  The creation of QLMMI follows a rigorous four-stage methodology. Initially,\nfoundational problems are developed using predefined templates, focusing on\ncritical areas such as synthetic Hamiltonians, QASM code generation,\nJordan-Wigner transformations, and Trotter-Suzuki quantum circuit\ndecompositions. Next, detailed and domain-specific solutions are crafted to\nensure accuracy and relevance. In the third stage, the dataset is enriched\nthrough advanced reasoning techniques, including Chain-of-Thought (CoT) and\nTask-Oriented Reasoning and Action (ToRA), which enhance problem-solution\ndiversity while adhering to strict mathematical standards. Lastly, a zero-shot\nJudge LLM performs self-assessments to validate the dataset's quality and\nreliability, minimizing human oversight requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present QuantumLLMInstruct (QLMMI), an innovative dataset featuring over\n500,000 meticulously curated instruction-following problem-solution pairs\ndesigned specifically for quantum computing - the largest and most\ncomprehensive dataset of its kind. Originating from over 90 primary seed\ndomains and encompassing hundreds of subdomains autonomously generated by LLMs,\nQLMMI marks a transformative step in the diversity and richness of quantum\ncomputing datasets.\n  Designed for instruction fine-tuning, QLMMI seeks to significantly improve\nLLM performance in addressing complex quantum computing challenges across a\nwide range of quantum physics topics. While Large Language Models (LLMs) have\npropelled advancements in computational science with datasets like Omni-MATH\nand OpenMathInstruct, these primarily target Olympiad-level mathematics,\nleaving quantum computing largely unexplored.\n  The creation of QLMMI follows a rigorous four-stage methodology. Initially,\nfoundational problems are developed using predefined templates, focusing on\ncritical areas such as synthetic Hamiltonians, QASM code generation,\nJordan-Wigner transformations, and Trotter-Suzuki quantum circuit\ndecompositions. Next, detailed and domain-specific solutions are crafted to\nensure accuracy and relevance. In the third stage, the dataset is enriched\nthrough advanced reasoning techniques, including Chain-of-Thought (CoT) and\nTask-Oriented Reasoning and Action (ToRA), which enhance problem-solution\ndiversity while adhering to strict mathematical standards. Lastly, a zero-shot\nJudge LLM performs self-assessments to validate the dataset's quality and\nreliability, minimizing human oversight requirements."
                },
                "authors": [
                    {
                        "name": "Shlomo Kashani"
                    }
                ],
                "author_detail": {
                    "name": "Shlomo Kashani"
                },
                "author": "Shlomo Kashani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09516v2",
                "updated": "2024-12-30T13:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    53,
                    16,
                    0,
                    365,
                    0
                ],
                "published": "2023-10-14T07:02:54Z",
                "published_parsed": [
                    2023,
                    10,
                    14,
                    7,
                    2,
                    54,
                    5,
                    287,
                    0
                ],
                "title": "Efficient Link Prediction via GNN Layers Induced by Negative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Link Prediction via GNN Layers Induced by Negative Sampling"
                },
                "summary": "Graph neural networks (GNNs) for link prediction can loosely be divided into\ntwo broad categories. First, \\emph{node-wise} architectures pre-compute\nindividual embeddings for each node that are later combined by a simple decoder\nto make predictions. While extremely efficient at inference time, model\nexpressiveness is limited such that isomorphic nodes contributing to candidate\nedges may not be distinguishable, compromising accuracy. In contrast,\n\\emph{edge-wise} methods rely on the formation of edge-specific subgraph\nembeddings to enrich the representation of pair-wise relationships,\ndisambiguating isomorphic nodes to improve accuracy, but with increased model\ncomplexity. To better navigate this trade-off, we propose a novel GNN\narchitecture whereby the \\emph{forward pass} explicitly depends on \\emph{both}\npositive (as is typical) and negative (unique to our approach) edges to inform\nmore flexible, yet still cheap node-wise embeddings. This is achieved by\nrecasting the embeddings themselves as minimizers of a forward-pass-specific\nenergy function that favors separation of positive and negative samples.\nNotably, this energy is distinct from the actual training loss shared by most\nexisting link prediction models, where contrastive pairs only influence the\n\\textit{backward pass}. As demonstrated by extensive empirical evaluations, the\nresulting architecture retains the inference speed of node-wise models, while\nproducing competitive accuracy with edge-wise alternatives. We released our\ncode at https://github.com/yxzwang/SubmissionverOfYinYanGNN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) for link prediction can loosely be divided into\ntwo broad categories. First, \\emph{node-wise} architectures pre-compute\nindividual embeddings for each node that are later combined by a simple decoder\nto make predictions. While extremely efficient at inference time, model\nexpressiveness is limited such that isomorphic nodes contributing to candidate\nedges may not be distinguishable, compromising accuracy. In contrast,\n\\emph{edge-wise} methods rely on the formation of edge-specific subgraph\nembeddings to enrich the representation of pair-wise relationships,\ndisambiguating isomorphic nodes to improve accuracy, but with increased model\ncomplexity. To better navigate this trade-off, we propose a novel GNN\narchitecture whereby the \\emph{forward pass} explicitly depends on \\emph{both}\npositive (as is typical) and negative (unique to our approach) edges to inform\nmore flexible, yet still cheap node-wise embeddings. This is achieved by\nrecasting the embeddings themselves as minimizers of a forward-pass-specific\nenergy function that favors separation of positive and negative samples.\nNotably, this energy is distinct from the actual training loss shared by most\nexisting link prediction models, where contrastive pairs only influence the\n\\textit{backward pass}. As demonstrated by extensive empirical evaluations, the\nresulting architecture retains the inference speed of node-wise models, while\nproducing competitive accuracy with edge-wise alternatives. We released our\ncode at https://github.com/yxzwang/SubmissionverOfYinYanGNN."
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Xiannian Hu"
                    },
                    {
                        "name": "Quan Gan"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "David Wipf"
                    }
                ],
                "author_detail": {
                    "name": "David Wipf"
                },
                "author": "David Wipf",
                "arxiv_doi": "10.1109/TKDE.2024.3481015",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TKDE.2024.3481015",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.09516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to TKDE. Citation information: DOI 10.1109/TKDE.2024.3481015",
                "arxiv_journal_ref": "IEEE Transactions on Knowledge and Data Engineering,2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20954v1",
                "updated": "2024-12-30T13:50:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    50,
                    20,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T13:50:20Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    50,
                    20,
                    0,
                    365,
                    0
                ],
                "title": "AGON: Automated Design Framework for Customizing Processors from ISA\n  Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGON: Automated Design Framework for Customizing Processors from ISA\n  Documents"
                },
                "summary": "Customized processors are attractive solutions for vast domain-specific\napplications due to their high energy efficiency. However, designing a\nprocessor in traditional flows is time-consuming and expensive. To address\nthis, researchers have explored methods including the use of agile development\ntools like Chisel or SpinalHDL, high-level synthesis (HLS) from programming\nlanguages like C or SystemC, and more recently, leveraging large language\nmodels (LLMs) to generate hardware description language (HDL) code from natural\nlanguage descriptions. However, each method has limitations in terms of\nexpressiveness, correctness, and performance, leading to a persistent\ncontradiction between the level of automation and the effectiveness of the\ndesign. Overall, how to automatically design highly efficient and practical\nprocessors with minimal human effort remains a challenge.\n  In this paper, we propose AGON, a novel framework designed to leverage LLMs\nfor the efficient design of out-of-order (OoO) customized processors with\nminimal human effort. Central to AGON is the nano-operator function (nOP\nfunction) based Intermediate Representation (IR), which bridges high-level\ndescriptions and hardware implementations while decoupling functionality from\nperformance optimization, thereby providing an automatic design framework that\nis expressive and efficient, has correctness guarantees, and enables PPA\n(Power, Performance, and Area) optimization.\n  Experimental results show that superior to previous LLM-assisted automatic\ndesign flows, AGON facilitates designing a series of customized OoO processors\nthat achieve on average 2.35 $\\times$ speedup compared with BOOM, a\ngeneral-purpose CPU designed by experts, with minimal design effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customized processors are attractive solutions for vast domain-specific\napplications due to their high energy efficiency. However, designing a\nprocessor in traditional flows is time-consuming and expensive. To address\nthis, researchers have explored methods including the use of agile development\ntools like Chisel or SpinalHDL, high-level synthesis (HLS) from programming\nlanguages like C or SystemC, and more recently, leveraging large language\nmodels (LLMs) to generate hardware description language (HDL) code from natural\nlanguage descriptions. However, each method has limitations in terms of\nexpressiveness, correctness, and performance, leading to a persistent\ncontradiction between the level of automation and the effectiveness of the\ndesign. Overall, how to automatically design highly efficient and practical\nprocessors with minimal human effort remains a challenge.\n  In this paper, we propose AGON, a novel framework designed to leverage LLMs\nfor the efficient design of out-of-order (OoO) customized processors with\nminimal human effort. Central to AGON is the nano-operator function (nOP\nfunction) based Intermediate Representation (IR), which bridges high-level\ndescriptions and hardware implementations while decoupling functionality from\nperformance optimization, thereby providing an automatic design framework that\nis expressive and efficient, has correctness guarantees, and enables PPA\n(Power, Performance, and Area) optimization.\n  Experimental results show that superior to previous LLM-assisted automatic\ndesign flows, AGON facilitates designing a series of customized OoO processors\nthat achieve on average 2.35 $\\times$ speedup compared with BOOM, a\ngeneral-purpose CPU designed by experts, with minimal design effort."
                },
                "authors": [
                    {
                        "name": "Chongxiao Li"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Pengwei Jin"
                    },
                    {
                        "name": "Tianyun Ma"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shuyao Cheng"
                    },
                    {
                        "name": "Yifan Hao"
                    },
                    {
                        "name": "Yongwei Zhao"
                    },
                    {
                        "name": "Guanglin Xu"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Xiaqing Li"
                    },
                    {
                        "name": "Yuanbo Wen"
                    },
                    {
                        "name": "Yanjun Wu"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07099v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07099v3",
                "updated": "2024-12-30T13:43:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    43,
                    46,
                    0,
                    365,
                    0
                ],
                "published": "2024-06-18T07:46:13Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    7,
                    46,
                    13,
                    1,
                    170,
                    0
                ],
                "title": "Nash CoT: Multi-Path Inference with Preference Equilibrium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nash CoT: Multi-Path Inference with Preference Equilibrium"
                },
                "summary": "Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na game system on each path that balances the generation from role-specific\nLLMs' and the general LLMs' generation, thereby ensuring both effective role\nadoption and diversity in LLM generation further maintaining the performance of\nmulti-path inference while reducing the requirement of the number of inference\npaths. We evaluate Nash CoT across various inference tasks, including Arabic\nReasoning, Commonsense Question Answering, and Symbolic Inference, achieving\nresults that are comparable to or better than those of multi-path CoT with the\nequal number of inference paths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na game system on each path that balances the generation from role-specific\nLLMs' and the general LLMs' generation, thereby ensuring both effective role\nadoption and diversity in LLM generation further maintaining the performance of\nmulti-path inference while reducing the requirement of the number of inference\npaths. We evaluate Nash CoT across various inference tasks, including Arabic\nReasoning, Commonsense Question Answering, and Symbolic Inference, achieving\nresults that are comparable to or better than those of multi-path CoT with the\nequal number of inference paths."
                },
                "authors": [
                    {
                        "name": "Ziqi Zhang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Xiong Xiao"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "arxiv_journal_ref": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07099v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07099v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.00576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.00576v2",
                "updated": "2024-12-30T13:42:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    42,
                    25,
                    0,
                    365,
                    0
                ],
                "published": "2023-01-02T09:35:42Z",
                "published_parsed": [
                    2023,
                    1,
                    2,
                    9,
                    35,
                    42,
                    0,
                    2,
                    0
                ],
                "title": "Time-averaging Polarimetric and Spectral Properties of Gamma-Ray Bursts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-averaging Polarimetric and Spectral Properties of Gamma-Ray Bursts"
                },
                "summary": "The composition and radiation mechanism of gamma-ray bursts (GRBs) within\njets continue to be hotly debated. Investigating the joint polarimetric and\nspectral properties is crucial for understanding GRB composition and radiation\nmechanism. Various jet properties, such as ``kinetic-energy-dominated\" (KED),\n``Poynting-flux-dominated\" (PFD), and ``hybrid-dominated\" (HD) relativistic\noutflows, have been inferred from observed GRB spectra, with expectations of\ndiffering polarization levels among them. In this study, we analyzed a sample\nof 27 GRBs detected by the Gamma-ray Burst Monitor on board the NASA Fermi\nGamma-ray Space Telescope, focusing on 26 bursts with significant polarization\nmeasurements. Our analysis revealed that 16 bursts (62\\%) were predominantly\nassociated with the ``PFD'' jet type, while 10 bursts (38\\%) were classified as\nHD, implying that photosphere emission may also be a possible mechanism\npowering the high levels of polarization. Notably, no bursts were identified as\nKED-type. We found distinct polarization properties, with HD-type bursts\nexhibiting consistently higher polarization levels than PFD-type bursts. We\nproposed models incorporating ordered and random magnetic field configurations\nspecific to hybrid jets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The composition and radiation mechanism of gamma-ray bursts (GRBs) within\njets continue to be hotly debated. Investigating the joint polarimetric and\nspectral properties is crucial for understanding GRB composition and radiation\nmechanism. Various jet properties, such as ``kinetic-energy-dominated\" (KED),\n``Poynting-flux-dominated\" (PFD), and ``hybrid-dominated\" (HD) relativistic\noutflows, have been inferred from observed GRB spectra, with expectations of\ndiffering polarization levels among them. In this study, we analyzed a sample\nof 27 GRBs detected by the Gamma-ray Burst Monitor on board the NASA Fermi\nGamma-ray Space Telescope, focusing on 26 bursts with significant polarization\nmeasurements. Our analysis revealed that 16 bursts (62\\%) were predominantly\nassociated with the ``PFD'' jet type, while 10 bursts (38\\%) were classified as\nHD, implying that photosphere emission may also be a possible mechanism\npowering the high levels of polarization. Notably, no bursts were identified as\nKED-type. We found distinct polarization properties, with HD-type bursts\nexhibiting consistently higher polarization levels than PFD-type bursts. We\nproposed models incorporating ordered and random magnetic field configurations\nspecific to hybrid jets."
                },
                "authors": [
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Soroush Shakeri"
                    }
                ],
                "author_detail": {
                    "name": "Soroush Shakeri"
                },
                "author": "Soroush Shakeri",
                "arxiv_doi": "10.3847/1538-4365/ad957f",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4365/ad957f",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2301.00576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.00576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "29 pages, 7 figures (including 62 panels), 4 tables, accepted for\n  publication in The Astrophysical Journal Supplement Series",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20942v1",
                "updated": "2024-12-30T13:36:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    36,
                    5,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T13:36:05Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    36,
                    5,
                    0,
                    365,
                    0
                ],
                "title": "Ontology-grounded Automatic Knowledge Graph Construction by LLM under\n  Wikidata schema",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-grounded Automatic Knowledge Graph Construction by LLM under\n  Wikidata schema"
                },
                "summary": "We propose an ontology-grounded approach to Knowledge Graph (KG) construction\nusing Large Language Models (LLMs) on a knowledge base. An ontology is authored\nby generating Competency Questions (CQ) on knowledge base to discover knowledge\nscope, extracting relations from CQs, and attempt to replace equivalent\nrelations by their counterpart in Wikidata. To ensure consistency and\ninterpretability in the resulting KG, we ground generation of KG with the\nauthored ontology based on extracted relations. Evaluation on benchmark\ndatasets demonstrates competitive performance in knowledge graph construction\ntask. Our work presents a promising direction for scalable KG construction\npipeline with minimal human intervention, that yields high quality and\nhuman-interpretable KGs, which are interoperable with Wikidata semantics for\npotential knowledge base expansion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an ontology-grounded approach to Knowledge Graph (KG) construction\nusing Large Language Models (LLMs) on a knowledge base. An ontology is authored\nby generating Competency Questions (CQ) on knowledge base to discover knowledge\nscope, extracting relations from CQs, and attempt to replace equivalent\nrelations by their counterpart in Wikidata. To ensure consistency and\ninterpretability in the resulting KG, we ground generation of KG with the\nauthored ontology based on extracted relations. Evaluation on benchmark\ndatasets demonstrates competitive performance in knowledge graph construction\ntask. Our work presents a promising direction for scalable KG construction\npipeline with minimal human intervention, that yields high quality and\nhuman-interpretable KGs, which are interoperable with Wikidata semantics for\npotential knowledge base expansion."
                },
                "authors": [
                    {
                        "name": "Xiaohan Feng"
                    },
                    {
                        "name": "Xixin Wu"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "arxiv_comment": "Presented at HI-AI@KDD, Human-Interpretable AI Workshop at the KDD\n  2024, 26th of August 2024, Barcelona, Spain",
                "arxiv_journal_ref": "CEUR Workshop Proceedings 3841 (2024) 117-135",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12075v2",
                "updated": "2024-12-30T13:34:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    34,
                    23,
                    0,
                    365,
                    0
                ],
                "published": "2024-10-15T21:29:26Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    21,
                    29,
                    26,
                    1,
                    289,
                    0
                ],
                "title": "WeatherDG: LLM-assisted Diffusion Model for Procedural Weather\n  Generation in Domain-Generalized Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeatherDG: LLM-assisted Diffusion Model for Procedural Weather\n  Generation in Domain-Generalized Semantic Segmentation"
                },
                "summary": "In this work, we propose a novel approach, namely WeatherDG, that can\ngenerate realistic, weather-diverse, and driving-screen images based on the\ncooperation of two foundation models, i.e, Stable Diffusion (SD) and Large\nLanguage Model (LLM). Specifically, we first fine-tune the SD with source data,\naligning the content and layout of generated samples with real-world driving\nscenarios. Then, we propose a procedural prompt generation method based on LLM,\nwhich can enrich scenario descriptions and help SD automatically generate more\ndiverse, detailed images. In addition, we introduce a balanced generation\nstrategy, which encourages the SD to generate high-quality objects of tailed\nclasses under various weather conditions, such as riders and motorcycles. This\nsegmentation-model-agnostic method can improve the generalization ability of\nexisting models by additionally adapting them with the generated synthetic\ndata. Experiments on three challenging datasets show that our method can\nsignificantly improve the segmentation performance of different\nstate-of-the-art models on target domains. Notably, in the setting of\n''Cityscapes to ACDC'', our method improves the baseline HRDA by 13.9% in mIoU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a novel approach, namely WeatherDG, that can\ngenerate realistic, weather-diverse, and driving-screen images based on the\ncooperation of two foundation models, i.e, Stable Diffusion (SD) and Large\nLanguage Model (LLM). Specifically, we first fine-tune the SD with source data,\naligning the content and layout of generated samples with real-world driving\nscenarios. Then, we propose a procedural prompt generation method based on LLM,\nwhich can enrich scenario descriptions and help SD automatically generate more\ndiverse, detailed images. In addition, we introduce a balanced generation\nstrategy, which encourages the SD to generate high-quality objects of tailed\nclasses under various weather conditions, such as riders and motorcycles. This\nsegmentation-model-agnostic method can improve the generalization ability of\nexisting models by additionally adapting them with the generated synthetic\ndata. Experiments on three challenging datasets show that our method can\nsignificantly improve the segmentation performance of different\nstate-of-the-art models on target domains. Notably, in the setting of\n''Cityscapes to ACDC'', our method improves the baseline HRDA by 13.9% in mIoU."
                },
                "authors": [
                    {
                        "name": "Chenghao Qian"
                    },
                    {
                        "name": "Yuhu Guo"
                    },
                    {
                        "name": "Yuhong Mo"
                    },
                    {
                        "name": "Wenjing Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjing Li"
                },
                "author": "Wenjing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20927v1",
                "updated": "2024-12-30T13:16:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    16,
                    8,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T13:16:08Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    16,
                    8,
                    0,
                    365,
                    0
                ],
                "title": "Enhanced Multimodal RAG-LLM for Accurate Visual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Multimodal RAG-LLM for Accurate Visual Question Answering"
                },
                "summary": "Multimodal large language models (MLLMs), such as GPT-4o, Gemini, LLaVA, and\nFlamingo, have made significant progress in integrating visual and textual\nmodalities, excelling in tasks like visual question answering (VQA), image\ncaptioning, and content retrieval. They can generate coherent and contextually\nrelevant descriptions of images. However, they still face challenges in\naccurately identifying and counting objects and determining their spatial\nlocations, particularly in complex scenes with overlapping or small objects. To\naddress these limitations, we propose a novel framework based on multimodal\nretrieval-augmented generation (RAG), which introduces structured scene graphs\nto enhance object recognition, relationship identification, and spatial\nunderstanding within images. Our framework improves the MLLM's capacity to\nhandle tasks requiring precise visual descriptions, especially in scenarios\nwith challenging perspectives, such as aerial views or scenes with dense object\narrangements. Finally, we conduct extensive experiments on the VG-150 dataset\nthat focuses on first-person visual understanding and the AUG dataset that\ninvolves aerial imagery. The results show that our approach consistently\noutperforms existing MLLMs in VQA tasks, which stands out in recognizing,\nlocalizing, and quantifying objects in different spatial contexts and provides\nmore accurate visual descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs), such as GPT-4o, Gemini, LLaVA, and\nFlamingo, have made significant progress in integrating visual and textual\nmodalities, excelling in tasks like visual question answering (VQA), image\ncaptioning, and content retrieval. They can generate coherent and contextually\nrelevant descriptions of images. However, they still face challenges in\naccurately identifying and counting objects and determining their spatial\nlocations, particularly in complex scenes with overlapping or small objects. To\naddress these limitations, we propose a novel framework based on multimodal\nretrieval-augmented generation (RAG), which introduces structured scene graphs\nto enhance object recognition, relationship identification, and spatial\nunderstanding within images. Our framework improves the MLLM's capacity to\nhandle tasks requiring precise visual descriptions, especially in scenarios\nwith challenging perspectives, such as aerial views or scenes with dense object\narrangements. Finally, we conduct extensive experiments on the VG-150 dataset\nthat focuses on first-person visual understanding and the AUG dataset that\ninvolves aerial imagery. The results show that our approach consistently\noutperforms existing MLLMs in VQA tasks, which stands out in recognizing,\nlocalizing, and quantifying objects in different spatial contexts and provides\nmore accurate visual descriptions."
                },
                "authors": [
                    {
                        "name": "Junxiao Xue"
                    },
                    {
                        "name": "Quan Deng"
                    },
                    {
                        "name": "Fei Yu"
                    },
                    {
                        "name": "Yanhao Wang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yuehua Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuehua Li"
                },
                "author": "Yuehua Li",
                "arxiv_comment": "6 pages, 3 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20912v1",
                "updated": "2024-12-30T12:44:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    12,
                    44,
                    48,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T12:44:48Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    12,
                    44,
                    48,
                    0,
                    365,
                    0
                ],
                "title": "Dynamics of Information Exchange in Zebrafish: The Role of U-Turns in\n  Visual Communication and Behavior Modulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamics of Information Exchange in Zebrafish: The Role of U-Turns in\n  Visual Communication and Behavior Modulation"
                },
                "summary": "Motions of visually coupled zebrafish pairs are studied to understand the\neffects of information exchange on their behavior as a function of their\nminimal separation ($d$). We find that when $d$ is small, the pair can display\na leader-follower relation (LFR) with trajectories of almost synchronized form.\nHowever, with larger $d$, although the same LFR is still maintained, the\noriginally similar trajectories turn into different forms. Detailed analysis of\ntheir motion trajectories suggests that the pair might be using U-turns (UTs)\nto exchange information and to maintain a LFR at the same time. A simulation\nmodel based on UTs with inferred and proposed rules is able to reproduce\nprominent features of observed trajectories; indicating that the transition of\ntrajectories can be understood as the result of a change in information\nexchange between the fish as $d$ increases. Our finding that UTs as important\nvisual signals is consistent with the fact that UTs can induce a large amount\nof firings in retinas of observing fish.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motions of visually coupled zebrafish pairs are studied to understand the\neffects of information exchange on their behavior as a function of their\nminimal separation ($d$). We find that when $d$ is small, the pair can display\na leader-follower relation (LFR) with trajectories of almost synchronized form.\nHowever, with larger $d$, although the same LFR is still maintained, the\noriginally similar trajectories turn into different forms. Detailed analysis of\ntheir motion trajectories suggests that the pair might be using U-turns (UTs)\nto exchange information and to maintain a LFR at the same time. A simulation\nmodel based on UTs with inferred and proposed rules is able to reproduce\nprominent features of observed trajectories; indicating that the transition of\ntrajectories can be understood as the result of a change in information\nexchange between the fish as $d$ increases. Our finding that UTs as important\nvisual signals is consistent with the fact that UTs can induce a large amount\nof firings in retinas of observing fish."
                },
                "authors": [
                    {
                        "name": "C. K. Chan"
                    },
                    {
                        "name": "Hao-Yun Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Hao-Yun Hsu"
                },
                "author": "Hao-Yun Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.08747v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.08747v4",
                "updated": "2024-12-30T12:32:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    12,
                    32,
                    49,
                    0,
                    365,
                    0
                ],
                "published": "2023-08-17T02:53:23Z",
                "published_parsed": [
                    2023,
                    8,
                    17,
                    2,
                    53,
                    23,
                    3,
                    229,
                    0
                ],
                "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models\n  During Continual Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Catastrophic Forgetting in Large Language Models\n  During Continual Fine-tuning"
                },
                "summary": "Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge for achieving a satisfactory performance in downstream tasks. As\nlarge language models (LLMs) have demonstrated remarkable performance, it is\nintriguing to investigate whether CF exists during the continual instruction\ntuning of LLMs. This study empirically evaluates the forgetting phenomenon in\nLLMs' knowledge during continual instruction tuning from the perspectives of\ndomain knowledge, reasoning, and reading comprehension. The experiments reveal\nthat catastrophic forgetting is generally observed in LLMs ranging from 1b to\n7b parameters. Surprisingly, as the model scale increases, the severity of\nforgetting intensifies in such a model sale range which may result from the\nmuch significant initial performance in the larger LLM. Comparing the\ndecoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits\nless forgetting and retains more knowledge. Interestingly, we also observe that\nLLMs can mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that general instruction tuning\ncan help alleviate the forgetting phenomenon in LLMs during subsequent\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge for achieving a satisfactory performance in downstream tasks. As\nlarge language models (LLMs) have demonstrated remarkable performance, it is\nintriguing to investigate whether CF exists during the continual instruction\ntuning of LLMs. This study empirically evaluates the forgetting phenomenon in\nLLMs' knowledge during continual instruction tuning from the perspectives of\ndomain knowledge, reasoning, and reading comprehension. The experiments reveal\nthat catastrophic forgetting is generally observed in LLMs ranging from 1b to\n7b parameters. Surprisingly, as the model scale increases, the severity of\nforgetting intensifies in such a model sale range which may result from the\nmuch significant initial performance in the larger LLM. Comparing the\ndecoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits\nless forgetting and retains more knowledge. Interestingly, we also observe that\nLLMs can mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that general instruction tuning\ncan help alleviate the forgetting phenomenon in LLMs during subsequent\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.08747v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.08747v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20903v1",
                "updated": "2024-12-30T12:29:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    12,
                    29,
                    2,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T12:29:02Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    12,
                    29,
                    2,
                    0,
                    365,
                    0
                ],
                "title": "WalkVLM:Aid Visually Impaired People Walking by Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WalkVLM:Aid Visually Impaired People Walking by Vision Language Model"
                },
                "summary": "Approximately 200 million individuals around the world suffer from varying\ndegrees of visual impairment, making it crucial to leverage AI technology to\noffer walking assistance for these people. With the recent progress of\nvision-language models (VLMs), employing VLMs to improve this field has emerged\nas a popular research topic. However, most existing methods are studied on\nself-built question-answering datasets, lacking a unified training and testing\nbenchmark for walk guidance. Moreover, in blind walking task, it is necessary\nto perform real-time streaming video parsing and generate concise yet\ninformative reminders, which poses a great challenge for VLMs that suffer from\nredundant responses and low inference efficiency. In this paper, we firstly\nrelease a diverse, extensive, and unbiased walking awareness dataset,\ncontaining 12k video-manual annotation pairs from Europe and Asia to provide a\nfair training and testing benchmark for blind walking task. Furthermore, a\nWalkVLM model is proposed, which employs chain of thought for hierarchical\nplanning to generate concise but informative reminders and utilizes\ntemporal-aware adaptive prediction to reduce the temporal redundancy of\nreminders. Finally, we have established a solid benchmark for blind walking\ntask and verified the advantages of WalkVLM in stream video processing for this\ntask compared to other VLMs. Our dataset and code will be released at anonymous\nlink https://walkvlm2024.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximately 200 million individuals around the world suffer from varying\ndegrees of visual impairment, making it crucial to leverage AI technology to\noffer walking assistance for these people. With the recent progress of\nvision-language models (VLMs), employing VLMs to improve this field has emerged\nas a popular research topic. However, most existing methods are studied on\nself-built question-answering datasets, lacking a unified training and testing\nbenchmark for walk guidance. Moreover, in blind walking task, it is necessary\nto perform real-time streaming video parsing and generate concise yet\ninformative reminders, which poses a great challenge for VLMs that suffer from\nredundant responses and low inference efficiency. In this paper, we firstly\nrelease a diverse, extensive, and unbiased walking awareness dataset,\ncontaining 12k video-manual annotation pairs from Europe and Asia to provide a\nfair training and testing benchmark for blind walking task. Furthermore, a\nWalkVLM model is proposed, which employs chain of thought for hierarchical\nplanning to generate concise but informative reminders and utilizes\ntemporal-aware adaptive prediction to reduce the temporal redundancy of\nreminders. Finally, we have established a solid benchmark for blind walking\ntask and verified the advantages of WalkVLM in stream video processing for this\ntask compared to other VLMs. Our dataset and code will be released at anonymous\nlink https://walkvlm2024.github.io."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Yuan"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Jiapei Zhang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jinchao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jinchao Zhang"
                },
                "author": "Jinchao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20891v1",
                "updated": "2024-12-30T12:00:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    12,
                    0,
                    47,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T12:00:47Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    12,
                    0,
                    47,
                    0,
                    365,
                    0
                ],
                "title": "DoTA: Weight-Decomposed Tensor Adaptation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoTA: Weight-Decomposed Tensor Adaptation for Large Language Models"
                },
                "summary": "Low-rank adaptation (LoRA) reduces the computational and memory demands of\nfine-tuning large language models (LLMs) by approximating updates with low-rank\nmatrices. However, low-rank approximation in two-dimensional space fails to\ncapture high-dimensional structures within the target matrix. Recently, tensor\ndecomposition methods have been explored for fine-tuning LLMs, leveraging their\nability to extract structured information. Yet, these approaches primarily rely\non random initialization, and the impact of initialization on tensor adaptation\nremains underexplored. In this paper, we reveal that random initialization\nsignificantly diverges from the validation loss achieved by full fine-tuning.\nTo address this, we propose Weight-Decomposed Tensor Adaptation (DoTA), which\nleverages the Matrix Product Operator (MPO) decomposition of pre-trained\nweights for effective initialization in fine-tuning LLMs. Additionally, we\nintroduce QDoTA, a quantized version of DoTA designed for 4-bit quantization.\nExperiments on commonsense and arithmetic reasoning tasks show that DoTA\noutperforms random initialization methods with fewer parameters. QDoTA further\nreduces memory consumption and achieves comparable performance to DoTA on\ncommonsense reasoning tasks. We will release our code to support future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank adaptation (LoRA) reduces the computational and memory demands of\nfine-tuning large language models (LLMs) by approximating updates with low-rank\nmatrices. However, low-rank approximation in two-dimensional space fails to\ncapture high-dimensional structures within the target matrix. Recently, tensor\ndecomposition methods have been explored for fine-tuning LLMs, leveraging their\nability to extract structured information. Yet, these approaches primarily rely\non random initialization, and the impact of initialization on tensor adaptation\nremains underexplored. In this paper, we reveal that random initialization\nsignificantly diverges from the validation loss achieved by full fine-tuning.\nTo address this, we propose Weight-Decomposed Tensor Adaptation (DoTA), which\nleverages the Matrix Product Operator (MPO) decomposition of pre-trained\nweights for effective initialization in fine-tuning LLMs. Additionally, we\nintroduce QDoTA, a quantized version of DoTA designed for 4-bit quantization.\nExperiments on commonsense and arithmetic reasoning tasks show that DoTA\noutperforms random initialization methods with fewer parameters. QDoTA further\nreduces memory consumption and achieves comparable performance to DoTA on\ncommonsense reasoning tasks. We will release our code to support future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Xiang Cheng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20885v1",
                "updated": "2024-12-30T11:52:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    52,
                    39,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:52:39Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    52,
                    39,
                    0,
                    365,
                    0
                ],
                "title": "CF-CGN: Channel Fingerprints Extrapolation for Multi-band Massive MIMO\n  Transmission based on Cycle-Consistent Generative Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CF-CGN: Channel Fingerprints Extrapolation for Multi-band Massive MIMO\n  Transmission based on Cycle-Consistent Generative Networks"
                },
                "summary": "Multi-band massive multiple-input multiple-output (MIMO) communication can\npromote the cooperation of licensed and unlicensed spectra, effectively\nenhancing spectrum efficiency for Wi-Fi and other wireless systems. As an\nenabler for multi-band transmission, channel fingerprints (CF), also known as\nthe channel knowledge map or radio environment map, are used to assist channel\nstate information (CSI) acquisition and reduce computational complexity. In\nthis paper, we propose CF-CGN (Channel Fingerprints with Cycle-consistent\nGenerative Networks) to extrapolate CF for multi-band massive MIMO transmission\nwhere licensed and unlicensed spectra cooperate to provide ubiquitous\nconnectivity. Specifically, we first model CF as a multichannel image and\ntransform the extrapolation problem into an image translation task, which\nconverts CF from one frequency to another by exploring the shared\ncharacteristics of statistical CSI in the beam domain. Then, paired generative\nnetworks are designed and coupled by variable-weight cycle consistency losses\nto fit the reciprocal relationship at different bands. Matched with the coupled\nnetworks, a joint training strategy is developed accordingly, supporting\nsynchronous optimization of all trainable parameters. During the inference\nprocess, we also introduce a refining scheme to improve the extrapolation\naccuracy based on the resolution of CF. Numerical results illustrate that our\nproposed CF-CGN can achieve bidirectional extrapolation with an error of 5-17\ndB lower than the benchmarks in different communication scenarios,\ndemonstrating its excellent generalization ability. We further show that the\nsum rate performance assisted by CF-CGN-based CF is close to that with perfect\nCSI for multi-band massive MIMO transmission.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-band massive multiple-input multiple-output (MIMO) communication can\npromote the cooperation of licensed and unlicensed spectra, effectively\nenhancing spectrum efficiency for Wi-Fi and other wireless systems. As an\nenabler for multi-band transmission, channel fingerprints (CF), also known as\nthe channel knowledge map or radio environment map, are used to assist channel\nstate information (CSI) acquisition and reduce computational complexity. In\nthis paper, we propose CF-CGN (Channel Fingerprints with Cycle-consistent\nGenerative Networks) to extrapolate CF for multi-band massive MIMO transmission\nwhere licensed and unlicensed spectra cooperate to provide ubiquitous\nconnectivity. Specifically, we first model CF as a multichannel image and\ntransform the extrapolation problem into an image translation task, which\nconverts CF from one frequency to another by exploring the shared\ncharacteristics of statistical CSI in the beam domain. Then, paired generative\nnetworks are designed and coupled by variable-weight cycle consistency losses\nto fit the reciprocal relationship at different bands. Matched with the coupled\nnetworks, a joint training strategy is developed accordingly, supporting\nsynchronous optimization of all trainable parameters. During the inference\nprocess, we also introduce a refining scheme to improve the extrapolation\naccuracy based on the resolution of CF. Numerical results illustrate that our\nproposed CF-CGN can achieve bidirectional extrapolation with an error of 5-17\ndB lower than the benchmarks in different communication scenarios,\ndemonstrating its excellent generalization ability. We further show that the\nsum rate performance assisted by CF-CGN-based CF is close to that with perfect\nCSI for multi-band massive MIMO transmission."
                },
                "authors": [
                    {
                        "name": "Chenjie Xie"
                    },
                    {
                        "name": "Li You"
                    },
                    {
                        "name": "Zhenzhou Jin"
                    },
                    {
                        "name": "Jinke Tang"
                    },
                    {
                        "name": "Xiqi Gao"
                    },
                    {
                        "name": "Xiang-Gen Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xiang-Gen Xia"
                },
                "author": "Xiang-Gen Xia",
                "arxiv_comment": "13 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20884v1",
                "updated": "2024-12-30T11:51:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    51,
                    14,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:51:14Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    51,
                    14,
                    0,
                    365,
                    0
                ],
                "title": "A gradient-based and determinant-free framework for fully Bayesian\n  Gaussian process regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A gradient-based and determinant-free framework for fully Bayesian\n  Gaussian process regression"
                },
                "summary": "Gaussian Process Regression (GPR) is widely used for inferring functions from\nnoisy data. GPR crucially relies on the choice of a kernel, which might be\nspecified in terms of a collection of hyperparameters that must be chosen or\nlearned. Fully Bayesian GPR seeks to infer these kernel hyperparameters in a\nBayesian sense, and the key computational challenge in sampling from their\nposterior distribution is the need for frequent determinant evaluations of\nlarge kernel matrices. This paper introduces a gradient-based, determinant-free\napproach for fully Bayesian GPR that combines a Gaussian integration trick for\navoiding the determinant with Hamiltonian Monte Carlo (HMC) sampling. Our\nframework permits a matrix-free formulation and reduces the difficulty of\ndealing with hyperparameter gradients to a simple automatic differentiation.\nOur implementation is highly flexible and leverages GPU acceleration with\nlinear-scaling memory footprint. Numerical experiments demonstrate the method's\nability to scale gracefully to both high-dimensional hyperparameter spaces and\nlarge kernel matrices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Process Regression (GPR) is widely used for inferring functions from\nnoisy data. GPR crucially relies on the choice of a kernel, which might be\nspecified in terms of a collection of hyperparameters that must be chosen or\nlearned. Fully Bayesian GPR seeks to infer these kernel hyperparameters in a\nBayesian sense, and the key computational challenge in sampling from their\nposterior distribution is the need for frequent determinant evaluations of\nlarge kernel matrices. This paper introduces a gradient-based, determinant-free\napproach for fully Bayesian GPR that combines a Gaussian integration trick for\navoiding the determinant with Hamiltonian Monte Carlo (HMC) sampling. Our\nframework permits a matrix-free formulation and reduces the difficulty of\ndealing with hyperparameter gradients to a simple automatic differentiation.\nOur implementation is highly flexible and leverages GPU acceleration with\nlinear-scaling memory footprint. Numerical experiments demonstrate the method's\nability to scale gracefully to both high-dimensional hyperparameter spaces and\nlarge kernel matrices."
                },
                "authors": [
                    {
                        "name": "P. Michael Kielstra"
                    },
                    {
                        "name": "Michael Lindsey"
                    }
                ],
                "author_detail": {
                    "name": "Michael Lindsey"
                },
                "author": "Michael Lindsey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65C05, 60G15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20875v1",
                "updated": "2024-12-30T11:25:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    25,
                    54,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:25:54Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    25,
                    54,
                    0,
                    365,
                    0
                ],
                "title": "Attention Is All You Need For Mixture-of-Depths Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Is All You Need For Mixture-of-Depths Routing"
                },
                "summary": "Advancements in deep learning are driven by training models with increasingly\nlarger numbers of parameters, which in turn heightens the computational\ndemands. To address this issue, Mixture-of-Depths (MoD) models have been\nproposed to dynamically assign computations only to the most relevant parts of\nthe inputs, thereby enabling the deployment of large-parameter models with high\nefficiency during inference and training. These MoD models utilize a routing\nmechanism to determine which tokens should be processed by a layer, or skipped.\nHowever, conventional MoD models employ additional network layers specifically\nfor the routing which are difficult to train, and add complexity and deployment\noverhead to the model. In this paper, we introduce a novel attention-based\nrouting mechanism A-MoD that leverages the existing attention map of the\npreceding layer for routing decisions within the current layer. Compared to\nstandard routing, A-MoD allows for more efficient training as it introduces no\nadditional trainable parameters and can be easily adapted from pretrained\ntransformer models. Furthermore, it can increase the performance of the MoD\nmodel. For instance, we observe up to 2% higher accuracy on ImageNet compared\nto standard routing and isoFLOP ViT baselines. Furthermore, A-MoD improves the\nMoD training convergence, leading to up to 2x faster transfer learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in deep learning are driven by training models with increasingly\nlarger numbers of parameters, which in turn heightens the computational\ndemands. To address this issue, Mixture-of-Depths (MoD) models have been\nproposed to dynamically assign computations only to the most relevant parts of\nthe inputs, thereby enabling the deployment of large-parameter models with high\nefficiency during inference and training. These MoD models utilize a routing\nmechanism to determine which tokens should be processed by a layer, or skipped.\nHowever, conventional MoD models employ additional network layers specifically\nfor the routing which are difficult to train, and add complexity and deployment\noverhead to the model. In this paper, we introduce a novel attention-based\nrouting mechanism A-MoD that leverages the existing attention map of the\npreceding layer for routing decisions within the current layer. Compared to\nstandard routing, A-MoD allows for more efficient training as it introduces no\nadditional trainable parameters and can be easily adapted from pretrained\ntransformer models. Furthermore, it can increase the performance of the MoD\nmodel. For instance, we observe up to 2% higher accuracy on ImageNet compared\nto standard routing and isoFLOP ViT baselines. Furthermore, A-MoD improves the\nMoD training convergence, leading to up to 2x faster transfer learning."
                },
                "authors": [
                    {
                        "name": "Advait Gadhikar"
                    },
                    {
                        "name": "Souptik Kumar Majumdar"
                    },
                    {
                        "name": "Niclas Popp"
                    },
                    {
                        "name": "Piyapat Saranrittichai"
                    },
                    {
                        "name": "Martin Rapp"
                    },
                    {
                        "name": "Lukas Schott"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Schott"
                },
                "author": "Lukas Schott",
                "arxiv_comment": "22 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18552v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18552v2",
                "updated": "2024-12-30T11:24:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    24,
                    32,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-24T17:05:26Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    5,
                    26,
                    1,
                    359,
                    0
                ],
                "title": "Distilling Fine-grained Sentiment Understanding from Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Fine-grained Sentiment Understanding from Large Language\n  Models"
                },
                "summary": "Fine-grained sentiment analysis (FSA) aims to extract and summarize user\nopinions from vast opinionated text. Recent studies demonstrate that large\nlanguage models (LLMs) possess exceptional sentiment understanding\ncapabilities. However, directly deploying LLMs for FSA applications incurs high\ninference costs. Therefore, this paper investigates the distillation of\nfine-grained sentiment understanding from LLMs into small language models\n(SLMs). We prompt LLMs to examine and interpret the sentiments of given reviews\nand then utilize the generated content to pretrain SLMs. Additionally, we\ndevelop a comprehensive FSA benchmark to evaluate both SLMs and LLMs. Extensive\nexperiments on this benchmark reveal that: (1) distillation significantly\nenhances the performance of SLMs in FSA tasks, achieving a 6.00\\% improvement\nin $F_1$-score, and the distilled model can outperform Llama-2-7b with only\n220M parameters; (2) distillation equips SLMs with excellent zero-shot\nsentiment classification capabilities, enabling them to match or even exceed\ntheir teacher models. These results suggest that distillation from LLMs is a\nhighly promising direction for FSA. We will release our code, data, and\npretrained model weights at https://github.com/HITSZ-HLT/FSA-Distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained sentiment analysis (FSA) aims to extract and summarize user\nopinions from vast opinionated text. Recent studies demonstrate that large\nlanguage models (LLMs) possess exceptional sentiment understanding\ncapabilities. However, directly deploying LLMs for FSA applications incurs high\ninference costs. Therefore, this paper investigates the distillation of\nfine-grained sentiment understanding from LLMs into small language models\n(SLMs). We prompt LLMs to examine and interpret the sentiments of given reviews\nand then utilize the generated content to pretrain SLMs. Additionally, we\ndevelop a comprehensive FSA benchmark to evaluate both SLMs and LLMs. Extensive\nexperiments on this benchmark reveal that: (1) distillation significantly\nenhances the performance of SLMs in FSA tasks, achieving a 6.00\\% improvement\nin $F_1$-score, and the distilled model can outperform Llama-2-7b with only\n220M parameters; (2) distillation equips SLMs with excellent zero-shot\nsentiment classification capabilities, enabling them to match or even exceed\ntheir teacher models. These results suggest that distillation from LLMs is a\nhighly promising direction for FSA. We will release our code, data, and\npretrained model weights at https://github.com/HITSZ-HLT/FSA-Distillation."
                },
                "authors": [
                    {
                        "name": "Yice Zhang"
                    },
                    {
                        "name": "Guangyu Xie"
                    },
                    {
                        "name": "Hongling Xu"
                    },
                    {
                        "name": "Kaiheng Hou"
                    },
                    {
                        "name": "Jianzhu Bao"
                    },
                    {
                        "name": "Qianlong Wang"
                    },
                    {
                        "name": "Shiwei Chen"
                    },
                    {
                        "name": "Ruifeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruifeng Xu"
                },
                "author": "Ruifeng Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18552v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18552v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20864v1",
                "updated": "2024-12-30T11:07:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    7,
                    5,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:07:05Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    7,
                    5,
                    0,
                    365,
                    0
                ],
                "title": "Enhancing Annotated Bibliography Generation with LLM Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Annotated Bibliography Generation with LLM Ensembles"
                },
                "summary": "This work proposes a novel approach to enhancing annotated bibliography\ngeneration through Large Language Model (LLM) ensembles. In particular,\nmultiple LLMs in different roles -- controllable text generation, evaluation,\nand summarization -- are introduced and validated using a systematic\nmethodology to enhance model performance in scholarly tasks. Output diversity\namong the ensemble that generates text is obtained using different LLM\nparameters, followed by an LLM acting as a judge to assess relevance, accuracy,\nand coherence. Responses selected by several combining strategies are then\nmerged and refined through summarization and redundancy removal techniques. The\npreliminary experimental validation demonstrates that the combined outputs from\nthe LLM ensemble improve coherence and relevance compared to individual\nresponses, leading to a 38% improvement in annotation quality and a 51%\nreduction in content redundancy, thus highlighting the potential for automating\ncomplex scholarly tasks while maintaining high-quality standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a novel approach to enhancing annotated bibliography\ngeneration through Large Language Model (LLM) ensembles. In particular,\nmultiple LLMs in different roles -- controllable text generation, evaluation,\nand summarization -- are introduced and validated using a systematic\nmethodology to enhance model performance in scholarly tasks. Output diversity\namong the ensemble that generates text is obtained using different LLM\nparameters, followed by an LLM acting as a judge to assess relevance, accuracy,\nand coherence. Responses selected by several combining strategies are then\nmerged and refined through summarization and redundancy removal techniques. The\npreliminary experimental validation demonstrates that the combined outputs from\nthe LLM ensemble improve coherence and relevance compared to individual\nresponses, leading to a 38% improvement in annotation quality and a 51%\nreduction in content redundancy, thus highlighting the potential for automating\ncomplex scholarly tasks while maintaining high-quality standards."
                },
                "authors": [
                    {
                        "name": "Sergio Bermejo"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Bermejo"
                },
                "author": "Sergio Bermejo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06691v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06691v3",
                "updated": "2024-12-30T11:05:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    5,
                    20,
                    0,
                    365,
                    0
                ],
                "published": "2024-09-10T17:54:28Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    54,
                    28,
                    1,
                    254,
                    0
                ],
                "title": "Geometric-Averaged Preference Optimization for Soft Preference Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric-Averaged Preference Optimization for Soft Preference Labels"
                },
                "summary": "Many algorithms for aligning LLMs with human preferences assume that human\npreferences are binary and deterministic. However, human preferences can vary\nacross individuals, and therefore should be represented distributionally. In\nthis work, we introduce the distributional soft preference labels and improve\nDirect Preference Optimization (DPO) with a weighted geometric average of the\nLLM output likelihood in the loss function. This approach adjusts the scale of\nlearning loss based on the soft labels such that the loss would approach zero\nwhen the responses are closer to equally preferred. This simple modification\ncan be easily applied to any DPO-based methods and mitigate over-optimization\nand objective mismatch, which prior works suffer from. Our experiments simulate\nthe soft preference labels with AI feedback from LLMs and demonstrate that\ngeometric averaging consistently improves performance on standard benchmarks\nfor alignment research. In particular, we observe more preferable responses\nthan binary labels and significant improvements where modestly-confident labels\nare in the majority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many algorithms for aligning LLMs with human preferences assume that human\npreferences are binary and deterministic. However, human preferences can vary\nacross individuals, and therefore should be represented distributionally. In\nthis work, we introduce the distributional soft preference labels and improve\nDirect Preference Optimization (DPO) with a weighted geometric average of the\nLLM output likelihood in the loss function. This approach adjusts the scale of\nlearning loss based on the soft labels such that the loss would approach zero\nwhen the responses are closer to equally preferred. This simple modification\ncan be easily applied to any DPO-based methods and mitigate over-optimization\nand objective mismatch, which prior works suffer from. Our experiments simulate\nthe soft preference labels with AI feedback from LLMs and demonstrate that\ngeometric averaging consistently improves performance on standard benchmarks\nfor alignment research. In particular, we observe more preferable responses\nthan binary labels and significant improvements where modestly-confident labels\nare in the majority."
                },
                "authors": [
                    {
                        "name": "Hiroki Furuta"
                    },
                    {
                        "name": "Kuang-Huei Lee"
                    },
                    {
                        "name": "Shixiang Shane Gu"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Aleksandra Faust"
                    },
                    {
                        "name": "Heiga Zen"
                    },
                    {
                        "name": "Izzeddin Gur"
                    }
                ],
                "author_detail": {
                    "name": "Izzeddin Gur"
                },
                "author": "Izzeddin Gur",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06691v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06691v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.13918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.13918v3",
                "updated": "2024-12-30T10:53:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    10,
                    53,
                    48,
                    0,
                    365,
                    0
                ],
                "published": "2023-07-26T02:34:57Z",
                "published_parsed": [
                    2023,
                    7,
                    26,
                    2,
                    34,
                    57,
                    2,
                    207,
                    0
                ],
                "title": "Simulation-based Inference for Cardiovascular Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based Inference for Cardiovascular Models"
                },
                "summary": "Over the past decades, hemodynamics simulators have steadily evolved and have\nbecome tools of choice for studying cardiovascular systems in-silico. While\nsuch tools are routinely used to simulate whole-body hemodynamics from\nphysiological parameters, solving the corresponding inverse problem of mapping\nwaveforms back to plausible physiological parameters remains both promising and\nchallenging. Motivated by advances in simulation-based inference (SBI), we cast\nthis inverse problem as statistical inference. In contrast to alternative\napproaches, SBI provides \\textit{posterior distributions} for the parameters of\ninterest, providing a \\textit{multi-dimensional} representation of uncertainty\nfor \\textit{individual} measurements. We showcase this ability by performing an\nin-silico uncertainty analysis of five biomarkers of clinical interest\ncomparing several measurement modalities. Beyond the corroboration of known\nfacts, such as the feasibility of estimating heart rate, our study highlights\nthe potential of estimating new biomarkers from standard-of-care measurements.\nSBI reveals practically relevant findings that cannot be captured by standard\nsensitivity analyses, such as the existence of sub-populations for which\nparameter estimation exhibits distinct uncertainty regimes. Finally, we study\nthe gap between in-vivo and in-silico with the MIMIC-III waveform database and\ncritically discuss how cardiovascular simulations can inform real-world data\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past decades, hemodynamics simulators have steadily evolved and have\nbecome tools of choice for studying cardiovascular systems in-silico. While\nsuch tools are routinely used to simulate whole-body hemodynamics from\nphysiological parameters, solving the corresponding inverse problem of mapping\nwaveforms back to plausible physiological parameters remains both promising and\nchallenging. Motivated by advances in simulation-based inference (SBI), we cast\nthis inverse problem as statistical inference. In contrast to alternative\napproaches, SBI provides \\textit{posterior distributions} for the parameters of\ninterest, providing a \\textit{multi-dimensional} representation of uncertainty\nfor \\textit{individual} measurements. We showcase this ability by performing an\nin-silico uncertainty analysis of five biomarkers of clinical interest\ncomparing several measurement modalities. Beyond the corroboration of known\nfacts, such as the feasibility of estimating heart rate, our study highlights\nthe potential of estimating new biomarkers from standard-of-care measurements.\nSBI reveals practically relevant findings that cannot be captured by standard\nsensitivity analyses, such as the existence of sub-populations for which\nparameter estimation exhibits distinct uncertainty regimes. Finally, we study\nthe gap between in-vivo and in-silico with the MIMIC-III waveform database and\ncritically discuss how cardiovascular simulations can inform real-world data\nanalysis."
                },
                "authors": [
                    {
                        "name": "Antoine Wehenkel"
                    },
                    {
                        "name": "Laura Manduchi"
                    },
                    {
                        "name": "Jens Behrmann"
                    },
                    {
                        "name": "Luca Pegolotti"
                    },
                    {
                        "name": "Andrew C. Miller"
                    },
                    {
                        "name": "Guillermo Sapiro"
                    },
                    {
                        "name": "Ozan Sener"
                    },
                    {
                        "name": "Marco Cuturi"
                    },
                    {
                        "name": "JÃ¶rn-Henrik Jacobsen"
                    }
                ],
                "author_detail": {
                    "name": "JÃ¶rn-Henrik Jacobsen"
                },
                "author": "JÃ¶rn-Henrik Jacobsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.13918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.13918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20858v1",
                "updated": "2024-12-30T10:52:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    10,
                    52,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T10:52:29Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    10,
                    52,
                    29,
                    0,
                    365,
                    0
                ],
                "title": "From sparse to dense functional time series: phase transitions of\n  detecting structural breaks and beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From sparse to dense functional time series: phase transitions of\n  detecting structural breaks and beyond"
                },
                "summary": "We develop a novel methodology for detecting abrupt break points in mean\nfunctions of functional time series, adaptable to arbitrary sampling schemes.\nBy employing B-spline smoothing, we introduce $\\mathcal L_{\\infty}$ and\n$\\mathcal L_2$ test statistics statistics based on a smoothed cumulative\nsummation (CUMSUM) process, and derive the corresponding asymptotic\ndistributions under the null and local alternative hypothesis, as well as the\nphase transition boundary from sparse to dense. We further establish the\nconvergence rate of the proposed break point estimators and conduct statistical\ninference on the jump magnitude based on the estimated break point, also\napplicable across sparsely, semi-densely, and densely, observed random\nfunctions. Extensive numerical experiments validate the effectiveness of the\nproposed procedures. To illustrate the practical relevance, we apply the\ndeveloped methods to analyze electricity price data and temperature data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a novel methodology for detecting abrupt break points in mean\nfunctions of functional time series, adaptable to arbitrary sampling schemes.\nBy employing B-spline smoothing, we introduce $\\mathcal L_{\\infty}$ and\n$\\mathcal L_2$ test statistics statistics based on a smoothed cumulative\nsummation (CUMSUM) process, and derive the corresponding asymptotic\ndistributions under the null and local alternative hypothesis, as well as the\nphase transition boundary from sparse to dense. We further establish the\nconvergence rate of the proposed break point estimators and conduct statistical\ninference on the jump magnitude based on the estimated break point, also\napplicable across sparsely, semi-densely, and densely, observed random\nfunctions. Extensive numerical experiments validate the effectiveness of the\nproposed procedures. To illustrate the practical relevance, we apply the\ndeveloped methods to analyze electricity price data and temperature data."
                },
                "authors": [
                    {
                        "name": "Leheng Cai"
                    },
                    {
                        "name": "Qirui Hu"
                    }
                ],
                "author_detail": {
                    "name": "Qirui Hu"
                },
                "author": "Qirui Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20848v1",
                "updated": "2024-12-30T10:35:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    10,
                    35,
                    3,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T10:35:03Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    10,
                    35,
                    3,
                    0,
                    365,
                    0
                ],
                "title": "Analog Alchemy: Neural Computation with In-Memory Inference, Learning\n  and Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog Alchemy: Neural Computation with In-Memory Inference, Learning\n  and Routing"
                },
                "summary": "As neural computation is revolutionizing the field of Artificial Intelligence\n(AI), rethinking the ideal neural hardware is becoming the next frontier. Fast\nand reliable von Neumann architecture has been the hosting platform for neural\ncomputation. Although capable, its separation of memory and computation creates\nthe bottleneck for the energy efficiency of neural computation, contrasting the\nbiological brain. The question remains: how can we efficiently combine memory\nand computation, while exploiting the physics of the substrate, to build\nintelligent systems? In this thesis, I explore an alternative way with\nmemristive devices for neural computation, where the unique physical dynamics\nof the devices are used for inference, learning and routing. Guided by the\nprinciples of gradient-based learning, we selected functions that need to be\nmaterialized, and analyzed connectomics principles for efficient wiring.\nDespite non-idealities and noise inherent in analog physics, I will provide\nhardware evidence of adaptability of local learning to memristive substrates,\nnew material stacks and circuit blocks that aid in solving the credit\nassignment problem and efficient routing between analog crossbars for scalable\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As neural computation is revolutionizing the field of Artificial Intelligence\n(AI), rethinking the ideal neural hardware is becoming the next frontier. Fast\nand reliable von Neumann architecture has been the hosting platform for neural\ncomputation. Although capable, its separation of memory and computation creates\nthe bottleneck for the energy efficiency of neural computation, contrasting the\nbiological brain. The question remains: how can we efficiently combine memory\nand computation, while exploiting the physics of the substrate, to build\nintelligent systems? In this thesis, I explore an alternative way with\nmemristive devices for neural computation, where the unique physical dynamics\nof the devices are used for inference, learning and routing. Guided by the\nprinciples of gradient-based learning, we selected functions that need to be\nmaterialized, and analyzed connectomics principles for efficient wiring.\nDespite non-idealities and noise inherent in analog physics, I will provide\nhardware evidence of adaptability of local learning to memristive substrates,\nnew material stacks and circuit blocks that aid in solving the credit\nassignment problem and efficient routing between analog crossbars for scalable\narchitectures."
                },
                "authors": [
                    {
                        "name": "Yigit Demirag"
                    }
                ],
                "author_detail": {
                    "name": "Yigit Demirag"
                },
                "author": "Yigit Demirag",
                "arxiv_doi": "10.5167/uzh-267127",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5167/uzh-267127",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Demirag, Yigit. Analog Alchemy: Neural Computation with In-Memory\n  Inference, Learning and Routing. 2024, University of Zurich, Faculty of\n  Science",
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20846v1",
                "updated": "2024-12-30T10:29:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    10,
                    29,
                    18,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T10:29:18Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    10,
                    29,
                    18,
                    0,
                    365,
                    0
                ],
                "title": "Are LLMs Really Not Knowledgable? Mining the Submerged Knowledge in\n  LLMs' Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Really Not Knowledgable? Mining the Submerged Knowledge in\n  LLMs' Memory"
                },
                "summary": "Large language models (LLMs) have shown promise as potential knowledge bases,\nyet they often struggle with question-answering tasks and are prone to\nhallucinations. While previous research attributes these issues to knowledge\ngaps in the model's parameters, our investigation reveals a different\nphenomenon: LLMs often retain correct knowledge even when generating incorrect\nanswers. Through analysis of model's internal representations, we find that\ncorrect answers frequently appear among high-probability tokens despite not\nbeing selected as final outputs. Based on this observation, we introduce\nHits@k, a new metric to assess knowledge retention independent of expression\naccuracy. Our extensive experiments demonstrate that LLMs store significantly\nmore knowledge than their QA performance suggests. Building on these findings,\nwe develop SkipUnsure, a method to improve answer accuracy by leveraging\ndetected but unexpressed knowledge. Experiments on both open-domain and\nspecific-domain datasets show consistent improvements, with accuracy gains of\nup to 11.8% on DBPedia and 6.3% on IMDB, without requiring model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise as potential knowledge bases,\nyet they often struggle with question-answering tasks and are prone to\nhallucinations. While previous research attributes these issues to knowledge\ngaps in the model's parameters, our investigation reveals a different\nphenomenon: LLMs often retain correct knowledge even when generating incorrect\nanswers. Through analysis of model's internal representations, we find that\ncorrect answers frequently appear among high-probability tokens despite not\nbeing selected as final outputs. Based on this observation, we introduce\nHits@k, a new metric to assess knowledge retention independent of expression\naccuracy. Our extensive experiments demonstrate that LLMs store significantly\nmore knowledge than their QA performance suggests. Building on these findings,\nwe develop SkipUnsure, a method to improve answer accuracy by leveraging\ndetected but unexpressed knowledge. Experiments on both open-domain and\nspecific-domain datasets show consistent improvements, with accuracy gains of\nup to 11.8% on DBPedia and 6.3% on IMDB, without requiring model retraining."
                },
                "authors": [
                    {
                        "name": "Xingjian Tao"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20840v1",
                "updated": "2024-12-30T10:09:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    10,
                    9,
                    51,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T10:09:51Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    10,
                    9,
                    51,
                    0,
                    365,
                    0
                ],
                "title": "Identifying average causal effect in regression discontinuity design\n  with auxiliary data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying average causal effect in regression discontinuity design\n  with auxiliary data"
                },
                "summary": "Regression discontinuity designs are widely used when treatment assignment is\ndetermined by whether a running variable exceeds a predefined threshold.\nHowever, most research focuses on estimating local causal effects at the\nthreshold, leaving the challenge of identifying treatment effects away from the\ncutoff largely unaddressed. The primary difficulty in this context is that the\ncounterfactual outcome under the alternative treatment status is\nunobservable.In this paper, we introduce a novel framework for identifying the\nglobal average causal effect in regression discontinuity designs.Our approach\nintegrates a latent variable and an additional data structure alongside the\ntraditional regression discontinuity design setup. This enhanced framework\nallows us to extend the analysis beyond the threshold, providing a more\ncomprehensive understanding of treatment effects.We develop asymptotically\nvalid estimation and inference procedures under this framework, ensuring the\nrobustness of our findings. To demonstrate the practical application of our\nmethod, we assess the causal effects of vitamin A supplementation on the\nseverity of autism spectrum disorders in children.Our approach offers a\nsignificant advancement in the analysis of regression discontinuity designs,\nenabling researchers to estimate causal effects across a broader range of\nvalues and providing more actionable insights in policy and medical research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression discontinuity designs are widely used when treatment assignment is\ndetermined by whether a running variable exceeds a predefined threshold.\nHowever, most research focuses on estimating local causal effects at the\nthreshold, leaving the challenge of identifying treatment effects away from the\ncutoff largely unaddressed. The primary difficulty in this context is that the\ncounterfactual outcome under the alternative treatment status is\nunobservable.In this paper, we introduce a novel framework for identifying the\nglobal average causal effect in regression discontinuity designs.Our approach\nintegrates a latent variable and an additional data structure alongside the\ntraditional regression discontinuity design setup. This enhanced framework\nallows us to extend the analysis beyond the threshold, providing a more\ncomprehensive understanding of treatment effects.We develop asymptotically\nvalid estimation and inference procedures under this framework, ensuring the\nrobustness of our findings. To demonstrate the practical application of our\nmethod, we assess the causal effects of vitamin A supplementation on the\nseverity of autism spectrum disorders in children.Our approach offers a\nsignificant advancement in the analysis of regression discontinuity designs,\nenabling researchers to estimate causal effects across a broader range of\nvalues and providing more actionable insights in policy and medical research."
                },
                "authors": [
                    {
                        "name": "Xinqin Feng"
                    },
                    {
                        "name": "Wenjie Hu"
                    },
                    {
                        "name": "Pu Yang"
                    },
                    {
                        "name": "Tingyu Li"
                    },
                    {
                        "name": "Xiao-Hua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Hua Zhou"
                },
                "author": "Xiao-Hua Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20834v1",
                "updated": "2024-12-30T09:58:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    58,
                    31,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T09:58:31Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    58,
                    31,
                    0,
                    365,
                    0
                ],
                "title": "Disentangling Preference Representation and Text Generation for\n  Efficient Individual Preference Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Preference Representation and Text Generation for\n  Efficient Individual Preference Alignment"
                },
                "summary": "Aligning Large Language Models (LLMs) with general human preferences has been\nproved crucial in improving the interaction quality between LLMs and human.\nHowever, human values are inherently diverse among different individuals,\nmaking it insufficient to align LLMs solely with general preferences. To\naddress this, personalizing LLMs according to individual feedback emerges as a\npromising solution. Nonetheless, this approach presents challenges in terms of\nthe efficiency of alignment algorithms. In this work, we introduce a flexible\nparadigm for individual preference alignment. Our method fundamentally improves\nefficiency by disentangling preference representation from text generation in\nLLMs. We validate our approach across multiple text generation tasks and\ndemonstrate that it can produce aligned quality as well as or better than\nPEFT-based methods, while reducing additional training time for each new\nindividual preference by $80\\%$ to $90\\%$ in comparison with them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models (LLMs) with general human preferences has been\nproved crucial in improving the interaction quality between LLMs and human.\nHowever, human values are inherently diverse among different individuals,\nmaking it insufficient to align LLMs solely with general preferences. To\naddress this, personalizing LLMs according to individual feedback emerges as a\npromising solution. Nonetheless, this approach presents challenges in terms of\nthe efficiency of alignment algorithms. In this work, we introduce a flexible\nparadigm for individual preference alignment. Our method fundamentally improves\nefficiency by disentangling preference representation from text generation in\nLLMs. We validate our approach across multiple text generation tasks and\ndemonstrate that it can produce aligned quality as well as or better than\nPEFT-based methods, while reducing additional training time for each new\nindividual preference by $80\\%$ to $90\\%$ in comparison with them."
                },
                "authors": [
                    {
                        "name": "Jianfei Zhang"
                    },
                    {
                        "name": "Jun Bai"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Yanmeng Wang"
                    },
                    {
                        "name": "Rumei Li"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Wenge Rong"
                    }
                ],
                "author_detail": {
                    "name": "Wenge Rong"
                },
                "author": "Wenge Rong",
                "arxiv_comment": "Coling 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.07395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.07395v2",
                "updated": "2024-12-30T09:51:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    51,
                    22,
                    0,
                    365,
                    0
                ],
                "published": "2023-12-12T16:10:19Z",
                "published_parsed": [
                    2023,
                    12,
                    12,
                    16,
                    10,
                    19,
                    1,
                    346,
                    0
                ],
                "title": "A Simple Recipe for Contrastively Pre-training Video-First Encoders\n  Beyond 16 Frames",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Recipe for Contrastively Pre-training Video-First Encoders\n  Beyond 16 Frames"
                },
                "summary": "Understanding long, real-world videos requires modeling of long-range visual\ndependencies. To this end, we explore video-first architectures, building on\nthe common paradigm of transferring large-scale, image--text models to video\nvia shallow temporal fusion. However, we expose two limitations to the\napproach: (1) decreased spatial capabilities, likely due to poor\nvideo--language alignment in standard video datasets, and (2) higher memory\nconsumption, bottlenecking the number of frames that can be processed. To\nmitigate the memory bottleneck, we systematically analyze the memory/accuracy\ntrade-off of various efficient methods: factorized attention,\nparameter-efficient image-to-video adaptation, input masking, and\nmulti-resolution patchification. Surprisingly, simply masking large portions of\nthe video (up to 75%) during contrastive pre-training proves to be one of the\nmost robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our\nsimple approach for training long video-to-text models, which scales to 1B\nparameters, does not add new architectural complexity and is able to outperform\nthe popular paradigm of using much larger LLMs as an information aggregator\nover segment-based information on benchmarks with long-range temporal\ndependencies (YouCook2, EgoSchema).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding long, real-world videos requires modeling of long-range visual\ndependencies. To this end, we explore video-first architectures, building on\nthe common paradigm of transferring large-scale, image--text models to video\nvia shallow temporal fusion. However, we expose two limitations to the\napproach: (1) decreased spatial capabilities, likely due to poor\nvideo--language alignment in standard video datasets, and (2) higher memory\nconsumption, bottlenecking the number of frames that can be processed. To\nmitigate the memory bottleneck, we systematically analyze the memory/accuracy\ntrade-off of various efficient methods: factorized attention,\nparameter-efficient image-to-video adaptation, input masking, and\nmulti-resolution patchification. Surprisingly, simply masking large portions of\nthe video (up to 75%) during contrastive pre-training proves to be one of the\nmost robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our\nsimple approach for training long video-to-text models, which scales to 1B\nparameters, does not add new architectural complexity and is able to outperform\nthe popular paradigm of using much larger LLMs as an information aggregator\nover segment-based information on benchmarks with long-range temporal\ndependencies (YouCook2, EgoSchema)."
                },
                "authors": [
                    {
                        "name": "Pinelopi Papalampidi"
                    },
                    {
                        "name": "Skanda Koppula"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Justin Chiu"
                    },
                    {
                        "name": "Joe Heyward"
                    },
                    {
                        "name": "Viorica Patraucean"
                    },
                    {
                        "name": "Jiajun Shen"
                    },
                    {
                        "name": "Antoine Miech"
                    },
                    {
                        "name": "Andrew Zisserman"
                    },
                    {
                        "name": "Aida Nematzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Aida Nematzadeh"
                },
                "author": "Aida Nematzadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.07395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.07395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19792v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19792v2",
                "updated": "2024-12-30T09:37:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    37,
                    33,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-27T18:45:36Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    45,
                    36,
                    4,
                    362,
                    0
                ],
                "title": "InfAlign: Inference-aware language model alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfAlign: Inference-aware language model alignment"
                },
                "summary": "Language model alignment has become a critical step in training modern\ngenerative language models. The goal of alignment is to finetune a reference\nmodel such that the win rate of a sample from the aligned model over a sample\nfrom the reference model is high, subject to a KL divergence constraint. Today,\nwe are increasingly using inference-time algorithms (e.g., Best-of-N,\ncontrolled decoding, tree search) to decode from language models rather than\nstandard sampling. However, the alignment objective does not capture such\ninference-time decoding procedures. We show that the existing alignment\nframework is sub-optimal in view of such inference-time methods. We then modify\nthe alignment objective and propose a framework for inference-aware alignment\n(IAPO). We prove that for any inference-time decoding algorithm, the optimal\nsolution that optimizes the inference-time win rate of the aligned policy\nagainst the reference policy is the solution to the typical RLHF problem with a\ntransformation of the reward. This motivates us to provide the KL-regularized\ncalibrate-and-transform RL (CTRL) algorithm to solve this problem, which\ninvolves a reward calibration step and a KL-regularized reward maximization\nstep with a transformation of the calibrated reward. We particularize our study\nto two important inference-time strategies: best-of-N sampling and best-of-N\njailbreaking, where N responses are sampled from the model and the one with the\nhighest or lowest reward is selected. We propose specific transformations for\nthese strategies and demonstrate that our framework offers significant\nimprovements over existing state-of-the-art methods for language model\nalignment. Empirically, we outperform baselines that are designed without\ntaking inference-time decoding into consideration by 8-12% and 4-9% on\ninference-time win rates over the Anthropic helpfulness and harmlessness dialog\nbenchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model alignment has become a critical step in training modern\ngenerative language models. The goal of alignment is to finetune a reference\nmodel such that the win rate of a sample from the aligned model over a sample\nfrom the reference model is high, subject to a KL divergence constraint. Today,\nwe are increasingly using inference-time algorithms (e.g., Best-of-N,\ncontrolled decoding, tree search) to decode from language models rather than\nstandard sampling. However, the alignment objective does not capture such\ninference-time decoding procedures. We show that the existing alignment\nframework is sub-optimal in view of such inference-time methods. We then modify\nthe alignment objective and propose a framework for inference-aware alignment\n(IAPO). We prove that for any inference-time decoding algorithm, the optimal\nsolution that optimizes the inference-time win rate of the aligned policy\nagainst the reference policy is the solution to the typical RLHF problem with a\ntransformation of the reward. This motivates us to provide the KL-regularized\ncalibrate-and-transform RL (CTRL) algorithm to solve this problem, which\ninvolves a reward calibration step and a KL-regularized reward maximization\nstep with a transformation of the calibrated reward. We particularize our study\nto two important inference-time strategies: best-of-N sampling and best-of-N\njailbreaking, where N responses are sampled from the model and the one with the\nhighest or lowest reward is selected. We propose specific transformations for\nthese strategies and demonstrate that our framework offers significant\nimprovements over existing state-of-the-art methods for language model\nalignment. Empirically, we outperform baselines that are designed without\ntaking inference-time decoding into consideration by 8-12% and 4-9% on\ninference-time win rates over the Anthropic helpfulness and harmlessness dialog\nbenchmark datasets."
                },
                "authors": [
                    {
                        "name": "Ananth Balashankar"
                    },
                    {
                        "name": "Ziteng Sun"
                    },
                    {
                        "name": "Jonathan Berant"
                    },
                    {
                        "name": "Jacob Eisenstein"
                    },
                    {
                        "name": "Michael Collins"
                    },
                    {
                        "name": "Adrian Hutter"
                    },
                    {
                        "name": "Jong Lee"
                    },
                    {
                        "name": "Chirag Nagpal"
                    },
                    {
                        "name": "Flavien Prost"
                    },
                    {
                        "name": "Aradhana Sinha"
                    },
                    {
                        "name": "Ananda Theertha Suresh"
                    },
                    {
                        "name": "Ahmad Beirami"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Beirami"
                },
                "author": "Ahmad Beirami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19792v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19792v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20820v1",
                "updated": "2024-12-30T09:30:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    30,
                    36,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T09:30:36Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    30,
                    36,
                    0,
                    365,
                    0
                ],
                "title": "Retrieval-Augmented Generation for Mobile Edge Computing via Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation for Mobile Edge Computing via Large\n  Language Model"
                },
                "summary": "The rapid evolution of mobile edge computing (MEC) has introduced significant\nchallenges in optimizing resource allocation in highly dynamic wireless\ncommunication systems, in which task offloading decisions should be made in\nreal-time. However, existing resource allocation strategies cannot well adapt\nto the dynamic and heterogeneous characteristics of MEC systems, since they are\nshort of scalability, context-awareness, and interpretability. To address these\nissues, this paper proposes a novel retrieval-augmented generation (RAG) method\nto improve the performance of MEC systems. Specifically, a latency minimization\nproblem is first proposed to jointly optimize the data offloading ratio,\ntransmit power allocation, and computing resource allocation. Then, an\nLLM-enabled information-retrieval mechanism is proposed to solve the problem\nefficiently. Extensive experiments across multi-user, multi-task, and highly\ndynamic offloading scenarios show that the proposed method consistently reduces\nlatency compared to several DL-based approaches, achieving 57% improvement\nunder varying user computing ability, 86% with different servers, 30% under\ndistinct transmit powers, and 42% for varying data volumes. These results show\nthe effectiveness of LLM-driven solutions to solve the resource allocation\nproblems in MEC systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of mobile edge computing (MEC) has introduced significant\nchallenges in optimizing resource allocation in highly dynamic wireless\ncommunication systems, in which task offloading decisions should be made in\nreal-time. However, existing resource allocation strategies cannot well adapt\nto the dynamic and heterogeneous characteristics of MEC systems, since they are\nshort of scalability, context-awareness, and interpretability. To address these\nissues, this paper proposes a novel retrieval-augmented generation (RAG) method\nto improve the performance of MEC systems. Specifically, a latency minimization\nproblem is first proposed to jointly optimize the data offloading ratio,\ntransmit power allocation, and computing resource allocation. Then, an\nLLM-enabled information-retrieval mechanism is proposed to solve the problem\nefficiently. Extensive experiments across multi-user, multi-task, and highly\ndynamic offloading scenarios show that the proposed method consistently reduces\nlatency compared to several DL-based approaches, achieving 57% improvement\nunder varying user computing ability, 86% with different servers, 30% under\ndistinct transmit powers, and 42% for varying data volumes. These results show\nthe effectiveness of LLM-driven solutions to solve the resource allocation\nproblems in MEC systems."
                },
                "authors": [
                    {
                        "name": "Runtao Ren"
                    },
                    {
                        "name": "Yinyu Wu"
                    },
                    {
                        "name": "Xuhui Zhang"
                    },
                    {
                        "name": "Jinke Ren"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Shuqiang Wang"
                    },
                    {
                        "name": "Kim-Fung Tsang"
                    }
                ],
                "author_detail": {
                    "name": "Kim-Fung Tsang"
                },
                "author": "Kim-Fung Tsang",
                "arxiv_comment": "This manuscript has been submitted to IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18176v2",
                "updated": "2024-12-30T09:24:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    24,
                    34,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-24T05:23:13Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    23,
                    13,
                    1,
                    359,
                    0
                ],
                "title": "Molar: Multimodal LLMs with Collaborative Filtering Alignment for\n  Enhanced Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molar: Multimodal LLMs with Collaborative Filtering Alignment for\n  Enhanced Sequential Recommendation"
                },
                "summary": "Sequential recommendation (SR) systems have evolved significantly over the\npast decade, transitioning from traditional collaborative filtering to deep\nlearning approaches and, more recently, to large language models (LLMs). While\nthe adoption of LLMs has driven substantial advancements, these models\ninherently lack collaborative filtering information, relying primarily on\ntextual content data neglecting other modalities and thus failing to achieve\noptimal recommendation performance. To address this limitation, we propose\nMolar, a Multimodal large language sequential recommendation framework that\nintegrates multiple content modalities with ID information to capture\ncollaborative signals effectively. Molar employs an MLLM to generate unified\nitem representations from both textual and non-textual data, facilitating\ncomprehensive multimodal modeling and enriching item embeddings. Additionally,\nit incorporates collaborative filtering signals through a post-alignment\nmechanism, which aligns user representations from content-based and ID-based\nmodels, ensuring precise personalization and robust performance. By seamlessly\ncombining multimodal content with collaborative filtering insights, Molar\ncaptures both user interests and contextual semantics, leading to superior\nrecommendation accuracy. Extensive experiments validate that Molar\nsignificantly outperforms traditional and LLM-based baselines, highlighting its\nstrength in utilizing multimodal data and collaborative signals for sequential\nrecommendation tasks. The source code is available at\nhttps://anonymous.4open.science/r/Molar-8B06/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation (SR) systems have evolved significantly over the\npast decade, transitioning from traditional collaborative filtering to deep\nlearning approaches and, more recently, to large language models (LLMs). While\nthe adoption of LLMs has driven substantial advancements, these models\ninherently lack collaborative filtering information, relying primarily on\ntextual content data neglecting other modalities and thus failing to achieve\noptimal recommendation performance. To address this limitation, we propose\nMolar, a Multimodal large language sequential recommendation framework that\nintegrates multiple content modalities with ID information to capture\ncollaborative signals effectively. Molar employs an MLLM to generate unified\nitem representations from both textual and non-textual data, facilitating\ncomprehensive multimodal modeling and enriching item embeddings. Additionally,\nit incorporates collaborative filtering signals through a post-alignment\nmechanism, which aligns user representations from content-based and ID-based\nmodels, ensuring precise personalization and robust performance. By seamlessly\ncombining multimodal content with collaborative filtering insights, Molar\ncaptures both user interests and contextual semantics, leading to superior\nrecommendation accuracy. Extensive experiments validate that Molar\nsignificantly outperforms traditional and LLM-based baselines, highlighting its\nstrength in utilizing multimodal data and collaborative signals for sequential\nrecommendation tasks. The source code is available at\nhttps://anonymous.4open.science/r/Molar-8B06/."
                },
                "authors": [
                    {
                        "name": "Yucong Luo"
                    },
                    {
                        "name": "Qitao Qin"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Ruiran Yan"
                    },
                    {
                        "name": "Kefan Wang"
                    },
                    {
                        "name": "Jie Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Ouyang"
                },
                "author": "Jie Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18185v2",
                "updated": "2024-12-30T09:22:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    22,
                    24,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-24T05:38:45Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    38,
                    45,
                    1,
                    359,
                    0
                ],
                "title": "TextMatch: Enhancing Image-Text Consistency Through Multimodal\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextMatch: Enhancing Image-Text Consistency Through Multimodal\n  Optimization"
                },
                "summary": "Text-to-image generative models excel in creating images from text but\nstruggle with ensuring alignment and consistency between outputs and prompts.\nThis paper introduces TextMatch, a novel framework that leverages multimodal\noptimization to address image-text discrepancies in text-to-image (T2I)\ngeneration and editing. TextMatch employs a scoring strategy powered by large\nlanguage models (LLMs) and visual question-answering (VQA) models to evaluate\nsemantic consistency between prompts and generated images. By integrating\nmultimodal in-context learning and chain of thought reasoning, our method\ndynamically refines prompts through iterative optimization. This process\nensures that the generated images better capture user intent of, resulting in\nhigher fidelity and relevance. Extensive experiments demonstrate that TextMatch\nsignificantly improves text-image consistency across multiple benchmarks,\nestablishing a reliable framework for advancing the capabilities of\ntext-to-image generative models. Our code is available at\nhttps://anonymous.4open.science/r/TextMatch-F55C/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generative models excel in creating images from text but\nstruggle with ensuring alignment and consistency between outputs and prompts.\nThis paper introduces TextMatch, a novel framework that leverages multimodal\noptimization to address image-text discrepancies in text-to-image (T2I)\ngeneration and editing. TextMatch employs a scoring strategy powered by large\nlanguage models (LLMs) and visual question-answering (VQA) models to evaluate\nsemantic consistency between prompts and generated images. By integrating\nmultimodal in-context learning and chain of thought reasoning, our method\ndynamically refines prompts through iterative optimization. This process\nensures that the generated images better capture user intent of, resulting in\nhigher fidelity and relevance. Extensive experiments demonstrate that TextMatch\nsignificantly improves text-image consistency across multiple benchmarks,\nestablishing a reliable framework for advancing the capabilities of\ntext-to-image generative models. Our code is available at\nhttps://anonymous.4open.science/r/TextMatch-F55C/."
                },
                "authors": [
                    {
                        "name": "Yucong Luo"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Jie Ouyang"
                    },
                    {
                        "name": "Xiaoyu Tao"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03552v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03552v3",
                "updated": "2024-12-30T09:22:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    22,
                    1,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-07T05:25:21Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    5,
                    25,
                    21,
                    2,
                    220,
                    0
                ],
                "title": "An In-depth Investigation of the Primordial Cluster Pair ASCC 19 and\n  ASCC 21",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An In-depth Investigation of the Primordial Cluster Pair ASCC 19 and\n  ASCC 21"
                },
                "summary": "Utilizing \\texttt{Gaia} data from the literature, we report a new young\n($\\sim$8.9~Myr) cluster pair, ASCC~19 and ASCC~21, located near the Orion\nstar-forming complex. The clusters are separated by a 3D distance of\n~27.00~$\\pm$~7.51~pc. Both clusters share a common age\n(Log(age)~=~6.95~$\\pm$~0.05), similar radial velocities\n($R_{v}$~=~21.34~$\\pm$~4.47~km s$^{-1}$ for ASCC~19 and\n$R_{v}$~=~20.05~$\\pm$~3.86~km s$^{-1}$ for ASCC~21), and comparable\nmetallicities ([Fe/H]~=~$-$0.14~$\\pm$~0.25~dex for ASCC~19 and\n[Fe/H]~=~$-$0.12~$\\pm$~0.04~dex for ASCC~21, from LAMOST-DR11). These\nsimilarities suggest that the clusters likely originated from the fragmentation\nof the same molecular cloud, forming a primordial cluster pair. Furthermore,\nthe formation of the two clusters is attributed to the coalescence of multiple\nsubclusters, as inferred from the distribution analysis between metal\nabundances and distances to clusters' centers. Neither cluster shows\nsignificant mass segregation. Their members with radial velocities exceeding\n100~km s$^{-1}$ are young variables. Additionally, a tidal interaction between\nthe clusters is observed. Comparisons of the Roche radius with tidal radii, and\nvelocity difference with orbital velocity, suggest that the pair is an unbound\nsystem, that is, a double cluster. Finally, orbital motion simulations show\nthat the clusters will not merge into a single system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing \\texttt{Gaia} data from the literature, we report a new young\n($\\sim$8.9~Myr) cluster pair, ASCC~19 and ASCC~21, located near the Orion\nstar-forming complex. The clusters are separated by a 3D distance of\n~27.00~$\\pm$~7.51~pc. Both clusters share a common age\n(Log(age)~=~6.95~$\\pm$~0.05), similar radial velocities\n($R_{v}$~=~21.34~$\\pm$~4.47~km s$^{-1}$ for ASCC~19 and\n$R_{v}$~=~20.05~$\\pm$~3.86~km s$^{-1}$ for ASCC~21), and comparable\nmetallicities ([Fe/H]~=~$-$0.14~$\\pm$~0.25~dex for ASCC~19 and\n[Fe/H]~=~$-$0.12~$\\pm$~0.04~dex for ASCC~21, from LAMOST-DR11). These\nsimilarities suggest that the clusters likely originated from the fragmentation\nof the same molecular cloud, forming a primordial cluster pair. Furthermore,\nthe formation of the two clusters is attributed to the coalescence of multiple\nsubclusters, as inferred from the distribution analysis between metal\nabundances and distances to clusters' centers. Neither cluster shows\nsignificant mass segregation. Their members with radial velocities exceeding\n100~km s$^{-1}$ are young variables. Additionally, a tidal interaction between\nthe clusters is observed. Comparisons of the Roche radius with tidal radii, and\nvelocity difference with orbital velocity, suggest that the pair is an unbound\nsystem, that is, a double cluster. Finally, orbital motion simulations show\nthat the clusters will not merge into a single system."
                },
                "authors": [
                    {
                        "name": "Qingshun Hu"
                    },
                    {
                        "name": "Yuting Li"
                    },
                    {
                        "name": "Mingfeng Qin"
                    },
                    {
                        "name": "Chenglong Lv"
                    },
                    {
                        "name": "Yang Pan"
                    },
                    {
                        "name": "Yangping Luo"
                    },
                    {
                        "name": "Shuo Ma"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Ma"
                },
                "author": "Shuo Ma",
                "arxiv_comment": "18 pages, 16 figures, 2 tables, accepted for publication in The\n  Astronomical Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03552v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03552v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10424v2",
                "updated": "2024-12-30T09:11:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    11,
                    50,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-10T15:00:32Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    0,
                    32,
                    1,
                    345,
                    0
                ],
                "title": "LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM\n  Evaluation"
                },
                "summary": "We introduce LLM-as-an-Interviewer, a novel paradigm for evaluating large\nlanguage models (LLMs). This approach leverages multi-turn interactions where\nthe LLM interviewer actively provides feedback on responses and poses follow-up\nquestions to the evaluated LLM. At the start of the interview, the LLM\ninterviewer dynamically modifies datasets to generate initial questions,\nmitigating data contamination. We apply the LLM-as-an-Interviewer framework to\nevaluate six models on the MATH and DepthQA tasks. Our results show that the\nframework effectively provides insights into LLM performance, including the\nquality of initial responses, adaptability to feedback, and ability to address\nfollow-up queries like clarification or additional knowledge requests. The\nframework also addresses key limitations of conventional methods like\nLLM-as-a-Judge, including verbosity bias and inconsistency across runs.\nFinally, we propose the Interview Report, which aggregates insights from the\ninterview process, providing examples and a comprehensive analysis of the LLM's\nstrengths and weaknesses. This report offers a detailed snapshot of the model's\nreal-world applicability. The code for our framework is publicly available at\nhttps://github.com/interview-eval/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LLM-as-an-Interviewer, a novel paradigm for evaluating large\nlanguage models (LLMs). This approach leverages multi-turn interactions where\nthe LLM interviewer actively provides feedback on responses and poses follow-up\nquestions to the evaluated LLM. At the start of the interview, the LLM\ninterviewer dynamically modifies datasets to generate initial questions,\nmitigating data contamination. We apply the LLM-as-an-Interviewer framework to\nevaluate six models on the MATH and DepthQA tasks. Our results show that the\nframework effectively provides insights into LLM performance, including the\nquality of initial responses, adaptability to feedback, and ability to address\nfollow-up queries like clarification or additional knowledge requests. The\nframework also addresses key limitations of conventional methods like\nLLM-as-a-Judge, including verbosity bias and inconsistency across runs.\nFinally, we propose the Interview Report, which aggregates insights from the\ninterview process, providing examples and a comprehensive analysis of the LLM's\nstrengths and weaknesses. This report offers a detailed snapshot of the model's\nreal-world applicability. The code for our framework is publicly available at\nhttps://github.com/interview-eval/."
                },
                "authors": [
                    {
                        "name": "Eunsu Kim"
                    },
                    {
                        "name": "Juyoung Suk"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Dongkwan Kim"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14040v2",
                "updated": "2024-12-30T09:02:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    2,
                    53,
                    0,
                    365,
                    0
                ],
                "published": "2024-05-22T22:22:26Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    22,
                    22,
                    26,
                    2,
                    143,
                    0
                ],
                "title": "Synchronized Video Storytelling: Generating Video Narrations with\n  Structured Storyline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synchronized Video Storytelling: Generating Video Narrations with\n  Structured Storyline"
                },
                "summary": "Video storytelling is engaging multimedia content that utilizes video and its\naccompanying narration to attract the audience, where a key challenge is\ncreating narrations for recorded visual scenes. Previous studies on dense video\ncaptioning and video story generation have made some progress. However, in\npractical applications, we typically require synchronized narrations for\nongoing visual scenes. In this work, we introduce a new task of Synchronized\nVideo Storytelling, which aims to generate synchronous and informative\nnarrations for videos. These narrations, associated with each video clip,\nshould relate to the visual content, integrate relevant knowledge, and have an\nappropriate word count corresponding to the clip's duration. Specifically, a\nstructured storyline is beneficial to guide the generation process, ensuring\ncoherence and integrity. To support the exploration of this task, we introduce\na new benchmark dataset E-SyncVidStory with rich annotations. Since existing\nMultimodal LLMs are not effective in addressing this task in one-shot or\nfew-shot settings, we propose a framework named VideoNarrator that can generate\na storyline for input videos and simultaneously generate narrations with the\nguidance of the generated or predefined storyline. We further introduce a set\nof evaluation metrics to thoroughly assess the generation. Both automatic and\nhuman evaluations validate the effectiveness of our approach. Our dataset,\ncodes, and evaluations will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video storytelling is engaging multimedia content that utilizes video and its\naccompanying narration to attract the audience, where a key challenge is\ncreating narrations for recorded visual scenes. Previous studies on dense video\ncaptioning and video story generation have made some progress. However, in\npractical applications, we typically require synchronized narrations for\nongoing visual scenes. In this work, we introduce a new task of Synchronized\nVideo Storytelling, which aims to generate synchronous and informative\nnarrations for videos. These narrations, associated with each video clip,\nshould relate to the visual content, integrate relevant knowledge, and have an\nappropriate word count corresponding to the clip's duration. Specifically, a\nstructured storyline is beneficial to guide the generation process, ensuring\ncoherence and integrity. To support the exploration of this task, we introduce\na new benchmark dataset E-SyncVidStory with rich annotations. Since existing\nMultimodal LLMs are not effective in addressing this task in one-shot or\nfew-shot settings, we propose a framework named VideoNarrator that can generate\na storyline for input videos and simultaneously generate narrations with the\nguidance of the generated or predefined storyline. We further introduce a set\nof evaluation metrics to thoroughly assess the generation. Both automatic and\nhuman evaluations validate the effectiveness of our approach. Our dataset,\ncodes, and evaluations will be released."
                },
                "authors": [
                    {
                        "name": "Dingyi Yang"
                    },
                    {
                        "name": "Chunru Zhan"
                    },
                    {
                        "name": "Ziheng Wang"
                    },
                    {
                        "name": "Biao Wang"
                    },
                    {
                        "name": "Tiezheng Ge"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Qin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Qin Jin"
                },
                "author": "Qin Jin",
                "arxiv_comment": "15 pages, 13 figures",
                "arxiv_journal_ref": "https://aclanthology.org/2024.acl-long.513/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19759v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19759v3",
                "updated": "2024-12-30T08:43:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    43,
                    6,
                    0,
                    365,
                    0
                ],
                "published": "2024-04-30T17:59:47Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    17,
                    59,
                    47,
                    1,
                    121,
                    0
                ],
                "title": "MotionLCM: Real-time Controllable Motion Generation via Latent\n  Consistency Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionLCM: Real-time Controllable Motion Generation via Latent\n  Consistency Model"
                },
                "summary": "This work introduces MotionLCM, extending controllable motion generation to a\nreal-time level. Existing methods for spatial-temporal control in\ntext-conditioned motion generation suffer from significant runtime\ninefficiency. To address this issue, we first propose the motion latent\nconsistency model (MotionLCM) for motion generation, building on the motion\nlatent diffusion model. By adopting one-step (or few-step) inference, we\nfurther improve the runtime efficiency of the motion latent diffusion model for\nmotion generation. To ensure effective controllability, we incorporate a motion\nControlNet within the latent space of MotionLCM and enable explicit control\nsignals (i.e., initial motions) in the vanilla motion space to further provide\nsupervision for the training process. By employing these techniques, our\napproach can generate human motions with text and control signals in real-time.\nExperimental results demonstrate the remarkable generation and controlling\ncapabilities of MotionLCM while maintaining real-time runtime efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces MotionLCM, extending controllable motion generation to a\nreal-time level. Existing methods for spatial-temporal control in\ntext-conditioned motion generation suffer from significant runtime\ninefficiency. To address this issue, we first propose the motion latent\nconsistency model (MotionLCM) for motion generation, building on the motion\nlatent diffusion model. By adopting one-step (or few-step) inference, we\nfurther improve the runtime efficiency of the motion latent diffusion model for\nmotion generation. To ensure effective controllability, we incorporate a motion\nControlNet within the latent space of MotionLCM and enable explicit control\nsignals (i.e., initial motions) in the vanilla motion space to further provide\nsupervision for the training process. By employing these techniques, our\napproach can generate human motions with text and control signals in real-time.\nExperimental results demonstrate the remarkable generation and controlling\ncapabilities of MotionLCM while maintaining real-time runtime efficiency."
                },
                "authors": [
                    {
                        "name": "Wenxun Dai"
                    },
                    {
                        "name": "Ling-Hao Chen"
                    },
                    {
                        "name": "Jingbo Wang"
                    },
                    {
                        "name": "Jinpeng Liu"
                    },
                    {
                        "name": "Bo Dai"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "MotionLCM project version 1.0 (ECCV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19759v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19759v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07687v2",
                "updated": "2024-12-30T08:29:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    29,
                    9,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-10T17:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    20,
                    47,
                    1,
                    345,
                    0
                ],
                "title": "Privacy-Preserving Customer Support: A Framework for Secure and Scalable\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Customer Support: A Framework for Secure and Scalable\n  Interactions"
                },
                "summary": "The growing reliance on artificial intelligence (AI) in customer support has\nsignificantly improved operational efficiency and user experience. However,\ntraditional machine learning (ML) approaches, which require extensive local\ntraining on sensitive datasets, pose substantial privacy risks and compliance\nchallenges with regulations like the General Data Protection Regulation (GDPR)\nand California Consumer Privacy Act (CCPA). Existing privacy-preserving\ntechniques, such as anonymization, differential privacy, and federated\nlearning, address some concerns but face limitations in utility, scalability,\nand complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning\n(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in\na zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates\nthe need for local training on sensitive data by utilizing pre-trained LLMs to\ngenerate responses directly. The framework incorporates real-time data\nanonymization to redact or mask sensitive information, retrieval-augmented\ngeneration (RAG) for domain-specific query resolution, and robust\npost-processing to ensure compliance with regulatory standards. This\ncombination reduces privacy risks, simplifies compliance, and enhances\nscalability and operational efficiency. Empirical analysis demonstrates that\nthe PP-ZSL framework provides accurate, privacy-compliant responses while\nsignificantly lowering the costs and complexities of deploying AI-driven\ncustomer support systems. The study highlights potential applications across\nindustries, including financial services, healthcare, e-commerce, legal\nsupport, telecommunications, and government services. By addressing the dual\nchallenges of privacy and performance, this framework establishes a foundation\nfor secure, efficient, and regulatory-compliant AI applications in customer\ninteractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing reliance on artificial intelligence (AI) in customer support has\nsignificantly improved operational efficiency and user experience. However,\ntraditional machine learning (ML) approaches, which require extensive local\ntraining on sensitive datasets, pose substantial privacy risks and compliance\nchallenges with regulations like the General Data Protection Regulation (GDPR)\nand California Consumer Privacy Act (CCPA). Existing privacy-preserving\ntechniques, such as anonymization, differential privacy, and federated\nlearning, address some concerns but face limitations in utility, scalability,\nand complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning\n(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in\na zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates\nthe need for local training on sensitive data by utilizing pre-trained LLMs to\ngenerate responses directly. The framework incorporates real-time data\nanonymization to redact or mask sensitive information, retrieval-augmented\ngeneration (RAG) for domain-specific query resolution, and robust\npost-processing to ensure compliance with regulatory standards. This\ncombination reduces privacy risks, simplifies compliance, and enhances\nscalability and operational efficiency. Empirical analysis demonstrates that\nthe PP-ZSL framework provides accurate, privacy-compliant responses while\nsignificantly lowering the costs and complexities of deploying AI-driven\ncustomer support systems. The study highlights potential applications across\nindustries, including financial services, healthcare, e-commerce, legal\nsupport, telecommunications, and government services. By addressing the dual\nchallenges of privacy and performance, this framework establishes a foundation\nfor secure, efficient, and regulatory-compliant AI applications in customer\ninteractions."
                },
                "authors": [
                    {
                        "name": "Anant Prakash Awasthi"
                    },
                    {
                        "name": "Girdhar Gopal Agarwal"
                    },
                    {
                        "name": "Chandraketu Singh"
                    },
                    {
                        "name": "Rakshit Varma"
                    },
                    {
                        "name": "Sanchit Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Sanchit Sharma"
                },
                "author": "Sanchit Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06387v2",
                "updated": "2024-12-30T08:27:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    27,
                    59,
                    0,
                    365,
                    0
                ],
                "published": "2024-07-08T20:56:17Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    20,
                    56,
                    17,
                    0,
                    190,
                    0
                ],
                "title": "Conditional Rank-Rank Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Rank-Rank Regression"
                },
                "summary": "Rank-rank regression is commonly employed in economic research as a way of\ncapturing the relationship between two economic variables. It frequently\nfeatures in studies of intergenerational mobility as the resulting coefficient,\ncapturing the rank correlation between the variables, is easy to interpret and\nmeasures overall persistence. However, in many applications it is common\npractice to include other covariates to account for differences in persistence\nlevels between groups defined by the values of these covariates. In these\ninstances the resulting coefficients can be difficult to interpret. We propose\nthe conditional rank-rank regression, which uses conditional ranks instead of\nunconditional ranks, to measure average within-group income persistence. The\ndifference between conditional and unconditional rank-rank regression\ncoefficients can then be interpreted as a measure of between-group persistence.\nWe develop a flexible estimation approach using distribution regression and\nestablish a theoretical framework for large sample inference. An empirical\nstudy on intergenerational income mobility in Switzerland demonstrates the\nadvantages of this approach. The study reveals stronger intergenerational\npersistence between fathers and sons compared to fathers and daughters, with\nthe within-group persistence explaining 62% of the overall income persistence\nfor sons and 52% for daughters. Smaller families and those with highly educated\nfathers exhibit greater persistence in economic status.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rank-rank regression is commonly employed in economic research as a way of\ncapturing the relationship between two economic variables. It frequently\nfeatures in studies of intergenerational mobility as the resulting coefficient,\ncapturing the rank correlation between the variables, is easy to interpret and\nmeasures overall persistence. However, in many applications it is common\npractice to include other covariates to account for differences in persistence\nlevels between groups defined by the values of these covariates. In these\ninstances the resulting coefficients can be difficult to interpret. We propose\nthe conditional rank-rank regression, which uses conditional ranks instead of\nunconditional ranks, to measure average within-group income persistence. The\ndifference between conditional and unconditional rank-rank regression\ncoefficients can then be interpreted as a measure of between-group persistence.\nWe develop a flexible estimation approach using distribution regression and\nestablish a theoretical framework for large sample inference. An empirical\nstudy on intergenerational income mobility in Switzerland demonstrates the\nadvantages of this approach. The study reveals stronger intergenerational\npersistence between fathers and sons compared to fathers and daughters, with\nthe within-group persistence explaining 62% of the overall income persistence\nfor sons and 52% for daughters. Smaller families and those with highly educated\nfathers exhibit greater persistence in economic status."
                },
                "authors": [
                    {
                        "name": "Victor Chernozhukov"
                    },
                    {
                        "name": "IvÃ¡n FernÃ¡ndez-Val"
                    },
                    {
                        "name": "Jonas Meier"
                    },
                    {
                        "name": "Aico van Vuuren"
                    },
                    {
                        "name": "Francis Vella"
                    }
                ],
                "author_detail": {
                    "name": "Francis Vella"
                },
                "author": "Francis Vella",
                "arxiv_comment": "41 pages, 3 figures, 9 tables; minor edits on main text and updated\n  simulation results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20793v1",
                "updated": "2024-12-30T08:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    24,
                    18,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T08:24:18Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    24,
                    18,
                    0,
                    365,
                    0
                ],
                "title": "Robust Bayesian inference with gapped LISA data using all-in-one\n  TDI-$\\infty$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Bayesian inference with gapped LISA data using all-in-one\n  TDI-$\\infty$"
                },
                "summary": "The Laser Interferometer Space Antenna (LISA), an ESA L-class mission, is\ndesigned to detect gravitational waves in the millihertz frequency band, with\noperations expected to begin in the next decade. LISA will enable studies of\nastrophysical phenomena such as massive black hole mergers, extreme mass ratio\ninspirals, and compact binary systems. A key challenge in analyzing LISA's data\nis the significant laser frequency noise, which must be suppressed using\ntime-delay interferometry (TDI). Classical TDI mitigates this noise by\nalgebraically combining phase measurements taken at different times and\nspacecraft. However, data gaps caused by instrumental issues or operational\ninterruptions complicate the process. These gaps affect multiple TDI samples\ndue to the time delays inherent to the algorithm, rendering surrounding\nmeasurements unusable for parameter inference. In this paper, we apply the\nrecently proposed variant of TDI known as TDI-$\\infty$ to astrophysical\nparameter inference, focusing on the challenge posed by data gaps. TDI-$\\infty$\nframes the LISA likelihood numerically in terms of raw measurements,\nmarginalizing over laser phase noises under the assumption of infinite noise\nvariance. Additionally, TDI-$\\infty$ is set up to incorporate and cancel other\nnoise sources beyond laser noise, including optical bench motion, clock noise,\nand modulation noise, establishing it as an all-in-one TDI solution. The method\ngracefully handles measurement interruptions, removing the need to explicitly\naddress discontinuities during template matching. We integrate TDI-$\\infty$\ninto a Bayesian framework, demonstrating its superior performance in scenarios\ninvolving gaps. Compared to classical TDI, the method preserves signal\nintegrity more effectively and is particularly interesting for low-latency\napplications, where the limited amount of available data makes data gaps\nparticularly disruptive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Laser Interferometer Space Antenna (LISA), an ESA L-class mission, is\ndesigned to detect gravitational waves in the millihertz frequency band, with\noperations expected to begin in the next decade. LISA will enable studies of\nastrophysical phenomena such as massive black hole mergers, extreme mass ratio\ninspirals, and compact binary systems. A key challenge in analyzing LISA's data\nis the significant laser frequency noise, which must be suppressed using\ntime-delay interferometry (TDI). Classical TDI mitigates this noise by\nalgebraically combining phase measurements taken at different times and\nspacecraft. However, data gaps caused by instrumental issues or operational\ninterruptions complicate the process. These gaps affect multiple TDI samples\ndue to the time delays inherent to the algorithm, rendering surrounding\nmeasurements unusable for parameter inference. In this paper, we apply the\nrecently proposed variant of TDI known as TDI-$\\infty$ to astrophysical\nparameter inference, focusing on the challenge posed by data gaps. TDI-$\\infty$\nframes the LISA likelihood numerically in terms of raw measurements,\nmarginalizing over laser phase noises under the assumption of infinite noise\nvariance. Additionally, TDI-$\\infty$ is set up to incorporate and cancel other\nnoise sources beyond laser noise, including optical bench motion, clock noise,\nand modulation noise, establishing it as an all-in-one TDI solution. The method\ngracefully handles measurement interruptions, removing the need to explicitly\naddress discontinuities during template matching. We integrate TDI-$\\infty$\ninto a Bayesian framework, demonstrating its superior performance in scenarios\ninvolving gaps. Compared to classical TDI, the method preserves signal\nintegrity more effectively and is particularly interesting for low-latency\napplications, where the limited amount of available data makes data gaps\nparticularly disruptive."
                },
                "authors": [
                    {
                        "name": "Niklas Houba"
                    },
                    {
                        "name": "Jean-Baptiste Bayle"
                    },
                    {
                        "name": "Michele Vallisneri"
                    }
                ],
                "author_detail": {
                    "name": "Michele Vallisneri"
                },
                "author": "Michele Vallisneri",
                "arxiv_comment": "47 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20790v1",
                "updated": "2024-12-30T08:12:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    12,
                    17,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T08:12:17Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    12,
                    17,
                    0,
                    365,
                    0
                ],
                "title": "Frequency-Masked Embedding Inference: A Non-Contrastive Approach for\n  Time Series Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frequency-Masked Embedding Inference: A Non-Contrastive Approach for\n  Time Series Representation Learning"
                },
                "summary": "Contrastive learning underpins most current self-supervised time series\nrepresentation methods. The strategy for constructing positive and negative\nsample pairs significantly affects the final representation quality. However,\ndue to the continuous nature of time series semantics, the modeling approach of\ncontrastive learning struggles to accommodate the characteristics of time\nseries data. This results in issues such as difficulties in constructing hard\nnegative samples and the potential introduction of inappropriate biases during\npositive sample construction. Although some recent works have developed several\nscientific strategies for constructing positive and negative sample pairs with\nimproved effectiveness, they remain constrained by the contrastive learning\nframework. To fundamentally overcome the limitations of contrastive learning,\nthis paper introduces Frequency-masked Embedding Inference (FEI), a novel\nnon-contrastive method that completely eliminates the need for positive and\nnegative samples. The proposed FEI constructs 2 inference branches based on a\nprompting strategy: 1) Using frequency masking as prompts to infer the\nembedding representation of the target series with missing frequency bands in\nthe embedding space, and 2) Using the target series as prompts to infer its\nfrequency masking embedding. In this way, FEI enables continuous semantic\nrelationship modeling for time series. Experiments on 8 widely used time series\ndatasets for classification and regression tasks, using linear evaluation and\nend-to-end fine-tuning, show that FEI significantly outperforms existing\ncontrastive-based methods in terms of generalization. This study provides new\ninsights into self-supervised representation learning for time series. The code\nis available at\nhttps://github.com/USTBInnovationPark/Frequency-masked-Embedding-Inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive learning underpins most current self-supervised time series\nrepresentation methods. The strategy for constructing positive and negative\nsample pairs significantly affects the final representation quality. However,\ndue to the continuous nature of time series semantics, the modeling approach of\ncontrastive learning struggles to accommodate the characteristics of time\nseries data. This results in issues such as difficulties in constructing hard\nnegative samples and the potential introduction of inappropriate biases during\npositive sample construction. Although some recent works have developed several\nscientific strategies for constructing positive and negative sample pairs with\nimproved effectiveness, they remain constrained by the contrastive learning\nframework. To fundamentally overcome the limitations of contrastive learning,\nthis paper introduces Frequency-masked Embedding Inference (FEI), a novel\nnon-contrastive method that completely eliminates the need for positive and\nnegative samples. The proposed FEI constructs 2 inference branches based on a\nprompting strategy: 1) Using frequency masking as prompts to infer the\nembedding representation of the target series with missing frequency bands in\nthe embedding space, and 2) Using the target series as prompts to infer its\nfrequency masking embedding. In this way, FEI enables continuous semantic\nrelationship modeling for time series. Experiments on 8 widely used time series\ndatasets for classification and regression tasks, using linear evaluation and\nend-to-end fine-tuning, show that FEI significantly outperforms existing\ncontrastive-based methods in terms of generalization. This study provides new\ninsights into self-supervised representation learning for time series. The code\nis available at\nhttps://github.com/USTBInnovationPark/Frequency-masked-Embedding-Inference."
                },
                "authors": [
                    {
                        "name": "En Fu"
                    },
                    {
                        "name": "Yanyan Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yanyan Hu"
                },
                "author": "Yanyan Hu",
                "arxiv_comment": "This paper has been accepted by AAAI-2025 main track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20787v1",
                "updated": "2024-12-30T08:11:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    11,
                    54,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T08:11:54Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    11,
                    54,
                    0,
                    365,
                    0
                ],
                "title": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for\n  LLMs in Cybersecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for\n  LLMs in Cybersecurity"
                },
                "summary": "Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs.Benchmarking\nresults on 13 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs.Benchmarking\nresults on 13 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link."
                },
                "authors": [
                    {
                        "name": "Pengfei Jing"
                    },
                    {
                        "name": "Mengyun Tang"
                    },
                    {
                        "name": "Xiaorong Shi"
                    },
                    {
                        "name": "Xing Zheng"
                    },
                    {
                        "name": "Sen Nie"
                    },
                    {
                        "name": "Shi Wu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Xiapu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Xiapu Luo"
                },
                "author": "Xiapu Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12543v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12543v3",
                "updated": "2024-12-30T07:57:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    57,
                    10,
                    0,
                    365,
                    0
                ],
                "published": "2024-10-16T13:21:46Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    21,
                    46,
                    2,
                    290,
                    0
                ],
                "title": "LLM-based Translation Inference with Iterative Bilingual Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Translation Inference with Iterative Bilingual Understanding"
                },
                "summary": "The remarkable understanding and generation capabilities of large language\nmodels (LLMs) have greatly improved translation performance. However, incorrect\nunderstanding of the sentence to be translated can degrade translation quality.\nTo address this issue, we proposed a novel Iterative Bilingual Understanding\nTranslation (IBUT) method based on the cross-lingual capabilities of LLMs and\nthe dual characteristics of translation tasks. The cross-lingual capability of\nLLMs enables the generation of contextual understanding for both the source and\ntarget languages separately. Furthermore, the dual characteristics allow IBUT\nto generate effective cross-lingual feedback, iteratively refining contextual\nunderstanding, thereby reducing errors and improving translation performance.\nExperimental results showed that the proposed IBUT outperforms several strong\ncomparison methods, especially being generalized to multiple domains (e.g.,\nnews, commonsense, and cultural translation benchmarks).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable understanding and generation capabilities of large language\nmodels (LLMs) have greatly improved translation performance. However, incorrect\nunderstanding of the sentence to be translated can degrade translation quality.\nTo address this issue, we proposed a novel Iterative Bilingual Understanding\nTranslation (IBUT) method based on the cross-lingual capabilities of LLMs and\nthe dual characteristics of translation tasks. The cross-lingual capability of\nLLMs enables the generation of contextual understanding for both the source and\ntarget languages separately. Furthermore, the dual characteristics allow IBUT\nto generate effective cross-lingual feedback, iteratively refining contextual\nunderstanding, thereby reducing errors and improving translation performance.\nExperimental results showed that the proposed IBUT outperforms several strong\ncomparison methods, especially being generalized to multiple domains (e.g.,\nnews, commonsense, and cultural translation benchmarks)."
                },
                "authors": [
                    {
                        "name": "Andong Chen"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min zhang"
                },
                "author": "Min zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12543v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12543v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19652v2",
                "updated": "2024-12-30T07:49:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    49,
                    13,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-27T13:56:51Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    56,
                    51,
                    4,
                    362,
                    0
                ],
                "title": "FreStega: A Plug-and-Play Method for Boosting Imperceptibility and\n  Capacity in Generative Linguistic Steganography for Real-World Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreStega: A Plug-and-Play Method for Boosting Imperceptibility and\n  Capacity in Generative Linguistic Steganography for Real-World Scenarios"
                },
                "summary": "Linguistic steganography embeds secret information in seemingly innocent\ntexts, safeguarding privacy in surveillance environments. Generative linguistic\nsteganography leverages the probability distribution of language models (LMs)\nand applies steganographic algorithms to generate stego tokens, gaining\nattention with recent Large Language Model (LLM) advancements. To enhance\nsecurity, researchers develop distribution-preserving stego algorithms to\nminimize the gap between stego sampling and LM sampling. However, the reliance\non language model distributions, coupled with deviations from real-world cover\ntexts, results in insufficient imperceptibility when facing steganalysis\ndetectors in real-world scenarios. Moreover, LLM distributions tend to be more\ndeterministic, resulting in reduced entropy and, consequently, lower embedding\ncapacity. In this paper, we propose FreStega, a plug-and-play method to\nreconstruct the distribution of language models used for generative linguistic\nsteganography. FreStega dynamically adjusts token probabilities from the\nlanguage model at each step of stegotext auto-regressive generation, leveraging\nboth sequential and spatial dimensions. In sequential adjustment, the\ntemperature is dynamically adjusted based on instantaneous entropy, enhancing\nthe diversity of stego texts and boosting embedding capacity. In the spatial\ndimension, the distribution is aligned with guidance from the target domain\ncorpus, closely mimicking real cover text in the target domain. By reforming\nthe distribution, FreStega enhances the imperceptibility of stego text in\npractical scenarios and improves steganographic capacity by 15.41\\%, all\nwithout compromising the quality of the generated text. FreStega serves as a\nplug-and-play remedy to enhance the imperceptibility and embedding capacity of\nexisting distribution-preserving steganography methods in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic steganography embeds secret information in seemingly innocent\ntexts, safeguarding privacy in surveillance environments. Generative linguistic\nsteganography leverages the probability distribution of language models (LMs)\nand applies steganographic algorithms to generate stego tokens, gaining\nattention with recent Large Language Model (LLM) advancements. To enhance\nsecurity, researchers develop distribution-preserving stego algorithms to\nminimize the gap between stego sampling and LM sampling. However, the reliance\non language model distributions, coupled with deviations from real-world cover\ntexts, results in insufficient imperceptibility when facing steganalysis\ndetectors in real-world scenarios. Moreover, LLM distributions tend to be more\ndeterministic, resulting in reduced entropy and, consequently, lower embedding\ncapacity. In this paper, we propose FreStega, a plug-and-play method to\nreconstruct the distribution of language models used for generative linguistic\nsteganography. FreStega dynamically adjusts token probabilities from the\nlanguage model at each step of stegotext auto-regressive generation, leveraging\nboth sequential and spatial dimensions. In sequential adjustment, the\ntemperature is dynamically adjusted based on instantaneous entropy, enhancing\nthe diversity of stego texts and boosting embedding capacity. In the spatial\ndimension, the distribution is aligned with guidance from the target domain\ncorpus, closely mimicking real cover text in the target domain. By reforming\nthe distribution, FreStega enhances the imperceptibility of stego text in\npractical scenarios and improves steganographic capacity by 15.41\\%, all\nwithout compromising the quality of the generated text. FreStega serves as a\nplug-and-play remedy to enhance the imperceptibility and embedding capacity of\nexisting distribution-preserving steganography methods in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Kaiyi Pang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyi Pang"
                },
                "author": "Kaiyi Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20772v1",
                "updated": "2024-12-30T07:47:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    47,
                    30,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T07:47:30Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    47,
                    30,
                    0,
                    365,
                    0
                ],
                "title": "Large Language Model Enabled Multi-Task Physical Layer Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Enabled Multi-Task Physical Layer Network"
                },
                "summary": "The recent advance of Artificial Intelligence (AI) is continuously reshaping\nthe future 6G wireless communications. Recently, the development of Large\nLanguage Models (LLMs) offers a promising approach to effectively improve the\nperformance and generalization for different physical layer tasks. However,\nmost existing works finetune dedicated LLM networks for a single wireless\ncommunication task separately. Thus performing diverse physical layer tasks\nintroduces extremely high training resources, memory usage, and deployment\ncosts. To solve the problem, we propose a LLM-enabled multi-task physical layer\nnetwork to unify multiple tasks with a single LLM. Specifically, we first\npropose a multi-task LLM framework, which finetunes LLM to perform multi-user\nprecoding, signal detection and channel prediction simultaneously. Besides,\nmulti-task instruction module, input encoders, as well as output decoders, are\nelaborately designed to distinguish multiple tasks and adapted the features of\ndifferent formats of wireless data for the features of LLM. Numerical\nsimulations are also displayed to verify the effectiveness of the proposed\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advance of Artificial Intelligence (AI) is continuously reshaping\nthe future 6G wireless communications. Recently, the development of Large\nLanguage Models (LLMs) offers a promising approach to effectively improve the\nperformance and generalization for different physical layer tasks. However,\nmost existing works finetune dedicated LLM networks for a single wireless\ncommunication task separately. Thus performing diverse physical layer tasks\nintroduces extremely high training resources, memory usage, and deployment\ncosts. To solve the problem, we propose a LLM-enabled multi-task physical layer\nnetwork to unify multiple tasks with a single LLM. Specifically, we first\npropose a multi-task LLM framework, which finetunes LLM to perform multi-user\nprecoding, signal detection and channel prediction simultaneously. Besides,\nmulti-task instruction module, input encoders, as well as output decoders, are\nelaborately designed to distinguish multiple tasks and adapted the features of\ndifferent formats of wireless data for the features of LLM. Numerical\nsimulations are also displayed to verify the effectiveness of the proposed\nmethod."
                },
                "authors": [
                    {
                        "name": "Tianyue Zheng"
                    },
                    {
                        "name": "Linglong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Linglong Dai"
                },
                "author": "Linglong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03963v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03963v2",
                "updated": "2024-12-30T07:46:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    46,
                    43,
                    0,
                    365,
                    0
                ],
                "published": "2024-07-04T14:33:03Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    14,
                    33,
                    3,
                    3,
                    186,
                    0
                ],
                "title": "LLM-jp: A Cross-organizational Project for the Research and Development\n  of Fully Open Japanese LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-jp: A Cross-organizational Project for the Research and Development\n  of Fully Open Japanese LLMs"
                },
                "summary": "This paper introduces LLM-jp, a cross-organizational project for the research\nand development of Japanese large language models (LLMs). LLM-jp aims to\ndevelop open-source and strong Japanese LLMs, and as of this writing, more than\n1,500 participants from academia and industry are working together for this\npurpose. This paper presents the background of the establishment of LLM-jp,\nsummaries of its activities, and technical reports on the LLMs developed by\nLLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces LLM-jp, a cross-organizational project for the research\nand development of Japanese large language models (LLMs). LLM-jp aims to\ndevelop open-source and strong Japanese LLMs, and as of this writing, more than\n1,500 participants from academia and industry are working together for this\npurpose. This paper presents the background of the establishment of LLM-jp,\nsummaries of its activities, and technical reports on the LLMs developed by\nLLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/."
                },
                "authors": [
                    {
                        "name": "LLM-jp"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Akiko Aizawa"
                    },
                    {
                        "name": "Eiji Aramaki"
                    },
                    {
                        "name": "Bowen Chen"
                    },
                    {
                        "name": "Fei Cheng"
                    },
                    {
                        "name": "Hiroyuki Deguchi"
                    },
                    {
                        "name": "Rintaro Enomoto"
                    },
                    {
                        "name": "Kazuki Fujii"
                    },
                    {
                        "name": "Kensuke Fukumoto"
                    },
                    {
                        "name": "Takuya Fukushima"
                    },
                    {
                        "name": "Namgi Han"
                    },
                    {
                        "name": "Yuto Harada"
                    },
                    {
                        "name": "Chikara Hashimoto"
                    },
                    {
                        "name": "Tatsuya Hiraoka"
                    },
                    {
                        "name": "Shohei Hisada"
                    },
                    {
                        "name": "Sosuke Hosokawa"
                    },
                    {
                        "name": "Lu Jie"
                    },
                    {
                        "name": "Keisuke Kamata"
                    },
                    {
                        "name": "Teruhito Kanazawa"
                    },
                    {
                        "name": "Hiroki Kanezashi"
                    },
                    {
                        "name": "Hiroshi Kataoka"
                    },
                    {
                        "name": "Satoru Katsumata"
                    },
                    {
                        "name": "Daisuke Kawahara"
                    },
                    {
                        "name": "Seiya Kawano"
                    },
                    {
                        "name": "Atsushi Keyaki"
                    },
                    {
                        "name": "Keisuke Kiryu"
                    },
                    {
                        "name": "Hirokazu Kiyomaru"
                    },
                    {
                        "name": "Takashi Kodama"
                    },
                    {
                        "name": "Takahiro Kubo"
                    },
                    {
                        "name": "Yohei Kuga"
                    },
                    {
                        "name": "Ryoma Kumon"
                    },
                    {
                        "name": "Shuhei Kurita"
                    },
                    {
                        "name": "Sadao Kurohashi"
                    },
                    {
                        "name": "Conglong Li"
                    },
                    {
                        "name": "Taiki Maekawa"
                    },
                    {
                        "name": "Hiroshi Matsuda"
                    },
                    {
                        "name": "Yusuke Miyao"
                    },
                    {
                        "name": "Kentaro Mizuki"
                    },
                    {
                        "name": "Sakae Mizuki"
                    },
                    {
                        "name": "Yugo Murawaki"
                    },
                    {
                        "name": "Akim Mousterou"
                    },
                    {
                        "name": "Ryo Nakamura"
                    },
                    {
                        "name": "Taishi Nakamura"
                    },
                    {
                        "name": "Kouta Nakayama"
                    },
                    {
                        "name": "Tomoka Nakazato"
                    },
                    {
                        "name": "Takuro Niitsuma"
                    },
                    {
                        "name": "Jiro Nishitoba"
                    },
                    {
                        "name": "Yusuke Oda"
                    },
                    {
                        "name": "Hayato Ogawa"
                    },
                    {
                        "name": "Takumi Okamoto"
                    },
                    {
                        "name": "Naoaki Okazaki"
                    },
                    {
                        "name": "Yohei Oseki"
                    },
                    {
                        "name": "Shintaro Ozaki"
                    },
                    {
                        "name": "Koki Ryu"
                    },
                    {
                        "name": "Rafal Rzepka"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "Shota Sasaki"
                    },
                    {
                        "name": "Satoshi Sekine"
                    },
                    {
                        "name": "Kohei Suda"
                    },
                    {
                        "name": "Saku Sugawara"
                    },
                    {
                        "name": "Issa Sugiura"
                    },
                    {
                        "name": "Hiroaki Sugiyama"
                    },
                    {
                        "name": "Hisami Suzuki"
                    },
                    {
                        "name": "Jun Suzuki"
                    },
                    {
                        "name": "Toyotaro Suzumura"
                    },
                    {
                        "name": "Kensuke Tachibana"
                    },
                    {
                        "name": "Yu Takagi"
                    },
                    {
                        "name": "Kyosuke Takami"
                    },
                    {
                        "name": "Koichi Takeda"
                    },
                    {
                        "name": "Masashi Takeshita"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    },
                    {
                        "name": "Kenjiro Taura"
                    },
                    {
                        "name": "Arseny Tolmachev"
                    },
                    {
                        "name": "Nobuhiro Ueda"
                    },
                    {
                        "name": "Zhen Wan"
                    },
                    {
                        "name": "Shuntaro Yada"
                    },
                    {
                        "name": "Sakiko Yahata"
                    },
                    {
                        "name": "Yuya Yamamoto"
                    },
                    {
                        "name": "Yusuke Yamauchi"
                    },
                    {
                        "name": "Hitomi Yanaka"
                    },
                    {
                        "name": "Rio Yokota"
                    },
                    {
                        "name": "Koichiro Yoshino"
                    }
                ],
                "author_detail": {
                    "name": "Koichiro Yoshino"
                },
                "author": "Koichiro Yoshino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03963v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03963v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16265v2",
                "updated": "2024-12-30T07:27:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    27,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-20T10:06:11Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    6,
                    11,
                    4,
                    355,
                    0
                ],
                "title": "Autoware.Flex: Human-Instructed Dynamically Reconfigurable Autonomous\n  Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoware.Flex: Human-Instructed Dynamically Reconfigurable Autonomous\n  Driving Systems"
                },
                "summary": "Existing Autonomous Driving Systems (ADS) independently make driving\ndecisions, but they face two significant limitations. First, in complex\nscenarios, ADS may misinterpret the environment and make inappropriate driving\ndecisions. Second, these systems are unable to incorporate human driving\npreferences in their decision-making processes. This paper proposes\nAutoware$.$Flex, a novel ADS system that incorporates human input into the\ndriving process, allowing users to guide the ADS in making more appropriate\ndecisions and ensuring their preferences are satisfied. Achieving this needs to\naddress two key challenges: (1) translating human instructions, expressed in\nnatural language, into a format the ADS can understand, and (2) ensuring these\ninstructions are executed safely and consistently within the ADS' s\ndecision-making framework. For the first challenge, we employ a Large Language\nModel (LLM) assisted by an ADS-specialized knowledge base to enhance\ndomain-specific translation. For the second challenge, we design a validation\nmechanism to ensure that human instructions result in safe and consistent\ndriving behavior. Experiments conducted on both simulators and a real-world\nautonomous vehicle demonstrate that Autoware$.$Flex effectively interprets\nhuman instructions and executes them safely.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Autonomous Driving Systems (ADS) independently make driving\ndecisions, but they face two significant limitations. First, in complex\nscenarios, ADS may misinterpret the environment and make inappropriate driving\ndecisions. Second, these systems are unable to incorporate human driving\npreferences in their decision-making processes. This paper proposes\nAutoware$.$Flex, a novel ADS system that incorporates human input into the\ndriving process, allowing users to guide the ADS in making more appropriate\ndecisions and ensuring their preferences are satisfied. Achieving this needs to\naddress two key challenges: (1) translating human instructions, expressed in\nnatural language, into a format the ADS can understand, and (2) ensuring these\ninstructions are executed safely and consistently within the ADS' s\ndecision-making framework. For the first challenge, we employ a Large Language\nModel (LLM) assisted by an ADS-specialized knowledge base to enhance\ndomain-specific translation. For the second challenge, we design a validation\nmechanism to ensure that human instructions result in safe and consistent\ndriving behavior. Experiments conducted on both simulators and a real-world\nautonomous vehicle demonstrate that Autoware$.$Flex effectively interprets\nhuman instructions and executes them safely."
                },
                "authors": [
                    {
                        "name": "Ziwei Song"
                    },
                    {
                        "name": "Mingsong Lv"
                    },
                    {
                        "name": "Tianchi Ren"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Jen-Ming Wu"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_comment": "14 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09945v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09945v4",
                "updated": "2024-12-30T07:26:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    26,
                    14,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-19T12:34:31Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    34,
                    31,
                    0,
                    232,
                    0
                ],
                "title": "Large Language Models for Classical Chinese Poetry Translation:\n  Benchmarking, Evaluating, and Improving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Classical Chinese Poetry Translation:\n  Benchmarking, Evaluating, and Improving"
                },
                "summary": "Different from the traditional translation tasks, classical Chinese poetry\ntranslation requires both adequacy and fluency in translating culturally and\nhistorically significant content and linguistic poetic elegance. Large language\nmodels (LLMs) with impressive multilingual capabilities may bring a ray of hope\nto achieve this extreme translation demand. This paper first introduces a\nsuitable benchmark (PoetMT) where each Chinese poetry has a recognized elegant\ntranslation. Meanwhile, we propose a new metric based on GPT-4 to evaluate the\nextent to which current LLMs can meet these demands. Our empirical evaluation\nreveals that the existing LLMs fall short in the challenging task. Hence, we\npropose a Retrieval-Augmented Machine Translation (RAT) method which\nincorporates knowledge related to classical poetry for advancing the\ntranslation of Chinese Poetry in LLMs. Experimental results show that RAT\nconsistently outperforms all comparison methods regarding wildly used BLEU,\nCOMET, BLEURT, our proposed metric, and human evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Different from the traditional translation tasks, classical Chinese poetry\ntranslation requires both adequacy and fluency in translating culturally and\nhistorically significant content and linguistic poetic elegance. Large language\nmodels (LLMs) with impressive multilingual capabilities may bring a ray of hope\nto achieve this extreme translation demand. This paper first introduces a\nsuitable benchmark (PoetMT) where each Chinese poetry has a recognized elegant\ntranslation. Meanwhile, we propose a new metric based on GPT-4 to evaluate the\nextent to which current LLMs can meet these demands. Our empirical evaluation\nreveals that the existing LLMs fall short in the challenging task. Hence, we\npropose a Retrieval-Augmented Machine Translation (RAT) method which\nincorporates knowledge related to classical poetry for advancing the\ntranslation of Chinese Poetry in LLMs. Experimental results show that RAT\nconsistently outperforms all comparison methods regarding wildly used BLEU,\nCOMET, BLEURT, our proposed metric, and human evaluation."
                },
                "authors": [
                    {
                        "name": "Andong Chen"
                    },
                    {
                        "name": "Lianzhang Lou"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09945v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09945v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13578v2",
                "updated": "2024-12-30T07:25:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    25,
                    13,
                    0,
                    365,
                    0
                ],
                "published": "2024-05-22T12:15:52Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    12,
                    15,
                    52,
                    2,
                    143,
                    0
                ],
                "title": "ConTrans: Weak-to-Strong Alignment Engineering via Concept\n  Transplantation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConTrans: Weak-to-Strong Alignment Engineering via Concept\n  Transplantation"
                },
                "summary": "Ensuring large language models (LLM) behave consistently with human goals,\nvalues, and intentions is crucial for their safety but yet computationally\nexpensive. To reduce the computational cost of alignment training of LLMs,\nespecially for those with a huge number of parameters, and to reutilize learned\nvalue alignment, we propose ConTrans, a novel framework that enables\nweak-to-strong alignment transfer via concept transplantation. From the\nperspective of representation engineering, ConTrans refines concept vectors in\nvalue alignment from a source LLM (usually a weak yet aligned LLM). The refined\nconcept vectors are then reformulated to adapt to the target LLM (usually a\nstrong yet unaligned base LLM) via affine transformation. In the third step,\nConTrans transplants the reformulated concept vectors into the residual stream\nof the target LLM. Experiments demonstrate the successful transplantation of a\nwide range of aligned concepts from 7B models to 13B and 70B models across\nmultiple LLMs and LLM families. Remarkably, ConTrans even surpasses\ninstruction-tuned models in terms of truthfulness. Experiment results validate\nthe effectiveness of both inter-LLM-family and intra-LLM-family concept\ntransplantation. Our work successfully demonstrates an alternative way to\nachieve weak-to-strong alignment generalization and control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring large language models (LLM) behave consistently with human goals,\nvalues, and intentions is crucial for their safety but yet computationally\nexpensive. To reduce the computational cost of alignment training of LLMs,\nespecially for those with a huge number of parameters, and to reutilize learned\nvalue alignment, we propose ConTrans, a novel framework that enables\nweak-to-strong alignment transfer via concept transplantation. From the\nperspective of representation engineering, ConTrans refines concept vectors in\nvalue alignment from a source LLM (usually a weak yet aligned LLM). The refined\nconcept vectors are then reformulated to adapt to the target LLM (usually a\nstrong yet unaligned base LLM) via affine transformation. In the third step,\nConTrans transplants the reformulated concept vectors into the residual stream\nof the target LLM. Experiments demonstrate the successful transplantation of a\nwide range of aligned concepts from 7B models to 13B and 70B models across\nmultiple LLMs and LLM families. Remarkably, ConTrans even surpasses\ninstruction-tuned models in terms of truthfulness. Experiment results validate\nthe effectiveness of both inter-LLM-family and intra-LLM-family concept\ntransplantation. Our work successfully demonstrates an alternative way to\nachieve weak-to-strong alignment generalization and control."
                },
                "authors": [
                    {
                        "name": "Weilong Dong"
                    },
                    {
                        "name": "Xinwei Wu"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Shaoyang Xu"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05395v2",
                "updated": "2024-12-30T07:20:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    20,
                    27,
                    0,
                    365,
                    0
                ],
                "published": "2024-06-08T08:12:41Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    8,
                    12,
                    41,
                    5,
                    160,
                    0
                ],
                "title": "Dynamic Importance Learning using Fisher Information Matrix (FIM) for\n  Nonlinear Dynamic Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Importance Learning using Fisher Information Matrix (FIM) for\n  Nonlinear Dynamic Mapping"
                },
                "summary": "Understanding output variance is critical in modeling nonlinear dynamic\nsystems, as it reflects the system's sensitivity to input variations and\nfeature interactions. This work presents a methodology for dynamically\ndetermining relevance scores in black-box models while ensuring\ninterpretability through an embedded decision module. This interpretable\nmodule, integrated into the first layer of the model, employs the Fisher\nInformation Matrix (FIM) and logistic regression to compute relevance scores,\ninterpreted as the probabilities of input neurons being active based on their\ncontribution to the output variance. The proposed method leverages a\ngradient-based framework to uncover the importance of variance-driven features,\ncapturing both individual contributions and complex feature interactions. These\nrelevance scores are applied through element-wise transformations of the\ninputs, enabling the black-box model to prioritize features dynamically based\non their impact on system output. This approach effectively bridges\ninterpretability with the intricate modeling of nonlinear dynamics and\ntime-dependent interactions. Simulation results demonstrate the method's\nability to infer feature interactions while achieving superior performance in\nfeature relevance compared to existing techniques. The practical utility of\nthis approach is showcased through its application to an industrial pH\nneutralization process, where critical system dynamics are uncovered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding output variance is critical in modeling nonlinear dynamic\nsystems, as it reflects the system's sensitivity to input variations and\nfeature interactions. This work presents a methodology for dynamically\ndetermining relevance scores in black-box models while ensuring\ninterpretability through an embedded decision module. This interpretable\nmodule, integrated into the first layer of the model, employs the Fisher\nInformation Matrix (FIM) and logistic regression to compute relevance scores,\ninterpreted as the probabilities of input neurons being active based on their\ncontribution to the output variance. The proposed method leverages a\ngradient-based framework to uncover the importance of variance-driven features,\ncapturing both individual contributions and complex feature interactions. These\nrelevance scores are applied through element-wise transformations of the\ninputs, enabling the black-box model to prioritize features dynamically based\non their impact on system output. This approach effectively bridges\ninterpretability with the intricate modeling of nonlinear dynamics and\ntime-dependent interactions. Simulation results demonstrate the method's\nability to infer feature interactions while achieving superior performance in\nfeature relevance compared to existing techniques. The practical utility of\nthis approach is showcased through its application to an industrial pH\nneutralization process, where critical system dynamics are uncovered."
                },
                "authors": [
                    {
                        "name": "Vahid MohammadZadeh Eivaghi"
                    },
                    {
                        "name": "Mahdi Aliyari Shoorehdeli"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Aliyari Shoorehdeli"
                },
                "author": "Mahdi Aliyari Shoorehdeli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20758v1",
                "updated": "2024-12-30T07:03:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    3,
                    54,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T07:03:54Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    3,
                    54,
                    0,
                    365,
                    0
                ],
                "title": "High-Sensitivity Vision-Based Tactile Sensing Enhanced by\n  Microstructures and Lightweight CNN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Sensitivity Vision-Based Tactile Sensing Enhanced by\n  Microstructures and Lightweight CNN"
                },
                "summary": "Tactile sensing is critical in advanced interactive systems by emulating the\nhuman sense of touch to detect stimuli. Vision-based tactile sensors (VBTSs)\nare promising for their ability to provide rich information, robustness,\nadaptability, low cost, and multimodal capabilities. However, current\ntechnologies still have limitations in sensitivity, spatial resolution, and the\nhigh computational demands of deep learning-based image processing. This paper\npresents a comprehensive approach combining a novel sensor structure with\nmicromachined structures and an efficient image processing method, and\ndemonstrates that carefully engineered microstructures within the sensor\nhardware can significantly enhance sensitivity while reducing computational\nload. Unlike traditional designs with tracking markers, our sensor incorporates\nan interface surface with micromachined trenches, as an example of\nmicrostructures, which modulate light transmission and amplify the variation in\nresponse to applied force. By capturing variations in brightness, wire width,\nand cross pattern locations with a camera, the sensor accurately infers the\ncontact location, the magnitude of displacement and applied force with a\nlightweight convolutional neural network (CNN). Theoretical and experimental\nresults demonstrated that the microstructures significantly enhance sensitivity\nby amplifying the visual effects of shape distortion. The sensor system\neffectively detected forces below 10 mN, and achieved a millimetre-level\nsingle-point spatial resolution. Using a model with only one convolutional\nlayer, a mean absolute error (MAE) below 0.05 mm have been achieved. Its soft\nsensor body ensures compatibility with soft robots and wearable electronics,\nwhile its immunity to electrical crosstalk and interference guarantees\nreliability in complex human-machine environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tactile sensing is critical in advanced interactive systems by emulating the\nhuman sense of touch to detect stimuli. Vision-based tactile sensors (VBTSs)\nare promising for their ability to provide rich information, robustness,\nadaptability, low cost, and multimodal capabilities. However, current\ntechnologies still have limitations in sensitivity, spatial resolution, and the\nhigh computational demands of deep learning-based image processing. This paper\npresents a comprehensive approach combining a novel sensor structure with\nmicromachined structures and an efficient image processing method, and\ndemonstrates that carefully engineered microstructures within the sensor\nhardware can significantly enhance sensitivity while reducing computational\nload. Unlike traditional designs with tracking markers, our sensor incorporates\nan interface surface with micromachined trenches, as an example of\nmicrostructures, which modulate light transmission and amplify the variation in\nresponse to applied force. By capturing variations in brightness, wire width,\nand cross pattern locations with a camera, the sensor accurately infers the\ncontact location, the magnitude of displacement and applied force with a\nlightweight convolutional neural network (CNN). Theoretical and experimental\nresults demonstrated that the microstructures significantly enhance sensitivity\nby amplifying the visual effects of shape distortion. The sensor system\neffectively detected forces below 10 mN, and achieved a millimetre-level\nsingle-point spatial resolution. Using a model with only one convolutional\nlayer, a mean absolute error (MAE) below 0.05 mm have been achieved. Its soft\nsensor body ensures compatibility with soft robots and wearable electronics,\nwhile its immunity to electrical crosstalk and interference guarantees\nreliability in complex human-machine environments."
                },
                "authors": [
                    {
                        "name": "Mayue Shi"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Xiaotong Guo"
                    },
                    {
                        "name": "Eric M. Yeatman"
                    }
                ],
                "author_detail": {
                    "name": "Eric M. Yeatman"
                },
                "author": "Eric M. Yeatman",
                "arxiv_comment": "26 pages, 17 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19478v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19478v2",
                "updated": "2024-12-30T06:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    6,
                    52,
                    12,
                    0,
                    365,
                    0
                ],
                "published": "2024-11-29T05:31:04Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    31,
                    4,
                    4,
                    334,
                    0
                ],
                "title": "Zero-Indexing Internet Search Augmented Generation for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Indexing Internet Search Augmented Generation for Large Language\n  Models"
                },
                "summary": "Retrieval augmented generation has emerged as an effective method to enhance\nlarge language model performance. This approach typically relies on an internal\nretrieval module that uses various indexing mechanisms to manage a static\npre-processed corpus. However, such a paradigm often falls short when it is\nnecessary to integrate the most up-to-date information that has not been\nupdated into the corpus during generative inference time. In this paper, we\nexplore an alternative approach that leverages standard search engine APIs to\ndynamically integrate the latest online information (without maintaining any\nindex for any fixed corpus), thereby improving the quality of generated\ncontent. We design a collaborative LLM-based paradigm, where we include: (i) a\nparser-LLM that determines if the Internet augmented generation is demanded and\nextracts the search keywords if so with a single inference; (ii) a mixed\nranking strategy that re-ranks the retrieved HTML files to eliminate bias\nintroduced from the search engine API; and (iii) an extractor-LLM that can\naccurately and efficiently extract relevant information from the fresh content\nin each HTML file. We conduct extensive empirical studies to evaluate the\nperformance of this Internet search augmented generation paradigm. The\nexperimental results demonstrate that our method generates content with\nsignificantly improved quality. Our system has been successfully deployed in a\nproduction environment to serve 01.AI's generative inference requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation has emerged as an effective method to enhance\nlarge language model performance. This approach typically relies on an internal\nretrieval module that uses various indexing mechanisms to manage a static\npre-processed corpus. However, such a paradigm often falls short when it is\nnecessary to integrate the most up-to-date information that has not been\nupdated into the corpus during generative inference time. In this paper, we\nexplore an alternative approach that leverages standard search engine APIs to\ndynamically integrate the latest online information (without maintaining any\nindex for any fixed corpus), thereby improving the quality of generated\ncontent. We design a collaborative LLM-based paradigm, where we include: (i) a\nparser-LLM that determines if the Internet augmented generation is demanded and\nextracts the search keywords if so with a single inference; (ii) a mixed\nranking strategy that re-ranks the retrieved HTML files to eliminate bias\nintroduced from the search engine API; and (iii) an extractor-LLM that can\naccurately and efficiently extract relevant information from the fresh content\nin each HTML file. We conduct extensive empirical studies to evaluate the\nperformance of this Internet search augmented generation paradigm. The\nexperimental results demonstrate that our method generates content with\nsignificantly improved quality. Our system has been successfully deployed in a\nproduction environment to serve 01.AI's generative inference requests."
                },
                "authors": [
                    {
                        "name": "Guangxin He"
                    },
                    {
                        "name": "Zonghong Dai"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Binqiang Zhao"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Chenyue Li"
                    },
                    {
                        "name": "You Peng"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19478v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19478v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18279v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18279v6",
                "updated": "2024-12-30T05:54:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    54,
                    24,
                    0,
                    365,
                    0
                ],
                "published": "2024-11-27T12:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "Large Language Model-Brained GUI Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Brained GUI Agents: A Survey"
                },
                "summary": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18279v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18279v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09843v2",
                "updated": "2024-12-30T05:27:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    27,
                    32,
                    0,
                    365,
                    0
                ],
                "published": "2024-10-13T13:54:30Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    13,
                    54,
                    30,
                    6,
                    287,
                    0
                ],
                "title": "ALMA-IMF XVIII: The assembly of a star cluster: Dense N$_2$H$^+$ (1-0)\n  kinematics in the massive G351.77 protocluster",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALMA-IMF XVIII: The assembly of a star cluster: Dense N$_2$H$^+$ (1-0)\n  kinematics in the massive G351.77 protocluster"
                },
                "summary": "ALMA-IMF observed 15 massive protoclusters capturing multiple spectral lines\nand the continuum emission. We focus on the G351.77 protocluster ($\\sim$ 2500\nM$_{\\odot}$, estimated from single-dish continuum observations) located at 2\nkpc. We trace the dense gas emission and kinematics with N$_2$H$^+$ (1-0) at\n$\\sim$ 4 kau resolution. We estimate an N$_2$H$^+$ relative abundance $\\sim\n(1.7 \\pm 0.5) \\times 10^{-10}$. We decompose the N$_2$H$^+$ emission into up to\ntwo velocity components, highlighting the kinematic complexity in the dense\ngas. By examining the position-velocity (PV) diagrams on small scales, we\nobserve clear inflow signatures (V-shapes) associated with dense cores. The\nmost prominent V-shape has a mass inflow rate of $\\sim 9.8 \\times 10^{-4}$\nM$_{\\odot}$ yr$^{-1}$ and a short timescale of $\\sim$ 15.6 kyr. We also observe\nV-shapes without associated cores. This suggests both that cores or centers of\naccretion exist below the 1.3 mm detection limit, and that the V-shapes may be\nviable tracers of very early accretion and star formation on $\\sim$ 4 kau\nscales. The large-scale PV diagram shows that the protocluster is separated\ninto 2 principal velocity structures. Combined with smaller scale DCN and\nH$_2$CO emission, we propose a scenario of larger scale slow contraction with\nrotation in the center based on simple toy models. This scenario leads the\nsuggestion of outside-in evolution of the protocluster as it collapses. The gas\ndepletion times implied by the V-shapes are short ($\\sim$ 0.3 Myr), requiring\neither very fast cluster formation, and/or continuous mass feeding of the\nprotocluster. The latter is possible via the Mother Filament G351.77 is forming\nout of. The similarities in the properties of G351.77 and the recently\npublished work in G353.41 indicate that many of the physical conditions\ninferred via the ALMA-IMF N$_2$H$^+$ observations may be generic to\nprotoclusters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALMA-IMF observed 15 massive protoclusters capturing multiple spectral lines\nand the continuum emission. We focus on the G351.77 protocluster ($\\sim$ 2500\nM$_{\\odot}$, estimated from single-dish continuum observations) located at 2\nkpc. We trace the dense gas emission and kinematics with N$_2$H$^+$ (1-0) at\n$\\sim$ 4 kau resolution. We estimate an N$_2$H$^+$ relative abundance $\\sim\n(1.7 \\pm 0.5) \\times 10^{-10}$. We decompose the N$_2$H$^+$ emission into up to\ntwo velocity components, highlighting the kinematic complexity in the dense\ngas. By examining the position-velocity (PV) diagrams on small scales, we\nobserve clear inflow signatures (V-shapes) associated with dense cores. The\nmost prominent V-shape has a mass inflow rate of $\\sim 9.8 \\times 10^{-4}$\nM$_{\\odot}$ yr$^{-1}$ and a short timescale of $\\sim$ 15.6 kyr. We also observe\nV-shapes without associated cores. This suggests both that cores or centers of\naccretion exist below the 1.3 mm detection limit, and that the V-shapes may be\nviable tracers of very early accretion and star formation on $\\sim$ 4 kau\nscales. The large-scale PV diagram shows that the protocluster is separated\ninto 2 principal velocity structures. Combined with smaller scale DCN and\nH$_2$CO emission, we propose a scenario of larger scale slow contraction with\nrotation in the center based on simple toy models. This scenario leads the\nsuggestion of outside-in evolution of the protocluster as it collapses. The gas\ndepletion times implied by the V-shapes are short ($\\sim$ 0.3 Myr), requiring\neither very fast cluster formation, and/or continuous mass feeding of the\nprotocluster. The latter is possible via the Mother Filament G351.77 is forming\nout of. The similarities in the properties of G351.77 and the recently\npublished work in G353.41 indicate that many of the physical conditions\ninferred via the ALMA-IMF N$_2$H$^+$ observations may be generic to\nprotoclusters."
                },
                "authors": [
                    {
                        "name": "N. A. Sandoval-Garrido"
                    },
                    {
                        "name": "A. M. Stutz"
                    },
                    {
                        "name": "R. H. Ãlvarez-GutiÃ©rrez"
                    },
                    {
                        "name": "R. GalvÃ¡n-Madrid"
                    },
                    {
                        "name": "F. Motte"
                    },
                    {
                        "name": "A. Ginsburg"
                    },
                    {
                        "name": "N. Cunningham"
                    },
                    {
                        "name": "S. Reyes-Reyes"
                    },
                    {
                        "name": "E. Redaelli"
                    },
                    {
                        "name": "M. Bonfand"
                    },
                    {
                        "name": "J. Salinas"
                    },
                    {
                        "name": "A. Koley"
                    },
                    {
                        "name": "J. Braine"
                    },
                    {
                        "name": "L. Bronfman"
                    },
                    {
                        "name": "G. Busquet"
                    },
                    {
                        "name": "T. Csengeri"
                    },
                    {
                        "name": "J. Di Francesco"
                    },
                    {
                        "name": "M. FernÃ¡ndez-LÃ³pez"
                    },
                    {
                        "name": "P. Garcia"
                    },
                    {
                        "name": "A. Gusdorf"
                    },
                    {
                        "name": "H. -L. Liu"
                    },
                    {
                        "name": "P. Sanhueza"
                    }
                ],
                "author_detail": {
                    "name": "P. Sanhueza"
                },
                "author": "P. Sanhueza",
                "arxiv_comment": "Submitted in A&A, 28 pages, 31 figures, 4 interactive figures, 7\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20718v1",
                "updated": "2024-12-30T05:18:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    18,
                    55,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T05:18:55Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    18,
                    55,
                    0,
                    365,
                    0
                ],
                "title": "M$^3$oralBench: A MultiModal Moral Benchmark for LVLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M$^3$oralBench: A MultiModal Moral Benchmark for LVLMs"
                },
                "summary": "Recently, large foundation models, including large language models (LLMs) and\nlarge vision-language models (LVLMs), have become essential tools in critical\nfields such as law, finance, and healthcare. As these models increasingly\nintegrate into our daily life, it is necessary to conduct moral evaluation to\nensure that their outputs align with human values and remain within moral\nboundaries. Previous works primarily focus on LLMs, proposing moral datasets\nand benchmarks limited to text modality. However, given the rapid development\nof LVLMs, there is still a lack of multimodal moral evaluation methods. To\nbridge this gap, we introduce M$^3$oralBench, the first MultiModal Moral\nBenchmark for LVLMs. M$^3$oralBench expands the everyday moral scenarios in\nMoral Foundations Vignettes (MFVs) and employs the text-to-image diffusion\nmodel, SD3.0, to create corresponding scenario images. It conducts moral\nevaluation across six moral foundations of Moral Foundations Theory (MFT) and\nencompasses tasks in moral judgement, moral classification, and moral response,\nproviding a comprehensive assessment of model performance in multimodal moral\nunderstanding and reasoning. Extensive experiments on 10 popular open-source\nand closed-source LVLMs demonstrate that M$^3$oralBench is a challenging\nbenchmark, exposing notable moral limitations in current models. Our benchmark\nis publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large foundation models, including large language models (LLMs) and\nlarge vision-language models (LVLMs), have become essential tools in critical\nfields such as law, finance, and healthcare. As these models increasingly\nintegrate into our daily life, it is necessary to conduct moral evaluation to\nensure that their outputs align with human values and remain within moral\nboundaries. Previous works primarily focus on LLMs, proposing moral datasets\nand benchmarks limited to text modality. However, given the rapid development\nof LVLMs, there is still a lack of multimodal moral evaluation methods. To\nbridge this gap, we introduce M$^3$oralBench, the first MultiModal Moral\nBenchmark for LVLMs. M$^3$oralBench expands the everyday moral scenarios in\nMoral Foundations Vignettes (MFVs) and employs the text-to-image diffusion\nmodel, SD3.0, to create corresponding scenario images. It conducts moral\nevaluation across six moral foundations of Moral Foundations Theory (MFT) and\nencompasses tasks in moral judgement, moral classification, and moral response,\nproviding a comprehensive assessment of model performance in multimodal moral\nunderstanding and reasoning. Extensive experiments on 10 popular open-source\nand closed-source LVLMs demonstrate that M$^3$oralBench is a challenging\nbenchmark, exposing notable moral limitations in current models. Our benchmark\nis publicly available."
                },
                "authors": [
                    {
                        "name": "Bei Yan"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "name": "Shiguang Shan"
                    },
                    {
                        "name": "Xilin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xilin Chen"
                },
                "author": "Xilin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10463v2",
                "updated": "2024-12-30T05:13:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    13,
                    23,
                    0,
                    365,
                    0
                ],
                "published": "2023-10-16T14:43:27Z",
                "published_parsed": [
                    2023,
                    10,
                    16,
                    14,
                    43,
                    27,
                    0,
                    289,
                    0
                ],
                "title": "Combating Label Noise With A General Surrogate Model For Sample\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combating Label Noise With A General Surrogate Model For Sample\n  Selection"
                },
                "summary": "Modern deep learning systems are data-hungry. Learning with web data is one\nof the feasible solutions, but will introduce label noise inevitably, which can\nhinder the performance of deep neural networks. Sample selection is an\neffective way to deal with label noise. The key is to separate clean samples\nbased on some criterion. Previous methods pay more attention to the small loss\ncriterion where small-loss samples are regarded as clean ones. Nevertheless,\nsuch a strategy relies on the learning dynamics of each data instance. Some\nnoisy samples are still memorized due to frequently occurring corrupted\nlearning patterns. To tackle this problem, a training-free surrogate model is\npreferred, freeing from the effect of memorization. In this work, we propose to\nleverage the vision-language surrogate model CLIP to filter noisy samples\nautomatically. CLIP brings external knowledge to facilitate the selection of\nclean samples with its ability of text-image alignment. Furthermore, a margin\nadaptive loss is designed to regularize the selection bias introduced by CLIP,\nproviding robustness to label noise. We validate the effectiveness of our\nproposed method on both real-world and synthetic noisy datasets. Our method\nachieves significant improvement without CLIP involved during the inference\nstage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern deep learning systems are data-hungry. Learning with web data is one\nof the feasible solutions, but will introduce label noise inevitably, which can\nhinder the performance of deep neural networks. Sample selection is an\neffective way to deal with label noise. The key is to separate clean samples\nbased on some criterion. Previous methods pay more attention to the small loss\ncriterion where small-loss samples are regarded as clean ones. Nevertheless,\nsuch a strategy relies on the learning dynamics of each data instance. Some\nnoisy samples are still memorized due to frequently occurring corrupted\nlearning patterns. To tackle this problem, a training-free surrogate model is\npreferred, freeing from the effect of memorization. In this work, we propose to\nleverage the vision-language surrogate model CLIP to filter noisy samples\nautomatically. CLIP brings external knowledge to facilitate the selection of\nclean samples with its ability of text-image alignment. Furthermore, a margin\nadaptive loss is designed to regularize the selection bias introduced by CLIP,\nproviding robustness to label noise. We validate the effectiveness of our\nproposed method on both real-world and synthetic noisy datasets. Our method\nachieves significant improvement without CLIP involved during the inference\nstage."
                },
                "authors": [
                    {
                        "name": "Chao Liang"
                    },
                    {
                        "name": "Linchao Zhu"
                    },
                    {
                        "name": "Humphrey Shi"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_doi": "10.1007/s11263-024-02324-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11263-024-02324-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.10463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IJCV 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08877v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08877v3",
                "updated": "2024-12-30T05:08:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    8,
                    0,
                    0,
                    365,
                    0
                ],
                "published": "2024-04-13T02:36:40Z",
                "published_parsed": [
                    2024,
                    4,
                    13,
                    2,
                    36,
                    40,
                    5,
                    104,
                    0
                ],
                "title": "Aligning the Objective of LLM-based Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning the Objective of LLM-based Program Repair"
                },
                "summary": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR."
                },
                "authors": [
                    {
                        "name": "Junjielong Xu"
                    },
                    {
                        "name": "Ying Fu"
                    },
                    {
                        "name": "Shin Hwei Tan"
                    },
                    {
                        "name": "Pinjia He"
                    }
                ],
                "author_detail": {
                    "name": "Pinjia He"
                },
                "author": "Pinjia He",
                "arxiv_comment": "Accepted by ICSE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08877v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08877v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20715v1",
                "updated": "2024-12-30T05:07:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    7,
                    34,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T05:07:34Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    7,
                    34,
                    0,
                    365,
                    0
                ],
                "title": "ChartAdapter: Large Vision-Language Model for Chart Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChartAdapter: Large Vision-Language Model for Chart Summarization"
                },
                "summary": "Chart summarization, which focuses on extracting key information from charts\nand interpreting it in natural language, is crucial for generating and\ndelivering insights through effective and accessible data analysis. Traditional\nmethods for chart understanding and summarization often rely on multi-stage\npipelines, which may produce suboptimal semantic alignment between visual and\ntextual information. In comparison, recently developed LLM-based methods are\nmore dependent on the capability of foundation images or languages, while\nignoring the characteristics of chart data and its relevant challenges. To\naddress these limitations, we propose ChartAdapter, a novel lightweight\ntransformer module designed to bridge the gap between charts and textual\nsummaries. ChartAdapter employs learnable query vectors to extract implicit\nsemantics from chart data and incorporates a cross-modal alignment projector to\nenhance vision-to-language generative learning. By integrating ChartAdapter\nwith an LLM, we enable end-to-end training and efficient chart summarization.\nTo further enhance the training, we introduce a three-stage hierarchical\ntraining procedure and develop a large-scale dataset specifically curated for\nchart summarization, comprising 190,618 samples. Experimental results on the\nstandard Chart-to-Text testing set demonstrate that our approach significantly\noutperforms existing methods, including state-of-the-art models, in generating\nhigh-quality chart summaries. Ablation studies further validate the\neffectiveness of key components in ChartAdapter. This work highlights the\npotential of tailored LLM-based approaches to advance chart understanding and\nsets a strong foundation for future research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chart summarization, which focuses on extracting key information from charts\nand interpreting it in natural language, is crucial for generating and\ndelivering insights through effective and accessible data analysis. Traditional\nmethods for chart understanding and summarization often rely on multi-stage\npipelines, which may produce suboptimal semantic alignment between visual and\ntextual information. In comparison, recently developed LLM-based methods are\nmore dependent on the capability of foundation images or languages, while\nignoring the characteristics of chart data and its relevant challenges. To\naddress these limitations, we propose ChartAdapter, a novel lightweight\ntransformer module designed to bridge the gap between charts and textual\nsummaries. ChartAdapter employs learnable query vectors to extract implicit\nsemantics from chart data and incorporates a cross-modal alignment projector to\nenhance vision-to-language generative learning. By integrating ChartAdapter\nwith an LLM, we enable end-to-end training and efficient chart summarization.\nTo further enhance the training, we introduce a three-stage hierarchical\ntraining procedure and develop a large-scale dataset specifically curated for\nchart summarization, comprising 190,618 samples. Experimental results on the\nstandard Chart-to-Text testing set demonstrate that our approach significantly\noutperforms existing methods, including state-of-the-art models, in generating\nhigh-quality chart summaries. Ablation studies further validate the\neffectiveness of key components in ChartAdapter. This work highlights the\npotential of tailored LLM-based approaches to advance chart understanding and\nsets a strong foundation for future research in this area."
                },
                "authors": [
                    {
                        "name": "Peixin Xu"
                    },
                    {
                        "name": "Yujuan Ding"
                    },
                    {
                        "name": "Wenqi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Fan"
                },
                "author": "Wenqi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20906v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20906v3",
                "updated": "2024-12-30T04:46:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    4,
                    46,
                    7,
                    0,
                    365,
                    0
                ],
                "published": "2024-07-30T15:26:36Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    15,
                    26,
                    36,
                    1,
                    212,
                    0
                ],
                "title": "Automated Review Generation Method Based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Review Generation Method Based on Large Language Models"
                },
                "summary": "Literature research, vital for scientific work, faces the challenge of\nsurging information volumes exceeding researchers' processing capabilities. We\npresent an automated review generation method based on large language models\n(LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our\nstatistically validated evaluation framework demonstrates that the generated\nreviews match or exceed manual quality, offering broad applicability across\nresearch fields without requiring users' domain knowledge. Applied to propane\ndehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles,\naveraging seconds per article per LLM account, producing comprehensive reviews\nspanning 35 topics, with extended analysis of 1041 articles providing insights\ninto catalysts' properties. Through multi-layered quality control, we\neffectively mitigated LLMs' hallucinations, with expert verification confirming\naccuracy and citation integrity while demonstrating hallucination risks reduced\nto below 0.5\\% with 95\\% confidence. Released Windows application enables\none-click review generation, enhancing research productivity and literature\nrecommendation efficiency while setting the stage for broader scientific\nexplorations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature research, vital for scientific work, faces the challenge of\nsurging information volumes exceeding researchers' processing capabilities. We\npresent an automated review generation method based on large language models\n(LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our\nstatistically validated evaluation framework demonstrates that the generated\nreviews match or exceed manual quality, offering broad applicability across\nresearch fields without requiring users' domain knowledge. Applied to propane\ndehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles,\naveraging seconds per article per LLM account, producing comprehensive reviews\nspanning 35 topics, with extended analysis of 1041 articles providing insights\ninto catalysts' properties. Through multi-layered quality control, we\neffectively mitigated LLMs' hallucinations, with expert verification confirming\naccuracy and citation integrity while demonstrating hallucination risks reduced\nto below 0.5\\% with 95\\% confidence. Released Windows application enables\none-click review generation, enhancing research productivity and literature\nrecommendation efficiency while setting the stage for broader scientific\nexplorations."
                },
                "authors": [
                    {
                        "name": "Shican Wu"
                    },
                    {
                        "name": "Xiao Ma"
                    },
                    {
                        "name": "Dehui Luo"
                    },
                    {
                        "name": "Lulu Li"
                    },
                    {
                        "name": "Xiangcheng Shi"
                    },
                    {
                        "name": "Xin Chang"
                    },
                    {
                        "name": "Xiaoyun Lin"
                    },
                    {
                        "name": "Ran Luo"
                    },
                    {
                        "name": "Chunlei Pei"
                    },
                    {
                        "name": "Changyin Du"
                    },
                    {
                        "name": "Zhi-Jian Zhao"
                    },
                    {
                        "name": "Jinlong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Gong"
                },
                "author": "Jinlong Gong",
                "arxiv_comment": "21 pages, 5 figures, 1 tables Code:\n  https://github.com/TJU-ECAT-AI/AutomaticReviewGeneration Data:\n  https://github.com/TJU-ECAT-AI/AutomaticReviewGenerationData This research\n  has been invited for a Short Oral presentation at the 18th ICC -\n  International Congress on Catalysis, taking place in Lyon, France from July\n  14-19, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20906v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20906v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09881v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09881v4",
                "updated": "2024-12-30T04:27:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    4,
                    27,
                    5,
                    0,
                    365,
                    0
                ],
                "published": "2023-10-15T16:40:19Z",
                "published_parsed": [
                    2023,
                    10,
                    15,
                    16,
                    40,
                    19,
                    6,
                    288,
                    0
                ],
                "title": "In-Context Learning with Iterative Demonstration Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning with Iterative Demonstration Selection"
                },
                "summary": "Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated strong few-shot learning ability via in-context learning (ICL).\nHowever, the performance of ICL has been shown to be highly sensitive to the\nselection of few-shot demonstrations. Selecting the most suitable examples as\ncontext remains an ongoing challenge and an open problem. Existing literature\nhas highlighted the importance of selecting examples that are diverse or\nsemantically similar to the test sample while ignoring the fact that the\noptimal selection dimension, i.e., diversity or similarity, is task-specific.\nBased on how the test sample is answered, we propose Iterative Demonstration\nSelection (IDS) to leverage the merits of both dimensions. Using zero-shot\nchain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples\nthat are diverse but still strongly correlated with the test sample as ICL\ndemonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample\nbefore demonstration selection. The output reasoning path is then used to\nchoose demonstrations that are prepended to the test sample for inference. The\ngenerated answer is followed by its corresponding reasoning path for extracting\na new set of demonstrations in the next iteration. After several iterations,\nIDS adopts majority voting to obtain the final result. Through extensive\nexperiments on tasks including reasoning, question answering, and topic\nclassification, we demonstrate that IDS can consistently outperform existing\nICL demonstration selection methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated strong few-shot learning ability via in-context learning (ICL).\nHowever, the performance of ICL has been shown to be highly sensitive to the\nselection of few-shot demonstrations. Selecting the most suitable examples as\ncontext remains an ongoing challenge and an open problem. Existing literature\nhas highlighted the importance of selecting examples that are diverse or\nsemantically similar to the test sample while ignoring the fact that the\noptimal selection dimension, i.e., diversity or similarity, is task-specific.\nBased on how the test sample is answered, we propose Iterative Demonstration\nSelection (IDS) to leverage the merits of both dimensions. Using zero-shot\nchain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples\nthat are diverse but still strongly correlated with the test sample as ICL\ndemonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample\nbefore demonstration selection. The output reasoning path is then used to\nchoose demonstrations that are prepended to the test sample for inference. The\ngenerated answer is followed by its corresponding reasoning path for extracting\na new set of demonstrations in the next iteration. After several iterations,\nIDS adopts majority voting to obtain the final result. Through extensive\nexperiments on tasks including reasoning, question answering, and topic\nclassification, we demonstrate that IDS can consistently outperform existing\nICL demonstration selection methods."
                },
                "authors": [
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Aston Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Anirudh Dagar"
                    },
                    {
                        "name": "Wenming Ye"
                    }
                ],
                "author_detail": {
                    "name": "Wenming Ye"
                },
                "author": "Wenming Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09881v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09881v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05961v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05961v4",
                "updated": "2024-12-30T04:22:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    4,
                    22,
                    28,
                    0,
                    365,
                    0
                ],
                "published": "2024-02-05T04:12:40Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    4,
                    12,
                    40,
                    0,
                    36,
                    0
                ],
                "title": "Genetic-guided GFlowNets for Sample Efficient Molecular Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genetic-guided GFlowNets for Sample Efficient Molecular Optimization"
                },
                "summary": "The challenge of discovering new molecules with desired properties is crucial\nin domains like drug discovery and material design. Recent advances in deep\nlearning-based generative methods have shown promise but face the issue of\nsample efficiency due to the computational expense of evaluating the reward\nfunction. This paper proposes a novel algorithm for sample-efficient molecular\noptimization by distilling a powerful genetic algorithm into deep generative\npolicy using GFlowNets training, the off-policy method for amortized inference.\nThis approach enables the deep generative policy to learn from domain\nknowledge, which has been explicitly integrated into the genetic algorithm. Our\nmethod achieves state-of-the-art performance in the official molecular\noptimization benchmark, significantly outperforming previous methods. It also\ndemonstrates effectiveness in designing inhibitors against SARS-CoV-2 with\nsubstantially fewer reward calls.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of discovering new molecules with desired properties is crucial\nin domains like drug discovery and material design. Recent advances in deep\nlearning-based generative methods have shown promise but face the issue of\nsample efficiency due to the computational expense of evaluating the reward\nfunction. This paper proposes a novel algorithm for sample-efficient molecular\noptimization by distilling a powerful genetic algorithm into deep generative\npolicy using GFlowNets training, the off-policy method for amortized inference.\nThis approach enables the deep generative policy to learn from domain\nknowledge, which has been explicitly integrated into the genetic algorithm. Our\nmethod achieves state-of-the-art performance in the official molecular\noptimization benchmark, significantly outperforming previous methods. It also\ndemonstrates effectiveness in designing inhibitors against SARS-CoV-2 with\nsubstantially fewer reward calls."
                },
                "authors": [
                    {
                        "name": "Hyeonah Kim"
                    },
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Sanghyeok Choi"
                    },
                    {
                        "name": "Jinkyoo Park"
                    }
                ],
                "author_detail": {
                    "name": "Jinkyoo Park"
                },
                "author": "Jinkyoo Park",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05961v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05961v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18819v2",
                "updated": "2024-12-30T04:15:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    4,
                    15,
                    42,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-25T08:17:37Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    8,
                    17,
                    37,
                    2,
                    360,
                    0
                ],
                "title": "LLM-assisted Vector Similarity Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-assisted Vector Similarity Search"
                },
                "summary": "As data retrieval demands become increasingly complex, traditional search\nmethods often fall short in addressing nuanced and conceptual queries. Vector\nsimilarity search has emerged as a promising technique for finding semantically\nsimilar information efficiently. However, its effectiveness diminishes when\nhandling intricate queries with contextual nuances. This paper explores a\nhybrid approach combining vector similarity search with Large Language Models\n(LLMs) to enhance search accuracy and relevance. The proposed two-step solution\nfirst employs vector similarity search to shortlist potential matches, followed\nby an LLM for context-aware ranking of the results. Experiments on structured\ndatasets demonstrate that while vector similarity search alone performs well\nfor straightforward queries, the LLM-assisted approach excels in processing\ncomplex queries involving constraints, negations, or conceptual requirements.\nBy leveraging the natural language understanding capabilities of LLMs, this\nmethod improves the accuracy of search results for complex tasks without\nsacrificing efficiency. We also discuss real-world applications and propose\ndirections for future research to refine and scale this technique for diverse\ndatasets and use cases.\n  Original article:\nhttps://engineering.grab.com/llm-assisted-vector-similarity-search",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As data retrieval demands become increasingly complex, traditional search\nmethods often fall short in addressing nuanced and conceptual queries. Vector\nsimilarity search has emerged as a promising technique for finding semantically\nsimilar information efficiently. However, its effectiveness diminishes when\nhandling intricate queries with contextual nuances. This paper explores a\nhybrid approach combining vector similarity search with Large Language Models\n(LLMs) to enhance search accuracy and relevance. The proposed two-step solution\nfirst employs vector similarity search to shortlist potential matches, followed\nby an LLM for context-aware ranking of the results. Experiments on structured\ndatasets demonstrate that while vector similarity search alone performs well\nfor straightforward queries, the LLM-assisted approach excels in processing\ncomplex queries involving constraints, negations, or conceptual requirements.\nBy leveraging the natural language understanding capabilities of LLMs, this\nmethod improves the accuracy of search results for complex tasks without\nsacrificing efficiency. We also discuss real-world applications and propose\ndirections for future research to refine and scale this technique for diverse\ndatasets and use cases.\n  Original article:\nhttps://engineering.grab.com/llm-assisted-vector-similarity-search"
                },
                "authors": [
                    {
                        "name": "Md Riyadh"
                    },
                    {
                        "name": "Muqi Li"
                    },
                    {
                        "name": "Felix Haryanto Lie"
                    },
                    {
                        "name": "Jia Long Loh"
                    },
                    {
                        "name": "Haotian Mi"
                    },
                    {
                        "name": "Sayam Bohra"
                    }
                ],
                "author_detail": {
                    "name": "Sayam Bohra"
                },
                "author": "Sayam Bohra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14368v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14368v3",
                "updated": "2024-12-30T04:09:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    4,
                    9,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-18T22:04:56Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    4,
                    56,
                    2,
                    353,
                    0
                ],
                "title": "Memorization Over Reasoning? Exposing and Mitigating Verbatim\n  Memorization in Large Language Models' Character Understanding Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization Over Reasoning? Exposing and Mitigating Verbatim\n  Memorization in Large Language Models' Character Understanding Evaluation"
                },
                "summary": "Recently, Large Language Models (LLMs) have shown impressive performance in\ncharacter understanding tasks, such as analyzing the roles, personalities, and\nrelationships of fictional characters. However, the extensive pre-training\ncorpora used by LLMs raise concerns that they may rely on memorizing popular\nfictional works rather than genuinely understanding and reasoning about them.\nIn this work, we argue that 'gist memory'-capturing essential meaning - should\nbe the primary mechanism for character understanding tasks, as opposed to\n'verbatim memory' - exact match of a string. We introduce a simple yet\neffective method to mitigate mechanized memorization in character understanding\nevaluations while preserving the essential implicit cues needed for\ncomprehension and reasoning. Our approach reduces memorization-driven\nperformance on popular fictional works from 96% accuracy to 72% and results in\nup to an 18% drop in accuracy across various character understanding tasks.\nThese findings underscore the issue of data contamination in existing\nbenchmarks, which often measure memorization rather than true character\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have shown impressive performance in\ncharacter understanding tasks, such as analyzing the roles, personalities, and\nrelationships of fictional characters. However, the extensive pre-training\ncorpora used by LLMs raise concerns that they may rely on memorizing popular\nfictional works rather than genuinely understanding and reasoning about them.\nIn this work, we argue that 'gist memory'-capturing essential meaning - should\nbe the primary mechanism for character understanding tasks, as opposed to\n'verbatim memory' - exact match of a string. We introduce a simple yet\neffective method to mitigate mechanized memorization in character understanding\nevaluations while preserving the essential implicit cues needed for\ncomprehension and reasoning. Our approach reduces memorization-driven\nperformance on popular fictional works from 96% accuracy to 72% and results in\nup to an 18% drop in accuracy across various character understanding tasks.\nThese findings underscore the issue of data contamination in existing\nbenchmarks, which often measure memorization rather than true character\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Yuxuan Jiang"
                    },
                    {
                        "name": "Francis Ferraro"
                    }
                ],
                "author_detail": {
                    "name": "Francis Ferraro"
                },
                "author": "Francis Ferraro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14368v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14368v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20694v1",
                "updated": "2024-12-30T04:05:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    4,
                    5,
                    22,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T04:05:22Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    4,
                    5,
                    22,
                    0,
                    365,
                    0
                ],
                "title": "UBER: Uncertainty-Based Evolution with Large Language Models for\n  Automatic Heuristic Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UBER: Uncertainty-Based Evolution with Large Language Models for\n  Automatic Heuristic Design"
                },
                "summary": "NP-hard problem-solving traditionally relies on heuristics, but manually\ncrafting effective heuristics for complex problems remains challenging. While\nrecent work like FunSearch has demonstrated that large language models (LLMs)\ncan be leveraged for heuristic design in evolutionary algorithm (EA)\nframeworks, their potential is not fully realized due to its deficiency in\nexploitation and exploration. We present UBER (Uncertainty-Based Evolution for\nRefinement), a method that enhances LLM+EA methods for automatic heuristic\ndesign by integrating uncertainty on top of the FunSearch framework. UBER\nintroduces two key innovations: an Uncertainty-Inclusive Evolution Process\n(UIEP) for adaptive exploration-exploitation balance, and a principled\nUncertainty-Inclusive Island Reset (UIIS) strategy for maintaining population\ndiversity. Through extensive experiments on challenging NP-complete problems,\nUBER demonstrates significant improvements over FunSearch. Our work provides a\nnew direction for the synergy of LLMs and EA, advancing the field of automatic\nheuristic design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NP-hard problem-solving traditionally relies on heuristics, but manually\ncrafting effective heuristics for complex problems remains challenging. While\nrecent work like FunSearch has demonstrated that large language models (LLMs)\ncan be leveraged for heuristic design in evolutionary algorithm (EA)\nframeworks, their potential is not fully realized due to its deficiency in\nexploitation and exploration. We present UBER (Uncertainty-Based Evolution for\nRefinement), a method that enhances LLM+EA methods for automatic heuristic\ndesign by integrating uncertainty on top of the FunSearch framework. UBER\nintroduces two key innovations: an Uncertainty-Inclusive Evolution Process\n(UIEP) for adaptive exploration-exploitation balance, and a principled\nUncertainty-Inclusive Island Reset (UIIS) strategy for maintaining population\ndiversity. Through extensive experiments on challenging NP-complete problems,\nUBER demonstrates significant improvements over FunSearch. Our work provides a\nnew direction for the synergy of LLMs and EA, advancing the field of automatic\nheuristic design."
                },
                "authors": [
                    {
                        "name": "Zijie Chen"
                    },
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Renjun Xu"
                    },
                    {
                        "name": "Lili Pan"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07010v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07010v2",
                "updated": "2024-12-30T03:52:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    52,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-09T21:36:42Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    21,
                    36,
                    42,
                    0,
                    344,
                    0
                ],
                "title": "TAEN: A Model-Constrained Tikhonov Autoencoder Network for Forward and\n  Inverse Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAEN: A Model-Constrained Tikhonov Autoencoder Network for Forward and\n  Inverse Problems"
                },
                "summary": "Efficient real-time solvers for forward and inverse problems are essential in\nengineering and science applications. Machine learning surrogate models have\nemerged as promising alternatives to traditional methods, offering\nsubstantially reduced computational time. Nevertheless, these models typically\ndemand extensive training datasets to achieve robust generalization across\ndiverse scenarios. While physics-based approaches can partially mitigate this\ndata dependency and ensure physics-interpretable solutions, addressing scarce\ndata regimes remains a challenge. Both purely data-driven and physics-based\nmachine learning approaches demonstrate severe overfitting issues when trained\nwith insufficient data. We propose a novel Tikhonov autoencoder\nmodel-constrained framework, called TAE, capable of learning both forward and\ninverse surrogate models using a single arbitrary observation sample. We\ndevelop comprehensive theoretical foundations including forward and inverse\ninference error bounds for the proposed approach for linear cases. For\ncomparative analysis, we derive equivalent formulations for pure data-driven\nand model-constrained approach counterparts. At the heart of our approach is a\ndata randomization strategy, which functions as a generative mechanism for\nexploring the training data space, enabling effective training of both forward\nand inverse surrogate models from a single observation, while regularizing the\nlearning process. We validate our approach through extensive numerical\nexperiments on two challenging inverse problems: 2D heat conductivity inversion\nand initial condition reconstruction for time-dependent 2D Navier-Stokes\nequations. Results demonstrate that TAE achieves accuracy comparable to\ntraditional Tikhonov solvers and numerical forward solvers for both inverse and\nforward problems, respectively, while delivering orders of magnitude\ncomputational speedups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-time solvers for forward and inverse problems are essential in\nengineering and science applications. Machine learning surrogate models have\nemerged as promising alternatives to traditional methods, offering\nsubstantially reduced computational time. Nevertheless, these models typically\ndemand extensive training datasets to achieve robust generalization across\ndiverse scenarios. While physics-based approaches can partially mitigate this\ndata dependency and ensure physics-interpretable solutions, addressing scarce\ndata regimes remains a challenge. Both purely data-driven and physics-based\nmachine learning approaches demonstrate severe overfitting issues when trained\nwith insufficient data. We propose a novel Tikhonov autoencoder\nmodel-constrained framework, called TAE, capable of learning both forward and\ninverse surrogate models using a single arbitrary observation sample. We\ndevelop comprehensive theoretical foundations including forward and inverse\ninference error bounds for the proposed approach for linear cases. For\ncomparative analysis, we derive equivalent formulations for pure data-driven\nand model-constrained approach counterparts. At the heart of our approach is a\ndata randomization strategy, which functions as a generative mechanism for\nexploring the training data space, enabling effective training of both forward\nand inverse surrogate models from a single observation, while regularizing the\nlearning process. We validate our approach through extensive numerical\nexperiments on two challenging inverse problems: 2D heat conductivity inversion\nand initial condition reconstruction for time-dependent 2D Navier-Stokes\nequations. Results demonstrate that TAE achieves accuracy comparable to\ntraditional Tikhonov solvers and numerical forward solvers for both inverse and\nforward problems, respectively, while delivering orders of magnitude\ncomputational speedups."
                },
                "authors": [
                    {
                        "name": "Hai V. Nguyen"
                    },
                    {
                        "name": "Tan Bui-Thanh"
                    },
                    {
                        "name": "Clint Dawson"
                    }
                ],
                "author_detail": {
                    "name": "Clint Dawson"
                },
                "author": "Clint Dawson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07010v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07010v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18619v2",
                "updated": "2024-12-30T03:00:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    0,
                    30,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T05:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    5,
                    2,
                    25,
                    0,
                    351,
                    0
                ],
                "title": "Next Token Prediction Towards Multimodal Intelligence: A Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Token Prediction Towards Multimodal Intelligence: A Comprehensive\n  Survey"
                },
                "summary": "Building on the foundations of language modeling in natural language\nprocessing, Next Token Prediction (NTP) has evolved into a versatile training\nobjective for machine learning tasks across various modalities, achieving\nconsiderable success. As Large Language Models (LLMs) have advanced to unify\nunderstanding and generation tasks within the textual modality, recent research\nhas shown that tasks from different modalities can also be effectively\nencapsulated within the NTP framework, transforming the multimodal information\ninto tokens and predict the next one given the context. This survey introduces\na comprehensive taxonomy that unifies both understanding and generation within\nmultimodal learning through the lens of NTP. The proposed taxonomy covers five\nkey aspects: Multimodal tokenization, MMNTP model architectures, unified task\nrepresentation, datasets \\& evaluation, and open challenges. This new taxonomy\naims to aid researchers in their exploration of multimodal intelligence. An\nassociated GitHub repository collecting the latest papers and repos is\navailable at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building on the foundations of language modeling in natural language\nprocessing, Next Token Prediction (NTP) has evolved into a versatile training\nobjective for machine learning tasks across various modalities, achieving\nconsiderable success. As Large Language Models (LLMs) have advanced to unify\nunderstanding and generation tasks within the textual modality, recent research\nhas shown that tasks from different modalities can also be effectively\nencapsulated within the NTP framework, transforming the multimodal information\ninto tokens and predict the next one given the context. This survey introduces\na comprehensive taxonomy that unifies both understanding and generation within\nmultimodal learning through the lens of NTP. The proposed taxonomy covers five\nkey aspects: Multimodal tokenization, MMNTP model architectures, unified task\nrepresentation, datasets \\& evaluation, and open challenges. This new taxonomy\naims to aid researchers in their exploration of multimodal intelligence. An\nassociated GitHub repository collecting the latest papers and repos is\navailable at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction"
                },
                "authors": [
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Shuhuai Ren"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Yunshui Li"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Ruoyu Wu"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Lingwei Meng"
                    },
                    {
                        "name": "Shujie Hu"
                    },
                    {
                        "name": "Yulong Chen"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Shuai Bai"
                    },
                    {
                        "name": "Andreas Vlachos"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Aaron Yee"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "69 papes, 18 figures, repo at\n  https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20662v1",
                "updated": "2024-12-30T02:40:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    2,
                    40,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T02:40:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    2,
                    40,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "Enhancing Table Recognition with Vision LLMs: A Benchmark and\n  Neighbor-Guided Toolchain Reasoner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Table Recognition with Vision LLMs: A Benchmark and\n  Neighbor-Guided Toolchain Reasoner"
                },
                "summary": "Pre-trained foundation models have recently significantly progressed in\nstructured table understanding and reasoning. However, despite advancements in\nareas such as table semantic understanding and table question answering,\nrecognizing the structure and content of unstructured tables using Vision Large\nLanguage Models (VLLMs) remains under-explored. In this work, we address this\nresearch gap by employing VLLMs in a training-free reasoning paradigm. First,\nwe design a benchmark with various hierarchical dimensions relevant to table\nrecognition. Subsequently, we conduct in-depth evaluations using pre-trained\nVLLMs, finding that low-quality image input is a significant bottleneck in the\nrecognition process. Drawing inspiration from these findings, we propose the\nNeighbor-Guided Toolchain Reasoner (NGTR) framework, which is characterized by\nintegrating multiple lightweight models for low-level visual processing\noperations aimed at mitigating issues with low-quality input images.\nSpecifically, we utilize a neighbor retrieval mechanism to guide the generation\nof multiple tool invocation plans, transferring tool selection experiences from\nsimilar neighbors to the given input, thereby facilitating suitable tool\nselection. Additionally, we introduce a reflection module to supervise the tool\ninvocation process. Extensive experiments on public table recognition datasets\ndemonstrate that our approach significantly enhances the recognition\ncapabilities of the vanilla VLLMs. We believe that the designed benchmark and\nthe proposed NGTR framework could provide an alternative solution in table\nrecognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained foundation models have recently significantly progressed in\nstructured table understanding and reasoning. However, despite advancements in\nareas such as table semantic understanding and table question answering,\nrecognizing the structure and content of unstructured tables using Vision Large\nLanguage Models (VLLMs) remains under-explored. In this work, we address this\nresearch gap by employing VLLMs in a training-free reasoning paradigm. First,\nwe design a benchmark with various hierarchical dimensions relevant to table\nrecognition. Subsequently, we conduct in-depth evaluations using pre-trained\nVLLMs, finding that low-quality image input is a significant bottleneck in the\nrecognition process. Drawing inspiration from these findings, we propose the\nNeighbor-Guided Toolchain Reasoner (NGTR) framework, which is characterized by\nintegrating multiple lightweight models for low-level visual processing\noperations aimed at mitigating issues with low-quality input images.\nSpecifically, we utilize a neighbor retrieval mechanism to guide the generation\nof multiple tool invocation plans, transferring tool selection experiences from\nsimilar neighbors to the given input, thereby facilitating suitable tool\nselection. Additionally, we introduce a reflection module to supervise the tool\ninvocation process. Extensive experiments on public table recognition datasets\ndemonstrate that our approach significantly enhances the recognition\ncapabilities of the vanilla VLLMs. We believe that the designed benchmark and\nthe proposed NGTR framework could provide an alternative solution in table\nrecognition."
                },
                "authors": [
                    {
                        "name": "Yitong Zhou"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Qingyang Mao"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Feiyang Xu"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06790v2",
                "updated": "2024-12-30T02:14:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    2,
                    14,
                    18,
                    0,
                    365,
                    0
                ],
                "published": "2024-11-11T08:36:49Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    36,
                    49,
                    0,
                    316,
                    0
                ],
                "title": "Large-scale moral machine experiment on large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale moral machine experiment on large language models"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) and their potential\nintegration into autonomous driving systems necessitates understanding their\nmoral decision-making capabilities. While our previous study examined four\nprominent LLMs using the Moral Machine experimental framework, the dynamic\nlandscape of LLM development demands a more comprehensive analysis. Here, we\nevaluate moral judgments across 52 different LLMs, including multiple versions\nof proprietary models (GPT, Claude, Gemini) and open-source alternatives\n(Llama, Gemma), to assess their alignment with human moral preferences in\nautonomous driving scenarios. Using a conjoint analysis framework, we evaluated\nhow closely LLM responses aligned with human preferences in ethical dilemmas\nand examined the effects of model size, updates, and architecture. Results\nshowed that proprietary models and open-source models exceeding 10 billion\nparameters demonstrated relatively close alignment with human judgments, with a\nsignificant negative correlation between model size and distance from human\njudgments in open-source models. However, model updates did not consistently\nimprove alignment with human preferences, and many LLMs showed excessive\nemphasis on specific ethical principles. These findings suggest that while\nincreasing model size may naturally lead to more human-like moral judgments,\npractical implementation in autonomous driving systems requires careful\nconsideration of the trade-off between judgment quality and computational\nefficiency. Our comprehensive analysis provides crucial insights for the\nethical design of autonomous systems and highlights the importance of\nconsidering cultural contexts in AI moral decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) and their potential\nintegration into autonomous driving systems necessitates understanding their\nmoral decision-making capabilities. While our previous study examined four\nprominent LLMs using the Moral Machine experimental framework, the dynamic\nlandscape of LLM development demands a more comprehensive analysis. Here, we\nevaluate moral judgments across 52 different LLMs, including multiple versions\nof proprietary models (GPT, Claude, Gemini) and open-source alternatives\n(Llama, Gemma), to assess their alignment with human moral preferences in\nautonomous driving scenarios. Using a conjoint analysis framework, we evaluated\nhow closely LLM responses aligned with human preferences in ethical dilemmas\nand examined the effects of model size, updates, and architecture. Results\nshowed that proprietary models and open-source models exceeding 10 billion\nparameters demonstrated relatively close alignment with human judgments, with a\nsignificant negative correlation between model size and distance from human\njudgments in open-source models. However, model updates did not consistently\nimprove alignment with human preferences, and many LLMs showed excessive\nemphasis on specific ethical principles. These findings suggest that while\nincreasing model size may naturally lead to more human-like moral judgments,\npractical implementation in autonomous driving systems requires careful\nconsideration of the trade-off between judgment quality and computational\nefficiency. Our comprehensive analysis provides crucial insights for the\nethical design of autonomous systems and highlights the importance of\nconsidering cultural contexts in AI moral decision-making."
                },
                "authors": [
                    {
                        "name": "Muhammad Shahrul Zaim bin Ahmad"
                    },
                    {
                        "name": "Kazuhiro Takemoto"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhiro Takemoto"
                },
                "author": "Kazuhiro Takemoto",
                "arxiv_comment": "21 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.21200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21200v1",
                "updated": "2024-12-30T18:59:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    59,
                    6,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:59:06Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    59,
                    6,
                    0,
                    365,
                    0
                ],
                "title": "Distributed Mixture-of-Agents for Edge Inference with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Mixture-of-Agents for Edge Inference with Large Language\n  Models"
                },
                "summary": "Mixture-of-Agents (MoA) has recently been proposed as a method to enhance\nperformance of large language models (LLMs), enabling multiple individual LLMs\nto work together for collaborative inference. This collaborative approach\nresults in improved responses to user prompts compared to relying on a single\nLLM. In this paper, we consider such an MoA architecture in a distributed\nsetting, where LLMs operate on individual edge devices, each uniquely\nassociated with a user and equipped with its own distributed computing power.\nThese devices exchange information using decentralized gossip algorithms,\nallowing different device nodes to talk without the supervision of a\ncentralized server. In the considered setup, different users have their own LLM\nmodels to address user prompts. Additionally, the devices gossip either their\nown user-specific prompts or augmented prompts to generate more refined answers\nto certain queries. User prompts are temporarily stored in the device queues\nwhen their corresponding LLMs are busy. Given the memory limitations of edge\ndevices, it is crucial to ensure that the average queue sizes in the system\nremain bounded. In this paper, we address this by theoretically calculating the\nqueuing stability conditions for the device queues under reasonable\nassumptions, which we validate experimentally as well. Further, we demonstrate\nthrough experiments, leveraging open-source LLMs for the implementation of\ndistributed MoA, that certain MoA configurations produce higher-quality\nresponses compared to others, as evaluated on AlpacaEval 2.0 benchmark. The\nimplementation is available at:\nhttps://github.com/purbeshmitra/distributed_moa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Agents (MoA) has recently been proposed as a method to enhance\nperformance of large language models (LLMs), enabling multiple individual LLMs\nto work together for collaborative inference. This collaborative approach\nresults in improved responses to user prompts compared to relying on a single\nLLM. In this paper, we consider such an MoA architecture in a distributed\nsetting, where LLMs operate on individual edge devices, each uniquely\nassociated with a user and equipped with its own distributed computing power.\nThese devices exchange information using decentralized gossip algorithms,\nallowing different device nodes to talk without the supervision of a\ncentralized server. In the considered setup, different users have their own LLM\nmodels to address user prompts. Additionally, the devices gossip either their\nown user-specific prompts or augmented prompts to generate more refined answers\nto certain queries. User prompts are temporarily stored in the device queues\nwhen their corresponding LLMs are busy. Given the memory limitations of edge\ndevices, it is crucial to ensure that the average queue sizes in the system\nremain bounded. In this paper, we address this by theoretically calculating the\nqueuing stability conditions for the device queues under reasonable\nassumptions, which we validate experimentally as well. Further, we demonstrate\nthrough experiments, leveraging open-source LLMs for the implementation of\ndistributed MoA, that certain MoA configurations produce higher-quality\nresponses compared to others, as evaluated on AlpacaEval 2.0 benchmark. The\nimplementation is available at:\nhttps://github.com/purbeshmitra/distributed_moa."
                },
                "authors": [
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Priyanka Kaswan"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21199v1",
                "updated": "2024-12-30T18:58:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    58,
                    58,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:58:58Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    58,
                    58,
                    0,
                    365,
                    0
                ],
                "title": "HumanEval Pro and MBPP Pro: Evaluating Large Language Models on\n  Self-invoking Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumanEval Pro and MBPP Pro: Evaluating Large Language Models on\n  Self-invoking Code Generation"
                },
                "summary": "We introduce self-invoking code generation, a new task designed to evaluate\nthe progressive reasoning and problem-solving capabilities of LLMs. In this\ntask, models are presented with a base problem and a related, more complex\nproblem. They must solve the base problem and then utilize its solution to\naddress the more complex one. This work features three key contributions.\nFirst, we propose a general recipe for generating more challenging versions of\nexisting benchmarks, resulting in three new benchmarks: HumanEval Pro, MBPP\nPro, and BigCodeBench-Lite Pro, specifically designed to assess LLMs on\nself-invoking code generation. Second, from the analysis of experimental\nresults over twenty LLMs on our benchmarks, we have two important observations:\n(i) Most LLMs excel in traditional code generation benchmarks like HumanEval\nand MBPP, but their performance declines on self-invoking tasks. For example,\no1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro.\n(ii) On self-invoking code generation task, the instruction-tuned models\ndemonstrate only marginal improvements compared to the base models. Third, we\ndisclose the types of failure modes that exist in our evaluation results. All\nthese results underscore the need for further advancements in self-invoking\ncode generation tasks and provide a new direction for future research on\nenhancing LLMs' code reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce self-invoking code generation, a new task designed to evaluate\nthe progressive reasoning and problem-solving capabilities of LLMs. In this\ntask, models are presented with a base problem and a related, more complex\nproblem. They must solve the base problem and then utilize its solution to\naddress the more complex one. This work features three key contributions.\nFirst, we propose a general recipe for generating more challenging versions of\nexisting benchmarks, resulting in three new benchmarks: HumanEval Pro, MBPP\nPro, and BigCodeBench-Lite Pro, specifically designed to assess LLMs on\nself-invoking code generation. Second, from the analysis of experimental\nresults over twenty LLMs on our benchmarks, we have two important observations:\n(i) Most LLMs excel in traditional code generation benchmarks like HumanEval\nand MBPP, but their performance declines on self-invoking tasks. For example,\no1-mini achieves 96.2% pass@1 on HumanEval but only 76.2% on HumanEval Pro.\n(ii) On self-invoking code generation task, the instruction-tuned models\ndemonstrate only marginal improvements compared to the base models. Third, we\ndisclose the types of failure modes that exist in our evaluation results. All\nthese results underscore the need for further advancements in self-invoking\ncode generation tasks and provide a new direction for future research on\nenhancing LLMs' code reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Zhaojian Yu"
                    },
                    {
                        "name": "Yilun Zhao"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Xiao-Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Ping Zhang"
                },
                "author": "Xiao-Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21187v1",
                "updated": "2024-12-30T18:55:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    55,
                    12,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:55:12Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    55,
                    12,
                    0,
                    365,
                    0
                ],
                "title": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs"
                },
                "summary": "The remarkable performance of models like the OpenAI o1 can be attributed to\ntheir ability to emulate human-like long-time thinking during inference. These\nmodels employ extended chain-of-thought (CoT) processes, exploring multiple\nstrategies to enhance problem-solving capabilities. However, a critical\nquestion remains: How to intelligently and efficiently scale computational\nresources during testing. This paper presents the first comprehensive study on\nthe prevalent issue of overthinking in these models, where excessive\ncomputational resources are allocated for simple problems with minimal benefit.\nWe introduce novel efficiency metrics from both outcome and process\nperspectives to evaluate the rational use of computational resources by o1-like\nmodels. Using a self-training paradigm, we propose strategies to mitigate\noverthinking, streamlining reasoning processes without compromising accuracy.\nExperimental results show that our approach successfully reduces computational\noverhead while preserving model performance across a range of testsets with\nvarying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable performance of models like the OpenAI o1 can be attributed to\ntheir ability to emulate human-like long-time thinking during inference. These\nmodels employ extended chain-of-thought (CoT) processes, exploring multiple\nstrategies to enhance problem-solving capabilities. However, a critical\nquestion remains: How to intelligently and efficiently scale computational\nresources during testing. This paper presents the first comprehensive study on\nthe prevalent issue of overthinking in these models, where excessive\ncomputational resources are allocated for simple problems with minimal benefit.\nWe introduce novel efficiency metrics from both outcome and process\nperspectives to evaluate the rational use of computational resources by o1-like\nmodels. Using a self-training paradigm, we propose strategies to mitigate\noverthinking, streamlining reasoning processes without compromising accuracy.\nExperimental results show that our approach successfully reduces computational\noverhead while preserving model performance across a range of testsets with\nvarying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME."
                },
                "authors": [
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Zhiwei He"
                    },
                    {
                        "name": "Jianhui Pang"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Linfeng Song"
                    },
                    {
                        "name": "Qiuzhi Liu"
                    },
                    {
                        "name": "Mengfei Zhou"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21162v1",
                "updated": "2024-12-30T18:41:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    41,
                    43,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:41:43Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    41,
                    43,
                    0,
                    365,
                    0
                ],
                "title": "Open-Source 5G Core Platforms: A Low-Cost Solution and Performance\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Source 5G Core Platforms: A Low-Cost Solution and Performance\n  Evaluation"
                },
                "summary": "An essential component for the Fifth Generation of Mobile Networks\ndeployments is the 5G Core (5GC), which bridges the 5G Radio Access Network\n(RAN) to the rest of the Internet. Some open-source platforms for the 5GC have\nemerged and been deployed in Common Off-the-Shelf (COTS)-based setups. Despite\nthese open-source 5GC initiatives following the 3GPP specifications, they\ndiffer in implementing some features and their stages in the timeline of 3GPP\nreleases. Besides that, they may yield different performance to metrics related\nto the data and control planes. This article reviews the major open-source 5GC\nplatforms and evaluates their performance in a 5G Standalone (SA) COTS-based\ntestbed. The results indicate that Open5GS provides the best latencies for\ncontrol plane procedures, OpenAirInterface offers the highest data rates, and\nFree5GC has the lowest resource consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An essential component for the Fifth Generation of Mobile Networks\ndeployments is the 5G Core (5GC), which bridges the 5G Radio Access Network\n(RAN) to the rest of the Internet. Some open-source platforms for the 5GC have\nemerged and been deployed in Common Off-the-Shelf (COTS)-based setups. Despite\nthese open-source 5GC initiatives following the 3GPP specifications, they\ndiffer in implementing some features and their stages in the timeline of 3GPP\nreleases. Besides that, they may yield different performance to metrics related\nto the data and control planes. This article reviews the major open-source 5GC\nplatforms and evaluates their performance in a 5G Standalone (SA) COTS-based\ntestbed. The results indicate that Open5GS provides the best latencies for\ncontrol plane procedures, OpenAirInterface offers the highest data rates, and\nFree5GC has the lowest resource consumption."
                },
                "authors": [
                    {
                        "name": "Maria Barbosa"
                    },
                    {
                        "name": "Marcelo Silva"
                    },
                    {
                        "name": "Ednelson Cavalcanti"
                    },
                    {
                        "name": "Kelvin Dias"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Dias"
                },
                "author": "Kelvin Dias",
                "arxiv_comment": "Accepted for publication in ICOIN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21154v1",
                "updated": "2024-12-30T18:33:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    33,
                    28,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:33:28Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    33,
                    28,
                    0,
                    365,
                    0
                ],
                "title": "Aviary: training language agents on challenging scientific tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aviary: training language agents on challenging scientific tasks"
                },
                "summary": "Solving complex real-world tasks requires cycles of actions and observations.\nThis is particularly true in science, where tasks require many cycles of\nanalysis, tool use, and experimentation. Language agents are promising for\nautomating intellectual tasks in science because they can interact with tools\nvia natural language or code. Yet their flexibility creates conceptual and\npractical challenges for software implementations, since agents may comprise\nnon-standard components such as internal reasoning, planning, tool usage, as\nwell as the inherent stochasticity of temperature-sampled language models.\nHere, we introduce Aviary, an extensible gymnasium for language agents. We\nformalize agents as policies solving language-grounded partially observable\nMarkov decision processes, which we term language decision processes. We then\nimplement five environments, including three challenging scientific\nenvironments: (1) manipulating DNA constructs for molecular cloning, (2)\nanswering research questions by accessing scientific literature, and (3)\nengineering protein stability. These environments were selected for their focus\non multi-step reasoning and their relevance to contemporary biology research.\nFinally, with online training and scaling inference-time compute, we show that\nlanguage agents backed by open-source, non-frontier LLMs can match and exceed\nboth frontier LLM agents and human experts on multiple tasks at up to 100x\nlower inference cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving complex real-world tasks requires cycles of actions and observations.\nThis is particularly true in science, where tasks require many cycles of\nanalysis, tool use, and experimentation. Language agents are promising for\nautomating intellectual tasks in science because they can interact with tools\nvia natural language or code. Yet their flexibility creates conceptual and\npractical challenges for software implementations, since agents may comprise\nnon-standard components such as internal reasoning, planning, tool usage, as\nwell as the inherent stochasticity of temperature-sampled language models.\nHere, we introduce Aviary, an extensible gymnasium for language agents. We\nformalize agents as policies solving language-grounded partially observable\nMarkov decision processes, which we term language decision processes. We then\nimplement five environments, including three challenging scientific\nenvironments: (1) manipulating DNA constructs for molecular cloning, (2)\nanswering research questions by accessing scientific literature, and (3)\nengineering protein stability. These environments were selected for their focus\non multi-step reasoning and their relevance to contemporary biology research.\nFinally, with online training and scaling inference-time compute, we show that\nlanguage agents backed by open-source, non-frontier LLMs can match and exceed\nboth frontier LLM agents and human experts on multiple tasks at up to 100x\nlower inference cost."
                },
                "authors": [
                    {
                        "name": "Siddharth Narayanan"
                    },
                    {
                        "name": "James D. Braza"
                    },
                    {
                        "name": "Ryan-Rhys Griffiths"
                    },
                    {
                        "name": "Manu Ponnapati"
                    },
                    {
                        "name": "Albert Bou"
                    },
                    {
                        "name": "Jon Laurent"
                    },
                    {
                        "name": "Ori Kabeli"
                    },
                    {
                        "name": "Geemi Wellawatte"
                    },
                    {
                        "name": "Sam Cox"
                    },
                    {
                        "name": "Samuel G. Rodriques"
                    },
                    {
                        "name": "Andrew D. White"
                    }
                ],
                "author_detail": {
                    "name": "Andrew D. White"
                },
                "author": "Andrew D. White",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05093v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05093v3",
                "updated": "2024-12-30T18:21:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    21,
                    8,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-09T14:34:32Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    34,
                    32,
                    4,
                    222,
                    0
                ],
                "title": "Order Matters in Hallucination: Reasoning Order as Benchmark and\n  Reflexive Prompting for Large-Language-Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Order Matters in Hallucination: Reasoning Order as Benchmark and\n  Reflexive Prompting for Large-Language-Models"
                },
                "summary": "Large language models (LLMs) have generated significant attention since their\ninception, finding applications across various academic and industrial domains.\nHowever, these models often suffer from the \"hallucination problem\", where\noutputs, though grammatically and logically coherent, lack factual accuracy or\nare entirely fabricated. A particularly troubling issue discovered and widely\ndiscussed recently is the numerical comparison error where multiple LLMs\nincorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs\ngenerate answers and reasoning impacts their consistency. Specifically, results\nvary significantly when an LLM generates an answer first and then provides the\nreasoning versus generating the reasoning process first and then the\nconclusion. Inspired by this, we propose a new benchmark method for assessing\nLLM consistency: comparing responses generated through these two different\napproaches. This benchmark effectively identifies instances where LLMs\nfabricate answers and subsequently generate justifications. Furthermore, we\nintroduce a novel and straightforward prompt strategy designed to mitigate this\nissue. Experimental results demonstrate that this strategy improves performance\nacross various LLMs compared to direct questioning. This work not only sheds\nlight on a critical flaw in LLMs but also offers a practical solution to\nenhance their reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have generated significant attention since their\ninception, finding applications across various academic and industrial domains.\nHowever, these models often suffer from the \"hallucination problem\", where\noutputs, though grammatically and logically coherent, lack factual accuracy or\nare entirely fabricated. A particularly troubling issue discovered and widely\ndiscussed recently is the numerical comparison error where multiple LLMs\nincorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs\ngenerate answers and reasoning impacts their consistency. Specifically, results\nvary significantly when an LLM generates an answer first and then provides the\nreasoning versus generating the reasoning process first and then the\nconclusion. Inspired by this, we propose a new benchmark method for assessing\nLLM consistency: comparing responses generated through these two different\napproaches. This benchmark effectively identifies instances where LLMs\nfabricate answers and subsequently generate justifications. Furthermore, we\nintroduce a novel and straightforward prompt strategy designed to mitigate this\nissue. Experimental results demonstrate that this strategy improves performance\nacross various LLMs compared to direct questioning. This work not only sheds\nlight on a critical flaw in LLMs but also offers a practical solution to\nenhance their reliability."
                },
                "authors": [
                    {
                        "name": "Zikai Xie"
                    }
                ],
                "author_detail": {
                    "name": "Zikai Xie"
                },
                "author": "Zikai Xie",
                "arxiv_comment": "8 pages, submitted to ACL22025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05093v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05093v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21140v1",
                "updated": "2024-12-30T18:15:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    15,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T18:15:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    18,
                    15,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Facilitating large language model Russian adaptation with Learned\n  Embedding Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facilitating large language model Russian adaptation with Learned\n  Embedding Propagation"
                },
                "summary": "Rapid advancements of large language model (LLM) technologies led to the\nintroduction of powerful open-source instruction-tuned LLMs that have the same\ntext generation quality as the state-of-the-art counterparts such as GPT-4.\nWhile the emergence of such models accelerates the adoption of LLM technologies\nin sensitive-information environments the authors of such models don not\ndisclose the training data necessary for replication of the results thus making\nthe achievements model-exclusive. Since those open-source models are also\nmultilingual this in turn reduces the benefits of training a language specific\nLLMs as improved inference computation efficiency becomes the only guaranteed\nadvantage of such costly procedure. More cost-efficient options such as\nvocabulary extension and subsequent continued pre-training are also inhibited\nby the lack of access to high-quality instruction-tuning data since it is the\nmajor factor behind the resulting LLM task-solving capabilities. To address the\nlimitations and cut the costs of the language adaptation pipeline we propose\nLearned Embedding Propagation (LEP). Unlike existing approaches our method has\nlower training data size requirements due to minimal impact on existing LLM\nknowledge which we reinforce using novel ad-hoc embedding propagation procedure\nthat allows to skip the instruction-tuning step and instead implant the new\nlanguage knowledge directly into any existing instruct-tuned variant. We\nevaluated four Russian vocabulary adaptations for LLaMa-3-8B and Mistral-7B,\nshowing that LEP is competitive with traditional instruction-tuning methods,\nachieving performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct, with\nfurther improvements via self-calibration and continued tuning enhancing\ntask-solving capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements of large language model (LLM) technologies led to the\nintroduction of powerful open-source instruction-tuned LLMs that have the same\ntext generation quality as the state-of-the-art counterparts such as GPT-4.\nWhile the emergence of such models accelerates the adoption of LLM technologies\nin sensitive-information environments the authors of such models don not\ndisclose the training data necessary for replication of the results thus making\nthe achievements model-exclusive. Since those open-source models are also\nmultilingual this in turn reduces the benefits of training a language specific\nLLMs as improved inference computation efficiency becomes the only guaranteed\nadvantage of such costly procedure. More cost-efficient options such as\nvocabulary extension and subsequent continued pre-training are also inhibited\nby the lack of access to high-quality instruction-tuning data since it is the\nmajor factor behind the resulting LLM task-solving capabilities. To address the\nlimitations and cut the costs of the language adaptation pipeline we propose\nLearned Embedding Propagation (LEP). Unlike existing approaches our method has\nlower training data size requirements due to minimal impact on existing LLM\nknowledge which we reinforce using novel ad-hoc embedding propagation procedure\nthat allows to skip the instruction-tuning step and instead implant the new\nlanguage knowledge directly into any existing instruct-tuned variant. We\nevaluated four Russian vocabulary adaptations for LLaMa-3-8B and Mistral-7B,\nshowing that LEP is competitive with traditional instruction-tuning methods,\nachieving performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct, with\nfurther improvements via self-calibration and continued tuning enhancing\ntask-solving capabilities."
                },
                "authors": [
                    {
                        "name": "Mikhail Tikhomirov"
                    },
                    {
                        "name": "Daniil Chernyshev"
                    }
                ],
                "author_detail": {
                    "name": "Daniil Chernyshev"
                },
                "author": "Daniil Chernyshev",
                "arxiv_comment": "Preprint version of an article published in the Journal of Language\n  and Education. Copyright held by the owner/author(s). Publication rights\n  licensed to the Journal of Language and Education",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21123v1",
                "updated": "2024-12-30T17:52:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    52,
                    2,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T17:52:02Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    52,
                    2,
                    0,
                    365,
                    0
                ],
                "title": "ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language\n  Modeling Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language\n  Modeling Exploitation"
                },
                "summary": "As large language models (LLMs) increasingly depend on web-scraped datasets,\nconcerns over unauthorized use of copyrighted or personal content for training\nhave intensified. Despite regulations such as the General Data Protection\nRegulation (GDPR), data owners still have limited control over the use of their\ncontent in model training. To address this, we propose ExpShield, a proactive\nself-guard mechanism that empowers content owners to embed invisible\nperturbations into their text, limiting data misuse in LLMs training without\naffecting readability. This preemptive approach enables data owners to protect\nsensitive content directly, without relying on a third-party to perform\ndefense. Starting from the random perturbation, we demonstrate the rationale\nfor using perturbation to conceal protected content. We further enhance the\nefficiency by identifying memorization triggers and creating pitfalls to\ndiverge the model memorization in a more focused way. To validate our defense's\neffectiveness, we propose a novel metric of instance exploitation which\ncaptures the individual risk raised by model training. The experimental results\nvalidate the effectiveness of our approach as the MIA AUC decreases from 0.95\nto 0.55, and instance exploitation approaches zero. This suggests that the\nindividual risk does not increase after training, underscoring the significance\nof proactive defenses in protecting copyrighted data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly depend on web-scraped datasets,\nconcerns over unauthorized use of copyrighted or personal content for training\nhave intensified. Despite regulations such as the General Data Protection\nRegulation (GDPR), data owners still have limited control over the use of their\ncontent in model training. To address this, we propose ExpShield, a proactive\nself-guard mechanism that empowers content owners to embed invisible\nperturbations into their text, limiting data misuse in LLMs training without\naffecting readability. This preemptive approach enables data owners to protect\nsensitive content directly, without relying on a third-party to perform\ndefense. Starting from the random perturbation, we demonstrate the rationale\nfor using perturbation to conceal protected content. We further enhance the\nefficiency by identifying memorization triggers and creating pitfalls to\ndiverge the model memorization in a more focused way. To validate our defense's\neffectiveness, we propose a novel metric of instance exploitation which\ncaptures the individual risk raised by model training. The experimental results\nvalidate the effectiveness of our approach as the MIA AUC decreases from 0.95\nto 0.55, and instance exploitation approaches zero. This suggests that the\nindividual risk does not increase after training, underscoring the significance\nof proactive defenses in protecting copyrighted data."
                },
                "authors": [
                    {
                        "name": "Ruixuan Liu"
                    },
                    {
                        "name": "Toan Tran"
                    },
                    {
                        "name": "Tianhao Wang"
                    },
                    {
                        "name": "Hongsheng Hu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Li Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiong"
                },
                "author": "Li Xiong",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21102v1",
                "updated": "2024-12-30T17:25:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    25,
                    58,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T17:25:58Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    17,
                    25,
                    58,
                    0,
                    365,
                    0
                ],
                "title": "Exploring and Controlling Diversity in LLM-Agent Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring and Controlling Diversity in LLM-Agent Conversation"
                },
                "summary": "Diversity is a critical aspect of multi-agent communication. In this paper,\nwe focus on controlling and exploring diversity in the context of open-domain\nmulti-agent conversations, particularly for world simulation applications. We\npropose Adaptive Prompt Pruning (APP), a novel method that dynamically adjusts\nthe content of the utterance generation prompt to control diversity using a\nsingle parameter, lambda. Through extensive experiments, we show that APP\neffectively controls the output diversity across models and datasets, with\npruning more information leading to more diverse output. We comprehensively\nanalyze the relationship between prompt content and conversational diversity.\nOur findings reveal that information from all components of the prompt\ngenerally constrains the diversity of the output, with the Memory block\nexerting the most significant influence. APP is compatible with established\ntechniques like temperature sampling and top-p sampling, providing a versatile\ntool for diversity management. To address the trade-offs of increased\ndiversity, such as inconsistencies with omitted information, we incorporate a\npost-generation correction step, which effectively balances diversity\nenhancement with output consistency. Additionally, we examine how prompt\nstructure, including component order and length, impacts diversity. This study\naddresses key questions surrounding diversity in multi-agent world simulation,\noffering insights into its control, influencing factors, and associated\ntrade-offs. Our contributions lay the foundation for systematically engineering\ndiversity in LLM-based multi-agent collaborations, advancing their\neffectiveness in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity is a critical aspect of multi-agent communication. In this paper,\nwe focus on controlling and exploring diversity in the context of open-domain\nmulti-agent conversations, particularly for world simulation applications. We\npropose Adaptive Prompt Pruning (APP), a novel method that dynamically adjusts\nthe content of the utterance generation prompt to control diversity using a\nsingle parameter, lambda. Through extensive experiments, we show that APP\neffectively controls the output diversity across models and datasets, with\npruning more information leading to more diverse output. We comprehensively\nanalyze the relationship between prompt content and conversational diversity.\nOur findings reveal that information from all components of the prompt\ngenerally constrains the diversity of the output, with the Memory block\nexerting the most significant influence. APP is compatible with established\ntechniques like temperature sampling and top-p sampling, providing a versatile\ntool for diversity management. To address the trade-offs of increased\ndiversity, such as inconsistencies with omitted information, we incorporate a\npost-generation correction step, which effectively balances diversity\nenhancement with output consistency. Additionally, we examine how prompt\nstructure, including component order and length, impacts diversity. This study\naddresses key questions surrounding diversity in multi-agent world simulation,\noffering insights into its control, influencing factors, and associated\ntrade-offs. Our contributions lay the foundation for systematically engineering\ndiversity in LLM-based multi-agent collaborations, advancing their\neffectiveness in real-world applications."
                },
                "authors": [
                    {
                        "name": "KuanChao Chu"
                    },
                    {
                        "name": "Yi-Pei Chen"
                    },
                    {
                        "name": "Hideki Nakayama"
                    }
                ],
                "author_detail": {
                    "name": "Hideki Nakayama"
                },
                "author": "Hideki Nakayama",
                "arxiv_comment": "Accepted for the AAAI 2025 Workshop on Advancing LLM-Based\n  Multi-Agent Collaboration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21080v1",
                "updated": "2024-12-30T16:57:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    57,
                    5,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T16:57:05Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    57,
                    5,
                    0,
                    365,
                    0
                ],
                "title": "Vinci: A Real-time Embodied Smart Assistant based on Egocentric\n  Vision-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vinci: A Real-time Embodied Smart Assistant based on Egocentric\n  Vision-Language Model"
                },
                "summary": "We introduce Vinci, a real-time embodied smart assistant built upon an\negocentric vision-language model. Designed for deployment on portable devices\nsuch as smartphones and wearable cameras, Vinci operates in an \"always on\"\nmode, continuously observing the environment to deliver seamless interaction\nand assistance. Users can wake up the system and engage in natural\nconversations to ask questions or seek assistance, with responses delivered\nthrough audio for hands-free convenience. With its ability to process long\nvideo streams in real-time, Vinci can answer user queries about current\nobservations and historical context while also providing task planning based on\npast interactions. To further enhance usability, Vinci integrates a video\ngeneration module that creates step-by-step visual demonstrations for tasks\nthat require detailed guidance. We hope that Vinci can establish a robust\nframework for portable, real-time egocentric AI systems, empowering users with\ncontextual and actionable insights. We release the complete implementation for\nthe development of the device in conjunction with a demo web platform to test\nuploaded videos at https://github.com/OpenGVLab/vinci.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Vinci, a real-time embodied smart assistant built upon an\negocentric vision-language model. Designed for deployment on portable devices\nsuch as smartphones and wearable cameras, Vinci operates in an \"always on\"\nmode, continuously observing the environment to deliver seamless interaction\nand assistance. Users can wake up the system and engage in natural\nconversations to ask questions or seek assistance, with responses delivered\nthrough audio for hands-free convenience. With its ability to process long\nvideo streams in real-time, Vinci can answer user queries about current\nobservations and historical context while also providing task planning based on\npast interactions. To further enhance usability, Vinci integrates a video\ngeneration module that creates step-by-step visual demonstrations for tasks\nthat require detailed guidance. We hope that Vinci can establish a robust\nframework for portable, real-time egocentric AI systems, empowering users with\ncontextual and actionable insights. We release the complete implementation for\nthe development of the device in conjunction with a demo web platform to test\nuploaded videos at https://github.com/OpenGVLab/vinci."
                },
                "authors": [
                    {
                        "name": "Yifei Huang"
                    },
                    {
                        "name": "Jilan Xu"
                    },
                    {
                        "name": "Baoqi Pei"
                    },
                    {
                        "name": "Yuping He"
                    },
                    {
                        "name": "Guo Chen"
                    },
                    {
                        "name": "Lijin Yang"
                    },
                    {
                        "name": "Xinyuan Chen"
                    },
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zheng Nie"
                    },
                    {
                        "name": "Jinyao Liu"
                    },
                    {
                        "name": "Guoshun Fan"
                    },
                    {
                        "name": "Dechen Lin"
                    },
                    {
                        "name": "Fang Fang"
                    },
                    {
                        "name": "Kunpeng Li"
                    },
                    {
                        "name": "Chang Yuan"
                    },
                    {
                        "name": "Yali Wang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09807v2",
                "updated": "2024-12-30T16:45:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    45,
                    50,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-13T02:48:36Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    48,
                    36,
                    4,
                    348,
                    0
                ],
                "title": "LLM Distillation for Efficient Few-Shot Multiple Choice Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Distillation for Efficient Few-Shot Multiple Choice Question\n  Answering"
                },
                "summary": "Multiple Choice Question Answering (MCQA) is an important problem with\nnumerous real-world applications, such as medicine, law, and education. The\nhigh cost of building MCQA datasets makes few-shot learning pivotal in this\ndomain. While Large Language Models (LLMs) can enable few-shot learning, their\ndirect application in real-world scenarios is often hindered by their high\ncomputational cost. To address this challenge, we propose a simple yet\neffective approach that uses LLMs for data generation and scoring. Our approach\nutilizes LLMs to create MCQA data which contains questions and choices, and to\nassign probability scores to the generated choices. We then use the generated\ndata and LLM-assigned scores to finetune a smaller and more efficient\nencoder-only model, DeBERTa-v3-base by leveraging distillation loss. Extensive\nexperiments on the Massive Multitask Language Understanding (MMLU) benchmark\ndemonstrate that our method improves accuracy from 28.9% to 39.3%, representing\na gain of over 10% compared to a baseline finetuned directly on 5-shot\nexamples. This shows the effectiveness of LLM-driven data generation and\nknowledge distillation for few-shot MCQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple Choice Question Answering (MCQA) is an important problem with\nnumerous real-world applications, such as medicine, law, and education. The\nhigh cost of building MCQA datasets makes few-shot learning pivotal in this\ndomain. While Large Language Models (LLMs) can enable few-shot learning, their\ndirect application in real-world scenarios is often hindered by their high\ncomputational cost. To address this challenge, we propose a simple yet\neffective approach that uses LLMs for data generation and scoring. Our approach\nutilizes LLMs to create MCQA data which contains questions and choices, and to\nassign probability scores to the generated choices. We then use the generated\ndata and LLM-assigned scores to finetune a smaller and more efficient\nencoder-only model, DeBERTa-v3-base by leveraging distillation loss. Extensive\nexperiments on the Massive Multitask Language Understanding (MMLU) benchmark\ndemonstrate that our method improves accuracy from 28.9% to 39.3%, representing\na gain of over 10% compared to a baseline finetuned directly on 5-shot\nexamples. This shows the effectiveness of LLM-driven data generation and\nknowledge distillation for few-shot MCQA."
                },
                "authors": [
                    {
                        "name": "Patrick Sutanto"
                    },
                    {
                        "name": "Joan Santoso"
                    },
                    {
                        "name": "Esther Irawati Setiawan"
                    },
                    {
                        "name": "Aji Prasetya Wibawa"
                    }
                ],
                "author_detail": {
                    "name": "Aji Prasetya Wibawa"
                },
                "author": "Aji Prasetya Wibawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21065v1",
                "updated": "2024-12-30T16:34:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    34,
                    11,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T16:34:11Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    34,
                    11,
                    0,
                    365,
                    0
                ],
                "title": "Efficient Multi-Task Inferencing with a Shared Backbone and Lightweight\n  Task-Specific Adapters for Automatic Scoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Multi-Task Inferencing with a Shared Backbone and Lightweight\n  Task-Specific Adapters for Automatic Scoring"
                },
                "summary": "The integration of Artificial Intelligence (AI) in education requires\nscalable and efficient frameworks that balance performance, adaptability, and\ncost. This paper addresses these needs by proposing a shared backbone model\narchitecture enhanced with lightweight LoRA adapters for task-specific\nfine-tuning, targeting the automated scoring of student responses across 27\nmutually exclusive tasks. By achieving competitive performance (average QWK of\n0.848 compared to 0.888 for fully fine-tuned models) while reducing GPU memory\nconsumption by 60% and inference latency by 40%, the framework demonstrates\nsignificant efficiency gains. This approach aligns with the workshops' focus on\nimproving language models for educational tasks, creating responsible\ninnovations for cost-sensitive deployment, and supporting educators by\nstreamlining assessment workflows. The findings underscore the potential of\nscalable AI to enhance learning outcomes while maintaining fairness and\ntransparency in automated scoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Artificial Intelligence (AI) in education requires\nscalable and efficient frameworks that balance performance, adaptability, and\ncost. This paper addresses these needs by proposing a shared backbone model\narchitecture enhanced with lightweight LoRA adapters for task-specific\nfine-tuning, targeting the automated scoring of student responses across 27\nmutually exclusive tasks. By achieving competitive performance (average QWK of\n0.848 compared to 0.888 for fully fine-tuned models) while reducing GPU memory\nconsumption by 60% and inference latency by 40%, the framework demonstrates\nsignificant efficiency gains. This approach aligns with the workshops' focus on\nimproving language models for educational tasks, creating responsible\ninnovations for cost-sensitive deployment, and supporting educators by\nstreamlining assessment workflows. The findings underscore the potential of\nscalable AI to enhance learning outcomes while maintaining fairness and\ntransparency in automated scoring systems."
                },
                "authors": [
                    {
                        "name": "Ehsan Latif"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Zhai"
                },
                "author": "Xiaoming Zhai",
                "arxiv_comment": "Accepted by AAAI-iRAISE Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17498v2",
                "updated": "2024-12-30T16:29:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    29,
                    36,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-23T11:55:33Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    11,
                    55,
                    33,
                    0,
                    358,
                    0
                ],
                "title": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought"
                },
                "summary": "Recently, O1-like models have emerged as representative examples,\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\ntasks such as math and coding tasks. In this paper, we introduce DRT-o1, an\nattempt to bring the success of long CoT to neural machine translation (MT).\nSpecifically, in view of the literature books that might involve similes and\nmetaphors, translating these texts to a target language is very difficult in\npractice due to cultural differences. In such cases, literal translation often\nfails to convey the intended meaning effectively. Even for professional human\ntranslators, considerable thought must be given to preserving semantics\nthroughout the translation process. To simulate LLMs' long thought ability in\nMT, we first mine sentences containing similes or metaphors from existing\nliterature books, and then develop a multi-agent framework to translate these\nsentences via long thought. In the multi-agent framework, a translator is used\nto iteratively translate the source sentence under the suggestions provided by\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\nalso employed to quantify the translation in each round. In this way, we\ncollect tens of thousands of long-thought MT data, which is used to train our\nDRT-o1. Using Qwen2.5 and LLama-3.1 as the backbones, DRT-o1 models can learn\nthe thought process during machine translation, and outperform vanilla LLMs as\nwell as existing O1-like LLMs, showing their effectiveness The project is\navailable at https://github.com/krystalan/DRT-o1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, O1-like models have emerged as representative examples,\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\ntasks such as math and coding tasks. In this paper, we introduce DRT-o1, an\nattempt to bring the success of long CoT to neural machine translation (MT).\nSpecifically, in view of the literature books that might involve similes and\nmetaphors, translating these texts to a target language is very difficult in\npractice due to cultural differences. In such cases, literal translation often\nfails to convey the intended meaning effectively. Even for professional human\ntranslators, considerable thought must be given to preserving semantics\nthroughout the translation process. To simulate LLMs' long thought ability in\nMT, we first mine sentences containing similes or metaphors from existing\nliterature books, and then develop a multi-agent framework to translate these\nsentences via long thought. In the multi-agent framework, a translator is used\nto iteratively translate the source sentence under the suggestions provided by\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\nalso employed to quantify the translation in each round. In this way, we\ncollect tens of thousands of long-thought MT data, which is used to train our\nDRT-o1. Using Qwen2.5 and LLama-3.1 as the backbones, DRT-o1 models can learn\nthe thought process during machine translation, and outperform vanilla LLMs as\nwell as existing O1-like LLMs, showing their effectiveness The project is\navailable at https://github.com/krystalan/DRT-o1"
                },
                "authors": [
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yunlong Liang"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21052v1",
                "updated": "2024-12-30T16:09:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    9,
                    33,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T16:09:33Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    9,
                    33,
                    0,
                    365,
                    0
                ],
                "title": "Towards Effective Discrimination Testing for Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Effective Discrimination Testing for Generative AI"
                },
                "summary": "Generative AI (GenAI) models present new challenges in regulating against\ndiscriminatory behavior. In this paper, we argue that GenAI fairness research\nstill has not met these challenges; instead, a significant gap remains between\nexisting bias assessment methods and regulatory goals. This leads to\nineffective regulation that can allow deployment of reportedly fair, yet\nactually discriminatory, GenAI systems. Towards remedying this problem, we\nconnect the legal and technical literature around GenAI bias evaluation and\nidentify areas of misalignment. Through four case studies, we demonstrate how\nthis misalignment between fairness testing techniques and regulatory goals can\nresult in discriminatory outcomes in real-world deployments, especially in\nadaptive or complex environments. We offer practical recommendations for\nimproving discrimination testing to better align with regulatory goals and\nenhance the reliability of fairness assessments in future deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) models present new challenges in regulating against\ndiscriminatory behavior. In this paper, we argue that GenAI fairness research\nstill has not met these challenges; instead, a significant gap remains between\nexisting bias assessment methods and regulatory goals. This leads to\nineffective regulation that can allow deployment of reportedly fair, yet\nactually discriminatory, GenAI systems. Towards remedying this problem, we\nconnect the legal and technical literature around GenAI bias evaluation and\nidentify areas of misalignment. Through four case studies, we demonstrate how\nthis misalignment between fairness testing techniques and regulatory goals can\nresult in discriminatory outcomes in real-world deployments, especially in\nadaptive or complex environments. We offer practical recommendations for\nimproving discrimination testing to better align with regulatory goals and\nenhance the reliability of fairness assessments in future deployments."
                },
                "authors": [
                    {
                        "name": "Thomas P. Zollo"
                    },
                    {
                        "name": "Nikita Rajaneesh"
                    },
                    {
                        "name": "Richard Zemel"
                    },
                    {
                        "name": "Talia B. Gillis"
                    },
                    {
                        "name": "Emily Black"
                    }
                ],
                "author_detail": {
                    "name": "Emily Black"
                },
                "author": "Emily Black",
                "arxiv_comment": "38 pages, 9 tables, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21051v1",
                "updated": "2024-12-30T16:09:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    9,
                    28,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T16:09:28Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    9,
                    28,
                    0,
                    365,
                    0
                ],
                "title": "Toward Intelligent and Secure Cloud: Large Language Model Empowered\n  Proactive Defense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Intelligent and Secure Cloud: Large Language Model Empowered\n  Proactive Defense"
                },
                "summary": "The rapid evolution of cloud computing technologies and the increasing number\nof cloud applications have provided a large number of benefits in daily lives.\nHowever, the diversity and complexity of different components pose a\nsignificant challenge to cloud security, especially when dealing with\nsophisticated and advanced cyberattacks. Recent advancements in generative\nfoundation models (GFMs), particularly in the large language models (LLMs),\noffer promising solutions for security intelligence. By exploiting the powerful\nabilities in language understanding, data analysis, task inference, action\nplanning, and code generation, we present LLM-PD, a novel proactive defense\narchitecture that defeats various threats in a proactive manner. LLM-PD can\nefficiently make a decision through comprehensive data analysis and sequential\nreasoning, as well as dynamically creating and deploying actionable defense\nmechanisms on the target cloud. Furthermore, it can flexibly self-evolve based\non experience learned from previous interactions and adapt to new attack\nscenarios without additional training. The experimental results demonstrate its\nremarkable ability in terms of defense effectiveness and efficiency,\nparticularly highlighting an outstanding success rate when compared with other\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of cloud computing technologies and the increasing number\nof cloud applications have provided a large number of benefits in daily lives.\nHowever, the diversity and complexity of different components pose a\nsignificant challenge to cloud security, especially when dealing with\nsophisticated and advanced cyberattacks. Recent advancements in generative\nfoundation models (GFMs), particularly in the large language models (LLMs),\noffer promising solutions for security intelligence. By exploiting the powerful\nabilities in language understanding, data analysis, task inference, action\nplanning, and code generation, we present LLM-PD, a novel proactive defense\narchitecture that defeats various threats in a proactive manner. LLM-PD can\nefficiently make a decision through comprehensive data analysis and sequential\nreasoning, as well as dynamically creating and deploying actionable defense\nmechanisms on the target cloud. Furthermore, it can flexibly self-evolve based\non experience learned from previous interactions and adapt to new attack\nscenarios without additional training. The experimental results demonstrate its\nremarkable ability in terms of defense effectiveness and efficiency,\nparticularly highlighting an outstanding success rate when compared with other\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Yuyang Zhou"
                    },
                    {
                        "name": "Guang Cheng"
                    },
                    {
                        "name": "Kang Du"
                    },
                    {
                        "name": "Zihan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Chen"
                },
                "author": "Zihan Chen",
                "arxiv_comment": "7 pages; In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; I.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21037v1",
                "updated": "2024-12-30T16:02:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    2,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T16:02:44Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    16,
                    2,
                    44,
                    0,
                    365,
                    0
                ],
                "title": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow\n  Matching and Clap-Ranked Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow\n  Matching and Clap-Ranked Preference Optimization"
                },
                "summary": "We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model\nwith 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio\nin just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models\nlies in the difficulty of creating preference pairs, as TTA lacks structured\nmechanisms like verifiable rewards or gold-standard answers available for Large\nLanguage Models (LLMs). To address this, we propose CLAP-Ranked Preference\nOptimization (CRPO), a novel framework that iteratively generates and optimizes\npreference data to enhance TTA alignment. We demonstrate that the audio\npreference dataset generated using CRPO outperforms existing alternatives. With\nthis framework, TangoFlux achieves state-of-the-art performance across both\nobjective and subjective benchmarks. We open source all code and models to\nsupport further research in TTA generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce TangoFlux, an efficient Text-to-Audio (TTA) generative model\nwith 515M parameters, capable of generating up to 30 seconds of 44.1kHz audio\nin just 3.7 seconds on a single A40 GPU. A key challenge in aligning TTA models\nlies in the difficulty of creating preference pairs, as TTA lacks structured\nmechanisms like verifiable rewards or gold-standard answers available for Large\nLanguage Models (LLMs). To address this, we propose CLAP-Ranked Preference\nOptimization (CRPO), a novel framework that iteratively generates and optimizes\npreference data to enhance TTA alignment. We demonstrate that the audio\npreference dataset generated using CRPO outperforms existing alternatives. With\nthis framework, TangoFlux achieves state-of-the-art performance across both\nobjective and subjective benchmarks. We open source all code and models to\nsupport further research in TTA generation."
                },
                "authors": [
                    {
                        "name": "Chia-Yu Hung"
                    },
                    {
                        "name": "Navonil Majumder"
                    },
                    {
                        "name": "Zhifeng Kong"
                    },
                    {
                        "name": "Ambuj Mehrish"
                    },
                    {
                        "name": "Rafael Valle"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Soujanya Poria"
                    }
                ],
                "author_detail": {
                    "name": "Soujanya Poria"
                },
                "author": "Soujanya Poria",
                "arxiv_comment": "https://tangoflux.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15639v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15639v3",
                "updated": "2024-12-30T15:59:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    59,
                    18,
                    0,
                    365,
                    0
                ],
                "published": "2024-04-24T04:25:04Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    4,
                    25,
                    4,
                    2,
                    115,
                    0
                ],
                "title": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models\n  of Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models\n  of Code"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable progress in code\ngeneration. It now becomes crucial to identify whether the code is AI-generated\nand to determine the specific model used, particularly for purposes such as\nprotecting Intellectual Property (IP) in industry and preventing cheating in\nprogramming exercises. To this end, several attempts have been made to insert\nwatermarks into machine-generated code. However, existing approaches are\nlimited to inserting only a single bit of information. In this paper, we\nintroduce CodeIP, a novel multi-bit watermarking technique that inserts\nadditional information to preserve crucial provenance details, such as the\nvendor ID of an LLM, thereby safeguarding the IPs of LLMs in code generation.\nFurthermore, to ensure the syntactical correctness of the generated code, we\npropose constraining the sampling process for predicting the next token by\ntraining a type predictor. Experiments conducted on a real-world dataset across\nfive programming languages demonstrate the effectiveness of CodeIP in\nwatermarking LLMs for code generation while maintaining the syntactical\ncorrectness of code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable progress in code\ngeneration. It now becomes crucial to identify whether the code is AI-generated\nand to determine the specific model used, particularly for purposes such as\nprotecting Intellectual Property (IP) in industry and preventing cheating in\nprogramming exercises. To this end, several attempts have been made to insert\nwatermarks into machine-generated code. However, existing approaches are\nlimited to inserting only a single bit of information. In this paper, we\nintroduce CodeIP, a novel multi-bit watermarking technique that inserts\nadditional information to preserve crucial provenance details, such as the\nvendor ID of an LLM, thereby safeguarding the IPs of LLMs in code generation.\nFurthermore, to ensure the syntactical correctness of the generated code, we\npropose constraining the sampling process for predicting the next token by\ntraining a type predictor. Experiments conducted on a real-world dataset across\nfive programming languages demonstrate the effectiveness of CodeIP in\nwatermarking LLMs for code generation while maintaining the syntactical\ncorrectness of code."
                },
                "authors": [
                    {
                        "name": "Batu Guan"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Zhangqian Bi"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_comment": "16 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15639v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15639v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21033v1",
                "updated": "2024-12-30T15:58:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    58,
                    41,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:58:41Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    58,
                    41,
                    0,
                    365,
                    0
                ],
                "title": "Plancraft: an evaluation dataset for planning with LLM agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plancraft: an evaluation dataset for planning with LLM agents"
                },
                "summary": "We present Plancraft, a multi-modal evaluation dataset for LLM agents.\nPlancraft has both a text-only and multi-modal interface, based on the\nMinecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and\nRetrieval Augmented Generation (RAG), as well as an oracle planner and oracle\nRAG information extractor, to ablate the different components of a modern agent\narchitecture. To evaluate decision-making, Plancraft also includes a subset of\nexamples that are intentionally unsolvable, providing a realistic challenge\nthat requires the agent not only to complete tasks but also to decide whether\nthey are solvable at all. We benchmark both open-source and closed-source LLMs\nand strategies on our task and compare their performance to a handcrafted\nplanner. We find that LLMs and VLMs struggle with the planning problems that\nPlancraft introduces, and we offer suggestions on how to improve their\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Plancraft, a multi-modal evaluation dataset for LLM agents.\nPlancraft has both a text-only and multi-modal interface, based on the\nMinecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and\nRetrieval Augmented Generation (RAG), as well as an oracle planner and oracle\nRAG information extractor, to ablate the different components of a modern agent\narchitecture. To evaluate decision-making, Plancraft also includes a subset of\nexamples that are intentionally unsolvable, providing a realistic challenge\nthat requires the agent not only to complete tasks but also to decide whether\nthey are solvable at all. We benchmark both open-source and closed-source LLMs\nand strategies on our task and compare their performance to a handcrafted\nplanner. We find that LLMs and VLMs struggle with the planning problems that\nPlancraft introduces, and we offer suggestions on how to improve their\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Gautier Dagan"
                    },
                    {
                        "name": "Frank Keller"
                    },
                    {
                        "name": "Alex Lascarides"
                    }
                ],
                "author_detail": {
                    "name": "Alex Lascarides"
                },
                "author": "Alex Lascarides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21016v1",
                "updated": "2024-12-30T15:33:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    34,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:34Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    34,
                    0,
                    365,
                    0
                ],
                "title": "Automated Robustness Testing for LLM-based NLP Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Robustness Testing for LLM-based NLP Software"
                },
                "summary": "Benefiting from the advancements in LLMs, NLP software has undergone rapid\ndevelopment. Such software is widely employed in various safety-critical tasks,\nsuch as financial sentiment analysis, toxic content moderation, and log\ngeneration. To our knowledge, there are no known automated robustness testing\nmethods specifically designed for LLM-based NLP software. Given the complexity\nof LLMs and the unpredictability of real-world inputs (including prompts and\nexamples), it is essential to examine the robustness of overall inputs to\nensure the safety of such software.\n  To this end, this paper introduces the first AutOmated Robustness Testing\nfrAmework, AORTA, which reconceptualizes the testing process into a\ncombinatorial optimization problem. Existing testing methods designed for\nDNN-based software can be applied to LLM-based software by AORTA, but their\neffectiveness is limited. To address this, we propose a novel testing method\nfor LLM-based software within AORTA called Adaptive Beam Search. ABS is\ntailored for the expansive feature space of LLMs and improves testing\neffectiveness through an adaptive beam width and the capability for\nbacktracking.\n  We successfully embed 18 test methods in the designed framework AORTA and\ncompared the test validity of ABS with three datasets and five threat models.\nABS facilitates a more comprehensive and accurate robustness assessment before\nsoftware deployment, with an average test success rate of 86.138%. Compared to\nthe currently best-performing baseline PWWS, ABS significantly reduces the\ncomputational overhead by up to 3441.895 seconds per successful test case and\ndecreases the number of queries by 218.762 times on average. Furthermore, test\ncases generated by ABS exhibit greater naturalness and transferability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the advancements in LLMs, NLP software has undergone rapid\ndevelopment. Such software is widely employed in various safety-critical tasks,\nsuch as financial sentiment analysis, toxic content moderation, and log\ngeneration. To our knowledge, there are no known automated robustness testing\nmethods specifically designed for LLM-based NLP software. Given the complexity\nof LLMs and the unpredictability of real-world inputs (including prompts and\nexamples), it is essential to examine the robustness of overall inputs to\nensure the safety of such software.\n  To this end, this paper introduces the first AutOmated Robustness Testing\nfrAmework, AORTA, which reconceptualizes the testing process into a\ncombinatorial optimization problem. Existing testing methods designed for\nDNN-based software can be applied to LLM-based software by AORTA, but their\neffectiveness is limited. To address this, we propose a novel testing method\nfor LLM-based software within AORTA called Adaptive Beam Search. ABS is\ntailored for the expansive feature space of LLMs and improves testing\neffectiveness through an adaptive beam width and the capability for\nbacktracking.\n  We successfully embed 18 test methods in the designed framework AORTA and\ncompared the test validity of ABS with three datasets and five threat models.\nABS facilitates a more comprehensive and accurate robustness assessment before\nsoftware deployment, with an average test success rate of 86.138%. Compared to\nthe currently best-performing baseline PWWS, ABS significantly reduces the\ncomputational overhead by up to 3441.895 seconds per successful test case and\ndecreases the number of queries by 218.762 times on average. Furthermore, test\ncases generated by ABS exhibit greater naturalness and transferability."
                },
                "authors": [
                    {
                        "name": "Mingxuan Xiao"
                    },
                    {
                        "name": "Yan Xiao"
                    },
                    {
                        "name": "Shunhui Ji"
                    },
                    {
                        "name": "Hanbo Cai"
                    },
                    {
                        "name": "Lei Xue"
                    },
                    {
                        "name": "Pengcheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Pengcheng Zhang"
                },
                "author": "Pengcheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19312v2",
                "updated": "2024-12-30T15:30:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    30,
                    23,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-26T18:19:53Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    19,
                    53,
                    3,
                    361,
                    0
                ],
                "title": "From Interests to Insights: An LLM Approach to Course Recommendations\n  Using Natural Language Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Interests to Insights: An LLM Approach to Course Recommendations\n  Using Natural Language Queries"
                },
                "summary": "Most universities in the United States encourage their students to explore\nacademic areas before declaring a major and to acquire academic breadth by\nsatisfying a variety of requirements. Each term, students must choose among\nmany thousands of offerings, spanning dozens of subject areas, a handful of\ncourses to take. The curricular environment is also dynamic, and poor\ncommunication and search functions on campus can limit a student's ability to\ndiscover new courses of interest. To support both students and their advisers\nin such a setting, we explore a novel Large Language Model (LLM) course\nrecommendation system that applies a Retrieval Augmented Generation (RAG)\nmethod to the corpus of course descriptions. The system first generates an\n'ideal' course description based on the user's query. This description is\nconverted into a search vector using embeddings, which is then used to find\nactual courses with similar content by comparing embedding similarities. We\ndescribe the method and assess the quality and fairness of some example\nprompts. Steps to deploy a pilot system on campus are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most universities in the United States encourage their students to explore\nacademic areas before declaring a major and to acquire academic breadth by\nsatisfying a variety of requirements. Each term, students must choose among\nmany thousands of offerings, spanning dozens of subject areas, a handful of\ncourses to take. The curricular environment is also dynamic, and poor\ncommunication and search functions on campus can limit a student's ability to\ndiscover new courses of interest. To support both students and their advisers\nin such a setting, we explore a novel Large Language Model (LLM) course\nrecommendation system that applies a Retrieval Augmented Generation (RAG)\nmethod to the corpus of course descriptions. The system first generates an\n'ideal' course description based on the user's query. This description is\nconverted into a search vector using embeddings, which is then used to find\nactual courses with similar content by comparing embedding similarities. We\ndescribe the method and assess the quality and fairness of some example\nprompts. Steps to deploy a pilot system on campus are discussed."
                },
                "authors": [
                    {
                        "name": "Hugh Van Deventer"
                    },
                    {
                        "name": "Mark Mills"
                    },
                    {
                        "name": "August Evrard"
                    }
                ],
                "author_detail": {
                    "name": "August Evrard"
                },
                "author": "August Evrard",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21006v1",
                "updated": "2024-12-30T15:15:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    15,
                    8,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:15:08Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    15,
                    8,
                    0,
                    365,
                    0
                ],
                "title": "Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant\n  Rationale via Principled Criteria",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant\n  Rationale via Principled Criteria"
                },
                "summary": "Large Language Models (LLMs) rely on generating extensive intermediate\nreasoning units (e.g., tokens, sentences) to enhance final answer quality\nacross a wide range of complex tasks. While generating multiple reasoning paths\nor iteratively refining rationales proves effective for improving performance,\nthese approaches inevitably result in significantly higher inference costs. In\nthis work, we propose a novel sentence-level rationale reduction training\nframework that leverages likelihood-based criteria, verbosity, to identify and\nremove redundant reasoning sentences. Unlike previous approaches that utilize\ntoken-level reduction, our sentence-level reduction framework maintains model\nperformance while reducing generation length. This preserves the original\nreasoning abilities of LLMs and achieves an average 17.15% reduction in\ngeneration costs across various models and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) rely on generating extensive intermediate\nreasoning units (e.g., tokens, sentences) to enhance final answer quality\nacross a wide range of complex tasks. While generating multiple reasoning paths\nor iteratively refining rationales proves effective for improving performance,\nthese approaches inevitably result in significantly higher inference costs. In\nthis work, we propose a novel sentence-level rationale reduction training\nframework that leverages likelihood-based criteria, verbosity, to identify and\nremove redundant reasoning sentences. Unlike previous approaches that utilize\ntoken-level reduction, our sentence-level reduction framework maintains model\nperformance while reducing generation length. This preserves the original\nreasoning abilities of LLMs and achieves an average 17.15% reduction in\ngeneration costs across various models and tasks."
                },
                "authors": [
                    {
                        "name": "Joonwon Jang"
                    },
                    {
                        "name": "Jaehee Kim"
                    },
                    {
                        "name": "Wonbin Kweon"
                    },
                    {
                        "name": "Hwanjo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjo Yu"
                },
                "author": "Hwanjo Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08603v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08603v3",
                "updated": "2024-12-30T15:08:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    8,
                    23,
                    0,
                    365,
                    0
                ],
                "published": "2024-05-14T13:42:05Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    13,
                    42,
                    5,
                    1,
                    135,
                    0
                ],
                "title": "A Comprehensive Survey of Large Language Models and Multimodal Large\n  Language Models in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Large Language Models and Multimodal Large\n  Language Models in Medicine"
                },
                "summary": "Since the release of ChatGPT and GPT-4, large language models (LLMs) and\nmultimodal large language models (MLLMs) have attracted widespread attention\nfor their exceptional capabilities in understanding, reasoning, and generation,\nintroducing transformative paradigms for integrating artificial intelligence\ninto medicine. This survey provides a comprehensive overview of the\ndevelopment, principles, application scenarios, challenges, and future\ndirections of LLMs and MLLMs in medicine. Specifically, it begins by examining\nthe paradigm shift, tracing the transition from traditional models to LLMs and\nMLLMs, and highlighting the unique advantages of these LLMs and MLLMs in\nmedical applications. Next, the survey reviews existing medical LLMs and MLLMs,\nproviding detailed guidance on their construction and evaluation in a clear and\nsystematic manner. Subsequently, to underscore the substantial value of LLMs\nand MLLMs in healthcare, the survey explores five promising applications in the\nfield. Finally, the survey addresses the challenges confronting medical LLMs\nand MLLMs and proposes practical strategies and future directions for their\nintegration into medicine. In summary, this survey offers a comprehensive\nanalysis of the technical methodologies and practical clinical applications of\nmedical LLMs and MLLMs, with the goal of bridging the gap between these\nadvanced technologies and clinical practice, thereby fostering the evolution of\nthe next generation of intelligent healthcare systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the release of ChatGPT and GPT-4, large language models (LLMs) and\nmultimodal large language models (MLLMs) have attracted widespread attention\nfor their exceptional capabilities in understanding, reasoning, and generation,\nintroducing transformative paradigms for integrating artificial intelligence\ninto medicine. This survey provides a comprehensive overview of the\ndevelopment, principles, application scenarios, challenges, and future\ndirections of LLMs and MLLMs in medicine. Specifically, it begins by examining\nthe paradigm shift, tracing the transition from traditional models to LLMs and\nMLLMs, and highlighting the unique advantages of these LLMs and MLLMs in\nmedical applications. Next, the survey reviews existing medical LLMs and MLLMs,\nproviding detailed guidance on their construction and evaluation in a clear and\nsystematic manner. Subsequently, to underscore the substantial value of LLMs\nand MLLMs in healthcare, the survey explores five promising applications in the\nfield. Finally, the survey addresses the challenges confronting medical LLMs\nand MLLMs and proposes practical strategies and future directions for their\nintegration into medicine. In summary, this survey offers a comprehensive\nanalysis of the technical methodologies and practical clinical applications of\nmedical LLMs and MLLMs, with the goal of bridging the gap between these\nadvanced technologies and clinical practice, thereby fostering the evolution of\nthe next generation of intelligent healthcare systems."
                },
                "authors": [
                    {
                        "name": "Hanguang Xiao"
                    },
                    {
                        "name": "Feizhong Zhou"
                    },
                    {
                        "name": "Xingyue Liu"
                    },
                    {
                        "name": "Tianqi Liu"
                    },
                    {
                        "name": "Zhipeng Li"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xiaoxuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxuan Huang"
                },
                "author": "Xiaoxuan Huang",
                "arxiv_doi": "10.1016/j.inffus.2024.102888",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.inffus.2024.102888",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.08603v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08603v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Information Fusion, 117 (2025) 102888",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20996v1",
                "updated": "2024-12-30T15:01:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    1,
                    48,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:01:48Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    1,
                    48,
                    0,
                    365,
                    0
                ],
                "title": "Plug-and-Play Training Framework for Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plug-and-Play Training Framework for Preference Optimization"
                },
                "summary": "Recently, preference optimization methods such as DPO have significantly\nenhanced large language models (LLMs) in wide tasks including dialogue and\nquestion-answering. However, current methods fail to account for the varying\ndifficulty levels of training samples during preference optimization, leading\nto mediocre performance in tasks with high accuracy requirements, particularly\nin mathematical reasoning. To address this limitation, we propose a novel\ntraining framework, which employs multiple sampling to analyze output\ndistributions, assign different weights to samples, and incorporate these\nweights into the preference optimization process. This plug-and-play approach\nenables LLMs to prioritize challenging examples during training, improving\nlearning efficiency. Experimental results demonstrate that our framework\nintegrates seamlessly with various preference optimization methods and achieves\nconsistent improvements in mathematical reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, preference optimization methods such as DPO have significantly\nenhanced large language models (LLMs) in wide tasks including dialogue and\nquestion-answering. However, current methods fail to account for the varying\ndifficulty levels of training samples during preference optimization, leading\nto mediocre performance in tasks with high accuracy requirements, particularly\nin mathematical reasoning. To address this limitation, we propose a novel\ntraining framework, which employs multiple sampling to analyze output\ndistributions, assign different weights to samples, and incorporate these\nweights into the preference optimization process. This plug-and-play approach\nenables LLMs to prioritize challenging examples during training, improving\nlearning efficiency. Experimental results demonstrate that our framework\nintegrates seamlessly with various preference optimization methods and achieves\nconsistent improvements in mathematical reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Jingyuan Ma"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "arxiv_comment": "12 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20995v1",
                "updated": "2024-12-30T14:58:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    58,
                    46,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T14:58:46Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    58,
                    46,
                    0,
                    365,
                    0
                ],
                "title": "KARPA: A Training-free Method of Adapting Knowledge Graph as References\n  for Large Language Model's Reasoning Path Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KARPA: A Training-free Method of Adapting Knowledge Graph as References\n  for Large Language Model's Reasoning Path Aggregation"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance across a\nvariety of tasks, yet they are often affected by hallucinations and the\ntimeliness of knowledge. Leveraging knowledge graphs (KGs) as external\nknowledge sources has emerged as a viable solution, but existing methods for\nLLM-based knowledge graph question answering (KGQA) are often limited by\nstep-by-step decision-making on KGs, restricting the global planning and\nreasoning capabilities of LLMs, or they require fine-tuning or pre-training on\nspecific KGs. To address these challenges, we propose Knowledge graph Assisted\nReasoning Path Aggregation (KARPA), a novel framework that harnesses the global\nplanning abilities of LLMs for efficient and accurate KG reasoning. KARPA\noperates in three steps: pre-planning relation paths using the LLM's global\nplanning capabilities, matching semantically relevant paths via an embedding\nmodel, and reasoning over these paths to generate answers. Unlike existing KGQA\nmethods, KARPA avoids stepwise traversal, requires no additional training, and\nis adaptable to various LLM architectures. Extensive experimental results show\nthat KARPA achieves state-of-the-art performance in KGQA tasks, delivering both\nhigh efficiency and accuracy. Our code will be available on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance across a\nvariety of tasks, yet they are often affected by hallucinations and the\ntimeliness of knowledge. Leveraging knowledge graphs (KGs) as external\nknowledge sources has emerged as a viable solution, but existing methods for\nLLM-based knowledge graph question answering (KGQA) are often limited by\nstep-by-step decision-making on KGs, restricting the global planning and\nreasoning capabilities of LLMs, or they require fine-tuning or pre-training on\nspecific KGs. To address these challenges, we propose Knowledge graph Assisted\nReasoning Path Aggregation (KARPA), a novel framework that harnesses the global\nplanning abilities of LLMs for efficient and accurate KG reasoning. KARPA\noperates in three steps: pre-planning relation paths using the LLM's global\nplanning capabilities, matching semantically relevant paths via an embedding\nmodel, and reasoning over these paths to generate answers. Unlike existing KGQA\nmethods, KARPA avoids stepwise traversal, requires no additional training, and\nis adaptable to various LLM architectures. Extensive experimental results show\nthat KARPA achieves state-of-the-art performance in KGQA tasks, delivering both\nhigh efficiency and accuracy. Our code will be available on Github."
                },
                "authors": [
                    {
                        "name": "Siyuan Fang"
                    },
                    {
                        "name": "Kaijing Ma"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Ningxuan Lu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Qingkun Tang"
                    }
                ],
                "author_detail": {
                    "name": "Qingkun Tang"
                },
                "author": "Qingkun Tang",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20993v1",
                "updated": "2024-12-30T14:57:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    57,
                    53,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T14:57:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    57,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "Efficiently Serving LLM Reasoning Programs with Certaindex",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving LLM Reasoning Programs with Certaindex"
                },
                "summary": "The rapid evolution of large language models (LLMs) has unlocked their\ncapabilities in advanced reasoning tasks like mathematical problem-solving,\ncode generation, and legal analysis. Central to this progress are\ninference-time reasoning algorithms, which refine outputs by exploring multiple\nsolution paths, at the cost of increasing compute demands and response\nlatencies. Existing serving systems fail to adapt to the scaling behaviors of\nthese algorithms or the varying difficulty of queries, leading to inefficient\nresource use and unmet latency targets.\n  We present Dynasor, a system that optimizes inference-time compute for LLM\nreasoning queries. Unlike traditional engines, Dynasor tracks and schedules\nrequests within reasoning queries and uses Certaindex, a proxy that measures\nstatistical reasoning progress based on model certainty, to guide compute\nallocation dynamically. Dynasor co-adapts scheduling with reasoning progress:\nit allocates more compute to hard queries, reduces compute for simpler ones,\nand terminates unpromising queries early, balancing accuracy, latency, and\ncost. On diverse datasets and algorithms, Dynasor reduces compute by up to 50%\nin batch processing and sustaining 3.3x higher query rates or 4.7x tighter\nlatency SLOs in online serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) has unlocked their\ncapabilities in advanced reasoning tasks like mathematical problem-solving,\ncode generation, and legal analysis. Central to this progress are\ninference-time reasoning algorithms, which refine outputs by exploring multiple\nsolution paths, at the cost of increasing compute demands and response\nlatencies. Existing serving systems fail to adapt to the scaling behaviors of\nthese algorithms or the varying difficulty of queries, leading to inefficient\nresource use and unmet latency targets.\n  We present Dynasor, a system that optimizes inference-time compute for LLM\nreasoning queries. Unlike traditional engines, Dynasor tracks and schedules\nrequests within reasoning queries and uses Certaindex, a proxy that measures\nstatistical reasoning progress based on model certainty, to guide compute\nallocation dynamically. Dynasor co-adapts scheduling with reasoning progress:\nit allocates more compute to hard queries, reduces compute for simpler ones,\nand terminates unpromising queries early, balancing accuracy, latency, and\ncost. On diverse datasets and algorithms, Dynasor reduces compute by up to 50%\nin batch processing and sustaining 3.3x higher query rates or 4.7x tighter\nlatency SLOs in online serving."
                },
                "authors": [
                    {
                        "name": "Yichao Fu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Siqi Zhu"
                    },
                    {
                        "name": "Zheyu Fu"
                    },
                    {
                        "name": "Zhongdongming Dai"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20960v1",
                "updated": "2024-12-30T13:55:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    55,
                    28,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T13:55:28Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    55,
                    28,
                    0,
                    365,
                    0
                ],
                "title": "Rise of Generative Artificial Intelligence in Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rise of Generative Artificial Intelligence in Science"
                },
                "summary": "Generative Artificial Intelligence (GenAI, generative AI) has rapidly become\navailable as a tool in scientific research. To explore the use of generative AI\nin science, we conduct an empirical analysis using OpenAlex. Analyzing GenAI\npublications and other AI publications from 2017 to 2023, we profile growth\npatterns, the diffusion of GenAI publications across fields of study, and the\ngeographical spread of scientific research on generative AI. We also\ninvestigate team size and international collaborations to explore whether\nGenAI, as an emerging scientific research area, shows different collaboration\npatterns compared to other AI technologies. The results indicate that\ngenerative AI has experienced rapid growth and increasing presence in\nscientific publications. The use of GenAI now extends beyond computer science\nto other scientific research domains. Over the study period, U.S. researchers\ncontributed nearly two-fifths of global GenAI publications. The U.S. is\nfollowed by China, with several small and medium-sized advanced economies\ndemonstrating relatively high levels of GenAI deployment in their research\npublications. Although scientific research overall is becoming increasingly\nspecialized and collaborative, our results suggest that GenAI research groups\ntend to have slightly smaller team sizes than found in other AI fields.\nFurthermore, notwithstanding recent geopolitical tensions, GenAI research\ncontinues to exhibit levels of international collaboration comparable to other\nAI technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI, generative AI) has rapidly become\navailable as a tool in scientific research. To explore the use of generative AI\nin science, we conduct an empirical analysis using OpenAlex. Analyzing GenAI\npublications and other AI publications from 2017 to 2023, we profile growth\npatterns, the diffusion of GenAI publications across fields of study, and the\ngeographical spread of scientific research on generative AI. We also\ninvestigate team size and international collaborations to explore whether\nGenAI, as an emerging scientific research area, shows different collaboration\npatterns compared to other AI technologies. The results indicate that\ngenerative AI has experienced rapid growth and increasing presence in\nscientific publications. The use of GenAI now extends beyond computer science\nto other scientific research domains. Over the study period, U.S. researchers\ncontributed nearly two-fifths of global GenAI publications. The U.S. is\nfollowed by China, with several small and medium-sized advanced economies\ndemonstrating relatively high levels of GenAI deployment in their research\npublications. Although scientific research overall is becoming increasingly\nspecialized and collaborative, our results suggest that GenAI research groups\ntend to have slightly smaller team sizes than found in other AI fields.\nFurthermore, notwithstanding recent geopolitical tensions, GenAI research\ncontinues to exhibit levels of international collaboration comparable to other\nAI technologies."
                },
                "authors": [
                    {
                        "name": "Liangping Ding"
                    },
                    {
                        "name": "Cornelia Lawson"
                    },
                    {
                        "name": "Philip Shapira"
                    }
                ],
                "author_detail": {
                    "name": "Philip Shapira"
                },
                "author": "Philip Shapira",
                "arxiv_comment": "26 pages, 4 tables, 1 figures, 1 appendix figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; K.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20956v1",
                "updated": "2024-12-30T13:53:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    53,
                    51,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T13:53:51Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    53,
                    51,
                    0,
                    365,
                    0
                ],
                "title": "QuantumLLMInstruct: A 500k LLM Instruction-Tuning Dataset with\n  Problem-Solution Pairs for Quantum Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantumLLMInstruct: A 500k LLM Instruction-Tuning Dataset with\n  Problem-Solution Pairs for Quantum Computing"
                },
                "summary": "We present QuantumLLMInstruct (QLMMI), an innovative dataset featuring over\n500,000 meticulously curated instruction-following problem-solution pairs\ndesigned specifically for quantum computing - the largest and most\ncomprehensive dataset of its kind. Originating from over 90 primary seed\ndomains and encompassing hundreds of subdomains autonomously generated by LLMs,\nQLMMI marks a transformative step in the diversity and richness of quantum\ncomputing datasets.\n  Designed for instruction fine-tuning, QLMMI seeks to significantly improve\nLLM performance in addressing complex quantum computing challenges across a\nwide range of quantum physics topics. While Large Language Models (LLMs) have\npropelled advancements in computational science with datasets like Omni-MATH\nand OpenMathInstruct, these primarily target Olympiad-level mathematics,\nleaving quantum computing largely unexplored.\n  The creation of QLMMI follows a rigorous four-stage methodology. Initially,\nfoundational problems are developed using predefined templates, focusing on\ncritical areas such as synthetic Hamiltonians, QASM code generation,\nJordan-Wigner transformations, and Trotter-Suzuki quantum circuit\ndecompositions. Next, detailed and domain-specific solutions are crafted to\nensure accuracy and relevance. In the third stage, the dataset is enriched\nthrough advanced reasoning techniques, including Chain-of-Thought (CoT) and\nTask-Oriented Reasoning and Action (ToRA), which enhance problem-solution\ndiversity while adhering to strict mathematical standards. Lastly, a zero-shot\nJudge LLM performs self-assessments to validate the dataset's quality and\nreliability, minimizing human oversight requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present QuantumLLMInstruct (QLMMI), an innovative dataset featuring over\n500,000 meticulously curated instruction-following problem-solution pairs\ndesigned specifically for quantum computing - the largest and most\ncomprehensive dataset of its kind. Originating from over 90 primary seed\ndomains and encompassing hundreds of subdomains autonomously generated by LLMs,\nQLMMI marks a transformative step in the diversity and richness of quantum\ncomputing datasets.\n  Designed for instruction fine-tuning, QLMMI seeks to significantly improve\nLLM performance in addressing complex quantum computing challenges across a\nwide range of quantum physics topics. While Large Language Models (LLMs) have\npropelled advancements in computational science with datasets like Omni-MATH\nand OpenMathInstruct, these primarily target Olympiad-level mathematics,\nleaving quantum computing largely unexplored.\n  The creation of QLMMI follows a rigorous four-stage methodology. Initially,\nfoundational problems are developed using predefined templates, focusing on\ncritical areas such as synthetic Hamiltonians, QASM code generation,\nJordan-Wigner transformations, and Trotter-Suzuki quantum circuit\ndecompositions. Next, detailed and domain-specific solutions are crafted to\nensure accuracy and relevance. In the third stage, the dataset is enriched\nthrough advanced reasoning techniques, including Chain-of-Thought (CoT) and\nTask-Oriented Reasoning and Action (ToRA), which enhance problem-solution\ndiversity while adhering to strict mathematical standards. Lastly, a zero-shot\nJudge LLM performs self-assessments to validate the dataset's quality and\nreliability, minimizing human oversight requirements."
                },
                "authors": [
                    {
                        "name": "Shlomo Kashani"
                    }
                ],
                "author_detail": {
                    "name": "Shlomo Kashani"
                },
                "author": "Shlomo Kashani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20954v1",
                "updated": "2024-12-30T13:50:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    50,
                    20,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T13:50:20Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    50,
                    20,
                    0,
                    365,
                    0
                ],
                "title": "AGON: Automated Design Framework for Customizing Processors from ISA\n  Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGON: Automated Design Framework for Customizing Processors from ISA\n  Documents"
                },
                "summary": "Customized processors are attractive solutions for vast domain-specific\napplications due to their high energy efficiency. However, designing a\nprocessor in traditional flows is time-consuming and expensive. To address\nthis, researchers have explored methods including the use of agile development\ntools like Chisel or SpinalHDL, high-level synthesis (HLS) from programming\nlanguages like C or SystemC, and more recently, leveraging large language\nmodels (LLMs) to generate hardware description language (HDL) code from natural\nlanguage descriptions. However, each method has limitations in terms of\nexpressiveness, correctness, and performance, leading to a persistent\ncontradiction between the level of automation and the effectiveness of the\ndesign. Overall, how to automatically design highly efficient and practical\nprocessors with minimal human effort remains a challenge.\n  In this paper, we propose AGON, a novel framework designed to leverage LLMs\nfor the efficient design of out-of-order (OoO) customized processors with\nminimal human effort. Central to AGON is the nano-operator function (nOP\nfunction) based Intermediate Representation (IR), which bridges high-level\ndescriptions and hardware implementations while decoupling functionality from\nperformance optimization, thereby providing an automatic design framework that\nis expressive and efficient, has correctness guarantees, and enables PPA\n(Power, Performance, and Area) optimization.\n  Experimental results show that superior to previous LLM-assisted automatic\ndesign flows, AGON facilitates designing a series of customized OoO processors\nthat achieve on average 2.35 $\\times$ speedup compared with BOOM, a\ngeneral-purpose CPU designed by experts, with minimal design effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customized processors are attractive solutions for vast domain-specific\napplications due to their high energy efficiency. However, designing a\nprocessor in traditional flows is time-consuming and expensive. To address\nthis, researchers have explored methods including the use of agile development\ntools like Chisel or SpinalHDL, high-level synthesis (HLS) from programming\nlanguages like C or SystemC, and more recently, leveraging large language\nmodels (LLMs) to generate hardware description language (HDL) code from natural\nlanguage descriptions. However, each method has limitations in terms of\nexpressiveness, correctness, and performance, leading to a persistent\ncontradiction between the level of automation and the effectiveness of the\ndesign. Overall, how to automatically design highly efficient and practical\nprocessors with minimal human effort remains a challenge.\n  In this paper, we propose AGON, a novel framework designed to leverage LLMs\nfor the efficient design of out-of-order (OoO) customized processors with\nminimal human effort. Central to AGON is the nano-operator function (nOP\nfunction) based Intermediate Representation (IR), which bridges high-level\ndescriptions and hardware implementations while decoupling functionality from\nperformance optimization, thereby providing an automatic design framework that\nis expressive and efficient, has correctness guarantees, and enables PPA\n(Power, Performance, and Area) optimization.\n  Experimental results show that superior to previous LLM-assisted automatic\ndesign flows, AGON facilitates designing a series of customized OoO processors\nthat achieve on average 2.35 $\\times$ speedup compared with BOOM, a\ngeneral-purpose CPU designed by experts, with minimal design effort."
                },
                "authors": [
                    {
                        "name": "Chongxiao Li"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Pengwei Jin"
                    },
                    {
                        "name": "Tianyun Ma"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shuyao Cheng"
                    },
                    {
                        "name": "Yifan Hao"
                    },
                    {
                        "name": "Yongwei Zhao"
                    },
                    {
                        "name": "Guanglin Xu"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Xiaqing Li"
                    },
                    {
                        "name": "Yuanbo Wen"
                    },
                    {
                        "name": "Yanjun Wu"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07099v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07099v3",
                "updated": "2024-12-30T13:43:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    43,
                    46,
                    0,
                    365,
                    0
                ],
                "published": "2024-06-18T07:46:13Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    7,
                    46,
                    13,
                    1,
                    170,
                    0
                ],
                "title": "Nash CoT: Multi-Path Inference with Preference Equilibrium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nash CoT: Multi-Path Inference with Preference Equilibrium"
                },
                "summary": "Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na game system on each path that balances the generation from role-specific\nLLMs' and the general LLMs' generation, thereby ensuring both effective role\nadoption and diversity in LLM generation further maintaining the performance of\nmulti-path inference while reducing the requirement of the number of inference\npaths. We evaluate Nash CoT across various inference tasks, including Arabic\nReasoning, Commonsense Question Answering, and Symbolic Inference, achieving\nresults that are comparable to or better than those of multi-path CoT with the\nequal number of inference paths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na game system on each path that balances the generation from role-specific\nLLMs' and the general LLMs' generation, thereby ensuring both effective role\nadoption and diversity in LLM generation further maintaining the performance of\nmulti-path inference while reducing the requirement of the number of inference\npaths. We evaluate Nash CoT across various inference tasks, including Arabic\nReasoning, Commonsense Question Answering, and Symbolic Inference, achieving\nresults that are comparable to or better than those of multi-path CoT with the\nequal number of inference paths."
                },
                "authors": [
                    {
                        "name": "Ziqi Zhang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Xiong Xiao"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "arxiv_journal_ref": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07099v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07099v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20943v1",
                "updated": "2024-12-30T13:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    36,
                    36,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T13:36:36Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    36,
                    36,
                    0,
                    365,
                    0
                ],
                "title": "Cluster-Based Time-Variant Channel Characterization and Modeling for\n  5G-Railways",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cluster-Based Time-Variant Channel Characterization and Modeling for\n  5G-Railways"
                },
                "summary": "With the development of high-speed railways, 5G for Railways (5G-R) is\ngradually replacing Global System for the Mobile Communications for Railway\n(GSM-R) worldwide to meet increasing demands. The large bandwidth, array\nantennas, and non-stationarity caused by high mobility has made 5G-R channel\ncharacterization more complex. Therefore, it is essential to develop an\naccurate channel model for 5G-R. However, researches on channel\ncharacterization and time-variant models specific to 5G-R frequency bands and\nscenarios is scarce. There are virtually no cluster-based time-variant channel\nmodels that capture statistical properties of 5G-R channel. In this paper, we\npropose a cluster-based time-variant channel model for 5G-R within an enhanced\n3GPP framework, which incorporates time evolution features. Extensive channel\nmeasurements are conducted on 5G-R private network test line in China. We then\nextract and analyze typical channel fading characteristics and multipath\ncluster characteristics. Furthermore, birth-death process of the clusters is\nmodeled by using a four-state Markov chain. Finally, a generalized clustered\ndelay line (CDL) model is established in accordance with 3GPP standard and\nvalidated by comparing the results of measurements and simulations. This work\nenhances the understanding of 5G-R channels and presents a flexible\ncluster-based time-variant channel model. The results can be used in the\ndesign, deployment, and optimization of 5G-R networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of high-speed railways, 5G for Railways (5G-R) is\ngradually replacing Global System for the Mobile Communications for Railway\n(GSM-R) worldwide to meet increasing demands. The large bandwidth, array\nantennas, and non-stationarity caused by high mobility has made 5G-R channel\ncharacterization more complex. Therefore, it is essential to develop an\naccurate channel model for 5G-R. However, researches on channel\ncharacterization and time-variant models specific to 5G-R frequency bands and\nscenarios is scarce. There are virtually no cluster-based time-variant channel\nmodels that capture statistical properties of 5G-R channel. In this paper, we\npropose a cluster-based time-variant channel model for 5G-R within an enhanced\n3GPP framework, which incorporates time evolution features. Extensive channel\nmeasurements are conducted on 5G-R private network test line in China. We then\nextract and analyze typical channel fading characteristics and multipath\ncluster characteristics. Furthermore, birth-death process of the clusters is\nmodeled by using a four-state Markov chain. Finally, a generalized clustered\ndelay line (CDL) model is established in accordance with 3GPP standard and\nvalidated by comparing the results of measurements and simulations. This work\nenhances the understanding of 5G-R channels and presents a flexible\ncluster-based time-variant channel model. The results can be used in the\ndesign, deployment, and optimization of 5G-R networks."
                },
                "authors": [
                    {
                        "name": "Xuejian Zhang"
                    },
                    {
                        "name": "Ruisi He"
                    },
                    {
                        "name": "Bo Ai"
                    },
                    {
                        "name": "Mi Yang"
                    },
                    {
                        "name": "Jianwen Ding"
                    },
                    {
                        "name": "Shuaiqi Gao"
                    },
                    {
                        "name": "Ziyi Qi"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Zhangdui Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Zhangdui Zhong"
                },
                "author": "Zhangdui Zhong",
                "arxiv_comment": "13 pages, 13 figures, submitted to IEEE Transactions on Wireless\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20942v1",
                "updated": "2024-12-30T13:36:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    36,
                    5,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T13:36:05Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    36,
                    5,
                    0,
                    365,
                    0
                ],
                "title": "Ontology-grounded Automatic Knowledge Graph Construction by LLM under\n  Wikidata schema",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology-grounded Automatic Knowledge Graph Construction by LLM under\n  Wikidata schema"
                },
                "summary": "We propose an ontology-grounded approach to Knowledge Graph (KG) construction\nusing Large Language Models (LLMs) on a knowledge base. An ontology is authored\nby generating Competency Questions (CQ) on knowledge base to discover knowledge\nscope, extracting relations from CQs, and attempt to replace equivalent\nrelations by their counterpart in Wikidata. To ensure consistency and\ninterpretability in the resulting KG, we ground generation of KG with the\nauthored ontology based on extracted relations. Evaluation on benchmark\ndatasets demonstrates competitive performance in knowledge graph construction\ntask. Our work presents a promising direction for scalable KG construction\npipeline with minimal human intervention, that yields high quality and\nhuman-interpretable KGs, which are interoperable with Wikidata semantics for\npotential knowledge base expansion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an ontology-grounded approach to Knowledge Graph (KG) construction\nusing Large Language Models (LLMs) on a knowledge base. An ontology is authored\nby generating Competency Questions (CQ) on knowledge base to discover knowledge\nscope, extracting relations from CQs, and attempt to replace equivalent\nrelations by their counterpart in Wikidata. To ensure consistency and\ninterpretability in the resulting KG, we ground generation of KG with the\nauthored ontology based on extracted relations. Evaluation on benchmark\ndatasets demonstrates competitive performance in knowledge graph construction\ntask. Our work presents a promising direction for scalable KG construction\npipeline with minimal human intervention, that yields high quality and\nhuman-interpretable KGs, which are interoperable with Wikidata semantics for\npotential knowledge base expansion."
                },
                "authors": [
                    {
                        "name": "Xiaohan Feng"
                    },
                    {
                        "name": "Xixin Wu"
                    },
                    {
                        "name": "Helen Meng"
                    }
                ],
                "author_detail": {
                    "name": "Helen Meng"
                },
                "author": "Helen Meng",
                "arxiv_comment": "Presented at HI-AI@KDD, Human-Interpretable AI Workshop at the KDD\n  2024, 26th of August 2024, Barcelona, Spain",
                "arxiv_journal_ref": "CEUR Workshop Proceedings 3841 (2024) 117-135",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12075v2",
                "updated": "2024-12-30T13:34:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    34,
                    23,
                    0,
                    365,
                    0
                ],
                "published": "2024-10-15T21:29:26Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    21,
                    29,
                    26,
                    1,
                    289,
                    0
                ],
                "title": "WeatherDG: LLM-assisted Diffusion Model for Procedural Weather\n  Generation in Domain-Generalized Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeatherDG: LLM-assisted Diffusion Model for Procedural Weather\n  Generation in Domain-Generalized Semantic Segmentation"
                },
                "summary": "In this work, we propose a novel approach, namely WeatherDG, that can\ngenerate realistic, weather-diverse, and driving-screen images based on the\ncooperation of two foundation models, i.e, Stable Diffusion (SD) and Large\nLanguage Model (LLM). Specifically, we first fine-tune the SD with source data,\naligning the content and layout of generated samples with real-world driving\nscenarios. Then, we propose a procedural prompt generation method based on LLM,\nwhich can enrich scenario descriptions and help SD automatically generate more\ndiverse, detailed images. In addition, we introduce a balanced generation\nstrategy, which encourages the SD to generate high-quality objects of tailed\nclasses under various weather conditions, such as riders and motorcycles. This\nsegmentation-model-agnostic method can improve the generalization ability of\nexisting models by additionally adapting them with the generated synthetic\ndata. Experiments on three challenging datasets show that our method can\nsignificantly improve the segmentation performance of different\nstate-of-the-art models on target domains. Notably, in the setting of\n''Cityscapes to ACDC'', our method improves the baseline HRDA by 13.9% in mIoU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a novel approach, namely WeatherDG, that can\ngenerate realistic, weather-diverse, and driving-screen images based on the\ncooperation of two foundation models, i.e, Stable Diffusion (SD) and Large\nLanguage Model (LLM). Specifically, we first fine-tune the SD with source data,\naligning the content and layout of generated samples with real-world driving\nscenarios. Then, we propose a procedural prompt generation method based on LLM,\nwhich can enrich scenario descriptions and help SD automatically generate more\ndiverse, detailed images. In addition, we introduce a balanced generation\nstrategy, which encourages the SD to generate high-quality objects of tailed\nclasses under various weather conditions, such as riders and motorcycles. This\nsegmentation-model-agnostic method can improve the generalization ability of\nexisting models by additionally adapting them with the generated synthetic\ndata. Experiments on three challenging datasets show that our method can\nsignificantly improve the segmentation performance of different\nstate-of-the-art models on target domains. Notably, in the setting of\n''Cityscapes to ACDC'', our method improves the baseline HRDA by 13.9% in mIoU."
                },
                "authors": [
                    {
                        "name": "Chenghao Qian"
                    },
                    {
                        "name": "Yuhu Guo"
                    },
                    {
                        "name": "Yuhong Mo"
                    },
                    {
                        "name": "Wenjing Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjing Li"
                },
                "author": "Wenjing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20927v1",
                "updated": "2024-12-30T13:16:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    16,
                    8,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T13:16:08Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    13,
                    16,
                    8,
                    0,
                    365,
                    0
                ],
                "title": "Enhanced Multimodal RAG-LLM for Accurate Visual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Multimodal RAG-LLM for Accurate Visual Question Answering"
                },
                "summary": "Multimodal large language models (MLLMs), such as GPT-4o, Gemini, LLaVA, and\nFlamingo, have made significant progress in integrating visual and textual\nmodalities, excelling in tasks like visual question answering (VQA), image\ncaptioning, and content retrieval. They can generate coherent and contextually\nrelevant descriptions of images. However, they still face challenges in\naccurately identifying and counting objects and determining their spatial\nlocations, particularly in complex scenes with overlapping or small objects. To\naddress these limitations, we propose a novel framework based on multimodal\nretrieval-augmented generation (RAG), which introduces structured scene graphs\nto enhance object recognition, relationship identification, and spatial\nunderstanding within images. Our framework improves the MLLM's capacity to\nhandle tasks requiring precise visual descriptions, especially in scenarios\nwith challenging perspectives, such as aerial views or scenes with dense object\narrangements. Finally, we conduct extensive experiments on the VG-150 dataset\nthat focuses on first-person visual understanding and the AUG dataset that\ninvolves aerial imagery. The results show that our approach consistently\noutperforms existing MLLMs in VQA tasks, which stands out in recognizing,\nlocalizing, and quantifying objects in different spatial contexts and provides\nmore accurate visual descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs), such as GPT-4o, Gemini, LLaVA, and\nFlamingo, have made significant progress in integrating visual and textual\nmodalities, excelling in tasks like visual question answering (VQA), image\ncaptioning, and content retrieval. They can generate coherent and contextually\nrelevant descriptions of images. However, they still face challenges in\naccurately identifying and counting objects and determining their spatial\nlocations, particularly in complex scenes with overlapping or small objects. To\naddress these limitations, we propose a novel framework based on multimodal\nretrieval-augmented generation (RAG), which introduces structured scene graphs\nto enhance object recognition, relationship identification, and spatial\nunderstanding within images. Our framework improves the MLLM's capacity to\nhandle tasks requiring precise visual descriptions, especially in scenarios\nwith challenging perspectives, such as aerial views or scenes with dense object\narrangements. Finally, we conduct extensive experiments on the VG-150 dataset\nthat focuses on first-person visual understanding and the AUG dataset that\ninvolves aerial imagery. The results show that our approach consistently\noutperforms existing MLLMs in VQA tasks, which stands out in recognizing,\nlocalizing, and quantifying objects in different spatial contexts and provides\nmore accurate visual descriptions."
                },
                "authors": [
                    {
                        "name": "Junxiao Xue"
                    },
                    {
                        "name": "Quan Deng"
                    },
                    {
                        "name": "Fei Yu"
                    },
                    {
                        "name": "Yanhao Wang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yuehua Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuehua Li"
                },
                "author": "Yuehua Li",
                "arxiv_comment": "6 pages, 3 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.08747v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.08747v4",
                "updated": "2024-12-30T12:32:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    12,
                    32,
                    49,
                    0,
                    365,
                    0
                ],
                "published": "2023-08-17T02:53:23Z",
                "published_parsed": [
                    2023,
                    8,
                    17,
                    2,
                    53,
                    23,
                    3,
                    229,
                    0
                ],
                "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models\n  During Continual Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Catastrophic Forgetting in Large Language Models\n  During Continual Fine-tuning"
                },
                "summary": "Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge for achieving a satisfactory performance in downstream tasks. As\nlarge language models (LLMs) have demonstrated remarkable performance, it is\nintriguing to investigate whether CF exists during the continual instruction\ntuning of LLMs. This study empirically evaluates the forgetting phenomenon in\nLLMs' knowledge during continual instruction tuning from the perspectives of\ndomain knowledge, reasoning, and reading comprehension. The experiments reveal\nthat catastrophic forgetting is generally observed in LLMs ranging from 1b to\n7b parameters. Surprisingly, as the model scale increases, the severity of\nforgetting intensifies in such a model sale range which may result from the\nmuch significant initial performance in the larger LLM. Comparing the\ndecoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits\nless forgetting and retains more knowledge. Interestingly, we also observe that\nLLMs can mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that general instruction tuning\ncan help alleviate the forgetting phenomenon in LLMs during subsequent\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning\nwhen a model forgets previously learned information while acquiring new\nknowledge for achieving a satisfactory performance in downstream tasks. As\nlarge language models (LLMs) have demonstrated remarkable performance, it is\nintriguing to investigate whether CF exists during the continual instruction\ntuning of LLMs. This study empirically evaluates the forgetting phenomenon in\nLLMs' knowledge during continual instruction tuning from the perspectives of\ndomain knowledge, reasoning, and reading comprehension. The experiments reveal\nthat catastrophic forgetting is generally observed in LLMs ranging from 1b to\n7b parameters. Surprisingly, as the model scale increases, the severity of\nforgetting intensifies in such a model sale range which may result from the\nmuch significant initial performance in the larger LLM. Comparing the\ndecoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits\nless forgetting and retains more knowledge. Interestingly, we also observe that\nLLMs can mitigate language biases, such as gender bias, during continual\nfine-tuning. Furthermore, our findings indicate that general instruction tuning\ncan help alleviate the forgetting phenomenon in LLMs during subsequent\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.08747v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.08747v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20891v1",
                "updated": "2024-12-30T12:00:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    12,
                    0,
                    47,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T12:00:47Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    12,
                    0,
                    47,
                    0,
                    365,
                    0
                ],
                "title": "DoTA: Weight-Decomposed Tensor Adaptation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DoTA: Weight-Decomposed Tensor Adaptation for Large Language Models"
                },
                "summary": "Low-rank adaptation (LoRA) reduces the computational and memory demands of\nfine-tuning large language models (LLMs) by approximating updates with low-rank\nmatrices. However, low-rank approximation in two-dimensional space fails to\ncapture high-dimensional structures within the target matrix. Recently, tensor\ndecomposition methods have been explored for fine-tuning LLMs, leveraging their\nability to extract structured information. Yet, these approaches primarily rely\non random initialization, and the impact of initialization on tensor adaptation\nremains underexplored. In this paper, we reveal that random initialization\nsignificantly diverges from the validation loss achieved by full fine-tuning.\nTo address this, we propose Weight-Decomposed Tensor Adaptation (DoTA), which\nleverages the Matrix Product Operator (MPO) decomposition of pre-trained\nweights for effective initialization in fine-tuning LLMs. Additionally, we\nintroduce QDoTA, a quantized version of DoTA designed for 4-bit quantization.\nExperiments on commonsense and arithmetic reasoning tasks show that DoTA\noutperforms random initialization methods with fewer parameters. QDoTA further\nreduces memory consumption and achieves comparable performance to DoTA on\ncommonsense reasoning tasks. We will release our code to support future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank adaptation (LoRA) reduces the computational and memory demands of\nfine-tuning large language models (LLMs) by approximating updates with low-rank\nmatrices. However, low-rank approximation in two-dimensional space fails to\ncapture high-dimensional structures within the target matrix. Recently, tensor\ndecomposition methods have been explored for fine-tuning LLMs, leveraging their\nability to extract structured information. Yet, these approaches primarily rely\non random initialization, and the impact of initialization on tensor adaptation\nremains underexplored. In this paper, we reveal that random initialization\nsignificantly diverges from the validation loss achieved by full fine-tuning.\nTo address this, we propose Weight-Decomposed Tensor Adaptation (DoTA), which\nleverages the Matrix Product Operator (MPO) decomposition of pre-trained\nweights for effective initialization in fine-tuning LLMs. Additionally, we\nintroduce QDoTA, a quantized version of DoTA designed for 4-bit quantization.\nExperiments on commonsense and arithmetic reasoning tasks show that DoTA\noutperforms random initialization methods with fewer parameters. QDoTA further\nreduces memory consumption and achieves comparable performance to DoTA on\ncommonsense reasoning tasks. We will release our code to support future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Xiang Cheng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20875v1",
                "updated": "2024-12-30T11:25:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    25,
                    54,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:25:54Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    25,
                    54,
                    0,
                    365,
                    0
                ],
                "title": "Attention Is All You Need For Mixture-of-Depths Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Is All You Need For Mixture-of-Depths Routing"
                },
                "summary": "Advancements in deep learning are driven by training models with increasingly\nlarger numbers of parameters, which in turn heightens the computational\ndemands. To address this issue, Mixture-of-Depths (MoD) models have been\nproposed to dynamically assign computations only to the most relevant parts of\nthe inputs, thereby enabling the deployment of large-parameter models with high\nefficiency during inference and training. These MoD models utilize a routing\nmechanism to determine which tokens should be processed by a layer, or skipped.\nHowever, conventional MoD models employ additional network layers specifically\nfor the routing which are difficult to train, and add complexity and deployment\noverhead to the model. In this paper, we introduce a novel attention-based\nrouting mechanism A-MoD that leverages the existing attention map of the\npreceding layer for routing decisions within the current layer. Compared to\nstandard routing, A-MoD allows for more efficient training as it introduces no\nadditional trainable parameters and can be easily adapted from pretrained\ntransformer models. Furthermore, it can increase the performance of the MoD\nmodel. For instance, we observe up to 2% higher accuracy on ImageNet compared\nto standard routing and isoFLOP ViT baselines. Furthermore, A-MoD improves the\nMoD training convergence, leading to up to 2x faster transfer learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in deep learning are driven by training models with increasingly\nlarger numbers of parameters, which in turn heightens the computational\ndemands. To address this issue, Mixture-of-Depths (MoD) models have been\nproposed to dynamically assign computations only to the most relevant parts of\nthe inputs, thereby enabling the deployment of large-parameter models with high\nefficiency during inference and training. These MoD models utilize a routing\nmechanism to determine which tokens should be processed by a layer, or skipped.\nHowever, conventional MoD models employ additional network layers specifically\nfor the routing which are difficult to train, and add complexity and deployment\noverhead to the model. In this paper, we introduce a novel attention-based\nrouting mechanism A-MoD that leverages the existing attention map of the\npreceding layer for routing decisions within the current layer. Compared to\nstandard routing, A-MoD allows for more efficient training as it introduces no\nadditional trainable parameters and can be easily adapted from pretrained\ntransformer models. Furthermore, it can increase the performance of the MoD\nmodel. For instance, we observe up to 2% higher accuracy on ImageNet compared\nto standard routing and isoFLOP ViT baselines. Furthermore, A-MoD improves the\nMoD training convergence, leading to up to 2x faster transfer learning."
                },
                "authors": [
                    {
                        "name": "Advait Gadhikar"
                    },
                    {
                        "name": "Souptik Kumar Majumdar"
                    },
                    {
                        "name": "Niclas Popp"
                    },
                    {
                        "name": "Piyapat Saranrittichai"
                    },
                    {
                        "name": "Martin Rapp"
                    },
                    {
                        "name": "Lukas Schott"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Schott"
                },
                "author": "Lukas Schott",
                "arxiv_comment": "22 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18552v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18552v2",
                "updated": "2024-12-30T11:24:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    24,
                    32,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-24T17:05:26Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    17,
                    5,
                    26,
                    1,
                    359,
                    0
                ],
                "title": "Distilling Fine-grained Sentiment Understanding from Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Fine-grained Sentiment Understanding from Large Language\n  Models"
                },
                "summary": "Fine-grained sentiment analysis (FSA) aims to extract and summarize user\nopinions from vast opinionated text. Recent studies demonstrate that large\nlanguage models (LLMs) possess exceptional sentiment understanding\ncapabilities. However, directly deploying LLMs for FSA applications incurs high\ninference costs. Therefore, this paper investigates the distillation of\nfine-grained sentiment understanding from LLMs into small language models\n(SLMs). We prompt LLMs to examine and interpret the sentiments of given reviews\nand then utilize the generated content to pretrain SLMs. Additionally, we\ndevelop a comprehensive FSA benchmark to evaluate both SLMs and LLMs. Extensive\nexperiments on this benchmark reveal that: (1) distillation significantly\nenhances the performance of SLMs in FSA tasks, achieving a 6.00\\% improvement\nin $F_1$-score, and the distilled model can outperform Llama-2-7b with only\n220M parameters; (2) distillation equips SLMs with excellent zero-shot\nsentiment classification capabilities, enabling them to match or even exceed\ntheir teacher models. These results suggest that distillation from LLMs is a\nhighly promising direction for FSA. We will release our code, data, and\npretrained model weights at https://github.com/HITSZ-HLT/FSA-Distillation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained sentiment analysis (FSA) aims to extract and summarize user\nopinions from vast opinionated text. Recent studies demonstrate that large\nlanguage models (LLMs) possess exceptional sentiment understanding\ncapabilities. However, directly deploying LLMs for FSA applications incurs high\ninference costs. Therefore, this paper investigates the distillation of\nfine-grained sentiment understanding from LLMs into small language models\n(SLMs). We prompt LLMs to examine and interpret the sentiments of given reviews\nand then utilize the generated content to pretrain SLMs. Additionally, we\ndevelop a comprehensive FSA benchmark to evaluate both SLMs and LLMs. Extensive\nexperiments on this benchmark reveal that: (1) distillation significantly\nenhances the performance of SLMs in FSA tasks, achieving a 6.00\\% improvement\nin $F_1$-score, and the distilled model can outperform Llama-2-7b with only\n220M parameters; (2) distillation equips SLMs with excellent zero-shot\nsentiment classification capabilities, enabling them to match or even exceed\ntheir teacher models. These results suggest that distillation from LLMs is a\nhighly promising direction for FSA. We will release our code, data, and\npretrained model weights at https://github.com/HITSZ-HLT/FSA-Distillation."
                },
                "authors": [
                    {
                        "name": "Yice Zhang"
                    },
                    {
                        "name": "Guangyu Xie"
                    },
                    {
                        "name": "Hongling Xu"
                    },
                    {
                        "name": "Kaiheng Hou"
                    },
                    {
                        "name": "Jianzhu Bao"
                    },
                    {
                        "name": "Qianlong Wang"
                    },
                    {
                        "name": "Shiwei Chen"
                    },
                    {
                        "name": "Ruifeng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruifeng Xu"
                },
                "author": "Ruifeng Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18552v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18552v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20866v1",
                "updated": "2024-12-30T11:10:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    10,
                    22,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:10:22Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    10,
                    22,
                    0,
                    365,
                    0
                ],
                "title": "An Infrastructure for Systematically Collecting Smart Contract Lineages\n  for Analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Infrastructure for Systematically Collecting Smart Contract Lineages\n  for Analyses"
                },
                "summary": "Tracking the evolution of smart contracts is a significant challenge,\nimpeding on the advancement of research on smart contract analysis. Indeed, due\nto the inherent immutability of the underlying blockchain technology, each\nsmart contract update results in a deployment at a new address, breaking the\nlinks between versions. Existing platforms like Etherscan lack the capability\nto trace the predecessor-successor relationships within a smart contract\nlineage, further hindering empirical research on contract evolution.\n  We address this challenge for the research community towards building a\nreliable dataset of linked versions for various smart contracts, i.e.,\nlineages: we introduce SCLineage, an automated infrastructure that accurately\nidentifies and collects smart contract lineages by leveraging proxy contracts.\nWe present SCLineageSet, an up-to-date, open-source dataset that facilitates\nextensive research on smart contract evolution. We illustrate the applicability\nof our proposal in software engineering research through a case study that\nexplores the evaluation of Locality-Sensitive Hashing (LSH) for forming\ncontract lineages. This example underscores how SCLineage provides valuable\ninsights for future research in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracking the evolution of smart contracts is a significant challenge,\nimpeding on the advancement of research on smart contract analysis. Indeed, due\nto the inherent immutability of the underlying blockchain technology, each\nsmart contract update results in a deployment at a new address, breaking the\nlinks between versions. Existing platforms like Etherscan lack the capability\nto trace the predecessor-successor relationships within a smart contract\nlineage, further hindering empirical research on contract evolution.\n  We address this challenge for the research community towards building a\nreliable dataset of linked versions for various smart contracts, i.e.,\nlineages: we introduce SCLineage, an automated infrastructure that accurately\nidentifies and collects smart contract lineages by leveraging proxy contracts.\nWe present SCLineageSet, an up-to-date, open-source dataset that facilitates\nextensive research on smart contract evolution. We illustrate the applicability\nof our proposal in software engineering research through a case study that\nexplores the evaluation of Locality-Sensitive Hashing (LSH) for forming\ncontract lineages. This example underscores how SCLineage provides valuable\ninsights for future research in the field."
                },
                "authors": [
                    {
                        "name": "Fatou Ndiaye Mbodji"
                    },
                    {
                        "name": "Vinny Adjibi"
                    },
                    {
                        "name": "Gervais Mendy"
                    },
                    {
                        "name": "Moustapha Awwalou Diouf"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawende Bissyande"
                    }
                ],
                "author_detail": {
                    "name": "Tegawende Bissyande"
                },
                "author": "Tegawende Bissyande",
                "arxiv_comment": "10 pages, 4 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20864v1",
                "updated": "2024-12-30T11:07:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    7,
                    5,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:07:05Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    7,
                    5,
                    0,
                    365,
                    0
                ],
                "title": "Enhancing Annotated Bibliography Generation with LLM Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Annotated Bibliography Generation with LLM Ensembles"
                },
                "summary": "This work proposes a novel approach to enhancing annotated bibliography\ngeneration through Large Language Model (LLM) ensembles. In particular,\nmultiple LLMs in different roles -- controllable text generation, evaluation,\nand summarization -- are introduced and validated using a systematic\nmethodology to enhance model performance in scholarly tasks. Output diversity\namong the ensemble that generates text is obtained using different LLM\nparameters, followed by an LLM acting as a judge to assess relevance, accuracy,\nand coherence. Responses selected by several combining strategies are then\nmerged and refined through summarization and redundancy removal techniques. The\npreliminary experimental validation demonstrates that the combined outputs from\nthe LLM ensemble improve coherence and relevance compared to individual\nresponses, leading to a 38% improvement in annotation quality and a 51%\nreduction in content redundancy, thus highlighting the potential for automating\ncomplex scholarly tasks while maintaining high-quality standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a novel approach to enhancing annotated bibliography\ngeneration through Large Language Model (LLM) ensembles. In particular,\nmultiple LLMs in different roles -- controllable text generation, evaluation,\nand summarization -- are introduced and validated using a systematic\nmethodology to enhance model performance in scholarly tasks. Output diversity\namong the ensemble that generates text is obtained using different LLM\nparameters, followed by an LLM acting as a judge to assess relevance, accuracy,\nand coherence. Responses selected by several combining strategies are then\nmerged and refined through summarization and redundancy removal techniques. The\npreliminary experimental validation demonstrates that the combined outputs from\nthe LLM ensemble improve coherence and relevance compared to individual\nresponses, leading to a 38% improvement in annotation quality and a 51%\nreduction in content redundancy, thus highlighting the potential for automating\ncomplex scholarly tasks while maintaining high-quality standards."
                },
                "authors": [
                    {
                        "name": "Sergio Bermejo"
                    }
                ],
                "author_detail": {
                    "name": "Sergio Bermejo"
                },
                "author": "Sergio Bermejo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06691v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06691v3",
                "updated": "2024-12-30T11:05:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    5,
                    20,
                    0,
                    365,
                    0
                ],
                "published": "2024-09-10T17:54:28Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    54,
                    28,
                    1,
                    254,
                    0
                ],
                "title": "Geometric-Averaged Preference Optimization for Soft Preference Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric-Averaged Preference Optimization for Soft Preference Labels"
                },
                "summary": "Many algorithms for aligning LLMs with human preferences assume that human\npreferences are binary and deterministic. However, human preferences can vary\nacross individuals, and therefore should be represented distributionally. In\nthis work, we introduce the distributional soft preference labels and improve\nDirect Preference Optimization (DPO) with a weighted geometric average of the\nLLM output likelihood in the loss function. This approach adjusts the scale of\nlearning loss based on the soft labels such that the loss would approach zero\nwhen the responses are closer to equally preferred. This simple modification\ncan be easily applied to any DPO-based methods and mitigate over-optimization\nand objective mismatch, which prior works suffer from. Our experiments simulate\nthe soft preference labels with AI feedback from LLMs and demonstrate that\ngeometric averaging consistently improves performance on standard benchmarks\nfor alignment research. In particular, we observe more preferable responses\nthan binary labels and significant improvements where modestly-confident labels\nare in the majority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many algorithms for aligning LLMs with human preferences assume that human\npreferences are binary and deterministic. However, human preferences can vary\nacross individuals, and therefore should be represented distributionally. In\nthis work, we introduce the distributional soft preference labels and improve\nDirect Preference Optimization (DPO) with a weighted geometric average of the\nLLM output likelihood in the loss function. This approach adjusts the scale of\nlearning loss based on the soft labels such that the loss would approach zero\nwhen the responses are closer to equally preferred. This simple modification\ncan be easily applied to any DPO-based methods and mitigate over-optimization\nand objective mismatch, which prior works suffer from. Our experiments simulate\nthe soft preference labels with AI feedback from LLMs and demonstrate that\ngeometric averaging consistently improves performance on standard benchmarks\nfor alignment research. In particular, we observe more preferable responses\nthan binary labels and significant improvements where modestly-confident labels\nare in the majority."
                },
                "authors": [
                    {
                        "name": "Hiroki Furuta"
                    },
                    {
                        "name": "Kuang-Huei Lee"
                    },
                    {
                        "name": "Shixiang Shane Gu"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    },
                    {
                        "name": "Aleksandra Faust"
                    },
                    {
                        "name": "Heiga Zen"
                    },
                    {
                        "name": "Izzeddin Gur"
                    }
                ],
                "author_detail": {
                    "name": "Izzeddin Gur"
                },
                "author": "Izzeddin Gur",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06691v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06691v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20846v1",
                "updated": "2024-12-30T10:29:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    10,
                    29,
                    18,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T10:29:18Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    10,
                    29,
                    18,
                    0,
                    365,
                    0
                ],
                "title": "Are LLMs Really Not Knowledgable? Mining the Submerged Knowledge in\n  LLMs' Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Really Not Knowledgable? Mining the Submerged Knowledge in\n  LLMs' Memory"
                },
                "summary": "Large language models (LLMs) have shown promise as potential knowledge bases,\nyet they often struggle with question-answering tasks and are prone to\nhallucinations. While previous research attributes these issues to knowledge\ngaps in the model's parameters, our investigation reveals a different\nphenomenon: LLMs often retain correct knowledge even when generating incorrect\nanswers. Through analysis of model's internal representations, we find that\ncorrect answers frequently appear among high-probability tokens despite not\nbeing selected as final outputs. Based on this observation, we introduce\nHits@k, a new metric to assess knowledge retention independent of expression\naccuracy. Our extensive experiments demonstrate that LLMs store significantly\nmore knowledge than their QA performance suggests. Building on these findings,\nwe develop SkipUnsure, a method to improve answer accuracy by leveraging\ndetected but unexpressed knowledge. Experiments on both open-domain and\nspecific-domain datasets show consistent improvements, with accuracy gains of\nup to 11.8% on DBPedia and 6.3% on IMDB, without requiring model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise as potential knowledge bases,\nyet they often struggle with question-answering tasks and are prone to\nhallucinations. While previous research attributes these issues to knowledge\ngaps in the model's parameters, our investigation reveals a different\nphenomenon: LLMs often retain correct knowledge even when generating incorrect\nanswers. Through analysis of model's internal representations, we find that\ncorrect answers frequently appear among high-probability tokens despite not\nbeing selected as final outputs. Based on this observation, we introduce\nHits@k, a new metric to assess knowledge retention independent of expression\naccuracy. Our extensive experiments demonstrate that LLMs store significantly\nmore knowledge than their QA performance suggests. Building on these findings,\nwe develop SkipUnsure, a method to improve answer accuracy by leveraging\ndetected but unexpressed knowledge. Experiments on both open-domain and\nspecific-domain datasets show consistent improvements, with accuracy gains of\nup to 11.8% on DBPedia and 6.3% on IMDB, without requiring model retraining."
                },
                "authors": [
                    {
                        "name": "Xingjian Tao"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Zhicheng Yang"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20834v1",
                "updated": "2024-12-30T09:58:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    58,
                    31,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T09:58:31Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    58,
                    31,
                    0,
                    365,
                    0
                ],
                "title": "Disentangling Preference Representation and Text Generation for\n  Efficient Individual Preference Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Preference Representation and Text Generation for\n  Efficient Individual Preference Alignment"
                },
                "summary": "Aligning Large Language Models (LLMs) with general human preferences has been\nproved crucial in improving the interaction quality between LLMs and human.\nHowever, human values are inherently diverse among different individuals,\nmaking it insufficient to align LLMs solely with general preferences. To\naddress this, personalizing LLMs according to individual feedback emerges as a\npromising solution. Nonetheless, this approach presents challenges in terms of\nthe efficiency of alignment algorithms. In this work, we introduce a flexible\nparadigm for individual preference alignment. Our method fundamentally improves\nefficiency by disentangling preference representation from text generation in\nLLMs. We validate our approach across multiple text generation tasks and\ndemonstrate that it can produce aligned quality as well as or better than\nPEFT-based methods, while reducing additional training time for each new\nindividual preference by $80\\%$ to $90\\%$ in comparison with them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models (LLMs) with general human preferences has been\nproved crucial in improving the interaction quality between LLMs and human.\nHowever, human values are inherently diverse among different individuals,\nmaking it insufficient to align LLMs solely with general preferences. To\naddress this, personalizing LLMs according to individual feedback emerges as a\npromising solution. Nonetheless, this approach presents challenges in terms of\nthe efficiency of alignment algorithms. In this work, we introduce a flexible\nparadigm for individual preference alignment. Our method fundamentally improves\nefficiency by disentangling preference representation from text generation in\nLLMs. We validate our approach across multiple text generation tasks and\ndemonstrate that it can produce aligned quality as well as or better than\nPEFT-based methods, while reducing additional training time for each new\nindividual preference by $80\\%$ to $90\\%$ in comparison with them."
                },
                "authors": [
                    {
                        "name": "Jianfei Zhang"
                    },
                    {
                        "name": "Jun Bai"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Yanmeng Wang"
                    },
                    {
                        "name": "Rumei Li"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Wenge Rong"
                    }
                ],
                "author_detail": {
                    "name": "Wenge Rong"
                },
                "author": "Wenge Rong",
                "arxiv_comment": "Coling 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19585v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19585v2",
                "updated": "2024-12-30T09:51:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    51,
                    38,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-27T11:03:26Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    11,
                    3,
                    26,
                    4,
                    362,
                    0
                ],
                "title": "Ultralight Signal Classification Model for Automatic Modulation\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultralight Signal Classification Model for Automatic Modulation\n  Recognition"
                },
                "summary": "The growing complexity of radar signals demands responsive and accurate\ndetection systems that can operate efficiently on resource-constrained edge\ndevices. Existing models, while effective, often rely on substantial\ncomputational resources and large datasets, making them impractical for edge\ndeployment. In this work, we propose an ultralight hybrid neural network\noptimized for edge applications, delivering robust performance across\nunfavorable signal-to-noise ratios (mean accuracy of 96.3% at 0 dB) using less\nthan 100 samples per class, and significantly reducing computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of radar signals demands responsive and accurate\ndetection systems that can operate efficiently on resource-constrained edge\ndevices. Existing models, while effective, often rely on substantial\ncomputational resources and large datasets, making them impractical for edge\ndeployment. In this work, we propose an ultralight hybrid neural network\noptimized for edge applications, delivering robust performance across\nunfavorable signal-to-noise ratios (mean accuracy of 96.3% at 0 dB) using less\nthan 100 samples per class, and significantly reducing computational overhead."
                },
                "authors": [
                    {
                        "name": "Alessandro Daniele Genuardi Oquendo"
                    },
                    {
                        "name": "AgustÃ­n MatÃ­as Galante CerviÃ±o"
                    },
                    {
                        "name": "Nilotpal Kanti Sinha"
                    },
                    {
                        "name": "Luc Andrea"
                    },
                    {
                        "name": "Sam Mugel"
                    },
                    {
                        "name": "RomÃ¡n OrÃºs"
                    }
                ],
                "author_detail": {
                    "name": "RomÃ¡n OrÃºs"
                },
                "author": "RomÃ¡n OrÃºs",
                "arxiv_comment": "8 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19585v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19585v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.07395v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.07395v2",
                "updated": "2024-12-30T09:51:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    51,
                    22,
                    0,
                    365,
                    0
                ],
                "published": "2023-12-12T16:10:19Z",
                "published_parsed": [
                    2023,
                    12,
                    12,
                    16,
                    10,
                    19,
                    1,
                    346,
                    0
                ],
                "title": "A Simple Recipe for Contrastively Pre-training Video-First Encoders\n  Beyond 16 Frames",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Recipe for Contrastively Pre-training Video-First Encoders\n  Beyond 16 Frames"
                },
                "summary": "Understanding long, real-world videos requires modeling of long-range visual\ndependencies. To this end, we explore video-first architectures, building on\nthe common paradigm of transferring large-scale, image--text models to video\nvia shallow temporal fusion. However, we expose two limitations to the\napproach: (1) decreased spatial capabilities, likely due to poor\nvideo--language alignment in standard video datasets, and (2) higher memory\nconsumption, bottlenecking the number of frames that can be processed. To\nmitigate the memory bottleneck, we systematically analyze the memory/accuracy\ntrade-off of various efficient methods: factorized attention,\nparameter-efficient image-to-video adaptation, input masking, and\nmulti-resolution patchification. Surprisingly, simply masking large portions of\nthe video (up to 75%) during contrastive pre-training proves to be one of the\nmost robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our\nsimple approach for training long video-to-text models, which scales to 1B\nparameters, does not add new architectural complexity and is able to outperform\nthe popular paradigm of using much larger LLMs as an information aggregator\nover segment-based information on benchmarks with long-range temporal\ndependencies (YouCook2, EgoSchema).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding long, real-world videos requires modeling of long-range visual\ndependencies. To this end, we explore video-first architectures, building on\nthe common paradigm of transferring large-scale, image--text models to video\nvia shallow temporal fusion. However, we expose two limitations to the\napproach: (1) decreased spatial capabilities, likely due to poor\nvideo--language alignment in standard video datasets, and (2) higher memory\nconsumption, bottlenecking the number of frames that can be processed. To\nmitigate the memory bottleneck, we systematically analyze the memory/accuracy\ntrade-off of various efficient methods: factorized attention,\nparameter-efficient image-to-video adaptation, input masking, and\nmulti-resolution patchification. Surprisingly, simply masking large portions of\nthe video (up to 75%) during contrastive pre-training proves to be one of the\nmost robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our\nsimple approach for training long video-to-text models, which scales to 1B\nparameters, does not add new architectural complexity and is able to outperform\nthe popular paradigm of using much larger LLMs as an information aggregator\nover segment-based information on benchmarks with long-range temporal\ndependencies (YouCook2, EgoSchema)."
                },
                "authors": [
                    {
                        "name": "Pinelopi Papalampidi"
                    },
                    {
                        "name": "Skanda Koppula"
                    },
                    {
                        "name": "Shreya Pathak"
                    },
                    {
                        "name": "Justin Chiu"
                    },
                    {
                        "name": "Joe Heyward"
                    },
                    {
                        "name": "Viorica Patraucean"
                    },
                    {
                        "name": "Jiajun Shen"
                    },
                    {
                        "name": "Antoine Miech"
                    },
                    {
                        "name": "Andrew Zisserman"
                    },
                    {
                        "name": "Aida Nematzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Aida Nematzadeh"
                },
                "author": "Aida Nematzadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.07395v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.07395v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20820v1",
                "updated": "2024-12-30T09:30:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    30,
                    36,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T09:30:36Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    30,
                    36,
                    0,
                    365,
                    0
                ],
                "title": "Retrieval-Augmented Generation for Mobile Edge Computing via Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation for Mobile Edge Computing via Large\n  Language Model"
                },
                "summary": "The rapid evolution of mobile edge computing (MEC) has introduced significant\nchallenges in optimizing resource allocation in highly dynamic wireless\ncommunication systems, in which task offloading decisions should be made in\nreal-time. However, existing resource allocation strategies cannot well adapt\nto the dynamic and heterogeneous characteristics of MEC systems, since they are\nshort of scalability, context-awareness, and interpretability. To address these\nissues, this paper proposes a novel retrieval-augmented generation (RAG) method\nto improve the performance of MEC systems. Specifically, a latency minimization\nproblem is first proposed to jointly optimize the data offloading ratio,\ntransmit power allocation, and computing resource allocation. Then, an\nLLM-enabled information-retrieval mechanism is proposed to solve the problem\nefficiently. Extensive experiments across multi-user, multi-task, and highly\ndynamic offloading scenarios show that the proposed method consistently reduces\nlatency compared to several DL-based approaches, achieving 57% improvement\nunder varying user computing ability, 86% with different servers, 30% under\ndistinct transmit powers, and 42% for varying data volumes. These results show\nthe effectiveness of LLM-driven solutions to solve the resource allocation\nproblems in MEC systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of mobile edge computing (MEC) has introduced significant\nchallenges in optimizing resource allocation in highly dynamic wireless\ncommunication systems, in which task offloading decisions should be made in\nreal-time. However, existing resource allocation strategies cannot well adapt\nto the dynamic and heterogeneous characteristics of MEC systems, since they are\nshort of scalability, context-awareness, and interpretability. To address these\nissues, this paper proposes a novel retrieval-augmented generation (RAG) method\nto improve the performance of MEC systems. Specifically, a latency minimization\nproblem is first proposed to jointly optimize the data offloading ratio,\ntransmit power allocation, and computing resource allocation. Then, an\nLLM-enabled information-retrieval mechanism is proposed to solve the problem\nefficiently. Extensive experiments across multi-user, multi-task, and highly\ndynamic offloading scenarios show that the proposed method consistently reduces\nlatency compared to several DL-based approaches, achieving 57% improvement\nunder varying user computing ability, 86% with different servers, 30% under\ndistinct transmit powers, and 42% for varying data volumes. These results show\nthe effectiveness of LLM-driven solutions to solve the resource allocation\nproblems in MEC systems."
                },
                "authors": [
                    {
                        "name": "Runtao Ren"
                    },
                    {
                        "name": "Yinyu Wu"
                    },
                    {
                        "name": "Xuhui Zhang"
                    },
                    {
                        "name": "Jinke Ren"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Shuqiang Wang"
                    },
                    {
                        "name": "Kim-Fung Tsang"
                    }
                ],
                "author_detail": {
                    "name": "Kim-Fung Tsang"
                },
                "author": "Kim-Fung Tsang",
                "arxiv_comment": "This manuscript has been submitted to IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18176v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18176v2",
                "updated": "2024-12-30T09:24:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    24,
                    34,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-24T05:23:13Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    23,
                    13,
                    1,
                    359,
                    0
                ],
                "title": "Molar: Multimodal LLMs with Collaborative Filtering Alignment for\n  Enhanced Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molar: Multimodal LLMs with Collaborative Filtering Alignment for\n  Enhanced Sequential Recommendation"
                },
                "summary": "Sequential recommendation (SR) systems have evolved significantly over the\npast decade, transitioning from traditional collaborative filtering to deep\nlearning approaches and, more recently, to large language models (LLMs). While\nthe adoption of LLMs has driven substantial advancements, these models\ninherently lack collaborative filtering information, relying primarily on\ntextual content data neglecting other modalities and thus failing to achieve\noptimal recommendation performance. To address this limitation, we propose\nMolar, a Multimodal large language sequential recommendation framework that\nintegrates multiple content modalities with ID information to capture\ncollaborative signals effectively. Molar employs an MLLM to generate unified\nitem representations from both textual and non-textual data, facilitating\ncomprehensive multimodal modeling and enriching item embeddings. Additionally,\nit incorporates collaborative filtering signals through a post-alignment\nmechanism, which aligns user representations from content-based and ID-based\nmodels, ensuring precise personalization and robust performance. By seamlessly\ncombining multimodal content with collaborative filtering insights, Molar\ncaptures both user interests and contextual semantics, leading to superior\nrecommendation accuracy. Extensive experiments validate that Molar\nsignificantly outperforms traditional and LLM-based baselines, highlighting its\nstrength in utilizing multimodal data and collaborative signals for sequential\nrecommendation tasks. The source code is available at\nhttps://anonymous.4open.science/r/Molar-8B06/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation (SR) systems have evolved significantly over the\npast decade, transitioning from traditional collaborative filtering to deep\nlearning approaches and, more recently, to large language models (LLMs). While\nthe adoption of LLMs has driven substantial advancements, these models\ninherently lack collaborative filtering information, relying primarily on\ntextual content data neglecting other modalities and thus failing to achieve\noptimal recommendation performance. To address this limitation, we propose\nMolar, a Multimodal large language sequential recommendation framework that\nintegrates multiple content modalities with ID information to capture\ncollaborative signals effectively. Molar employs an MLLM to generate unified\nitem representations from both textual and non-textual data, facilitating\ncomprehensive multimodal modeling and enriching item embeddings. Additionally,\nit incorporates collaborative filtering signals through a post-alignment\nmechanism, which aligns user representations from content-based and ID-based\nmodels, ensuring precise personalization and robust performance. By seamlessly\ncombining multimodal content with collaborative filtering insights, Molar\ncaptures both user interests and contextual semantics, leading to superior\nrecommendation accuracy. Extensive experiments validate that Molar\nsignificantly outperforms traditional and LLM-based baselines, highlighting its\nstrength in utilizing multimodal data and collaborative signals for sequential\nrecommendation tasks. The source code is available at\nhttps://anonymous.4open.science/r/Molar-8B06/."
                },
                "authors": [
                    {
                        "name": "Yucong Luo"
                    },
                    {
                        "name": "Qitao Qin"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Ruiran Yan"
                    },
                    {
                        "name": "Kefan Wang"
                    },
                    {
                        "name": "Jie Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Ouyang"
                },
                "author": "Jie Ouyang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18176v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18176v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18185v2",
                "updated": "2024-12-30T09:22:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    22,
                    24,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-24T05:38:45Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    38,
                    45,
                    1,
                    359,
                    0
                ],
                "title": "TextMatch: Enhancing Image-Text Consistency Through Multimodal\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextMatch: Enhancing Image-Text Consistency Through Multimodal\n  Optimization"
                },
                "summary": "Text-to-image generative models excel in creating images from text but\nstruggle with ensuring alignment and consistency between outputs and prompts.\nThis paper introduces TextMatch, a novel framework that leverages multimodal\noptimization to address image-text discrepancies in text-to-image (T2I)\ngeneration and editing. TextMatch employs a scoring strategy powered by large\nlanguage models (LLMs) and visual question-answering (VQA) models to evaluate\nsemantic consistency between prompts and generated images. By integrating\nmultimodal in-context learning and chain of thought reasoning, our method\ndynamically refines prompts through iterative optimization. This process\nensures that the generated images better capture user intent of, resulting in\nhigher fidelity and relevance. Extensive experiments demonstrate that TextMatch\nsignificantly improves text-image consistency across multiple benchmarks,\nestablishing a reliable framework for advancing the capabilities of\ntext-to-image generative models. Our code is available at\nhttps://anonymous.4open.science/r/TextMatch-F55C/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generative models excel in creating images from text but\nstruggle with ensuring alignment and consistency between outputs and prompts.\nThis paper introduces TextMatch, a novel framework that leverages multimodal\noptimization to address image-text discrepancies in text-to-image (T2I)\ngeneration and editing. TextMatch employs a scoring strategy powered by large\nlanguage models (LLMs) and visual question-answering (VQA) models to evaluate\nsemantic consistency between prompts and generated images. By integrating\nmultimodal in-context learning and chain of thought reasoning, our method\ndynamically refines prompts through iterative optimization. This process\nensures that the generated images better capture user intent of, resulting in\nhigher fidelity and relevance. Extensive experiments demonstrate that TextMatch\nsignificantly improves text-image consistency across multiple benchmarks,\nestablishing a reliable framework for advancing the capabilities of\ntext-to-image generative models. Our code is available at\nhttps://anonymous.4open.science/r/TextMatch-F55C/."
                },
                "authors": [
                    {
                        "name": "Yucong Luo"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Jie Ouyang"
                    },
                    {
                        "name": "Xiaoyu Tao"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10424v2",
                "updated": "2024-12-30T09:11:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    11,
                    50,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-10T15:00:32Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    15,
                    0,
                    32,
                    1,
                    345,
                    0
                ],
                "title": "LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM\n  Evaluation"
                },
                "summary": "We introduce LLM-as-an-Interviewer, a novel paradigm for evaluating large\nlanguage models (LLMs). This approach leverages multi-turn interactions where\nthe LLM interviewer actively provides feedback on responses and poses follow-up\nquestions to the evaluated LLM. At the start of the interview, the LLM\ninterviewer dynamically modifies datasets to generate initial questions,\nmitigating data contamination. We apply the LLM-as-an-Interviewer framework to\nevaluate six models on the MATH and DepthQA tasks. Our results show that the\nframework effectively provides insights into LLM performance, including the\nquality of initial responses, adaptability to feedback, and ability to address\nfollow-up queries like clarification or additional knowledge requests. The\nframework also addresses key limitations of conventional methods like\nLLM-as-a-Judge, including verbosity bias and inconsistency across runs.\nFinally, we propose the Interview Report, which aggregates insights from the\ninterview process, providing examples and a comprehensive analysis of the LLM's\nstrengths and weaknesses. This report offers a detailed snapshot of the model's\nreal-world applicability. The code for our framework is publicly available at\nhttps://github.com/interview-eval/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce LLM-as-an-Interviewer, a novel paradigm for evaluating large\nlanguage models (LLMs). This approach leverages multi-turn interactions where\nthe LLM interviewer actively provides feedback on responses and poses follow-up\nquestions to the evaluated LLM. At the start of the interview, the LLM\ninterviewer dynamically modifies datasets to generate initial questions,\nmitigating data contamination. We apply the LLM-as-an-Interviewer framework to\nevaluate six models on the MATH and DepthQA tasks. Our results show that the\nframework effectively provides insights into LLM performance, including the\nquality of initial responses, adaptability to feedback, and ability to address\nfollow-up queries like clarification or additional knowledge requests. The\nframework also addresses key limitations of conventional methods like\nLLM-as-a-Judge, including verbosity bias and inconsistency across runs.\nFinally, we propose the Interview Report, which aggregates insights from the\ninterview process, providing examples and a comprehensive analysis of the LLM's\nstrengths and weaknesses. This report offers a detailed snapshot of the model's\nreal-world applicability. The code for our framework is publicly available at\nhttps://github.com/interview-eval/."
                },
                "authors": [
                    {
                        "name": "Eunsu Kim"
                    },
                    {
                        "name": "Juyoung Suk"
                    },
                    {
                        "name": "Seungone Kim"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Dongkwan Kim"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14040v2",
                "updated": "2024-12-30T09:02:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    9,
                    2,
                    53,
                    0,
                    365,
                    0
                ],
                "published": "2024-05-22T22:22:26Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    22,
                    22,
                    26,
                    2,
                    143,
                    0
                ],
                "title": "Synchronized Video Storytelling: Generating Video Narrations with\n  Structured Storyline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synchronized Video Storytelling: Generating Video Narrations with\n  Structured Storyline"
                },
                "summary": "Video storytelling is engaging multimedia content that utilizes video and its\naccompanying narration to attract the audience, where a key challenge is\ncreating narrations for recorded visual scenes. Previous studies on dense video\ncaptioning and video story generation have made some progress. However, in\npractical applications, we typically require synchronized narrations for\nongoing visual scenes. In this work, we introduce a new task of Synchronized\nVideo Storytelling, which aims to generate synchronous and informative\nnarrations for videos. These narrations, associated with each video clip,\nshould relate to the visual content, integrate relevant knowledge, and have an\nappropriate word count corresponding to the clip's duration. Specifically, a\nstructured storyline is beneficial to guide the generation process, ensuring\ncoherence and integrity. To support the exploration of this task, we introduce\na new benchmark dataset E-SyncVidStory with rich annotations. Since existing\nMultimodal LLMs are not effective in addressing this task in one-shot or\nfew-shot settings, we propose a framework named VideoNarrator that can generate\na storyline for input videos and simultaneously generate narrations with the\nguidance of the generated or predefined storyline. We further introduce a set\nof evaluation metrics to thoroughly assess the generation. Both automatic and\nhuman evaluations validate the effectiveness of our approach. Our dataset,\ncodes, and evaluations will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video storytelling is engaging multimedia content that utilizes video and its\naccompanying narration to attract the audience, where a key challenge is\ncreating narrations for recorded visual scenes. Previous studies on dense video\ncaptioning and video story generation have made some progress. However, in\npractical applications, we typically require synchronized narrations for\nongoing visual scenes. In this work, we introduce a new task of Synchronized\nVideo Storytelling, which aims to generate synchronous and informative\nnarrations for videos. These narrations, associated with each video clip,\nshould relate to the visual content, integrate relevant knowledge, and have an\nappropriate word count corresponding to the clip's duration. Specifically, a\nstructured storyline is beneficial to guide the generation process, ensuring\ncoherence and integrity. To support the exploration of this task, we introduce\na new benchmark dataset E-SyncVidStory with rich annotations. Since existing\nMultimodal LLMs are not effective in addressing this task in one-shot or\nfew-shot settings, we propose a framework named VideoNarrator that can generate\na storyline for input videos and simultaneously generate narrations with the\nguidance of the generated or predefined storyline. We further introduce a set\nof evaluation metrics to thoroughly assess the generation. Both automatic and\nhuman evaluations validate the effectiveness of our approach. Our dataset,\ncodes, and evaluations will be released."
                },
                "authors": [
                    {
                        "name": "Dingyi Yang"
                    },
                    {
                        "name": "Chunru Zhan"
                    },
                    {
                        "name": "Ziheng Wang"
                    },
                    {
                        "name": "Biao Wang"
                    },
                    {
                        "name": "Tiezheng Ge"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Qin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Qin Jin"
                },
                "author": "Qin Jin",
                "arxiv_comment": "15 pages, 13 figures",
                "arxiv_journal_ref": "https://aclanthology.org/2024.acl-long.513/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07687v2",
                "updated": "2024-12-30T08:29:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    29,
                    9,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-10T17:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    17,
                    20,
                    47,
                    1,
                    345,
                    0
                ],
                "title": "Privacy-Preserving Customer Support: A Framework for Secure and Scalable\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Customer Support: A Framework for Secure and Scalable\n  Interactions"
                },
                "summary": "The growing reliance on artificial intelligence (AI) in customer support has\nsignificantly improved operational efficiency and user experience. However,\ntraditional machine learning (ML) approaches, which require extensive local\ntraining on sensitive datasets, pose substantial privacy risks and compliance\nchallenges with regulations like the General Data Protection Regulation (GDPR)\nand California Consumer Privacy Act (CCPA). Existing privacy-preserving\ntechniques, such as anonymization, differential privacy, and federated\nlearning, address some concerns but face limitations in utility, scalability,\nand complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning\n(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in\na zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates\nthe need for local training on sensitive data by utilizing pre-trained LLMs to\ngenerate responses directly. The framework incorporates real-time data\nanonymization to redact or mask sensitive information, retrieval-augmented\ngeneration (RAG) for domain-specific query resolution, and robust\npost-processing to ensure compliance with regulatory standards. This\ncombination reduces privacy risks, simplifies compliance, and enhances\nscalability and operational efficiency. Empirical analysis demonstrates that\nthe PP-ZSL framework provides accurate, privacy-compliant responses while\nsignificantly lowering the costs and complexities of deploying AI-driven\ncustomer support systems. The study highlights potential applications across\nindustries, including financial services, healthcare, e-commerce, legal\nsupport, telecommunications, and government services. By addressing the dual\nchallenges of privacy and performance, this framework establishes a foundation\nfor secure, efficient, and regulatory-compliant AI applications in customer\ninteractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing reliance on artificial intelligence (AI) in customer support has\nsignificantly improved operational efficiency and user experience. However,\ntraditional machine learning (ML) approaches, which require extensive local\ntraining on sensitive datasets, pose substantial privacy risks and compliance\nchallenges with regulations like the General Data Protection Regulation (GDPR)\nand California Consumer Privacy Act (CCPA). Existing privacy-preserving\ntechniques, such as anonymization, differential privacy, and federated\nlearning, address some concerns but face limitations in utility, scalability,\nand complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning\n(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in\na zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates\nthe need for local training on sensitive data by utilizing pre-trained LLMs to\ngenerate responses directly. The framework incorporates real-time data\nanonymization to redact or mask sensitive information, retrieval-augmented\ngeneration (RAG) for domain-specific query resolution, and robust\npost-processing to ensure compliance with regulatory standards. This\ncombination reduces privacy risks, simplifies compliance, and enhances\nscalability and operational efficiency. Empirical analysis demonstrates that\nthe PP-ZSL framework provides accurate, privacy-compliant responses while\nsignificantly lowering the costs and complexities of deploying AI-driven\ncustomer support systems. The study highlights potential applications across\nindustries, including financial services, healthcare, e-commerce, legal\nsupport, telecommunications, and government services. By addressing the dual\nchallenges of privacy and performance, this framework establishes a foundation\nfor secure, efficient, and regulatory-compliant AI applications in customer\ninteractions."
                },
                "authors": [
                    {
                        "name": "Anant Prakash Awasthi"
                    },
                    {
                        "name": "Girdhar Gopal Agarwal"
                    },
                    {
                        "name": "Chandraketu Singh"
                    },
                    {
                        "name": "Rakshit Varma"
                    },
                    {
                        "name": "Sanchit Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Sanchit Sharma"
                },
                "author": "Sanchit Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20787v1",
                "updated": "2024-12-30T08:11:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    11,
                    54,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T08:11:54Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    8,
                    11,
                    54,
                    0,
                    365,
                    0
                ],
                "title": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for\n  LLMs in Cybersecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for\n  LLMs in Cybersecurity"
                },
                "summary": "Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs.Benchmarking\nresults on 13 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs.Benchmarking\nresults on 13 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link."
                },
                "authors": [
                    {
                        "name": "Pengfei Jing"
                    },
                    {
                        "name": "Mengyun Tang"
                    },
                    {
                        "name": "Xiaorong Shi"
                    },
                    {
                        "name": "Xing Zheng"
                    },
                    {
                        "name": "Sen Nie"
                    },
                    {
                        "name": "Shi Wu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Xiapu Luo"
                    }
                ],
                "author_detail": {
                    "name": "Xiapu Luo"
                },
                "author": "Xiapu Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12543v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12543v3",
                "updated": "2024-12-30T07:57:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    57,
                    10,
                    0,
                    365,
                    0
                ],
                "published": "2024-10-16T13:21:46Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    13,
                    21,
                    46,
                    2,
                    290,
                    0
                ],
                "title": "LLM-based Translation Inference with Iterative Bilingual Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Translation Inference with Iterative Bilingual Understanding"
                },
                "summary": "The remarkable understanding and generation capabilities of large language\nmodels (LLMs) have greatly improved translation performance. However, incorrect\nunderstanding of the sentence to be translated can degrade translation quality.\nTo address this issue, we proposed a novel Iterative Bilingual Understanding\nTranslation (IBUT) method based on the cross-lingual capabilities of LLMs and\nthe dual characteristics of translation tasks. The cross-lingual capability of\nLLMs enables the generation of contextual understanding for both the source and\ntarget languages separately. Furthermore, the dual characteristics allow IBUT\nto generate effective cross-lingual feedback, iteratively refining contextual\nunderstanding, thereby reducing errors and improving translation performance.\nExperimental results showed that the proposed IBUT outperforms several strong\ncomparison methods, especially being generalized to multiple domains (e.g.,\nnews, commonsense, and cultural translation benchmarks).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable understanding and generation capabilities of large language\nmodels (LLMs) have greatly improved translation performance. However, incorrect\nunderstanding of the sentence to be translated can degrade translation quality.\nTo address this issue, we proposed a novel Iterative Bilingual Understanding\nTranslation (IBUT) method based on the cross-lingual capabilities of LLMs and\nthe dual characteristics of translation tasks. The cross-lingual capability of\nLLMs enables the generation of contextual understanding for both the source and\ntarget languages separately. Furthermore, the dual characteristics allow IBUT\nto generate effective cross-lingual feedback, iteratively refining contextual\nunderstanding, thereby reducing errors and improving translation performance.\nExperimental results showed that the proposed IBUT outperforms several strong\ncomparison methods, especially being generalized to multiple domains (e.g.,\nnews, commonsense, and cultural translation benchmarks)."
                },
                "authors": [
                    {
                        "name": "Andong Chen"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min zhang"
                },
                "author": "Min zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12543v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12543v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19652v2",
                "updated": "2024-12-30T07:49:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    49,
                    13,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-27T13:56:51Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    56,
                    51,
                    4,
                    362,
                    0
                ],
                "title": "FreStega: A Plug-and-Play Method for Boosting Imperceptibility and\n  Capacity in Generative Linguistic Steganography for Real-World Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreStega: A Plug-and-Play Method for Boosting Imperceptibility and\n  Capacity in Generative Linguistic Steganography for Real-World Scenarios"
                },
                "summary": "Linguistic steganography embeds secret information in seemingly innocent\ntexts, safeguarding privacy in surveillance environments. Generative linguistic\nsteganography leverages the probability distribution of language models (LMs)\nand applies steganographic algorithms to generate stego tokens, gaining\nattention with recent Large Language Model (LLM) advancements. To enhance\nsecurity, researchers develop distribution-preserving stego algorithms to\nminimize the gap between stego sampling and LM sampling. However, the reliance\non language model distributions, coupled with deviations from real-world cover\ntexts, results in insufficient imperceptibility when facing steganalysis\ndetectors in real-world scenarios. Moreover, LLM distributions tend to be more\ndeterministic, resulting in reduced entropy and, consequently, lower embedding\ncapacity. In this paper, we propose FreStega, a plug-and-play method to\nreconstruct the distribution of language models used for generative linguistic\nsteganography. FreStega dynamically adjusts token probabilities from the\nlanguage model at each step of stegotext auto-regressive generation, leveraging\nboth sequential and spatial dimensions. In sequential adjustment, the\ntemperature is dynamically adjusted based on instantaneous entropy, enhancing\nthe diversity of stego texts and boosting embedding capacity. In the spatial\ndimension, the distribution is aligned with guidance from the target domain\ncorpus, closely mimicking real cover text in the target domain. By reforming\nthe distribution, FreStega enhances the imperceptibility of stego text in\npractical scenarios and improves steganographic capacity by 15.41\\%, all\nwithout compromising the quality of the generated text. FreStega serves as a\nplug-and-play remedy to enhance the imperceptibility and embedding capacity of\nexisting distribution-preserving steganography methods in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic steganography embeds secret information in seemingly innocent\ntexts, safeguarding privacy in surveillance environments. Generative linguistic\nsteganography leverages the probability distribution of language models (LMs)\nand applies steganographic algorithms to generate stego tokens, gaining\nattention with recent Large Language Model (LLM) advancements. To enhance\nsecurity, researchers develop distribution-preserving stego algorithms to\nminimize the gap between stego sampling and LM sampling. However, the reliance\non language model distributions, coupled with deviations from real-world cover\ntexts, results in insufficient imperceptibility when facing steganalysis\ndetectors in real-world scenarios. Moreover, LLM distributions tend to be more\ndeterministic, resulting in reduced entropy and, consequently, lower embedding\ncapacity. In this paper, we propose FreStega, a plug-and-play method to\nreconstruct the distribution of language models used for generative linguistic\nsteganography. FreStega dynamically adjusts token probabilities from the\nlanguage model at each step of stegotext auto-regressive generation, leveraging\nboth sequential and spatial dimensions. In sequential adjustment, the\ntemperature is dynamically adjusted based on instantaneous entropy, enhancing\nthe diversity of stego texts and boosting embedding capacity. In the spatial\ndimension, the distribution is aligned with guidance from the target domain\ncorpus, closely mimicking real cover text in the target domain. By reforming\nthe distribution, FreStega enhances the imperceptibility of stego text in\npractical scenarios and improves steganographic capacity by 15.41\\%, all\nwithout compromising the quality of the generated text. FreStega serves as a\nplug-and-play remedy to enhance the imperceptibility and embedding capacity of\nexisting distribution-preserving steganography methods in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Kaiyi Pang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyi Pang"
                },
                "author": "Kaiyi Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20772v1",
                "updated": "2024-12-30T07:47:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    47,
                    30,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T07:47:30Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    47,
                    30,
                    0,
                    365,
                    0
                ],
                "title": "Large Language Model Enabled Multi-Task Physical Layer Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Enabled Multi-Task Physical Layer Network"
                },
                "summary": "The recent advance of Artificial Intelligence (AI) is continuously reshaping\nthe future 6G wireless communications. Recently, the development of Large\nLanguage Models (LLMs) offers a promising approach to effectively improve the\nperformance and generalization for different physical layer tasks. However,\nmost existing works finetune dedicated LLM networks for a single wireless\ncommunication task separately. Thus performing diverse physical layer tasks\nintroduces extremely high training resources, memory usage, and deployment\ncosts. To solve the problem, we propose a LLM-enabled multi-task physical layer\nnetwork to unify multiple tasks with a single LLM. Specifically, we first\npropose a multi-task LLM framework, which finetunes LLM to perform multi-user\nprecoding, signal detection and channel prediction simultaneously. Besides,\nmulti-task instruction module, input encoders, as well as output decoders, are\nelaborately designed to distinguish multiple tasks and adapted the features of\ndifferent formats of wireless data for the features of LLM. Numerical\nsimulations are also displayed to verify the effectiveness of the proposed\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advance of Artificial Intelligence (AI) is continuously reshaping\nthe future 6G wireless communications. Recently, the development of Large\nLanguage Models (LLMs) offers a promising approach to effectively improve the\nperformance and generalization for different physical layer tasks. However,\nmost existing works finetune dedicated LLM networks for a single wireless\ncommunication task separately. Thus performing diverse physical layer tasks\nintroduces extremely high training resources, memory usage, and deployment\ncosts. To solve the problem, we propose a LLM-enabled multi-task physical layer\nnetwork to unify multiple tasks with a single LLM. Specifically, we first\npropose a multi-task LLM framework, which finetunes LLM to perform multi-user\nprecoding, signal detection and channel prediction simultaneously. Besides,\nmulti-task instruction module, input encoders, as well as output decoders, are\nelaborately designed to distinguish multiple tasks and adapted the features of\ndifferent formats of wireless data for the features of LLM. Numerical\nsimulations are also displayed to verify the effectiveness of the proposed\nmethod."
                },
                "authors": [
                    {
                        "name": "Tianyue Zheng"
                    },
                    {
                        "name": "Linglong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Linglong Dai"
                },
                "author": "Linglong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03963v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03963v2",
                "updated": "2024-12-30T07:46:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    46,
                    43,
                    0,
                    365,
                    0
                ],
                "published": "2024-07-04T14:33:03Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    14,
                    33,
                    3,
                    3,
                    186,
                    0
                ],
                "title": "LLM-jp: A Cross-organizational Project for the Research and Development\n  of Fully Open Japanese LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-jp: A Cross-organizational Project for the Research and Development\n  of Fully Open Japanese LLMs"
                },
                "summary": "This paper introduces LLM-jp, a cross-organizational project for the research\nand development of Japanese large language models (LLMs). LLM-jp aims to\ndevelop open-source and strong Japanese LLMs, and as of this writing, more than\n1,500 participants from academia and industry are working together for this\npurpose. This paper presents the background of the establishment of LLM-jp,\nsummaries of its activities, and technical reports on the LLMs developed by\nLLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces LLM-jp, a cross-organizational project for the research\nand development of Japanese large language models (LLMs). LLM-jp aims to\ndevelop open-source and strong Japanese LLMs, and as of this writing, more than\n1,500 participants from academia and industry are working together for this\npurpose. This paper presents the background of the establishment of LLM-jp,\nsummaries of its activities, and technical reports on the LLMs developed by\nLLM-jp. For the latest activities, visit https://llm-jp.nii.ac.jp/en/."
                },
                "authors": [
                    {
                        "name": "LLM-jp"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Akiko Aizawa"
                    },
                    {
                        "name": "Eiji Aramaki"
                    },
                    {
                        "name": "Bowen Chen"
                    },
                    {
                        "name": "Fei Cheng"
                    },
                    {
                        "name": "Hiroyuki Deguchi"
                    },
                    {
                        "name": "Rintaro Enomoto"
                    },
                    {
                        "name": "Kazuki Fujii"
                    },
                    {
                        "name": "Kensuke Fukumoto"
                    },
                    {
                        "name": "Takuya Fukushima"
                    },
                    {
                        "name": "Namgi Han"
                    },
                    {
                        "name": "Yuto Harada"
                    },
                    {
                        "name": "Chikara Hashimoto"
                    },
                    {
                        "name": "Tatsuya Hiraoka"
                    },
                    {
                        "name": "Shohei Hisada"
                    },
                    {
                        "name": "Sosuke Hosokawa"
                    },
                    {
                        "name": "Lu Jie"
                    },
                    {
                        "name": "Keisuke Kamata"
                    },
                    {
                        "name": "Teruhito Kanazawa"
                    },
                    {
                        "name": "Hiroki Kanezashi"
                    },
                    {
                        "name": "Hiroshi Kataoka"
                    },
                    {
                        "name": "Satoru Katsumata"
                    },
                    {
                        "name": "Daisuke Kawahara"
                    },
                    {
                        "name": "Seiya Kawano"
                    },
                    {
                        "name": "Atsushi Keyaki"
                    },
                    {
                        "name": "Keisuke Kiryu"
                    },
                    {
                        "name": "Hirokazu Kiyomaru"
                    },
                    {
                        "name": "Takashi Kodama"
                    },
                    {
                        "name": "Takahiro Kubo"
                    },
                    {
                        "name": "Yohei Kuga"
                    },
                    {
                        "name": "Ryoma Kumon"
                    },
                    {
                        "name": "Shuhei Kurita"
                    },
                    {
                        "name": "Sadao Kurohashi"
                    },
                    {
                        "name": "Conglong Li"
                    },
                    {
                        "name": "Taiki Maekawa"
                    },
                    {
                        "name": "Hiroshi Matsuda"
                    },
                    {
                        "name": "Yusuke Miyao"
                    },
                    {
                        "name": "Kentaro Mizuki"
                    },
                    {
                        "name": "Sakae Mizuki"
                    },
                    {
                        "name": "Yugo Murawaki"
                    },
                    {
                        "name": "Akim Mousterou"
                    },
                    {
                        "name": "Ryo Nakamura"
                    },
                    {
                        "name": "Taishi Nakamura"
                    },
                    {
                        "name": "Kouta Nakayama"
                    },
                    {
                        "name": "Tomoka Nakazato"
                    },
                    {
                        "name": "Takuro Niitsuma"
                    },
                    {
                        "name": "Jiro Nishitoba"
                    },
                    {
                        "name": "Yusuke Oda"
                    },
                    {
                        "name": "Hayato Ogawa"
                    },
                    {
                        "name": "Takumi Okamoto"
                    },
                    {
                        "name": "Naoaki Okazaki"
                    },
                    {
                        "name": "Yohei Oseki"
                    },
                    {
                        "name": "Shintaro Ozaki"
                    },
                    {
                        "name": "Koki Ryu"
                    },
                    {
                        "name": "Rafal Rzepka"
                    },
                    {
                        "name": "Keisuke Sakaguchi"
                    },
                    {
                        "name": "Shota Sasaki"
                    },
                    {
                        "name": "Satoshi Sekine"
                    },
                    {
                        "name": "Kohei Suda"
                    },
                    {
                        "name": "Saku Sugawara"
                    },
                    {
                        "name": "Issa Sugiura"
                    },
                    {
                        "name": "Hiroaki Sugiyama"
                    },
                    {
                        "name": "Hisami Suzuki"
                    },
                    {
                        "name": "Jun Suzuki"
                    },
                    {
                        "name": "Toyotaro Suzumura"
                    },
                    {
                        "name": "Kensuke Tachibana"
                    },
                    {
                        "name": "Yu Takagi"
                    },
                    {
                        "name": "Kyosuke Takami"
                    },
                    {
                        "name": "Koichi Takeda"
                    },
                    {
                        "name": "Masashi Takeshita"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    },
                    {
                        "name": "Kenjiro Taura"
                    },
                    {
                        "name": "Arseny Tolmachev"
                    },
                    {
                        "name": "Nobuhiro Ueda"
                    },
                    {
                        "name": "Zhen Wan"
                    },
                    {
                        "name": "Shuntaro Yada"
                    },
                    {
                        "name": "Sakiko Yahata"
                    },
                    {
                        "name": "Yuya Yamamoto"
                    },
                    {
                        "name": "Yusuke Yamauchi"
                    },
                    {
                        "name": "Hitomi Yanaka"
                    },
                    {
                        "name": "Rio Yokota"
                    },
                    {
                        "name": "Koichiro Yoshino"
                    }
                ],
                "author_detail": {
                    "name": "Koichiro Yoshino"
                },
                "author": "Koichiro Yoshino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03963v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03963v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20770v1",
                "updated": "2024-12-30T07:41:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    41,
                    1,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T07:41:01Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    41,
                    1,
                    0,
                    365,
                    0
                ],
                "title": "Humanoid Robot RHP Friends: Seamless Combination of Autonomous and\n  Teleoperated Tasks in a Nursing Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanoid Robot RHP Friends: Seamless Combination of Autonomous and\n  Teleoperated Tasks in a Nursing Context"
                },
                "summary": "This paper describes RHP Friends, a social humanoid robot developed to enable\nassistive robotic deployments in human-coexisting environments. As a use-case\napplication, we present its potential use in nursing by extending its\ncapabilities to operate human devices and tools according to the task and by\nenabling remote assistance operations. To meet a wide variety of tasks and\nsituations in environments designed by and for humans, we developed a system\nthat seamlessly integrates the slim and lightweight robot and several\ntechnologies: locomanipulation, multi-contact motion, teleoperation, and object\ndetection and tracking. We demonstrated the system's usage in a nursing\napplication. The robot efficiently performed the daily task of patient transfer\nand a non-routine task, represented by a request to operate a circuit breaker.\nThis demonstration, held at the 2023 International Robot Exhibition (IREX),\nconducted three times a day over three days.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper describes RHP Friends, a social humanoid robot developed to enable\nassistive robotic deployments in human-coexisting environments. As a use-case\napplication, we present its potential use in nursing by extending its\ncapabilities to operate human devices and tools according to the task and by\nenabling remote assistance operations. To meet a wide variety of tasks and\nsituations in environments designed by and for humans, we developed a system\nthat seamlessly integrates the slim and lightweight robot and several\ntechnologies: locomanipulation, multi-contact motion, teleoperation, and object\ndetection and tracking. We demonstrated the system's usage in a nursing\napplication. The robot efficiently performed the daily task of patient transfer\nand a non-routine task, represented by a request to operate a circuit breaker.\nThis demonstration, held at the 2023 International Robot Exhibition (IREX),\nconducted three times a day over three days."
                },
                "authors": [
                    {
                        "name": "Mehdi Benallegue"
                    },
                    {
                        "name": "Guillaume Lorthioir"
                    },
                    {
                        "name": "Antonin Dallard"
                    },
                    {
                        "name": "Rafael Cisneros-LimÃ³n"
                    },
                    {
                        "name": "Iori Kumagai"
                    },
                    {
                        "name": "Mitsuharu Morisawa"
                    },
                    {
                        "name": "Hiroshi Kaminaga"
                    },
                    {
                        "name": "Masaki Murooka"
                    },
                    {
                        "name": "Antoine Andre"
                    },
                    {
                        "name": "Pierre Gergondet"
                    },
                    {
                        "name": "Kenji Kaneko"
                    },
                    {
                        "name": "Guillaume Caron"
                    },
                    {
                        "name": "Fumio Kanehiro"
                    },
                    {
                        "name": "Abderrahmane Kheddar"
                    },
                    {
                        "name": "Soh Yukizaki"
                    },
                    {
                        "name": "Junichi Karasuyama"
                    },
                    {
                        "name": "Junichi Murakami"
                    },
                    {
                        "name": "Masayuki Kamon"
                    }
                ],
                "author_detail": {
                    "name": "Masayuki Kamon"
                },
                "arxiv_affiliation": "CNRS-AIST JRL, LIRMM",
                "author": "Masayuki Kamon",
                "arxiv_comment": "IEEE Robotics and Automation Magazine, In press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16265v2",
                "updated": "2024-12-30T07:27:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    27,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-20T10:06:11Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    6,
                    11,
                    4,
                    355,
                    0
                ],
                "title": "Autoware.Flex: Human-Instructed Dynamically Reconfigurable Autonomous\n  Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoware.Flex: Human-Instructed Dynamically Reconfigurable Autonomous\n  Driving Systems"
                },
                "summary": "Existing Autonomous Driving Systems (ADS) independently make driving\ndecisions, but they face two significant limitations. First, in complex\nscenarios, ADS may misinterpret the environment and make inappropriate driving\ndecisions. Second, these systems are unable to incorporate human driving\npreferences in their decision-making processes. This paper proposes\nAutoware$.$Flex, a novel ADS system that incorporates human input into the\ndriving process, allowing users to guide the ADS in making more appropriate\ndecisions and ensuring their preferences are satisfied. Achieving this needs to\naddress two key challenges: (1) translating human instructions, expressed in\nnatural language, into a format the ADS can understand, and (2) ensuring these\ninstructions are executed safely and consistently within the ADS' s\ndecision-making framework. For the first challenge, we employ a Large Language\nModel (LLM) assisted by an ADS-specialized knowledge base to enhance\ndomain-specific translation. For the second challenge, we design a validation\nmechanism to ensure that human instructions result in safe and consistent\ndriving behavior. Experiments conducted on both simulators and a real-world\nautonomous vehicle demonstrate that Autoware$.$Flex effectively interprets\nhuman instructions and executes them safely.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Autonomous Driving Systems (ADS) independently make driving\ndecisions, but they face two significant limitations. First, in complex\nscenarios, ADS may misinterpret the environment and make inappropriate driving\ndecisions. Second, these systems are unable to incorporate human driving\npreferences in their decision-making processes. This paper proposes\nAutoware$.$Flex, a novel ADS system that incorporates human input into the\ndriving process, allowing users to guide the ADS in making more appropriate\ndecisions and ensuring their preferences are satisfied. Achieving this needs to\naddress two key challenges: (1) translating human instructions, expressed in\nnatural language, into a format the ADS can understand, and (2) ensuring these\ninstructions are executed safely and consistently within the ADS' s\ndecision-making framework. For the first challenge, we employ a Large Language\nModel (LLM) assisted by an ADS-specialized knowledge base to enhance\ndomain-specific translation. For the second challenge, we design a validation\nmechanism to ensure that human instructions result in safe and consistent\ndriving behavior. Experiments conducted on both simulators and a real-world\nautonomous vehicle demonstrate that Autoware$.$Flex effectively interprets\nhuman instructions and executes them safely."
                },
                "authors": [
                    {
                        "name": "Ziwei Song"
                    },
                    {
                        "name": "Mingsong Lv"
                    },
                    {
                        "name": "Tianchi Ren"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Jen-Ming Wu"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_comment": "14 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09945v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09945v4",
                "updated": "2024-12-30T07:26:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    26,
                    14,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-19T12:34:31Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    12,
                    34,
                    31,
                    0,
                    232,
                    0
                ],
                "title": "Large Language Models for Classical Chinese Poetry Translation:\n  Benchmarking, Evaluating, and Improving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Classical Chinese Poetry Translation:\n  Benchmarking, Evaluating, and Improving"
                },
                "summary": "Different from the traditional translation tasks, classical Chinese poetry\ntranslation requires both adequacy and fluency in translating culturally and\nhistorically significant content and linguistic poetic elegance. Large language\nmodels (LLMs) with impressive multilingual capabilities may bring a ray of hope\nto achieve this extreme translation demand. This paper first introduces a\nsuitable benchmark (PoetMT) where each Chinese poetry has a recognized elegant\ntranslation. Meanwhile, we propose a new metric based on GPT-4 to evaluate the\nextent to which current LLMs can meet these demands. Our empirical evaluation\nreveals that the existing LLMs fall short in the challenging task. Hence, we\npropose a Retrieval-Augmented Machine Translation (RAT) method which\nincorporates knowledge related to classical poetry for advancing the\ntranslation of Chinese Poetry in LLMs. Experimental results show that RAT\nconsistently outperforms all comparison methods regarding wildly used BLEU,\nCOMET, BLEURT, our proposed metric, and human evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Different from the traditional translation tasks, classical Chinese poetry\ntranslation requires both adequacy and fluency in translating culturally and\nhistorically significant content and linguistic poetic elegance. Large language\nmodels (LLMs) with impressive multilingual capabilities may bring a ray of hope\nto achieve this extreme translation demand. This paper first introduces a\nsuitable benchmark (PoetMT) where each Chinese poetry has a recognized elegant\ntranslation. Meanwhile, we propose a new metric based on GPT-4 to evaluate the\nextent to which current LLMs can meet these demands. Our empirical evaluation\nreveals that the existing LLMs fall short in the challenging task. Hence, we\npropose a Retrieval-Augmented Machine Translation (RAT) method which\nincorporates knowledge related to classical poetry for advancing the\ntranslation of Chinese Poetry in LLMs. Experimental results show that RAT\nconsistently outperforms all comparison methods regarding wildly used BLEU,\nCOMET, BLEURT, our proposed metric, and human evaluation."
                },
                "authors": [
                    {
                        "name": "Andong Chen"
                    },
                    {
                        "name": "Lianzhang Lou"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Xuefeng Bai"
                    },
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09945v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09945v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13578v2",
                "updated": "2024-12-30T07:25:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    7,
                    25,
                    13,
                    0,
                    365,
                    0
                ],
                "published": "2024-05-22T12:15:52Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    12,
                    15,
                    52,
                    2,
                    143,
                    0
                ],
                "title": "ConTrans: Weak-to-Strong Alignment Engineering via Concept\n  Transplantation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConTrans: Weak-to-Strong Alignment Engineering via Concept\n  Transplantation"
                },
                "summary": "Ensuring large language models (LLM) behave consistently with human goals,\nvalues, and intentions is crucial for their safety but yet computationally\nexpensive. To reduce the computational cost of alignment training of LLMs,\nespecially for those with a huge number of parameters, and to reutilize learned\nvalue alignment, we propose ConTrans, a novel framework that enables\nweak-to-strong alignment transfer via concept transplantation. From the\nperspective of representation engineering, ConTrans refines concept vectors in\nvalue alignment from a source LLM (usually a weak yet aligned LLM). The refined\nconcept vectors are then reformulated to adapt to the target LLM (usually a\nstrong yet unaligned base LLM) via affine transformation. In the third step,\nConTrans transplants the reformulated concept vectors into the residual stream\nof the target LLM. Experiments demonstrate the successful transplantation of a\nwide range of aligned concepts from 7B models to 13B and 70B models across\nmultiple LLMs and LLM families. Remarkably, ConTrans even surpasses\ninstruction-tuned models in terms of truthfulness. Experiment results validate\nthe effectiveness of both inter-LLM-family and intra-LLM-family concept\ntransplantation. Our work successfully demonstrates an alternative way to\nachieve weak-to-strong alignment generalization and control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring large language models (LLM) behave consistently with human goals,\nvalues, and intentions is crucial for their safety but yet computationally\nexpensive. To reduce the computational cost of alignment training of LLMs,\nespecially for those with a huge number of parameters, and to reutilize learned\nvalue alignment, we propose ConTrans, a novel framework that enables\nweak-to-strong alignment transfer via concept transplantation. From the\nperspective of representation engineering, ConTrans refines concept vectors in\nvalue alignment from a source LLM (usually a weak yet aligned LLM). The refined\nconcept vectors are then reformulated to adapt to the target LLM (usually a\nstrong yet unaligned base LLM) via affine transformation. In the third step,\nConTrans transplants the reformulated concept vectors into the residual stream\nof the target LLM. Experiments demonstrate the successful transplantation of a\nwide range of aligned concepts from 7B models to 13B and 70B models across\nmultiple LLMs and LLM families. Remarkably, ConTrans even surpasses\ninstruction-tuned models in terms of truthfulness. Experiment results validate\nthe effectiveness of both inter-LLM-family and intra-LLM-family concept\ntransplantation. Our work successfully demonstrates an alternative way to\nachieve weak-to-strong alignment generalization and control."
                },
                "authors": [
                    {
                        "name": "Weilong Dong"
                    },
                    {
                        "name": "Xinwei Wu"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Shaoyang Xu"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19478v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19478v2",
                "updated": "2024-12-30T06:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    6,
                    52,
                    12,
                    0,
                    365,
                    0
                ],
                "published": "2024-11-29T05:31:04Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    31,
                    4,
                    4,
                    334,
                    0
                ],
                "title": "Zero-Indexing Internet Search Augmented Generation for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Indexing Internet Search Augmented Generation for Large Language\n  Models"
                },
                "summary": "Retrieval augmented generation has emerged as an effective method to enhance\nlarge language model performance. This approach typically relies on an internal\nretrieval module that uses various indexing mechanisms to manage a static\npre-processed corpus. However, such a paradigm often falls short when it is\nnecessary to integrate the most up-to-date information that has not been\nupdated into the corpus during generative inference time. In this paper, we\nexplore an alternative approach that leverages standard search engine APIs to\ndynamically integrate the latest online information (without maintaining any\nindex for any fixed corpus), thereby improving the quality of generated\ncontent. We design a collaborative LLM-based paradigm, where we include: (i) a\nparser-LLM that determines if the Internet augmented generation is demanded and\nextracts the search keywords if so with a single inference; (ii) a mixed\nranking strategy that re-ranks the retrieved HTML files to eliminate bias\nintroduced from the search engine API; and (iii) an extractor-LLM that can\naccurately and efficiently extract relevant information from the fresh content\nin each HTML file. We conduct extensive empirical studies to evaluate the\nperformance of this Internet search augmented generation paradigm. The\nexperimental results demonstrate that our method generates content with\nsignificantly improved quality. Our system has been successfully deployed in a\nproduction environment to serve 01.AI's generative inference requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation has emerged as an effective method to enhance\nlarge language model performance. This approach typically relies on an internal\nretrieval module that uses various indexing mechanisms to manage a static\npre-processed corpus. However, such a paradigm often falls short when it is\nnecessary to integrate the most up-to-date information that has not been\nupdated into the corpus during generative inference time. In this paper, we\nexplore an alternative approach that leverages standard search engine APIs to\ndynamically integrate the latest online information (without maintaining any\nindex for any fixed corpus), thereby improving the quality of generated\ncontent. We design a collaborative LLM-based paradigm, where we include: (i) a\nparser-LLM that determines if the Internet augmented generation is demanded and\nextracts the search keywords if so with a single inference; (ii) a mixed\nranking strategy that re-ranks the retrieved HTML files to eliminate bias\nintroduced from the search engine API; and (iii) an extractor-LLM that can\naccurately and efficiently extract relevant information from the fresh content\nin each HTML file. We conduct extensive empirical studies to evaluate the\nperformance of this Internet search augmented generation paradigm. The\nexperimental results demonstrate that our method generates content with\nsignificantly improved quality. Our system has been successfully deployed in a\nproduction environment to serve 01.AI's generative inference requests."
                },
                "authors": [
                    {
                        "name": "Guangxin He"
                    },
                    {
                        "name": "Zonghong Dai"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Binqiang Zhao"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Chenyue Li"
                    },
                    {
                        "name": "You Peng"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19478v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19478v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18279v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18279v6",
                "updated": "2024-12-30T05:54:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    54,
                    24,
                    0,
                    365,
                    0
                ],
                "published": "2024-11-27T12:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "Large Language Model-Brained GUI Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Brained GUI Agents: A Survey"
                },
                "summary": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18279v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18279v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20718v1",
                "updated": "2024-12-30T05:18:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    18,
                    55,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T05:18:55Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    18,
                    55,
                    0,
                    365,
                    0
                ],
                "title": "M$^3$oralBench: A MultiModal Moral Benchmark for LVLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M$^3$oralBench: A MultiModal Moral Benchmark for LVLMs"
                },
                "summary": "Recently, large foundation models, including large language models (LLMs) and\nlarge vision-language models (LVLMs), have become essential tools in critical\nfields such as law, finance, and healthcare. As these models increasingly\nintegrate into our daily life, it is necessary to conduct moral evaluation to\nensure that their outputs align with human values and remain within moral\nboundaries. Previous works primarily focus on LLMs, proposing moral datasets\nand benchmarks limited to text modality. However, given the rapid development\nof LVLMs, there is still a lack of multimodal moral evaluation methods. To\nbridge this gap, we introduce M$^3$oralBench, the first MultiModal Moral\nBenchmark for LVLMs. M$^3$oralBench expands the everyday moral scenarios in\nMoral Foundations Vignettes (MFVs) and employs the text-to-image diffusion\nmodel, SD3.0, to create corresponding scenario images. It conducts moral\nevaluation across six moral foundations of Moral Foundations Theory (MFT) and\nencompasses tasks in moral judgement, moral classification, and moral response,\nproviding a comprehensive assessment of model performance in multimodal moral\nunderstanding and reasoning. Extensive experiments on 10 popular open-source\nand closed-source LVLMs demonstrate that M$^3$oralBench is a challenging\nbenchmark, exposing notable moral limitations in current models. Our benchmark\nis publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large foundation models, including large language models (LLMs) and\nlarge vision-language models (LVLMs), have become essential tools in critical\nfields such as law, finance, and healthcare. As these models increasingly\nintegrate into our daily life, it is necessary to conduct moral evaluation to\nensure that their outputs align with human values and remain within moral\nboundaries. Previous works primarily focus on LLMs, proposing moral datasets\nand benchmarks limited to text modality. However, given the rapid development\nof LVLMs, there is still a lack of multimodal moral evaluation methods. To\nbridge this gap, we introduce M$^3$oralBench, the first MultiModal Moral\nBenchmark for LVLMs. M$^3$oralBench expands the everyday moral scenarios in\nMoral Foundations Vignettes (MFVs) and employs the text-to-image diffusion\nmodel, SD3.0, to create corresponding scenario images. It conducts moral\nevaluation across six moral foundations of Moral Foundations Theory (MFT) and\nencompasses tasks in moral judgement, moral classification, and moral response,\nproviding a comprehensive assessment of model performance in multimodal moral\nunderstanding and reasoning. Extensive experiments on 10 popular open-source\nand closed-source LVLMs demonstrate that M$^3$oralBench is a challenging\nbenchmark, exposing notable moral limitations in current models. Our benchmark\nis publicly available."
                },
                "authors": [
                    {
                        "name": "Bei Yan"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "name": "Shiguang Shan"
                    },
                    {
                        "name": "Xilin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xilin Chen"
                },
                "author": "Xilin Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08877v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08877v3",
                "updated": "2024-12-30T05:08:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    8,
                    0,
                    0,
                    365,
                    0
                ],
                "published": "2024-04-13T02:36:40Z",
                "published_parsed": [
                    2024,
                    4,
                    13,
                    2,
                    36,
                    40,
                    5,
                    104,
                    0
                ],
                "title": "Aligning the Objective of LLM-based Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning the Objective of LLM-based Program Repair"
                },
                "summary": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved decent results on automated\nprogram repair (APR). However, the next token prediction training objective of\ndecoder-only LLMs (e.g., GPT-4) is misaligned with the masked span prediction\nobjective of current infilling-style methods, which impedes LLMs from fully\nleveraging pre-trained knowledge for program repair. In addition, while some\nLLMs can locate and repair bugs in certain functions using the related\nartifacts (e.g., test cases), existing methods still depend on statement-level\nfault localization methods to provide a list of buggy hunks for repair. This\nrestriction hinders LLMs from exploring potential patches beyond the given\nlocations.\n  In this paper, we investigate a new approach to adapt LLMs to program repair.\nOur core insight is that LLM's APR capability can be greatly improved by simply\naligning the output to their training objective and allowing them to refine the\nwhole program without first identifying faulty statements. Based on this\ninsight, we designed D4C, a straightforward prompting framework for APR. D4C\ncan repair 180 bugs correctly in Defects4J, with each patch being sampled only\n10 times. This surpasses the SOTA APR methods with perfect fault localization\nby 10% and reduces the patch sampling number by 90%. Our findings reveal that\n(1) objective alignment is crucial for fully exploiting LLM's pre-trained\ncapability, and (2) replacing the traditional localize-buggy-hunks-then-repair\nworkflow with direct debugging is more effective for LLM-based APR methods.\nThus, we believe this paper introduces a new mindset for harnessing LLMs in\nAPR."
                },
                "authors": [
                    {
                        "name": "Junjielong Xu"
                    },
                    {
                        "name": "Ying Fu"
                    },
                    {
                        "name": "Shin Hwei Tan"
                    },
                    {
                        "name": "Pinjia He"
                    }
                ],
                "author_detail": {
                    "name": "Pinjia He"
                },
                "author": "Pinjia He",
                "arxiv_comment": "Accepted by ICSE'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08877v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08877v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20715v1",
                "updated": "2024-12-30T05:07:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    7,
                    34,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T05:07:34Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    7,
                    34,
                    0,
                    365,
                    0
                ],
                "title": "ChartAdapter: Large Vision-Language Model for Chart Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChartAdapter: Large Vision-Language Model for Chart Summarization"
                },
                "summary": "Chart summarization, which focuses on extracting key information from charts\nand interpreting it in natural language, is crucial for generating and\ndelivering insights through effective and accessible data analysis. Traditional\nmethods for chart understanding and summarization often rely on multi-stage\npipelines, which may produce suboptimal semantic alignment between visual and\ntextual information. In comparison, recently developed LLM-based methods are\nmore dependent on the capability of foundation images or languages, while\nignoring the characteristics of chart data and its relevant challenges. To\naddress these limitations, we propose ChartAdapter, a novel lightweight\ntransformer module designed to bridge the gap between charts and textual\nsummaries. ChartAdapter employs learnable query vectors to extract implicit\nsemantics from chart data and incorporates a cross-modal alignment projector to\nenhance vision-to-language generative learning. By integrating ChartAdapter\nwith an LLM, we enable end-to-end training and efficient chart summarization.\nTo further enhance the training, we introduce a three-stage hierarchical\ntraining procedure and develop a large-scale dataset specifically curated for\nchart summarization, comprising 190,618 samples. Experimental results on the\nstandard Chart-to-Text testing set demonstrate that our approach significantly\noutperforms existing methods, including state-of-the-art models, in generating\nhigh-quality chart summaries. Ablation studies further validate the\neffectiveness of key components in ChartAdapter. This work highlights the\npotential of tailored LLM-based approaches to advance chart understanding and\nsets a strong foundation for future research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chart summarization, which focuses on extracting key information from charts\nand interpreting it in natural language, is crucial for generating and\ndelivering insights through effective and accessible data analysis. Traditional\nmethods for chart understanding and summarization often rely on multi-stage\npipelines, which may produce suboptimal semantic alignment between visual and\ntextual information. In comparison, recently developed LLM-based methods are\nmore dependent on the capability of foundation images or languages, while\nignoring the characteristics of chart data and its relevant challenges. To\naddress these limitations, we propose ChartAdapter, a novel lightweight\ntransformer module designed to bridge the gap between charts and textual\nsummaries. ChartAdapter employs learnable query vectors to extract implicit\nsemantics from chart data and incorporates a cross-modal alignment projector to\nenhance vision-to-language generative learning. By integrating ChartAdapter\nwith an LLM, we enable end-to-end training and efficient chart summarization.\nTo further enhance the training, we introduce a three-stage hierarchical\ntraining procedure and develop a large-scale dataset specifically curated for\nchart summarization, comprising 190,618 samples. Experimental results on the\nstandard Chart-to-Text testing set demonstrate that our approach significantly\noutperforms existing methods, including state-of-the-art models, in generating\nhigh-quality chart summaries. Ablation studies further validate the\neffectiveness of key components in ChartAdapter. This work highlights the\npotential of tailored LLM-based approaches to advance chart understanding and\nsets a strong foundation for future research in this area."
                },
                "authors": [
                    {
                        "name": "Peixin Xu"
                    },
                    {
                        "name": "Yujuan Ding"
                    },
                    {
                        "name": "Wenqi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Fan"
                },
                "author": "Wenqi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20906v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20906v3",
                "updated": "2024-12-30T04:46:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    4,
                    46,
                    7,
                    0,
                    365,
                    0
                ],
                "published": "2024-07-30T15:26:36Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    15,
                    26,
                    36,
                    1,
                    212,
                    0
                ],
                "title": "Automated Review Generation Method Based on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Review Generation Method Based on Large Language Models"
                },
                "summary": "Literature research, vital for scientific work, faces the challenge of\nsurging information volumes exceeding researchers' processing capabilities. We\npresent an automated review generation method based on large language models\n(LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our\nstatistically validated evaluation framework demonstrates that the generated\nreviews match or exceed manual quality, offering broad applicability across\nresearch fields without requiring users' domain knowledge. Applied to propane\ndehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles,\naveraging seconds per article per LLM account, producing comprehensive reviews\nspanning 35 topics, with extended analysis of 1041 articles providing insights\ninto catalysts' properties. Through multi-layered quality control, we\neffectively mitigated LLMs' hallucinations, with expert verification confirming\naccuracy and citation integrity while demonstrating hallucination risks reduced\nto below 0.5\\% with 95\\% confidence. Released Windows application enables\none-click review generation, enhancing research productivity and literature\nrecommendation efficiency while setting the stage for broader scientific\nexplorations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature research, vital for scientific work, faces the challenge of\nsurging information volumes exceeding researchers' processing capabilities. We\npresent an automated review generation method based on large language models\n(LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our\nstatistically validated evaluation framework demonstrates that the generated\nreviews match or exceed manual quality, offering broad applicability across\nresearch fields without requiring users' domain knowledge. Applied to propane\ndehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles,\naveraging seconds per article per LLM account, producing comprehensive reviews\nspanning 35 topics, with extended analysis of 1041 articles providing insights\ninto catalysts' properties. Through multi-layered quality control, we\neffectively mitigated LLMs' hallucinations, with expert verification confirming\naccuracy and citation integrity while demonstrating hallucination risks reduced\nto below 0.5\\% with 95\\% confidence. Released Windows application enables\none-click review generation, enhancing research productivity and literature\nrecommendation efficiency while setting the stage for broader scientific\nexplorations."
                },
                "authors": [
                    {
                        "name": "Shican Wu"
                    },
                    {
                        "name": "Xiao Ma"
                    },
                    {
                        "name": "Dehui Luo"
                    },
                    {
                        "name": "Lulu Li"
                    },
                    {
                        "name": "Xiangcheng Shi"
                    },
                    {
                        "name": "Xin Chang"
                    },
                    {
                        "name": "Xiaoyun Lin"
                    },
                    {
                        "name": "Ran Luo"
                    },
                    {
                        "name": "Chunlei Pei"
                    },
                    {
                        "name": "Changyin Du"
                    },
                    {
                        "name": "Zhi-Jian Zhao"
                    },
                    {
                        "name": "Jinlong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Gong"
                },
                "author": "Jinlong Gong",
                "arxiv_comment": "21 pages, 5 figures, 1 tables Code:\n  https://github.com/TJU-ECAT-AI/AutomaticReviewGeneration Data:\n  https://github.com/TJU-ECAT-AI/AutomaticReviewGenerationData This research\n  has been invited for a Short Oral presentation at the 18th ICC -\n  International Congress on Catalysis, taking place in Lyon, France from July\n  14-19, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20906v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20906v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.09881v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.09881v4",
                "updated": "2024-12-30T04:27:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    4,
                    27,
                    5,
                    0,
                    365,
                    0
                ],
                "published": "2023-10-15T16:40:19Z",
                "published_parsed": [
                    2023,
                    10,
                    15,
                    16,
                    40,
                    19,
                    6,
                    288,
                    0
                ],
                "title": "In-Context Learning with Iterative Demonstration Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Context Learning with Iterative Demonstration Selection"
                },
                "summary": "Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated strong few-shot learning ability via in-context learning (ICL).\nHowever, the performance of ICL has been shown to be highly sensitive to the\nselection of few-shot demonstrations. Selecting the most suitable examples as\ncontext remains an ongoing challenge and an open problem. Existing literature\nhas highlighted the importance of selecting examples that are diverse or\nsemantically similar to the test sample while ignoring the fact that the\noptimal selection dimension, i.e., diversity or similarity, is task-specific.\nBased on how the test sample is answered, we propose Iterative Demonstration\nSelection (IDS) to leverage the merits of both dimensions. Using zero-shot\nchain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples\nthat are diverse but still strongly correlated with the test sample as ICL\ndemonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample\nbefore demonstration selection. The output reasoning path is then used to\nchoose demonstrations that are prepended to the test sample for inference. The\ngenerated answer is followed by its corresponding reasoning path for extracting\na new set of demonstrations in the next iteration. After several iterations,\nIDS adopts majority voting to obtain the final result. Through extensive\nexperiments on tasks including reasoning, question answering, and topic\nclassification, we demonstrate that IDS can consistently outperform existing\nICL demonstration selection methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated strong few-shot learning ability via in-context learning (ICL).\nHowever, the performance of ICL has been shown to be highly sensitive to the\nselection of few-shot demonstrations. Selecting the most suitable examples as\ncontext remains an ongoing challenge and an open problem. Existing literature\nhas highlighted the importance of selecting examples that are diverse or\nsemantically similar to the test sample while ignoring the fact that the\noptimal selection dimension, i.e., diversity or similarity, is task-specific.\nBased on how the test sample is answered, we propose Iterative Demonstration\nSelection (IDS) to leverage the merits of both dimensions. Using zero-shot\nchain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples\nthat are diverse but still strongly correlated with the test sample as ICL\ndemonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample\nbefore demonstration selection. The output reasoning path is then used to\nchoose demonstrations that are prepended to the test sample for inference. The\ngenerated answer is followed by its corresponding reasoning path for extracting\na new set of demonstrations in the next iteration. After several iterations,\nIDS adopts majority voting to obtain the final result. Through extensive\nexperiments on tasks including reasoning, question answering, and topic\nclassification, we demonstrate that IDS can consistently outperform existing\nICL demonstration selection methods."
                },
                "authors": [
                    {
                        "name": "Chengwei Qin"
                    },
                    {
                        "name": "Aston Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Anirudh Dagar"
                    },
                    {
                        "name": "Wenming Ye"
                    }
                ],
                "author_detail": {
                    "name": "Wenming Ye"
                },
                "author": "Wenming Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.09881v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.09881v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18819v2",
                "updated": "2024-12-30T04:15:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    4,
                    15,
                    42,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-25T08:17:37Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    8,
                    17,
                    37,
                    2,
                    360,
                    0
                ],
                "title": "LLM-assisted Vector Similarity Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-assisted Vector Similarity Search"
                },
                "summary": "As data retrieval demands become increasingly complex, traditional search\nmethods often fall short in addressing nuanced and conceptual queries. Vector\nsimilarity search has emerged as a promising technique for finding semantically\nsimilar information efficiently. However, its effectiveness diminishes when\nhandling intricate queries with contextual nuances. This paper explores a\nhybrid approach combining vector similarity search with Large Language Models\n(LLMs) to enhance search accuracy and relevance. The proposed two-step solution\nfirst employs vector similarity search to shortlist potential matches, followed\nby an LLM for context-aware ranking of the results. Experiments on structured\ndatasets demonstrate that while vector similarity search alone performs well\nfor straightforward queries, the LLM-assisted approach excels in processing\ncomplex queries involving constraints, negations, or conceptual requirements.\nBy leveraging the natural language understanding capabilities of LLMs, this\nmethod improves the accuracy of search results for complex tasks without\nsacrificing efficiency. We also discuss real-world applications and propose\ndirections for future research to refine and scale this technique for diverse\ndatasets and use cases.\n  Original article:\nhttps://engineering.grab.com/llm-assisted-vector-similarity-search",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As data retrieval demands become increasingly complex, traditional search\nmethods often fall short in addressing nuanced and conceptual queries. Vector\nsimilarity search has emerged as a promising technique for finding semantically\nsimilar information efficiently. However, its effectiveness diminishes when\nhandling intricate queries with contextual nuances. This paper explores a\nhybrid approach combining vector similarity search with Large Language Models\n(LLMs) to enhance search accuracy and relevance. The proposed two-step solution\nfirst employs vector similarity search to shortlist potential matches, followed\nby an LLM for context-aware ranking of the results. Experiments on structured\ndatasets demonstrate that while vector similarity search alone performs well\nfor straightforward queries, the LLM-assisted approach excels in processing\ncomplex queries involving constraints, negations, or conceptual requirements.\nBy leveraging the natural language understanding capabilities of LLMs, this\nmethod improves the accuracy of search results for complex tasks without\nsacrificing efficiency. We also discuss real-world applications and propose\ndirections for future research to refine and scale this technique for diverse\ndatasets and use cases.\n  Original article:\nhttps://engineering.grab.com/llm-assisted-vector-similarity-search"
                },
                "authors": [
                    {
                        "name": "Md Riyadh"
                    },
                    {
                        "name": "Muqi Li"
                    },
                    {
                        "name": "Felix Haryanto Lie"
                    },
                    {
                        "name": "Jia Long Loh"
                    },
                    {
                        "name": "Haotian Mi"
                    },
                    {
                        "name": "Sayam Bohra"
                    }
                ],
                "author_detail": {
                    "name": "Sayam Bohra"
                },
                "author": "Sayam Bohra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14368v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14368v3",
                "updated": "2024-12-30T04:09:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    4,
                    9,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-18T22:04:56Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    4,
                    56,
                    2,
                    353,
                    0
                ],
                "title": "Memorization Over Reasoning? Exposing and Mitigating Verbatim\n  Memorization in Large Language Models' Character Understanding Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization Over Reasoning? Exposing and Mitigating Verbatim\n  Memorization in Large Language Models' Character Understanding Evaluation"
                },
                "summary": "Recently, Large Language Models (LLMs) have shown impressive performance in\ncharacter understanding tasks, such as analyzing the roles, personalities, and\nrelationships of fictional characters. However, the extensive pre-training\ncorpora used by LLMs raise concerns that they may rely on memorizing popular\nfictional works rather than genuinely understanding and reasoning about them.\nIn this work, we argue that 'gist memory'-capturing essential meaning - should\nbe the primary mechanism for character understanding tasks, as opposed to\n'verbatim memory' - exact match of a string. We introduce a simple yet\neffective method to mitigate mechanized memorization in character understanding\nevaluations while preserving the essential implicit cues needed for\ncomprehension and reasoning. Our approach reduces memorization-driven\nperformance on popular fictional works from 96% accuracy to 72% and results in\nup to an 18% drop in accuracy across various character understanding tasks.\nThese findings underscore the issue of data contamination in existing\nbenchmarks, which often measure memorization rather than true character\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have shown impressive performance in\ncharacter understanding tasks, such as analyzing the roles, personalities, and\nrelationships of fictional characters. However, the extensive pre-training\ncorpora used by LLMs raise concerns that they may rely on memorizing popular\nfictional works rather than genuinely understanding and reasoning about them.\nIn this work, we argue that 'gist memory'-capturing essential meaning - should\nbe the primary mechanism for character understanding tasks, as opposed to\n'verbatim memory' - exact match of a string. We introduce a simple yet\neffective method to mitigate mechanized memorization in character understanding\nevaluations while preserving the essential implicit cues needed for\ncomprehension and reasoning. Our approach reduces memorization-driven\nperformance on popular fictional works from 96% accuracy to 72% and results in\nup to an 18% drop in accuracy across various character understanding tasks.\nThese findings underscore the issue of data contamination in existing\nbenchmarks, which often measure memorization rather than true character\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Yuxuan Jiang"
                    },
                    {
                        "name": "Francis Ferraro"
                    }
                ],
                "author_detail": {
                    "name": "Francis Ferraro"
                },
                "author": "Francis Ferraro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14368v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14368v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20694v1",
                "updated": "2024-12-30T04:05:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    4,
                    5,
                    22,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T04:05:22Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    4,
                    5,
                    22,
                    0,
                    365,
                    0
                ],
                "title": "UBER: Uncertainty-Based Evolution with Large Language Models for\n  Automatic Heuristic Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UBER: Uncertainty-Based Evolution with Large Language Models for\n  Automatic Heuristic Design"
                },
                "summary": "NP-hard problem-solving traditionally relies on heuristics, but manually\ncrafting effective heuristics for complex problems remains challenging. While\nrecent work like FunSearch has demonstrated that large language models (LLMs)\ncan be leveraged for heuristic design in evolutionary algorithm (EA)\nframeworks, their potential is not fully realized due to its deficiency in\nexploitation and exploration. We present UBER (Uncertainty-Based Evolution for\nRefinement), a method that enhances LLM+EA methods for automatic heuristic\ndesign by integrating uncertainty on top of the FunSearch framework. UBER\nintroduces two key innovations: an Uncertainty-Inclusive Evolution Process\n(UIEP) for adaptive exploration-exploitation balance, and a principled\nUncertainty-Inclusive Island Reset (UIIS) strategy for maintaining population\ndiversity. Through extensive experiments on challenging NP-complete problems,\nUBER demonstrates significant improvements over FunSearch. Our work provides a\nnew direction for the synergy of LLMs and EA, advancing the field of automatic\nheuristic design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NP-hard problem-solving traditionally relies on heuristics, but manually\ncrafting effective heuristics for complex problems remains challenging. While\nrecent work like FunSearch has demonstrated that large language models (LLMs)\ncan be leveraged for heuristic design in evolutionary algorithm (EA)\nframeworks, their potential is not fully realized due to its deficiency in\nexploitation and exploration. We present UBER (Uncertainty-Based Evolution for\nRefinement), a method that enhances LLM+EA methods for automatic heuristic\ndesign by integrating uncertainty on top of the FunSearch framework. UBER\nintroduces two key innovations: an Uncertainty-Inclusive Evolution Process\n(UIEP) for adaptive exploration-exploitation balance, and a principled\nUncertainty-Inclusive Island Reset (UIIS) strategy for maintaining population\ndiversity. Through extensive experiments on challenging NP-complete problems,\nUBER demonstrates significant improvements over FunSearch. Our work provides a\nnew direction for the synergy of LLMs and EA, advancing the field of automatic\nheuristic design."
                },
                "authors": [
                    {
                        "name": "Zijie Chen"
                    },
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Renjun Xu"
                    },
                    {
                        "name": "Lili Pan"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20682v1",
                "updated": "2024-12-30T03:26:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    26,
                    53,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:26:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    26,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "Learning to Rank Pre-trained Vision-Language Models for Downstream Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Rank Pre-trained Vision-Language Models for Downstream Tasks"
                },
                "summary": "Vision language models (VLMs) like CLIP show stellar zero-shot capability on\nclassification benchmarks. However, selecting the VLM with the highest\nperformance on the unlabeled downstream task is non-trivial. Existing VLM\nselection methods focus on the class-name-only setting, relying on a supervised\nlarge-scale dataset and large language models, which may not be accessible or\nfeasible during deployment. This paper introduces the problem of\n\\textbf{unsupervised vision-language model selection}, where only unsupervised\ndownstream datasets are available, with no additional information provided. To\nsolve this problem, we propose a method termed Visual-tExtual Graph Alignment\n(VEGA), to select VLMs without any annotations by measuring the alignment of\nthe VLM between the two modalities on the downstream task. VEGA is motivated by\nthe pretraining paradigm of VLMs, which aligns features with the same semantics\nfrom the visual and textual modalities, thereby mapping both modalities into a\nshared representation space. Specifically, we first construct two graphs on the\nvision and textual features, respectively. VEGA is then defined as the overall\nsimilarity between the visual and textual graphs at both node and edge levels.\nExtensive experiments across three different benchmarks, covering a variety of\napplication scenarios and downstream datasets, demonstrate that VEGA\nconsistently provides reliable and accurate estimates of VLMs' performance on\nunlabeled downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision language models (VLMs) like CLIP show stellar zero-shot capability on\nclassification benchmarks. However, selecting the VLM with the highest\nperformance on the unlabeled downstream task is non-trivial. Existing VLM\nselection methods focus on the class-name-only setting, relying on a supervised\nlarge-scale dataset and large language models, which may not be accessible or\nfeasible during deployment. This paper introduces the problem of\n\\textbf{unsupervised vision-language model selection}, where only unsupervised\ndownstream datasets are available, with no additional information provided. To\nsolve this problem, we propose a method termed Visual-tExtual Graph Alignment\n(VEGA), to select VLMs without any annotations by measuring the alignment of\nthe VLM between the two modalities on the downstream task. VEGA is motivated by\nthe pretraining paradigm of VLMs, which aligns features with the same semantics\nfrom the visual and textual modalities, thereby mapping both modalities into a\nshared representation space. Specifically, we first construct two graphs on the\nvision and textual features, respectively. VEGA is then defined as the overall\nsimilarity between the visual and textual graphs at both node and edge levels.\nExtensive experiments across three different benchmarks, covering a variety of\napplication scenarios and downstream datasets, demonstrate that VEGA\nconsistently provides reliable and accurate estimates of VLMs' performance on\nunlabeled downstream tasks."
                },
                "authors": [
                    {
                        "name": "Yuhe Ding"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Aihua Zheng"
                    },
                    {
                        "name": "Qin Xu"
                    },
                    {
                        "name": "Jian Liang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Liang"
                },
                "author": "Jian Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18619v2",
                "updated": "2024-12-30T03:00:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    0,
                    30,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T05:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    5,
                    2,
                    25,
                    0,
                    351,
                    0
                ],
                "title": "Next Token Prediction Towards Multimodal Intelligence: A Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Token Prediction Towards Multimodal Intelligence: A Comprehensive\n  Survey"
                },
                "summary": "Building on the foundations of language modeling in natural language\nprocessing, Next Token Prediction (NTP) has evolved into a versatile training\nobjective for machine learning tasks across various modalities, achieving\nconsiderable success. As Large Language Models (LLMs) have advanced to unify\nunderstanding and generation tasks within the textual modality, recent research\nhas shown that tasks from different modalities can also be effectively\nencapsulated within the NTP framework, transforming the multimodal information\ninto tokens and predict the next one given the context. This survey introduces\na comprehensive taxonomy that unifies both understanding and generation within\nmultimodal learning through the lens of NTP. The proposed taxonomy covers five\nkey aspects: Multimodal tokenization, MMNTP model architectures, unified task\nrepresentation, datasets \\& evaluation, and open challenges. This new taxonomy\naims to aid researchers in their exploration of multimodal intelligence. An\nassociated GitHub repository collecting the latest papers and repos is\navailable at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building on the foundations of language modeling in natural language\nprocessing, Next Token Prediction (NTP) has evolved into a versatile training\nobjective for machine learning tasks across various modalities, achieving\nconsiderable success. As Large Language Models (LLMs) have advanced to unify\nunderstanding and generation tasks within the textual modality, recent research\nhas shown that tasks from different modalities can also be effectively\nencapsulated within the NTP framework, transforming the multimodal information\ninto tokens and predict the next one given the context. This survey introduces\na comprehensive taxonomy that unifies both understanding and generation within\nmultimodal learning through the lens of NTP. The proposed taxonomy covers five\nkey aspects: Multimodal tokenization, MMNTP model architectures, unified task\nrepresentation, datasets \\& evaluation, and open challenges. This new taxonomy\naims to aid researchers in their exploration of multimodal intelligence. An\nassociated GitHub repository collecting the latest papers and repos is\navailable at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction"
                },
                "authors": [
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Shuhuai Ren"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Yunshui Li"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Hongcheng Guo"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Ruoyu Wu"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Lingwei Meng"
                    },
                    {
                        "name": "Shujie Hu"
                    },
                    {
                        "name": "Yulong Chen"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Shuai Bai"
                    },
                    {
                        "name": "Andreas Vlachos"
                    },
                    {
                        "name": "Xu Tan"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Aaron Yee"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "69 papes, 18 figures, repo at\n  https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20662v1",
                "updated": "2024-12-30T02:40:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    2,
                    40,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T02:40:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    2,
                    40,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "Enhancing Table Recognition with Vision LLMs: A Benchmark and\n  Neighbor-Guided Toolchain Reasoner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Table Recognition with Vision LLMs: A Benchmark and\n  Neighbor-Guided Toolchain Reasoner"
                },
                "summary": "Pre-trained foundation models have recently significantly progressed in\nstructured table understanding and reasoning. However, despite advancements in\nareas such as table semantic understanding and table question answering,\nrecognizing the structure and content of unstructured tables using Vision Large\nLanguage Models (VLLMs) remains under-explored. In this work, we address this\nresearch gap by employing VLLMs in a training-free reasoning paradigm. First,\nwe design a benchmark with various hierarchical dimensions relevant to table\nrecognition. Subsequently, we conduct in-depth evaluations using pre-trained\nVLLMs, finding that low-quality image input is a significant bottleneck in the\nrecognition process. Drawing inspiration from these findings, we propose the\nNeighbor-Guided Toolchain Reasoner (NGTR) framework, which is characterized by\nintegrating multiple lightweight models for low-level visual processing\noperations aimed at mitigating issues with low-quality input images.\nSpecifically, we utilize a neighbor retrieval mechanism to guide the generation\nof multiple tool invocation plans, transferring tool selection experiences from\nsimilar neighbors to the given input, thereby facilitating suitable tool\nselection. Additionally, we introduce a reflection module to supervise the tool\ninvocation process. Extensive experiments on public table recognition datasets\ndemonstrate that our approach significantly enhances the recognition\ncapabilities of the vanilla VLLMs. We believe that the designed benchmark and\nthe proposed NGTR framework could provide an alternative solution in table\nrecognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained foundation models have recently significantly progressed in\nstructured table understanding and reasoning. However, despite advancements in\nareas such as table semantic understanding and table question answering,\nrecognizing the structure and content of unstructured tables using Vision Large\nLanguage Models (VLLMs) remains under-explored. In this work, we address this\nresearch gap by employing VLLMs in a training-free reasoning paradigm. First,\nwe design a benchmark with various hierarchical dimensions relevant to table\nrecognition. Subsequently, we conduct in-depth evaluations using pre-trained\nVLLMs, finding that low-quality image input is a significant bottleneck in the\nrecognition process. Drawing inspiration from these findings, we propose the\nNeighbor-Guided Toolchain Reasoner (NGTR) framework, which is characterized by\nintegrating multiple lightweight models for low-level visual processing\noperations aimed at mitigating issues with low-quality input images.\nSpecifically, we utilize a neighbor retrieval mechanism to guide the generation\nof multiple tool invocation plans, transferring tool selection experiences from\nsimilar neighbors to the given input, thereby facilitating suitable tool\nselection. Additionally, we introduce a reflection module to supervise the tool\ninvocation process. Extensive experiments on public table recognition datasets\ndemonstrate that our approach significantly enhances the recognition\ncapabilities of the vanilla VLLMs. We believe that the designed benchmark and\nthe proposed NGTR framework could provide an alternative solution in table\nrecognition."
                },
                "authors": [
                    {
                        "name": "Yitong Zhou"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Qingyang Mao"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Feiyang Xu"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06790v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06790v2",
                "updated": "2024-12-30T02:14:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    2,
                    14,
                    18,
                    0,
                    365,
                    0
                ],
                "published": "2024-11-11T08:36:49Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    8,
                    36,
                    49,
                    0,
                    316,
                    0
                ],
                "title": "Large-scale moral machine experiment on large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale moral machine experiment on large language models"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) and their potential\nintegration into autonomous driving systems necessitates understanding their\nmoral decision-making capabilities. While our previous study examined four\nprominent LLMs using the Moral Machine experimental framework, the dynamic\nlandscape of LLM development demands a more comprehensive analysis. Here, we\nevaluate moral judgments across 52 different LLMs, including multiple versions\nof proprietary models (GPT, Claude, Gemini) and open-source alternatives\n(Llama, Gemma), to assess their alignment with human moral preferences in\nautonomous driving scenarios. Using a conjoint analysis framework, we evaluated\nhow closely LLM responses aligned with human preferences in ethical dilemmas\nand examined the effects of model size, updates, and architecture. Results\nshowed that proprietary models and open-source models exceeding 10 billion\nparameters demonstrated relatively close alignment with human judgments, with a\nsignificant negative correlation between model size and distance from human\njudgments in open-source models. However, model updates did not consistently\nimprove alignment with human preferences, and many LLMs showed excessive\nemphasis on specific ethical principles. These findings suggest that while\nincreasing model size may naturally lead to more human-like moral judgments,\npractical implementation in autonomous driving systems requires careful\nconsideration of the trade-off between judgment quality and computational\nefficiency. Our comprehensive analysis provides crucial insights for the\nethical design of autonomous systems and highlights the importance of\nconsidering cultural contexts in AI moral decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) and their potential\nintegration into autonomous driving systems necessitates understanding their\nmoral decision-making capabilities. While our previous study examined four\nprominent LLMs using the Moral Machine experimental framework, the dynamic\nlandscape of LLM development demands a more comprehensive analysis. Here, we\nevaluate moral judgments across 52 different LLMs, including multiple versions\nof proprietary models (GPT, Claude, Gemini) and open-source alternatives\n(Llama, Gemma), to assess their alignment with human moral preferences in\nautonomous driving scenarios. Using a conjoint analysis framework, we evaluated\nhow closely LLM responses aligned with human preferences in ethical dilemmas\nand examined the effects of model size, updates, and architecture. Results\nshowed that proprietary models and open-source models exceeding 10 billion\nparameters demonstrated relatively close alignment with human judgments, with a\nsignificant negative correlation between model size and distance from human\njudgments in open-source models. However, model updates did not consistently\nimprove alignment with human preferences, and many LLMs showed excessive\nemphasis on specific ethical principles. These findings suggest that while\nincreasing model size may naturally lead to more human-like moral judgments,\npractical implementation in autonomous driving systems requires careful\nconsideration of the trade-off between judgment quality and computational\nefficiency. Our comprehensive analysis provides crucial insights for the\nethical design of autonomous systems and highlights the importance of\nconsidering cultural contexts in AI moral decision-making."
                },
                "authors": [
                    {
                        "name": "Muhammad Shahrul Zaim bin Ahmad"
                    },
                    {
                        "name": "Kazuhiro Takemoto"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhiro Takemoto"
                },
                "author": "Kazuhiro Takemoto",
                "arxiv_comment": "21 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06790v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06790v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22316v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22316v2",
                "updated": "2024-12-30T01:48:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    1,
                    48,
                    26,
                    0,
                    365,
                    0
                ],
                "published": "2024-10-29T17:55:00Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    55,
                    0,
                    1,
                    303,
                    0
                ],
                "title": "Understanding Synthetic Context Extension via Retrieval Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Synthetic Context Extension via Retrieval Heads"
                },
                "summary": "Long-context LLMs are increasingly in demand for applications such as\nretrieval-augmented generation. To defray the cost of pretraining LLMs over\nlong contexts, recent work takes an approach of synthetic context extension:\nfine-tuning LLMs with synthetically generated long-context data in a\npost-training stage. However, it remains unclear how and why this synthetic\ncontext extension imparts abilities for downstream long-context tasks. In this\npaper, we investigate fine-tuning on synthetic data for three long-context\ntasks that require retrieval and reasoning. We vary the realism of \"needle\"\nconcepts to be retrieved and diversity of the surrounding \"haystack\" context,\nfrom using LLMs to construct synthetic documents to using templated relations\nand creating symbolic datasets. We find that models trained on synthetic data\nfall short of the real data, but surprisingly, the mismatch can be interpreted\nand even predicted in terms of a special set of attention heads that are\nresponsible for retrieval over long context, retrieval heads (Wu et al., 2024).\nThe retrieval heads learned on synthetic data have high overlap with retrieval\nheads learned on real data, and there is a strong correlation between the\nrecall of heads learned and the downstream performance of a model. Furthermore,\nwith attention knockout and activation patching, we mechanistically show that\nretrieval heads are necessary and explain model performance, although they are\nnot totally sufficient. Our results shed light on how to interpret synthetic\ndata fine-tuning performance and how to approach creating better data for\nlearning real-world capabilities over long contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs are increasingly in demand for applications such as\nretrieval-augmented generation. To defray the cost of pretraining LLMs over\nlong contexts, recent work takes an approach of synthetic context extension:\nfine-tuning LLMs with synthetically generated long-context data in a\npost-training stage. However, it remains unclear how and why this synthetic\ncontext extension imparts abilities for downstream long-context tasks. In this\npaper, we investigate fine-tuning on synthetic data for three long-context\ntasks that require retrieval and reasoning. We vary the realism of \"needle\"\nconcepts to be retrieved and diversity of the surrounding \"haystack\" context,\nfrom using LLMs to construct synthetic documents to using templated relations\nand creating symbolic datasets. We find that models trained on synthetic data\nfall short of the real data, but surprisingly, the mismatch can be interpreted\nand even predicted in terms of a special set of attention heads that are\nresponsible for retrieval over long context, retrieval heads (Wu et al., 2024).\nThe retrieval heads learned on synthetic data have high overlap with retrieval\nheads learned on real data, and there is a strong correlation between the\nrecall of heads learned and the downstream performance of a model. Furthermore,\nwith attention knockout and activation patching, we mechanistically show that\nretrieval heads are necessary and explain model performance, although they are\nnot totally sufficient. Our results shed light on how to interpret synthetic\ndata fine-tuning performance and how to approach creating better data for\nlearning real-world capabilities over long contexts."
                },
                "authors": [
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Fangcong Yin"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22316v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22316v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02059v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02059v3",
                "updated": "2024-12-30T01:42:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    1,
                    42,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2023-10-03T14:01:28Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    14,
                    1,
                    28,
                    1,
                    276,
                    0
                ],
                "title": "Security Weaknesses of Copilot-Generated Code in GitHub Projects: An\n  Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security Weaknesses of Copilot-Generated Code in GitHub Projects: An\n  Empirical Study"
                },
                "summary": "Modern code generation tools utilizing AI models like Large Language Models\n(LLMs) have gained increased popularity due to their ability to produce\nfunctional code. However, their usage presents security challenges, often\nresulting in insecure code merging into the code base. Thus, evaluating the\nquality of generated code, especially its security, is crucial. While prior\nresearch explored various aspects of code generation, the focus on security has\nbeen limited, mostly examining code produced in controlled environments rather\nthan open source development scenarios. To address this gap, we conducted an\nempirical study, analyzing code snippets generated by GitHub Copilot and two\nother AI code generation tools (i.e., CodeWhisperer and Codeium) from GitHub\nprojects. Our analysis identified 733 snippets, revealing a high likelihood of\nsecurity weaknesses, with 29.5% of Python and 24.2% of JavaScript snippets\naffected. These issues span 43 Common Weakness Enumeration (CWE) categories,\nincluding significant ones like CWE-330: Use of Insufficiently Random Values,\nCWE-94: Improper Control of Generation of Code, and CWE-79: Cross-site\nScripting. Notably, eight of those CWEs are among the 2023 CWE Top-25,\nhighlighting their severity. We further examined using Copilot Chat to fix\nsecurity issues in Copilot-generated code by providing Copilot Chat with\nwarning messages from the static analysis tools, and up to 55.5% of the\nsecurity issues can be fixed. We finally provide the suggestions for mitigating\nsecurity issues in generated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern code generation tools utilizing AI models like Large Language Models\n(LLMs) have gained increased popularity due to their ability to produce\nfunctional code. However, their usage presents security challenges, often\nresulting in insecure code merging into the code base. Thus, evaluating the\nquality of generated code, especially its security, is crucial. While prior\nresearch explored various aspects of code generation, the focus on security has\nbeen limited, mostly examining code produced in controlled environments rather\nthan open source development scenarios. To address this gap, we conducted an\nempirical study, analyzing code snippets generated by GitHub Copilot and two\nother AI code generation tools (i.e., CodeWhisperer and Codeium) from GitHub\nprojects. Our analysis identified 733 snippets, revealing a high likelihood of\nsecurity weaknesses, with 29.5% of Python and 24.2% of JavaScript snippets\naffected. These issues span 43 Common Weakness Enumeration (CWE) categories,\nincluding significant ones like CWE-330: Use of Insufficiently Random Values,\nCWE-94: Improper Control of Generation of Code, and CWE-79: Cross-site\nScripting. Notably, eight of those CWEs are among the 2023 CWE Top-25,\nhighlighting their severity. We further examined using Copilot Chat to fix\nsecurity issues in Copilot-generated code by providing Copilot Chat with\nwarning messages from the static analysis tools, and up to 55.5% of the\nsecurity issues can be fixed. We finally provide the suggestions for mitigating\nsecurity issues in generated code."
                },
                "authors": [
                    {
                        "name": "Yujia Fu"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Amjed Tahir"
                    },
                    {
                        "name": "Zengyang Li"
                    },
                    {
                        "name": "Mojtaba Shahin"
                    },
                    {
                        "name": "Jiaxin Yu"
                    },
                    {
                        "name": "Jinfu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jinfu Chen"
                },
                "author": "Jinfu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02059v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02059v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14285v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14285v3",
                "updated": "2024-12-30T01:41:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    1,
                    41,
                    37,
                    0,
                    365,
                    0
                ],
                "published": "2024-04-22T15:35:33Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    15,
                    35,
                    33,
                    0,
                    113,
                    0
                ],
                "title": "LLM-Personalize: Aligning LLM Planners with Human Preferences via\n  Reinforced Self-Training for Housekeeping Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Personalize: Aligning LLM Planners with Human Preferences via\n  Reinforced Self-Training for Housekeeping Robots"
                },
                "summary": "Large language models (LLMs) have shown significant potential for robotics\napplications, particularly task planning, by harnessing their language\ncomprehension and text generation capabilities. However, in applications such\nas household robotics, a critical gap remains in the personalization of these\nmodels to individual user preferences. We introduce LLM-Personalize, a novel\nframework with an optimization pipeline designed to personalize LLM planners\nfor household robotics. Our LLM-Personalize framework features an LLM planner\nthat performs iterative planning in multi-room, partially-observable household\nscenarios, making use of a scene graph constructed with local observations. The\ngenerated plan consists of a sequence of high-level actions which are\nsubsequently executed by a controller. Central to our approach is the\noptimization pipeline, which combines imitation learning and iterative\nself-training to personalize the LLM planner. In particular, the imitation\nlearning phase performs initial LLM alignment from demonstrations, and\nbootstraps the model to facilitate effective iterative self-training, which\nfurther explores and aligns the model to user preferences. We evaluate\nLLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark\nfor household rearrangements, and show that LLM-Personalize achieves more than\na 30 percent increase in success rate over existing LLM planners, showcasing\nsignificantly improved alignment with human preferences. Project page:\nhttps://gdg94.github.io/projectllmpersonalize/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant potential for robotics\napplications, particularly task planning, by harnessing their language\ncomprehension and text generation capabilities. However, in applications such\nas household robotics, a critical gap remains in the personalization of these\nmodels to individual user preferences. We introduce LLM-Personalize, a novel\nframework with an optimization pipeline designed to personalize LLM planners\nfor household robotics. Our LLM-Personalize framework features an LLM planner\nthat performs iterative planning in multi-room, partially-observable household\nscenarios, making use of a scene graph constructed with local observations. The\ngenerated plan consists of a sequence of high-level actions which are\nsubsequently executed by a controller. Central to our approach is the\noptimization pipeline, which combines imitation learning and iterative\nself-training to personalize the LLM planner. In particular, the imitation\nlearning phase performs initial LLM alignment from demonstrations, and\nbootstraps the model to facilitate effective iterative self-training, which\nfurther explores and aligns the model to user preferences. We evaluate\nLLM-Personalize on Housekeep, a challenging simulated real-world 3D benchmark\nfor household rearrangements, and show that LLM-Personalize achieves more than\na 30 percent increase in success rate over existing LLM planners, showcasing\nsignificantly improved alignment with human preferences. Project page:\nhttps://gdg94.github.io/projectllmpersonalize/."
                },
                "authors": [
                    {
                        "name": "Dongge Han"
                    },
                    {
                        "name": "Trevor McInroe"
                    },
                    {
                        "name": "Adam Jelley"
                    },
                    {
                        "name": "Stefano V. Albrecht"
                    },
                    {
                        "name": "Peter Bell"
                    },
                    {
                        "name": "Amos Storkey"
                    }
                ],
                "author_detail": {
                    "name": "Amos Storkey"
                },
                "author": "Amos Storkey",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14285v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14285v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07066v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07066v6",
                "updated": "2024-12-30T01:25:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    1,
                    25,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-04-10T14:56:40Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    14,
                    56,
                    40,
                    2,
                    101,
                    0
                ],
                "title": "Exploring Concept Depth: How Large Language Models Acquire Knowledge at\n  Different Layers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Concept Depth: How Large Language Models Acquire Knowledge at\n  Different Layers?"
                },
                "summary": "Large language models (LLMs) have shown remarkable performances across a wide\nrange of tasks. However, the mechanisms by which these models encode tasks of\nvarying complexities remain poorly understood. In this paper, we explore the\nhypothesis that LLMs process concepts of varying complexities in different\nlayers, introducing the idea of ``Concept Depth'' to suggest that more complex\nconcepts are typically acquired in deeper layers. Specifically, we categorize\nconcepts based on their level of abstraction, defining them in the order of\nincreasing complexity within factual, emotional, and inferential tasks. We\nconduct extensive probing experiments using layer-wise representations across\nvarious LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the\nthree domains of tasks. Our findings reveal that models could efficiently\nconduct probing for simpler tasks in shallow layers, and more complex tasks\ntypically necessitate deeper layers for accurate understanding. Additionally,\nwe examine how external factors, such as adding noise to the input and\nquantizing the model weights, might affect layer-wise representations. Our\nfindings suggest that these factors can impede the development of a conceptual\nunderstanding of LLMs until deeper layers are explored. We hope that our\nproposed concept and experimental insights will enhance the understanding of\nthe mechanisms underlying LLMs. Our codes are available at\n\\url{https://github.com/Luckfort/CD}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performances across a wide\nrange of tasks. However, the mechanisms by which these models encode tasks of\nvarying complexities remain poorly understood. In this paper, we explore the\nhypothesis that LLMs process concepts of varying complexities in different\nlayers, introducing the idea of ``Concept Depth'' to suggest that more complex\nconcepts are typically acquired in deeper layers. Specifically, we categorize\nconcepts based on their level of abstraction, defining them in the order of\nincreasing complexity within factual, emotional, and inferential tasks. We\nconduct extensive probing experiments using layer-wise representations across\nvarious LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the\nthree domains of tasks. Our findings reveal that models could efficiently\nconduct probing for simpler tasks in shallow layers, and more complex tasks\ntypically necessitate deeper layers for accurate understanding. Additionally,\nwe examine how external factors, such as adding noise to the input and\nquantizing the model weights, might affect layer-wise representations. Our\nfindings suggest that these factors can impede the development of a conceptual\nunderstanding of LLMs until deeper layers are explored. We hope that our\nproposed concept and experimental insights will enhance the understanding of\nthe mechanisms underlying LLMs. Our codes are available at\n\\url{https://github.com/Luckfort/CD}."
                },
                "authors": [
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Qinkai Yu"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Qingcheng Zeng"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Haiyan Zhao"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Yanda Meng"
                    },
                    {
                        "name": "Kaize Ding"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07066v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07066v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20641v1",
                "updated": "2024-12-30T01:10:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    1,
                    10,
                    10,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T01:10:10Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    1,
                    10,
                    10,
                    0,
                    365,
                    0
                ],
                "title": "SafeSynthDP: Leveraging Large Language Models for Privacy-Preserving\n  Synthetic Data Generation Using Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeSynthDP: Leveraging Large Language Models for Privacy-Preserving\n  Synthetic Data Generation Using Differential Privacy"
                },
                "summary": "Machine learning (ML) models frequently rely on training data that may\ninclude sensitive or personal information, raising substantial privacy\nconcerns. Legislative frameworks such as the General Data Protection Regulation\n(GDPR) and the California Consumer Privacy Act (CCPA) have necessitated the\ndevelopment of strategies that preserve privacy while maintaining the utility\nof data. In this paper, we investigate the capability of Large Language Models\n(LLMs) to generate synthetic datasets integrated with Differential Privacy (DP)\nmechanisms, thereby enabling data-driven research and model training without\ndirect exposure of sensitive information. Our approach incorporates DP-based\nnoise injection methods, including Laplace and Gaussian distributions, into the\ndata generation process. We then evaluate the utility of these DP-enhanced\nsynthetic datasets by comparing the performance of ML models trained on them\nagainst models trained on the original data. To substantiate privacy\nguarantees, we assess the resilience of the generated synthetic data to\nmembership inference attacks and related threats. The experimental results\ndemonstrate that integrating DP within LLM-driven synthetic data generation\noffers a viable balance between privacy protection and data utility. This study\nprovides a foundational methodology and insight into the privacy-preserving\ncapabilities of LLMs, paving the way for compliant and effective ML research\nand applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) models frequently rely on training data that may\ninclude sensitive or personal information, raising substantial privacy\nconcerns. Legislative frameworks such as the General Data Protection Regulation\n(GDPR) and the California Consumer Privacy Act (CCPA) have necessitated the\ndevelopment of strategies that preserve privacy while maintaining the utility\nof data. In this paper, we investigate the capability of Large Language Models\n(LLMs) to generate synthetic datasets integrated with Differential Privacy (DP)\nmechanisms, thereby enabling data-driven research and model training without\ndirect exposure of sensitive information. Our approach incorporates DP-based\nnoise injection methods, including Laplace and Gaussian distributions, into the\ndata generation process. We then evaluate the utility of these DP-enhanced\nsynthetic datasets by comparing the performance of ML models trained on them\nagainst models trained on the original data. To substantiate privacy\nguarantees, we assess the resilience of the generated synthetic data to\nmembership inference attacks and related threats. The experimental results\ndemonstrate that integrating DP within LLM-driven synthetic data generation\noffers a viable balance between privacy protection and data utility. This study\nprovides a foundational methodology and insight into the privacy-preserving\ncapabilities of LLMs, paving the way for compliant and effective ML research\nand applications."
                },
                "authors": [
                    {
                        "name": "Md Mahadi Hasan Nahid"
                    },
                    {
                        "name": "Sadid Bin Hasan"
                    }
                ],
                "author_detail": {
                    "name": "Sadid Bin Hasan"
                },
                "author": "Sadid Bin Hasan",
                "arxiv_comment": "15 pages, 1 figure, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18547v2",
                "updated": "2024-12-30T01:07:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    1,
                    7,
                    39,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-24T16:55:45Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    16,
                    55,
                    45,
                    1,
                    359,
                    0
                ],
                "title": "Token-Budget-Aware LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-Budget-Aware LLM Reasoning"
                },
                "summary": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning enhance LLM\nperformance by decomposing problems into intermediate steps, they also incur\nsignificant overhead in token usage, leading to increased costs. We find that\nthe reasoning process of current LLMs is unnecessarily lengthy and it can be\ncompressed by including a reasonable token budget in the prompt, but the choice\nof token budget plays a crucial role in the actual compression effectiveness.\nWe then propose a token-budget-aware LLM reasoning framework, which dynamically\nestimates token budgets for different problems based on reasoning complexity\nand uses the estimated token budgets to guide the reasoning process.\nExperiments show that our method effectively reduces token costs in CoT\nreasoning with only a slight performance reduction, offering a practical\nsolution to balance efficiency and accuracy in LLM reasoning. Code:\nhttps://github.com/GeniusHTX/TALE."
                },
                "authors": [
                    {
                        "name": "Tingxu Han"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Zhenyu Chen"
                    },
                    {
                        "name": "Zhenting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenting Wang"
                },
                "author": "Zhenting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14170v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14170v3",
                "updated": "2024-12-30T00:53:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    0,
                    53,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-05-23T04:54:37Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    4,
                    54,
                    37,
                    3,
                    144,
                    0
                ],
                "title": "Large Language Models-guided Dynamic Adaptation for Temporal Knowledge\n  Graph Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models-guided Dynamic Adaptation for Temporal Knowledge\n  Graph Reasoning"
                },
                "summary": "Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing\ntemporal information to capture complex relations within a Temporal Knowledge\nGraph (TKG) to infer new knowledge. Conventional methods in TKGR typically\ndepend on deep learning algorithms or temporal logical rules. However, deep\nlearning-based TKGRs often lack interpretability, whereas rule-based TKGRs\nstruggle to effectively learn temporal rules that capture temporal patterns.\nRecently, Large Language Models (LLMs) have demonstrated extensive knowledge\nand remarkable proficiency in temporal reasoning. Consequently, the employment\nof LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing\ninterest among researchers. Nonetheless, LLMs are known to function as black\nboxes, making it challenging to comprehend their reasoning process.\nAdditionally, due to the resource-intensive nature of fine-tuning, promptly\nupdating LLMs to integrate evolving knowledge within TKGs for reasoning is\nimpractical. To address these challenges, in this paper, we propose a Large\nLanguage Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on\nTKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze\nhistorical data and extract temporal logical rules. These rules unveil temporal\npatterns and facilitate interpretable reasoning. To account for the evolving\nnature of TKGs, a dynamic adaptation strategy is proposed to update the\nLLM-generated rules with the latest events. This ensures that the extracted\nrules always incorporate the most recent knowledge and better generalize to the\npredictions on future events. Experimental results show that without the need\nof fine-tuning, LLM-DA significantly improves the accuracy of reasoning over\nseveral common datasets, providing a robust framework for TKGR tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Knowledge Graph Reasoning (TKGR) is the process of utilizing\ntemporal information to capture complex relations within a Temporal Knowledge\nGraph (TKG) to infer new knowledge. Conventional methods in TKGR typically\ndepend on deep learning algorithms or temporal logical rules. However, deep\nlearning-based TKGRs often lack interpretability, whereas rule-based TKGRs\nstruggle to effectively learn temporal rules that capture temporal patterns.\nRecently, Large Language Models (LLMs) have demonstrated extensive knowledge\nand remarkable proficiency in temporal reasoning. Consequently, the employment\nof LLMs for Temporal Knowledge Graph Reasoning (TKGR) has sparked increasing\ninterest among researchers. Nonetheless, LLMs are known to function as black\nboxes, making it challenging to comprehend their reasoning process.\nAdditionally, due to the resource-intensive nature of fine-tuning, promptly\nupdating LLMs to integrate evolving knowledge within TKGs for reasoning is\nimpractical. To address these challenges, in this paper, we propose a Large\nLanguage Models-guided Dynamic Adaptation (LLM-DA) method for reasoning on\nTKGs. Specifically, LLM-DA harnesses the capabilities of LLMs to analyze\nhistorical data and extract temporal logical rules. These rules unveil temporal\npatterns and facilitate interpretable reasoning. To account for the evolving\nnature of TKGs, a dynamic adaptation strategy is proposed to update the\nLLM-generated rules with the latest events. This ensures that the extracted\nrules always incorporate the most recent knowledge and better generalize to the\npredictions on future events. Experimental results show that without the need\nof fine-tuning, LLM-DA significantly improves the accuracy of reasoning over\nseveral common datasets, providing a robust framework for TKGR tasks."
                },
                "authors": [
                    {
                        "name": "Jiapu Wang"
                    },
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Linhao Luo"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yongli Hu"
                    },
                    {
                        "name": "Alan Wee-Chung Liew"
                    },
                    {
                        "name": "Shirui Pan"
                    },
                    {
                        "name": "Baocai Yin"
                    }
                ],
                "author_detail": {
                    "name": "Baocai Yin"
                },
                "author": "Baocai Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14170v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14170v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20635v1",
                "updated": "2024-12-30T00:47:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    0,
                    47,
                    49,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T00:47:49Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    0,
                    47,
                    49,
                    0,
                    365,
                    0
                ],
                "title": "NetFlowGen: Leveraging Generative Pre-training for Network Traffic\n  Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetFlowGen: Leveraging Generative Pre-training for Network Traffic\n  Dynamics"
                },
                "summary": "Understanding the traffic dynamics in networks is a core capability for\nautomated systems to monitor and analyze networking behaviors, reducing\nexpensive human efforts and economic risks through tasks such as traffic\nclassification, congestion prediction, and attack detection. However, it is\nstill challenging to accurately model network traffic with machine learning\napproaches in an efficient and broadly applicable manner. Task-specific models\ntrained from scratch are used for different networking applications, which\nlimits the efficiency of model development and generalization of model\ndeployment. Furthermore, while networking data is abundant, high-quality\ntask-specific labels are often insufficient for training individual models.\nLarge-scale self-supervised learning on unlabeled data provides a natural\npathway for tackling these challenges. We propose to pre-train a\ngeneral-purpose machine learning model to capture traffic dynamics with only\ntraffic data from NetFlow records, with the goal of fine-tuning for different\ndownstream tasks with small amount of labels. Our presented NetFlowGen\nframework goes beyond a proof-of-concept for network traffic pre-training and\naddresses specific challenges such as unifying network feature representations,\nlearning from large unlabeled traffic data volume, and testing on real\ndownstream tasks in DDoS attack detection. Experiments demonstrate promising\nresults of our pre-training framework on capturing traffic dynamics and\nadapting to different networking tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the traffic dynamics in networks is a core capability for\nautomated systems to monitor and analyze networking behaviors, reducing\nexpensive human efforts and economic risks through tasks such as traffic\nclassification, congestion prediction, and attack detection. However, it is\nstill challenging to accurately model network traffic with machine learning\napproaches in an efficient and broadly applicable manner. Task-specific models\ntrained from scratch are used for different networking applications, which\nlimits the efficiency of model development and generalization of model\ndeployment. Furthermore, while networking data is abundant, high-quality\ntask-specific labels are often insufficient for training individual models.\nLarge-scale self-supervised learning on unlabeled data provides a natural\npathway for tackling these challenges. We propose to pre-train a\ngeneral-purpose machine learning model to capture traffic dynamics with only\ntraffic data from NetFlow records, with the goal of fine-tuning for different\ndownstream tasks with small amount of labels. Our presented NetFlowGen\nframework goes beyond a proof-of-concept for network traffic pre-training and\naddresses specific challenges such as unifying network feature representations,\nlearning from large unlabeled traffic data volume, and testing on real\ndownstream tasks in DDoS attack detection. Experiments demonstrate promising\nresults of our pre-training framework on capturing traffic dynamics and\nadapting to different networking tasks."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Zhiying Xu"
                    },
                    {
                        "name": "Alexander M. Rush"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20632v1",
                "updated": "2024-12-30T00:43:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    0,
                    43,
                    31,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T00:43:31Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    0,
                    43,
                    31,
                    0,
                    365,
                    0
                ],
                "title": "EVOLVE: Emotion and Visual Output Learning via LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOLVE: Emotion and Visual Output Learning via LLM Evaluation"
                },
                "summary": "Human acceptance of social robots is greatly effected by empathy and\nperceived understanding. This necessitates accurate and flexible responses to\nvarious input data from the user. While systems such as this can become\nincreasingly complex as more states or response types are included, new\nresearch in the application of large language models towards human-robot\ninteraction has allowed for more streamlined perception and reaction pipelines.\nLLM-selected actions and emotional expressions can help reinforce the realism\nof displayed empathy and allow for improved communication between the robot and\nuser. Beyond portraying empathy in spoken or written responses, this shows the\npossibilities of using LLMs in actuated, real world scenarios. In this work we\nextend research in LLM-driven nonverbal behavior for social robots by\nconsidering more open-ended emotional response selection leveraging new\nadvances in vision-language models, along with emotionally aligned motion and\ncolor pattern selections that strengthen conveyance of meaning and empathy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human acceptance of social robots is greatly effected by empathy and\nperceived understanding. This necessitates accurate and flexible responses to\nvarious input data from the user. While systems such as this can become\nincreasingly complex as more states or response types are included, new\nresearch in the application of large language models towards human-robot\ninteraction has allowed for more streamlined perception and reaction pipelines.\nLLM-selected actions and emotional expressions can help reinforce the realism\nof displayed empathy and allow for improved communication between the robot and\nuser. Beyond portraying empathy in spoken or written responses, this shows the\npossibilities of using LLMs in actuated, real world scenarios. In this work we\nextend research in LLM-driven nonverbal behavior for social robots by\nconsidering more open-ended emotional response selection leveraging new\nadvances in vision-language models, along with emotionally aligned motion and\ncolor pattern selections that strengthen conveyance of meaning and empathy."
                },
                "authors": [
                    {
                        "name": "Jordan Sinclair"
                    },
                    {
                        "name": "Christopher Reardon"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Reardon"
                },
                "author": "Christopher Reardon",
                "arxiv_comment": "This work was presented at the WARN, Weighing the Benefits of\n  Autonomous Robot Personalization, workshop at the 33rd IEEE RO-MAN 2024\n  conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04163v2",
                "updated": "2024-12-30T00:27:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    0,
                    27,
                    42,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-08T02:08:59Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    2,
                    8,
                    59,
                    3,
                    221,
                    0
                ],
                "title": "Academic collaboration on large language model studies increases overall\n  but varies across disciplines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Academic collaboration on large language model studies increases overall\n  but varies across disciplines"
                },
                "summary": "Interdisciplinary collaboration is crucial for addressing complex scientific\nchallenges. Recent advancements in large language models (LLMs) have shown\nsignificant potential in benefiting researchers across various fields. To\nexplore their potential for interdisciplinary collaboration, we collect and\nanalyze data from OpenAlex, an open-source academic database. Our dataset\ncomprises 59,293 LLM-related papers, along with 70,945 machine learning (ML)\npapers and 73,110 papers from non-LLM/ML fields as control groups. We first\nemploy Shannon Entropy to assess the diversity of collaboration. Our results\nreveal that many fields have exhibited a more significant increasing trend\nfollowing the release of ChatGPT as compared to the control groups. In\nparticular, Computer Science and Social Science display a consistent increase\nin both institution and department entropy. Other fields such as Decision\nScience, Psychology, and Health Professions have shown minor to significant\nincreases. Our difference-in-difference analysis also indicates that the\nrelease of ChatGPT leads to a statistically significant increase in\ncollaboration in several fields, such as Computer Science and Social Science.\nIn addition, we analyze the author networks and find that Computer Science,\nMedicine, and other Computer Science-related departments are the most\nprominent. Regarding authors' institutions, our analysis reveals that entities\nsuch as Stanford University, Harvard University, and University College London\nare key players, either dominating centrality or playing crucial roles in\nconnecting research networks. Overall, this study provides valuable information\non the current landscape and evolving dynamics of collaboration networks in LLM\nresearch. It also suggests potential areas for fostering more diverse\ncollaborations and highlights the need for continued research on the impact of\nLLMs on scientific practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interdisciplinary collaboration is crucial for addressing complex scientific\nchallenges. Recent advancements in large language models (LLMs) have shown\nsignificant potential in benefiting researchers across various fields. To\nexplore their potential for interdisciplinary collaboration, we collect and\nanalyze data from OpenAlex, an open-source academic database. Our dataset\ncomprises 59,293 LLM-related papers, along with 70,945 machine learning (ML)\npapers and 73,110 papers from non-LLM/ML fields as control groups. We first\nemploy Shannon Entropy to assess the diversity of collaboration. Our results\nreveal that many fields have exhibited a more significant increasing trend\nfollowing the release of ChatGPT as compared to the control groups. In\nparticular, Computer Science and Social Science display a consistent increase\nin both institution and department entropy. Other fields such as Decision\nScience, Psychology, and Health Professions have shown minor to significant\nincreases. Our difference-in-difference analysis also indicates that the\nrelease of ChatGPT leads to a statistically significant increase in\ncollaboration in several fields, such as Computer Science and Social Science.\nIn addition, we analyze the author networks and find that Computer Science,\nMedicine, and other Computer Science-related departments are the most\nprominent. Regarding authors' institutions, our analysis reveals that entities\nsuch as Stanford University, Harvard University, and University College London\nare key players, either dominating centrality or playing crucial roles in\nconnecting research networks. Overall, this study provides valuable information\non the current landscape and evolving dynamics of collaboration networks in LLM\nresearch. It also suggests potential areas for fostering more diverse\ncollaborations and highlights the need for continued research on the impact of\nLLMs on scientific practices."
                },
                "authors": [
                    {
                        "name": "Lingyao Li"
                    },
                    {
                        "name": "Ly Dinh"
                    },
                    {
                        "name": "Songhua Hu"
                    },
                    {
                        "name": "Libby Hemphill"
                    }
                ],
                "author_detail": {
                    "name": "Libby Hemphill"
                },
                "author": "Libby Hemphill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20621v1",
                "updated": "2024-12-29T23:52:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    23,
                    52,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T23:52:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    23,
                    52,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "FreqMixFormerV2: Lightweight Frequency-aware Mixed Transformer for Human\n  Skeleton Action Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqMixFormerV2: Lightweight Frequency-aware Mixed Transformer for Human\n  Skeleton Action Recognition"
                },
                "summary": "Transformer-based human skeleton action recognition has been developed for\nyears. However, the complexity and high parameter count demands of these models\nhinder their practical applications, especially in resource-constrained\nenvironments. In this work, we propose FreqMixForemrV2, which was built upon\nthe Frequency-aware Mixed Transformer (FreqMixFormer) for identifying subtle\nand discriminative actions with pioneered frequency-domain analysis. We design\na lightweight architecture that maintains robust performance while\nsignificantly reducing the model complexity. This is achieved through a\nredesigned frequency operator that optimizes high-frequency and low-frequency\nparameter adjustments, and a simplified frequency-aware attention module. These\nimprovements result in a substantial reduction in model parameters, enabling\nefficient deployment with only a minimal sacrifice in accuracy. Comprehensive\nevaluations of standard datasets (NTU RGB+D, NTU RGB+D 120, and NW-UCLA\ndatasets) demonstrate that the proposed model achieves a superior balance\nbetween efficiency and accuracy, outperforming state-of-the-art methods with\nonly 60% of the parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based human skeleton action recognition has been developed for\nyears. However, the complexity and high parameter count demands of these models\nhinder their practical applications, especially in resource-constrained\nenvironments. In this work, we propose FreqMixForemrV2, which was built upon\nthe Frequency-aware Mixed Transformer (FreqMixFormer) for identifying subtle\nand discriminative actions with pioneered frequency-domain analysis. We design\na lightweight architecture that maintains robust performance while\nsignificantly reducing the model complexity. This is achieved through a\nredesigned frequency operator that optimizes high-frequency and low-frequency\nparameter adjustments, and a simplified frequency-aware attention module. These\nimprovements result in a substantial reduction in model parameters, enabling\nefficient deployment with only a minimal sacrifice in accuracy. Comprehensive\nevaluations of standard datasets (NTU RGB+D, NTU RGB+D 120, and NW-UCLA\ndatasets) demonstrate that the proposed model achieves a superior balance\nbetween efficiency and accuracy, outperforming state-of-the-art methods with\nonly 60% of the parameters."
                },
                "authors": [
                    {
                        "name": "Wenhan Wu"
                    },
                    {
                        "name": "Pengfei Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Aidong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Aidong Lu"
                },
                "author": "Aidong Lu",
                "arxiv_comment": "IEEE FG2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20613v1",
                "updated": "2024-12-29T23:20:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    23,
                    20,
                    1,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T23:20:01Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    23,
                    20,
                    1,
                    6,
                    364,
                    0
                ],
                "title": "Do Current Video LLMs Have Strong OCR Abilities? A Preliminary Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Current Video LLMs Have Strong OCR Abilities? A Preliminary Study"
                },
                "summary": "With the rise of multimodal large language models, accurately extracting and\nunderstanding textual information from video content, referred to as video\nbased optical character recognition (Video OCR), has become a crucial\ncapability. This paper introduces a novel benchmark designed to evaluate the\nvideo OCR performance of multi-modal models in videos. Comprising 1,028 videos\nand 2,961 question-answer pairs, this benchmark proposes several key challenges\nthrough 6 distinct subtasks: (1) Recognition of text content itself and its\nbasic visual attributes, (2)Semantic and Spatial Comprehension of OCR objects\nin videos (3) Dynamic Motion detection and Temporal Localization. We developed\nthis benchmark using a semi-automated approach that integrates the OCR ability\nof image LLMs with manual refinement, balancing efficiency, cost, and data\nquality. Our resource aims to help advance research in video LLMs and\nunderscores the need for improving OCR ability for video LLMs. The benchmark\nwill be released on https://github.com/YuHuiGao/FG-Bench.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of multimodal large language models, accurately extracting and\nunderstanding textual information from video content, referred to as video\nbased optical character recognition (Video OCR), has become a crucial\ncapability. This paper introduces a novel benchmark designed to evaluate the\nvideo OCR performance of multi-modal models in videos. Comprising 1,028 videos\nand 2,961 question-answer pairs, this benchmark proposes several key challenges\nthrough 6 distinct subtasks: (1) Recognition of text content itself and its\nbasic visual attributes, (2)Semantic and Spatial Comprehension of OCR objects\nin videos (3) Dynamic Motion detection and Temporal Localization. We developed\nthis benchmark using a semi-automated approach that integrates the OCR ability\nof image LLMs with manual refinement, balancing efficiency, cost, and data\nquality. Our resource aims to help advance research in video LLMs and\nunderscores the need for improving OCR ability for video LLMs. The benchmark\nwill be released on https://github.com/YuHuiGao/FG-Bench.git."
                },
                "authors": [
                    {
                        "name": "Yulin Fei"
                    },
                    {
                        "name": "Yuhui Gao"
                    },
                    {
                        "name": "Xingyuan Xian"
                    },
                    {
                        "name": "Xiaojin Zhang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "arxiv_comment": "Accepted by CoLing 2025 (The 31st International Conference on\n  Computational Linguistics)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20602v1",
                "updated": "2024-12-29T22:14:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    22,
                    14,
                    59,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T22:14:59Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    22,
                    14,
                    59,
                    6,
                    364,
                    0
                ],
                "title": "NLP-based Regulatory Compliance -- Using GPT 4.0 to Decode Regulatory\n  Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NLP-based Regulatory Compliance -- Using GPT 4.0 to Decode Regulatory\n  Documents"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4.0 have shown significant promise\nin addressing the semantic complexities of regulatory documents, particularly\nin detecting inconsistencies and contradictions. This study evaluates GPT-4.0's\nability to identify conflicts within regulatory requirements by analyzing a\ncurated corpus with artificially injected ambiguities and contradictions,\ndesigned in collaboration with architects and compliance engineers. Using\nmetrics such as precision, recall, and F1 score, the experiment demonstrates\nGPT-4.0's effectiveness in detecting inconsistencies, with findings validated\nby human experts. The results highlight the potential of LLMs to enhance\nregulatory compliance processes, though further testing with larger datasets\nand domain-specific fine-tuning is needed to maximize accuracy and practical\napplicability. Future work will explore automated conflict resolution and\nreal-world implementation through pilot projects with industry partners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4.0 have shown significant promise\nin addressing the semantic complexities of regulatory documents, particularly\nin detecting inconsistencies and contradictions. This study evaluates GPT-4.0's\nability to identify conflicts within regulatory requirements by analyzing a\ncurated corpus with artificially injected ambiguities and contradictions,\ndesigned in collaboration with architects and compliance engineers. Using\nmetrics such as precision, recall, and F1 score, the experiment demonstrates\nGPT-4.0's effectiveness in detecting inconsistencies, with findings validated\nby human experts. The results highlight the potential of LLMs to enhance\nregulatory compliance processes, though further testing with larger datasets\nand domain-specific fine-tuning is needed to maximize accuracy and practical\napplicability. Future work will explore automated conflict resolution and\nreal-world implementation through pilot projects with industry partners."
                },
                "authors": [
                    {
                        "name": "Bimal Kumar"
                    },
                    {
                        "name": "Dmitri Roussinov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitri Roussinov"
                },
                "author": "Dmitri Roussinov",
                "arxiv_comment": "accepted for presentation at Georg Nemetschek Institute Symposium &\n  Expo on Artificial Intelligence for the Built World - Munich, Germany. 12\n  Sept 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20595v1",
                "updated": "2024-12-29T21:54:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    21,
                    54,
                    39,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T21:54:39Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    21,
                    54,
                    39,
                    6,
                    364,
                    0
                ],
                "title": "Controlling Out-of-Domain Gaps in LLMs for Genre Classification and\n  Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Out-of-Domain Gaps in LLMs for Genre Classification and\n  Generated Text Detection"
                },
                "summary": "This study demonstrates that the modern generation of Large Language Models\n(LLMs, such as GPT-4) suffers from the same out-of-domain (OOD) performance gap\nobserved in prior research on pre-trained Language Models (PLMs, such as BERT).\nWe demonstrate this across two non-topical classification tasks: 1) genre\nclassification and 2) generated text detection. Our results show that when\ndemonstration examples for In-Context Learning (ICL) come from one domain\n(e.g., travel) and the system is tested on another domain (e.g., history),\nclassification performance declines significantly.\n  To address this, we introduce a method that controls which predictive\nindicators are used and which are excluded during classification. For the two\ntasks studied here, this ensures that topical features are omitted, while the\nmodel is guided to focus on stylistic rather than content-based attributes.\nThis approach reduces the OOD gap by up to 20 percentage points in a few-shot\nsetup. Straightforward Chain-of-Thought (CoT) methods, used as the baseline,\nprove insufficient, while our approach consistently enhances domain transfer\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates that the modern generation of Large Language Models\n(LLMs, such as GPT-4) suffers from the same out-of-domain (OOD) performance gap\nobserved in prior research on pre-trained Language Models (PLMs, such as BERT).\nWe demonstrate this across two non-topical classification tasks: 1) genre\nclassification and 2) generated text detection. Our results show that when\ndemonstration examples for In-Context Learning (ICL) come from one domain\n(e.g., travel) and the system is tested on another domain (e.g., history),\nclassification performance declines significantly.\n  To address this, we introduce a method that controls which predictive\nindicators are used and which are excluded during classification. For the two\ntasks studied here, this ensures that topical features are omitted, while the\nmodel is guided to focus on stylistic rather than content-based attributes.\nThis approach reduces the OOD gap by up to 20 percentage points in a few-shot\nsetup. Straightforward Chain-of-Thought (CoT) methods, used as the baseline,\nprove insufficient, while our approach consistently enhances domain transfer\nperformance."
                },
                "authors": [
                    {
                        "name": "Dmitri Roussinov"
                    },
                    {
                        "name": "Serge Sharoff"
                    },
                    {
                        "name": "Nadezhda Puchnina"
                    }
                ],
                "author_detail": {
                    "name": "Nadezhda Puchnina"
                },
                "author": "Nadezhda Puchnina",
                "arxiv_comment": "The 31st International Conference on Computational Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18295v2",
                "updated": "2024-12-29T21:25:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    21,
                    25,
                    4,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-24T09:03:57Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    9,
                    3,
                    57,
                    1,
                    359,
                    0
                ],
                "title": "Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases"
                },
                "summary": "The growing ubiquity of Retrieval-Augmented Generation (RAG) systems in\nseveral real-world services triggers severe concerns about their security. A\nRAG system improves the generative capabilities of a Large Language Models\n(LLM) by a retrieval mechanism which operates on a private knowledge base,\nwhose unintended exposure could lead to severe consequences, including breaches\nof private and sensitive information. This paper presents a black-box attack to\nforce a RAG system to leak its private knowledge base which, differently from\nexisting approaches, is adaptive and automatic. A relevance-based mechanism and\nan attacker-side open-source LLM favor the generation of effective queries to\nleak most of the (hidden) knowledge base. Extensive experimentation proves the\nquality of the proposed algorithm in different RAG pipelines and domains,\ncomparing to very recent related approaches, which turn out to be either not\nfully black-box, not adaptive, or not based on open-source models. The findings\nfrom our study remark the urgent need for more robust privacy safeguards in the\ndesign and deployment of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing ubiquity of Retrieval-Augmented Generation (RAG) systems in\nseveral real-world services triggers severe concerns about their security. A\nRAG system improves the generative capabilities of a Large Language Models\n(LLM) by a retrieval mechanism which operates on a private knowledge base,\nwhose unintended exposure could lead to severe consequences, including breaches\nof private and sensitive information. This paper presents a black-box attack to\nforce a RAG system to leak its private knowledge base which, differently from\nexisting approaches, is adaptive and automatic. A relevance-based mechanism and\nan attacker-side open-source LLM favor the generation of effective queries to\nleak most of the (hidden) knowledge base. Extensive experimentation proves the\nquality of the proposed algorithm in different RAG pipelines and domains,\ncomparing to very recent related approaches, which turn out to be either not\nfully black-box, not adaptive, or not based on open-source models. The findings\nfrom our study remark the urgent need for more robust privacy safeguards in the\ndesign and deployment of RAG systems."
                },
                "authors": [
                    {
                        "name": "Christian Di Maio"
                    },
                    {
                        "name": "Cristian Cosci"
                    },
                    {
                        "name": "Marco Maggini"
                    },
                    {
                        "name": "Valentina Poggioni"
                    },
                    {
                        "name": "Stefano Melacci"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Melacci"
                },
                "author": "Stefano Melacci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20584v1",
                "updated": "2024-12-29T21:12:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    21,
                    12,
                    39,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T21:12:39Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    21,
                    12,
                    39,
                    6,
                    364,
                    0
                ],
                "title": "Towards Neural No-Resource Language Translation: A Comparative\n  Evaluation of Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Neural No-Resource Language Translation: A Comparative\n  Evaluation of Approaches"
                },
                "summary": "No-resource languages - those with minimal or no digital representation -\npose unique challenges for machine translation (MT). Unlike low-resource\nlanguages, which rely on limited but existent corpora, no-resource languages\noften have fewer than 100 sentences available for training. This work explores\nthe problem of no-resource translation through three distinct workflows:\nfine-tuning of translation-specific models, in-context learning with large\nlanguage models (LLMs) using chain-of-reasoning prompting, and direct prompting\nwithout reasoning. Using Owens Valley Paiute as a case study, we demonstrate\nthat no-resource translation demands fundamentally different approaches from\nlow-resource scenarios, as traditional approaches to machine translation, such\nas those that work for low-resource languages, fail. Empirical results reveal\nthat, although traditional approaches fail, the in-context learning\ncapabilities of general-purpose large language models enable no-resource\nlanguage translation that outperforms low-resource translation approaches and\nrivals human translations (BLEU 0.45-0.6); specifically, chain-of-reasoning\nprompting outperforms other methods for larger corpora, while direct prompting\nexhibits advantages in smaller datasets. As these approaches are\nlanguage-agnostic, they have potential to be generalized to translation tasks\nfrom a wide variety of no-resource languages without expert input. These\nfindings establish no-resource translation as a distinct paradigm requiring\ninnovative solutions, providing practical and theoretical insights for language\npreservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No-resource languages - those with minimal or no digital representation -\npose unique challenges for machine translation (MT). Unlike low-resource\nlanguages, which rely on limited but existent corpora, no-resource languages\noften have fewer than 100 sentences available for training. This work explores\nthe problem of no-resource translation through three distinct workflows:\nfine-tuning of translation-specific models, in-context learning with large\nlanguage models (LLMs) using chain-of-reasoning prompting, and direct prompting\nwithout reasoning. Using Owens Valley Paiute as a case study, we demonstrate\nthat no-resource translation demands fundamentally different approaches from\nlow-resource scenarios, as traditional approaches to machine translation, such\nas those that work for low-resource languages, fail. Empirical results reveal\nthat, although traditional approaches fail, the in-context learning\ncapabilities of general-purpose large language models enable no-resource\nlanguage translation that outperforms low-resource translation approaches and\nrivals human translations (BLEU 0.45-0.6); specifically, chain-of-reasoning\nprompting outperforms other methods for larger corpora, while direct prompting\nexhibits advantages in smaller datasets. As these approaches are\nlanguage-agnostic, they have potential to be generalized to translation tasks\nfrom a wide variety of no-resource languages without expert input. These\nfindings establish no-resource translation as a distinct paradigm requiring\ninnovative solutions, providing practical and theoretical insights for language\npreservation."
                },
                "authors": [
                    {
                        "name": "Madhavendra Thakur"
                    }
                ],
                "author_detail": {
                    "name": "Madhavendra Thakur"
                },
                "author": "Madhavendra Thakur",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20563v1",
                "updated": "2024-12-29T20:18:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    20,
                    18,
                    52,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T20:18:52Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    20,
                    18,
                    52,
                    6,
                    364,
                    0
                ],
                "title": "Counterfactual Samples Constructing and Training for Commonsense\n  Statements Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual Samples Constructing and Training for Commonsense\n  Statements Estimation"
                },
                "summary": "Plausibility Estimation (PE) plays a crucial role for enabling language\nmodels to objectively comprehend the real world. While large language models\n(LLMs) demonstrate remarkable capabilities in PE tasks but sometimes produce\ntrivial commonsense errors due to the complexity of commonsense knowledge. They\nlack two key traits of an ideal PE model: a) Language-explainable: relying on\ncritical word segments for decisions, and b) Commonsense-sensitive: detecting\nsubtle linguistic variations in commonsense. To address these issues, we\npropose a novel model-agnostic method, referred to as Commonsense\nCounterfactual Samples Generating (CCSG). By training PE models with CCSG, we\nencourage them to focus on critical words, thereby enhancing both their\nlanguage-explainable and commonsense-sensitive capabilities. Specifically, CCSG\ngenerates counterfactual samples by strategically replacing key words and\nintroducing low-level dropout within sentences. These counterfactual samples\nare then incorporated into a sentence-level contrastive training framework to\nfurther enhance the model's learning process. Experimental results across nine\ndiverse datasets demonstrate the effectiveness of CCSG in addressing\ncommonsense reasoning challenges, with our CCSG method showing 3.07%\nimprovement against the SOTA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plausibility Estimation (PE) plays a crucial role for enabling language\nmodels to objectively comprehend the real world. While large language models\n(LLMs) demonstrate remarkable capabilities in PE tasks but sometimes produce\ntrivial commonsense errors due to the complexity of commonsense knowledge. They\nlack two key traits of an ideal PE model: a) Language-explainable: relying on\ncritical word segments for decisions, and b) Commonsense-sensitive: detecting\nsubtle linguistic variations in commonsense. To address these issues, we\npropose a novel model-agnostic method, referred to as Commonsense\nCounterfactual Samples Generating (CCSG). By training PE models with CCSG, we\nencourage them to focus on critical words, thereby enhancing both their\nlanguage-explainable and commonsense-sensitive capabilities. Specifically, CCSG\ngenerates counterfactual samples by strategically replacing key words and\nintroducing low-level dropout within sentences. These counterfactual samples\nare then incorporated into a sentence-level contrastive training framework to\nfurther enhance the model's learning process. Experimental results across nine\ndiverse datasets demonstrate the effectiveness of CCSG in addressing\ncommonsense reasoning challenges, with our CCSG method showing 3.07%\nimprovement against the SOTA methods."
                },
                "authors": [
                    {
                        "name": "Chong Liu"
                    },
                    {
                        "name": "Zaiwen Feng"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Zhenyun Deng"
                    },
                    {
                        "name": "Jiuyong Li"
                    },
                    {
                        "name": "Ruifang Zhai"
                    },
                    {
                        "name": "Debo Cheng"
                    },
                    {
                        "name": "Li Qin"
                    }
                ],
                "author_detail": {
                    "name": "Li Qin"
                },
                "author": "Li Qin",
                "arxiv_comment": "14 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06360v2",
                "updated": "2024-12-29T18:43:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    18,
                    43,
                    4,
                    6,
                    364,
                    0
                ],
                "published": "2024-11-10T04:56:14Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    4,
                    56,
                    14,
                    6,
                    315,
                    0
                ],
                "title": "An Efficient Matrix Multiplication Algorithm for Accelerating Inference\n  in Binary and Ternary Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Matrix Multiplication Algorithm for Accelerating Inference\n  in Binary and Ternary Neural Networks"
                },
                "summary": "Despite their tremendous success and versatility, Large Language Models\n(LLMs) suffer from inference inefficiency while relying on advanced\ncomputational infrastructure. To address these challenges and make LLMs more\naccessible and cost-effective, in this paper, we propose algorithms to improve\nthe inference time and memory efficiency of 1.58-bit LLMs with ternary weight\nmatrices. Particularly focusing on matrix multiplication as the bottle-neck\noperation of inference, we observe that, once trained, the weight matrices of a\nmodel no longer change. This allows us to preprocess these matrices and create\nindices that help reduce the storage requirements by a logarithmic factor while\nenabling our efficient inference algorithms. Specifically, for a $n$ by $n$\nweight matrix, our efficient algorithm guarantees a time complexity of\n$O(\\frac{n^2}{\\log n})$, a logarithmic factor improvement over the standard\n$O(n^2)$ vector-matrix multiplication. Besides theoretical analysis, we conduct\nextensive experiments to evaluate the practical efficiency of our algorithms.\nOur results confirm the superiority of the approach both with respect to time\nand memory, as we observed a reduction in inference time up to 29x and memory\nusage up to 6x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their tremendous success and versatility, Large Language Models\n(LLMs) suffer from inference inefficiency while relying on advanced\ncomputational infrastructure. To address these challenges and make LLMs more\naccessible and cost-effective, in this paper, we propose algorithms to improve\nthe inference time and memory efficiency of 1.58-bit LLMs with ternary weight\nmatrices. Particularly focusing on matrix multiplication as the bottle-neck\noperation of inference, we observe that, once trained, the weight matrices of a\nmodel no longer change. This allows us to preprocess these matrices and create\nindices that help reduce the storage requirements by a logarithmic factor while\nenabling our efficient inference algorithms. Specifically, for a $n$ by $n$\nweight matrix, our efficient algorithm guarantees a time complexity of\n$O(\\frac{n^2}{\\log n})$, a logarithmic factor improvement over the standard\n$O(n^2)$ vector-matrix multiplication. Besides theoretical analysis, we conduct\nextensive experiments to evaluate the practical efficiency of our algorithms.\nOur results confirm the superiority of the approach both with respect to time\nand memory, as we observed a reduction in inference time up to 29x and memory\nusage up to 6x."
                },
                "authors": [
                    {
                        "name": "Mohsen Dehghankar"
                    },
                    {
                        "name": "Mahdi Erfanian"
                    },
                    {
                        "name": "Abolfazl Asudeh"
                    }
                ],
                "author_detail": {
                    "name": "Abolfazl Asudeh"
                },
                "author": "Abolfazl Asudeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20545v1",
                "updated": "2024-12-29T18:34:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    18,
                    34,
                    10,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T18:34:10Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    18,
                    34,
                    10,
                    6,
                    364,
                    0
                ],
                "title": "The Impact of Prompt Programming on Function-Level Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Prompt Programming on Function-Level Code Generation"
                },
                "summary": "Large Language Models (LLMs) are increasingly used by software engineers for\ncode generation. However, limitations of LLMs such as irrelevant or incorrect\ncode have highlighted the need for prompt programming (or prompt engineering)\nwhere engineers apply specific prompt techniques (e.g., chain-of-thought or\ninput-output examples) to improve the generated code. Despite this, the impact\nof different prompt techniques -- and their combinations -- on code generation\nremains underexplored. In this study, we introduce CodePromptEval, a dataset of\n7072 prompts designed to evaluate five prompt techniques (few-shot, persona,\nchain-of-thought, function signature, list of packages) and their effect on the\ncorrectness, similarity, and quality of complete functions generated by three\nLLMs (GPT-4o, Llama3, and Mistral). Our findings show that while certain prompt\ntechniques significantly influence the generated code, combining multiple\ntechniques does not necessarily improve the outcome. Additionally, we observed\na trade-off between correctness and quality when using prompt techniques. Our\ndataset and replication package enable future research on improving\nLLM-generated code and evaluating new prompt techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used by software engineers for\ncode generation. However, limitations of LLMs such as irrelevant or incorrect\ncode have highlighted the need for prompt programming (or prompt engineering)\nwhere engineers apply specific prompt techniques (e.g., chain-of-thought or\ninput-output examples) to improve the generated code. Despite this, the impact\nof different prompt techniques -- and their combinations -- on code generation\nremains underexplored. In this study, we introduce CodePromptEval, a dataset of\n7072 prompts designed to evaluate five prompt techniques (few-shot, persona,\nchain-of-thought, function signature, list of packages) and their effect on the\ncorrectness, similarity, and quality of complete functions generated by three\nLLMs (GPT-4o, Llama3, and Mistral). Our findings show that while certain prompt\ntechniques significantly influence the generated code, combining multiple\ntechniques does not necessarily improve the outcome. Additionally, we observed\na trade-off between correctness and quality when using prompt techniques. Our\ndataset and replication package enable future research on improving\nLLM-generated code and evaluating new prompt techniques."
                },
                "authors": [
                    {
                        "name": "Ranim Khojah"
                    },
                    {
                        "name": "Francisco Gomes de Oliveira Neto"
                    },
                    {
                        "name": "Mazen Mohamad"
                    },
                    {
                        "name": "Philipp Leitner"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Leitner"
                },
                "author": "Philipp Leitner",
                "arxiv_comment": "CodePromptEval dataset and replication package on GitHub:\n  https://github.com/icetlab/CodePromptEval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14651v2",
                "updated": "2024-12-29T18:22:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    18,
                    22,
                    27,
                    6,
                    364,
                    0
                ],
                "published": "2024-10-18T17:47:11Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    47,
                    11,
                    4,
                    292,
                    0
                ],
                "title": "Real-time Fake News from Adversarial Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Fake News from Adversarial Feedback"
                },
                "summary": "We show that existing evaluations for fake news detection based on\nconventional sources, such as claims on fact-checking websites, result in high\naccuracies over time for LLM-based detectors -- even after their knowledge\ncutoffs. This suggests that recent popular fake news from such sources can be\neasily detected due to pre-training and retrieval corpus contamination or\nincreasingly salient shallow patterns. Instead, we argue that a proper fake\nnews detection dataset should test a model's ability to reason factually about\nthe current world by retrieving and reading related evidence. To this end, we\ndevelop a novel pipeline that leverages natural language feedback from a\nRAG-based detector to iteratively modify real-time news into deceptive fake\nnews that challenges LLMs. Our iterative rewrite decreases the binary\nclassification ROC-AUC by an absolute 17.5 percent for a strong RAG-based\nGPT-4o detector. Our experiments reveal the important role of RAG in both\ndetecting and generating fake news, as retrieval-free LLM detectors are\nvulnerable to unseen events and adversarial attacks, while feedback from RAG\ndetection helps discover more deceitful patterns in fake news.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that existing evaluations for fake news detection based on\nconventional sources, such as claims on fact-checking websites, result in high\naccuracies over time for LLM-based detectors -- even after their knowledge\ncutoffs. This suggests that recent popular fake news from such sources can be\neasily detected due to pre-training and retrieval corpus contamination or\nincreasingly salient shallow patterns. Instead, we argue that a proper fake\nnews detection dataset should test a model's ability to reason factually about\nthe current world by retrieving and reading related evidence. To this end, we\ndevelop a novel pipeline that leverages natural language feedback from a\nRAG-based detector to iteratively modify real-time news into deceptive fake\nnews that challenges LLMs. Our iterative rewrite decreases the binary\nclassification ROC-AUC by an absolute 17.5 percent for a strong RAG-based\nGPT-4o detector. Our experiments reveal the important role of RAG in both\ndetecting and generating fake news, as retrieval-free LLM detectors are\nvulnerable to unseen events and adversarial attacks, while feedback from RAG\ndetection helps discover more deceitful patterns in fake news."
                },
                "authors": [
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Yukun Huang"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    }
                ],
                "author_detail": {
                    "name": "Bhuwan Dhingra"
                },
                "author": "Bhuwan Dhingra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.00625v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.00625v4",
                "updated": "2024-12-29T17:38:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    38,
                    32,
                    6,
                    364,
                    0
                ],
                "published": "2024-01-01T01:12:42Z",
                "published_parsed": [
                    2024,
                    1,
                    1,
                    1,
                    12,
                    42,
                    0,
                    1,
                    0
                ],
                "title": "Beyond Efficiency: A Systematic Survey of Resource-Efficient Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Efficiency: A Systematic Survey of Resource-Efficient Large\n  Language Models"
                },
                "summary": "The burgeoning field of Large Language Models (LLMs), exemplified by\nsophisticated models like OpenAI's ChatGPT, represents a significant\nadvancement in artificial intelligence. These models, however, bring forth\nsubstantial challenges in the high consumption of computational, memory,\nenergy, and financial resources, especially in environments with limited\nresource capabilities. This survey aims to systematically address these\nchallenges by reviewing a broad spectrum of techniques designed to enhance the\nresource efficiency of LLMs. We categorize methods based on their optimization\nfocus: computational, memory, energy, financial, and network resources and\ntheir applicability across various stages of an LLM's lifecycle, including\narchitecture design, pretraining, finetuning, and system design. Additionally,\nthe survey introduces a nuanced categorization of resource efficiency\ntechniques by their specific resource types, which uncovers the intricate\nrelationships and mappings between various resources and corresponding\noptimization techniques. A standardized set of evaluation metrics and datasets\nis also presented to facilitate consistent and fair comparisons across\ndifferent models and techniques. By offering a comprehensive overview of the\ncurrent sota and identifying open research avenues, this survey serves as a\nfoundational reference for researchers and practitioners, aiding them in\ndeveloping more sustainable and efficient LLMs in a rapidly evolving landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The burgeoning field of Large Language Models (LLMs), exemplified by\nsophisticated models like OpenAI's ChatGPT, represents a significant\nadvancement in artificial intelligence. These models, however, bring forth\nsubstantial challenges in the high consumption of computational, memory,\nenergy, and financial resources, especially in environments with limited\nresource capabilities. This survey aims to systematically address these\nchallenges by reviewing a broad spectrum of techniques designed to enhance the\nresource efficiency of LLMs. We categorize methods based on their optimization\nfocus: computational, memory, energy, financial, and network resources and\ntheir applicability across various stages of an LLM's lifecycle, including\narchitecture design, pretraining, finetuning, and system design. Additionally,\nthe survey introduces a nuanced categorization of resource efficiency\ntechniques by their specific resource types, which uncovers the intricate\nrelationships and mappings between various resources and corresponding\noptimization techniques. A standardized set of evaluation metrics and datasets\nis also presented to facilitate consistent and fair comparisons across\ndifferent models and techniques. By offering a comprehensive overview of the\ncurrent sota and identifying open research avenues, this survey serves as a\nfoundational reference for researchers and practitioners, aiding them in\ndeveloping more sustainable and efficient LLMs in a rapidly evolving landscape."
                },
                "authors": [
                    {
                        "name": "Guangji Bai"
                    },
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Chen Ling"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Jiaying Lu"
                    },
                    {
                        "name": "Nan Zhang"
                    },
                    {
                        "name": "Tingwei Shi"
                    },
                    {
                        "name": "Ziyang Yu"
                    },
                    {
                        "name": "Mengdan Zhu"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Carl Yang"
                    },
                    {
                        "name": "Yue Cheng"
                    },
                    {
                        "name": "Liang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Liang Zhao"
                },
                "author": "Liang Zhao",
                "arxiv_comment": "GitHub repo:\n  https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.00625v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.00625v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20505v1",
                "updated": "2024-12-29T15:43:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    43,
                    25,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T15:43:25Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    43,
                    25,
                    6,
                    364,
                    0
                ],
                "title": "Planning, Living and Judging: A Multi-agent LLM-based Framework for\n  Cyclical Urban Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning, Living and Judging: A Multi-agent LLM-based Framework for\n  Cyclical Urban Planning"
                },
                "summary": "Urban regeneration presents significant challenges within the context of\nurbanization, requiring adaptive approaches to tackle evolving needs.\nLeveraging advancements in large language models (LLMs), we propose Cyclical\nUrban Planning (CUP), a new paradigm that continuously generates, evaluates,\nand refines urban plans in a closed-loop. Specifically, our multi-agent\nLLM-based framework consists of three key components: (1) Planning, where LLM\nagents generate and refine urban plans based on contextual data; (2) Living,\nwhere agents simulate the behaviors and interactions of residents, modeling\nlife in the urban environment; and (3) Judging, which involves evaluating plan\neffectiveness and providing iterative feedback for improvement. The cyclical\nprocess enables a dynamic and responsive planning approach. Experiments on the\nreal-world dataset demonstrate the effectiveness of our framework as a\ncontinuous and adaptive planning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Urban regeneration presents significant challenges within the context of\nurbanization, requiring adaptive approaches to tackle evolving needs.\nLeveraging advancements in large language models (LLMs), we propose Cyclical\nUrban Planning (CUP), a new paradigm that continuously generates, evaluates,\nand refines urban plans in a closed-loop. Specifically, our multi-agent\nLLM-based framework consists of three key components: (1) Planning, where LLM\nagents generate and refine urban plans based on contextual data; (2) Living,\nwhere agents simulate the behaviors and interactions of residents, modeling\nlife in the urban environment; and (3) Judging, which involves evaluating plan\neffectiveness and providing iterative feedback for improvement. The cyclical\nprocess enables a dynamic and responsive planning approach. Experiments on the\nreal-world dataset demonstrate the effectiveness of our framework as a\ncontinuous and adaptive planning process."
                },
                "authors": [
                    {
                        "name": "Hang Ni"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "arxiv_comment": "4 pages, 2 figures, accepted by The 1st Workshop on AI for Urban\n  Planning (AAAI 2025's Workshop)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v1",
                "updated": "2024-12-29T15:42:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]